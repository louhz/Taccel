//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32345990
// Cuda compilation tools, release 12.1, V12.1.55
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_86
.address_size 64

	// .globl	negate_arr_vec12d_cuda_kernel_forward
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.func  (.param .b64 func_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;
.const .align 4 .b8 pnanovdb_grid_type_value_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 1, 0, 0, 0, 32, 0, 0, 0, 4, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_table_strides_bits[108] = {64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_minmax_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_minmax_aligns_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_stat_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_leaf_type[108] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_constants[3024] = {28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 4, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 40, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 68, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 44, 32, 0, 0, 56, 32, 0, 0, 60, 32, 0, 0, 64, 32, 0, 0, 64, 32, 8, 0, 32, 4, 0, 0, 44, 4, 0, 0, 56, 4, 0, 0, 60, 4, 0, 0, 64, 4, 0, 0, 64, 4, 1, 0, 80, 0, 0, 0, 92, 0, 0, 0, 104, 0, 0, 0, 108, 0, 0, 0, 128, 0, 0, 0, 128, 24, 0, 0, 32, 0, 0, 0, 56, 0, 0, 0, 80, 0, 0, 0, 104, 0, 0, 0, 112, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 24, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 56, 32, 0, 0, 80, 32, 0, 0, 88, 32, 0, 0, 96, 32, 0, 0, 96, 32, 12, 0, 32, 4, 0, 0, 56, 4, 0, 0, 80, 4, 0, 0, 88, 4, 0, 0, 96, 4, 0, 0, 96, 132, 1, 0, 80, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 160, 48, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 1, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 160, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 1, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 2, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 44, 0, 0, 0, 60, 0, 0, 0, 76, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 68, 32, 0, 0, 96, 32, 0, 0, 96, 32, 8, 0, 32, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 68, 4, 0, 0, 96, 4, 0, 0, 96, 4, 1, 0, 80, 0, 0, 0, 96, 0, 0, 0, 112, 0, 0, 0, 116, 0, 0, 0, 128, 0, 0, 0, 128, 32, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 0, 1, 0, 0, 32, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 64, 32, 0, 0, 96, 32, 0, 0, 104, 32, 0, 0, 128, 32, 0, 0, 128, 32, 16, 0, 32, 4, 0, 0, 64, 4, 0, 0, 96, 4, 0, 0, 104, 4, 0, 0, 128, 4, 0, 0, 128, 4, 2, 0, 80, 0, 0, 0, 112, 0, 0, 0, 144, 0, 0, 0, 152, 0, 0, 0, 160, 0, 0, 0, 160, 64, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 31, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 35, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 35, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 83, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 6, 0, 0, 28, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 38, 32, 0, 0, 44, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 38, 4, 0, 0, 44, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 86, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 128, 12, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 81, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 2, 0, 0};
.const .align 4 .b8 pnanovdb_dither_lut[2048] = {70, 182, 19, 62, 172, 173, 36, 63, 175, 149, 84, 63, 42, 171, 169, 62, 33, 148, 215, 61, 175, 178, 26, 63, 21, 170, 43, 62, 176, 170, 42, 63, 193, 141, 100, 63, 44, 155, 201, 62, 36, 172, 167, 61, 170, 181, 20, 63, 180, 146, 90, 63, 51, 165, 181, 62, 181, 138, 106, 63, 54, 149, 213, 62, 171, 177, 28, 63, 0, 140, 231, 61, 175, 153, 76, 63, 41, 179, 153, 62, 157, 190, 2, 63, 41, 160, 63, 60, 182, 134, 114, 63, 55, 141, 229, 62, 43, 163, 185, 62, 176, 145, 92, 63, 62, 152, 79, 61, 170, 185, 12, 63, 48, 189, 133, 62, 178, 158, 66, 63, 23, 154, 75, 62, 177, 166, 50, 63, 44, 184, 15, 62, 37, 174, 35, 63, 36, 182, 19, 63, 39, 176, 159, 61, 31, 189, 5, 63, 41, 160, 191, 60, 59, 138, 235, 62, 56, 133, 117, 63, 59, 142, 99, 63, 65, 156, 199, 62, 29, 172, 167, 62, 58, 150, 83, 63, 19, 186, 139, 62, 52, 157, 69, 63, 51, 165, 53, 63, 33, 148, 87, 62, 69, 132, 247, 62, 61, 130, 123, 63, 28, 180, 151, 62, 57, 154, 75, 63, 51, 136, 239, 61, 33, 177, 29, 63, 57, 144, 95, 61, 31, 185, 13, 63, 39, 162, 59, 63, 51, 136, 111, 62, 35, 186, 11, 63, 41, 160, 63, 61, 54, 145, 93, 63, 56, 162, 187, 62, 53, 153, 77, 63, 53, 178, 155, 62, 169, 189, 4, 63, 52, 176, 159, 60, 47, 139, 233, 62, 194, 133, 116, 63, 177, 162, 58, 63, 26, 138, 107, 62, 157, 186, 10, 63, 47, 168, 47, 61, 40, 187, 137, 62, 174, 157, 68, 63, 173, 165, 52, 63, 74, 150, 83, 62, 56, 133, 245, 62, 199, 130, 122, 63, 49, 181, 149, 62, 179, 154, 74, 63, 77, 134, 115, 62, 173, 161, 60, 63, 45, 147, 217, 62, 194, 137, 108, 63, 19, 186, 11, 62, 176, 174, 34, 63, 50, 173, 165, 62, 179, 150, 82, 63, 195, 129, 124, 63, 48, 131, 249, 62, 172, 169, 44, 63, 72, 166, 51, 62, 180, 142, 98, 63, 52, 157, 197, 62, 158, 182, 18, 63, 41, 180, 151, 61, 35, 190, 3, 63, 19, 128, 127, 60, 49, 152, 79, 62, 38, 166, 51, 63, 28, 180, 23, 62, 50, 173, 37, 63, 54, 149, 85, 63, 55, 170, 171, 62, 27, 188, 135, 62, 56, 158, 67, 63, 60, 134, 115, 63, 67, 140, 231, 62, 55, 141, 101, 63, 57, 154, 203, 62, 47, 168, 175, 61, 32, 181, 21, 63, 58, 146, 91, 63, 30, 164, 183, 62, 59, 138, 107, 63, 66, 148, 215, 62, 52, 161, 61, 63, 35, 132, 119, 62, 51, 169, 45, 63, 30, 164, 55, 62, 84, 144, 223, 61, 37, 178, 27, 63, 47, 168, 47, 62, 38, 170, 43, 63, 61, 130, 251, 62, 56, 129, 125, 63, 58, 146, 219, 62, 55, 137, 109, 63, 34, 188, 135, 61, 162, 183, 16, 63, 163, 175, 32, 63, 35, 190, 3, 62, 56, 162, 59, 62, 168, 168, 46, 63, 169, 160, 62, 63, 61, 130, 123, 62, 183, 151, 80, 63, 25, 175, 161, 62, 60, 159, 193, 62, 184, 143, 96, 63, 190, 136, 110, 63, 71, 145, 221, 62, 73, 129, 253, 62, 191, 128, 126, 63, 31, 184, 15, 61, 161, 187, 8, 63, 186, 131, 120, 63, 64, 135, 241, 62, 58, 146, 91, 62, 169, 164, 54, 63, 165, 188, 6, 63, 30, 144, 223, 60, 183, 155, 72, 63, 23, 183, 145, 62, 42, 142, 99, 62, 181, 163, 56, 63, 190, 132, 118, 63, 72, 137, 237, 62, 31, 185, 141, 62, 170, 156, 70, 63, 69, 132, 119, 63, 51, 136, 239, 62, 49, 152, 207, 62, 51, 140, 103, 63, 31, 184, 143, 61, 40, 183, 17, 63, 63, 143, 97, 63, 40, 158, 195, 62, 84, 144, 95, 62, 47, 164, 55, 63, 46, 172, 39, 63, 12, 176, 31, 62, 45, 151, 81, 63, 37, 174, 163, 62, 60, 188, 7, 62, 41, 175, 33, 63, 73, 128, 127, 61, 44, 184, 15, 63, 48, 160, 63, 63, 86, 128, 127, 62, 63, 139, 105, 63, 41, 150, 211, 62, 65, 131, 121, 63, 77, 134, 243, 62, 49, 152, 79, 63, 45, 176, 159, 62, 52, 128, 255, 62, 69, 128, 127, 63, 63, 172, 39, 62, 42, 171, 41, 63, 67, 140, 103, 62, 43, 163, 57, 63, 164, 167, 48, 63, 40, 158, 67, 62, 83, 128, 127, 59, 161, 191, 0, 63, 166, 184, 14, 63, 51, 136, 111, 61, 102, 132, 247, 61, 167, 176, 30, 63, 63, 143, 225, 62, 186, 135, 112, 63, 182, 159, 64, 63, 22, 191, 129, 62, 33, 177, 157, 62, 187, 152, 78, 63, 188, 144, 94, 63, 35, 161, 189, 62, 164, 171, 40, 63, 37, 174, 35, 62, 59, 167, 177, 62, 184, 147, 88, 63, 166, 180, 22, 63, 44, 164, 183, 61, 53, 178, 27, 62, 168, 172, 38, 63, 62, 151, 209, 62, 185, 139, 104, 63, 162, 179, 24, 63, 52, 156, 199, 61, 34, 169, 173, 62, 188, 148, 86, 63, 189, 140, 102, 63, 36, 153, 205, 62, 47, 168, 175, 62, 49, 148, 87, 63, 48, 156, 71, 63, 44, 184, 143, 62, 42, 167, 49, 63, 65, 156, 71, 62, 35, 190, 131, 62, 44, 159, 65, 63, 45, 180, 23, 63, 54, 160, 191, 61, 73, 128, 255, 60, 27, 188, 7, 63, 42, 142, 227, 62, 64, 135, 113, 63, 39, 191, 1, 63, 62, 128, 255, 59, 47, 168, 47, 63, 81, 160, 63, 62, 19, 128, 255, 61, 45, 176, 31, 63, 36, 182, 147, 62, 44, 155, 73, 63, 38, 166, 179, 62, 62, 147, 89, 63, 50, 144, 223, 62, 68, 136, 111, 63, 50, 144, 95, 63, 48, 160, 191, 62, 40, 187, 9, 63, 52, 176, 31, 61, 41, 179, 25, 63, 116, 152, 207, 61, 227, 54, 18, 63, 43, 182, 147, 61, 247, 30, 66, 63, 152, 189, 132, 62, 234, 35, 56, 63, 63, 143, 97, 62, 230, 59, 8, 63, 34, 188, 7, 61, 155, 173, 164, 62, 248, 22, 82, 63, 41, 176, 31, 60, 226, 62, 2, 63, 202, 135, 240, 62, 255, 3, 120, 63, 162, 183, 144, 62, 235, 27, 72, 63, 226, 58, 10, 63, 49, 172, 39, 61, 247, 34, 58, 63, 47, 139, 105, 62, 230, 63, 0, 63, 83, 128, 255, 58, 231, 55, 16, 63, 35, 190, 131, 61, 154, 181, 148, 62, 248, 26, 74, 63, 194, 133, 244, 62, 251, 2, 122, 63, 161, 191, 128, 62, 235, 31, 64, 63, 163, 175, 160, 62, 236, 23, 80, 63, 116, 7, 113, 63, 180, 142, 226, 62, 115, 15, 97, 63, 178, 158, 194, 62, 186, 160, 190, 62, 119, 16, 95, 63, 184, 176, 158, 62, 118, 24, 79, 63, 19, 157, 69, 62, 112, 39, 49, 63, 14, 189, 5, 62, 111, 47, 33, 63, 98, 48, 31, 63, 61, 130, 251, 61, 97, 56, 15, 63, 75, 132, 119, 61, 111, 43, 41, 63, 16, 173, 37, 62, 112, 35, 57, 63, 88, 141, 101, 62, 187, 152, 206, 62, 120, 12, 103, 63, 119, 20, 87, 63, 185, 168, 174, 62, 179, 150, 210, 62, 116, 11, 105, 63, 182, 134, 242, 62, 134, 3, 121, 63, 98, 44, 39, 63, 33, 177, 29, 62, 42, 162, 187, 61, 97, 52, 23, 63, 44, 155, 73, 62, 229, 38, 50, 63, 191, 157, 196, 62, 250, 14, 98, 63, 53, 158, 195, 61, 232, 51, 24, 63, 58, 175, 33, 62, 233, 43, 40, 63, 251, 6, 114, 63, 193, 141, 228, 62, 228, 46, 34, 63, 40, 187, 9, 62, 253, 19, 88, 63, 164, 167, 176, 62, 254, 11, 104, 63, 166, 151, 208, 62, 42, 171, 41, 62, 229, 42, 42, 63, 74, 150, 211, 61, 228, 50, 26, 63, 56, 191, 1, 62, 232, 47, 32, 63, 60, 159, 65, 62, 233, 39, 48, 63, 250, 10, 106, 63, 192, 149, 212, 62, 249, 18, 90, 63, 190, 165, 180, 62, 254, 15, 96, 63, 165, 159, 192, 62, 255, 7, 112, 63, 168, 143, 224, 62, 176, 174, 162, 62, 114, 23, 81, 63, 173, 190, 130, 62, 113, 31, 65, 63, 122, 0, 127, 63, 191, 128, 254, 62, 120, 8, 111, 63, 188, 144, 222, 62, 93, 55, 17, 63, 32, 186, 139, 61, 91, 63, 1, 63, 41, 160, 191, 59, 40, 129, 125, 62, 117, 32, 63, 63, 35, 161, 61, 62, 116, 40, 47, 63, 28, 180, 23, 61, 92, 59, 9, 63, 50, 154, 203, 61, 110, 51, 25, 63, 118, 28, 71, 63, 149, 184, 142, 62, 190, 136, 238, 62, 121, 4, 119, 63, 113, 27, 73, 63, 174, 182, 146, 62, 115, 19, 89, 63, 177, 166, 178, 62, 78, 136, 239, 60, 96, 60, 7, 63, 116, 36, 55, 63, 37, 145, 93, 62, 175, 153, 204, 62, 2, 13, 102, 63, 3, 5, 118, 63, 177, 137, 236, 62, 38, 156, 71, 61, 222, 57, 12, 63, 42, 142, 227, 61, 223, 49, 28, 63, 237, 44, 38, 63, 7, 179, 25, 62, 79, 147, 89, 62, 238, 36, 54, 63, 244, 25, 76, 63, 179, 179, 152, 62, 245, 17, 92, 63, 181, 163, 184, 62, 10, 134, 243, 61, 236, 48, 30, 63, 54, 140, 103, 61, 235, 56, 14, 63, 180, 171, 168, 62, 244, 21, 84, 63, 222, 61, 4, 63, 58, 184, 143, 60, 241, 16, 94, 63, 173, 161, 188, 62, 240, 24, 78, 63, 171, 177, 156, 62, 223, 53, 20, 63, 37, 174, 163, 61, 178, 187, 136, 62, 243, 29, 68, 63, 157, 186, 138, 62, 105, 29, 69, 63, 104, 37, 53, 63, 54, 149, 85, 62, 107, 42, 43, 63, 67, 169, 45, 62, 106, 50, 27, 63, 247, 145, 219, 61, 100, 61, 5, 63, 47, 168, 175, 60, 198, 138, 234, 62, 125, 5, 117, 63, 171, 148, 214, 62, 129, 10, 107, 63, 169, 164, 182, 62, 127, 18, 91, 63, 126, 1, 125, 63, 199, 130, 250, 62, 197, 146, 218, 62, 125, 9, 109, 63, 172, 140, 230, 62, 129, 6, 115, 63, 104, 62, 3, 63, 30, 144, 95, 60, 56, 133, 117, 62, 104, 33, 61, 63, 103, 41, 45, 63, 51, 165, 53, 62, 108, 38, 51, 63, 70, 153, 77, 62, 165, 188, 134, 62, 109, 30, 67, 63, 239, 28, 70, 63, 170, 185, 140, 62, 172, 169, 172, 62, 240, 20, 86, 63, 241, 41, 44, 63, 26, 167, 49, 62, 243, 33, 60, 63, 30, 135, 113, 62, 35, 152, 207, 60, 218, 60, 6, 63, 236, 52, 22, 63, 45, 166, 179, 61, 184, 147, 216, 62, 246, 9, 108, 63, 186, 131, 248, 62, 247, 1, 124, 63, 239, 32, 62, 63, 81, 131, 121, 62, 237, 40, 46, 63, 77, 163, 57, 62, 247, 5, 116, 63, 185, 139, 232, 62, 23, 183, 17, 62, 241, 45, 36, 63, 178, 129, 252, 62, 4, 1, 126, 63, 176, 145, 220, 62, 3, 9, 110, 63, 28, 151, 81, 62, 242, 37, 52, 63, 246, 13, 100, 63, 183, 155, 200, 62, 124, 13, 101, 63, 195, 154, 202, 62, 48, 170, 171, 61, 101, 53, 21, 63, 44, 164, 55, 61, 105, 58, 11, 63, 72, 137, 109, 62, 108, 34, 59, 63, 49, 181, 21, 62, 102, 45, 37, 63, 123, 21, 85, 63, 159, 170, 170, 62, 109, 26, 75, 63, 166, 180, 150, 62, 130, 2, 123, 63, 173, 132, 246, 62, 161, 162, 186, 62, 123, 17, 93, 63, 122, 25, 77, 63, 158, 178, 154, 62, 110, 22, 83, 63, 168, 172, 166, 62, 65, 185, 13, 62, 106, 46, 35, 63, 102, 49, 29, 63, 93, 138, 235, 61, 60, 148, 87, 61, 101, 57, 13, 63, 40, 178, 155, 61, 105, 54, 19, 63, 128, 14, 99, 63, 170, 156, 198, 62};
.global .align 8 .f64 _ZN2wp11_svd_configIdE17QR_GIVENS_EPSILONE = 0d3D719799812DEA11;
.global .align 4 .u32 _ZN2wp11_svd_configIdE17JACOBI_ITERATIONSE = 8;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIiE9GRID_TYPEE = 4;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIxE9GRID_TYPEE = 5;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIjE9GRID_TYPEE = 10;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIfE9GRID_TYPEE = 1;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIdE9GRID_TYPEE = 2;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EfEEE9GRID_TYPEE = 6;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EdEEE9GRID_TYPEE = 7;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EfEEE9GRID_TYPEE = 17;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EdEEE9GRID_TYPEE = 18;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9all_hostsE = 1;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_35_bitE = 2;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_37_bitE = 4;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_50_bitE = 8;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_52_bitE = 16;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_53_bitE = 32;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_60_bitE = 64;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_61_bitE = 128;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_62_bitE = 256;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_70_bitE = 512;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_72_bitE = 1024;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_75_bitE = 2048;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_80_bitE = 4096;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_86_bitE = 8192;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_87_bitE = 16384;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_89_bitE = 32768;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail9sm_90_bitE = 65536;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target6detail11all_devicesE = 131070;
.global .align 8 .b8 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target7is_hostE[8] = {1, 0, 0, 0, 0, 0, 0, 0};
.global .align 8 .b8 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target9is_deviceE[8] = {254, 255, 1, 0, 0, 0, 0, 0};
.global .align 8 .b8 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target10any_targetE[8] = {255, 255, 1, 0, 0, 0, 0, 0};
.global .align 8 .b8 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target9no_targetE[8];
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_35E = 35;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_37E = 37;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_50E = 50;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_52E = 52;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_53E = 53;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_60E = 60;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_61E = 61;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_62E = 62;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_70E = 70;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_72E = 72;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_75E = 75;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_80E = 80;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_86E = 86;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_87E = 87;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_89E = 89;
.global .align 8 .u64 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2nv6target5sm_90E = 90;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp6volume7CLOSESTE;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp6volume6LINEARE = 1;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp15LAUNCH_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp14ARRAY_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp18ARRAY_TYPE_REGULARE;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp18ARRAY_TYPE_INDEXEDE = 1;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp17ARRAY_TYPE_FABRICE = 2;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp25ARRAY_TYPE_FABRIC_INDEXEDE = 3;
.global .align 1 .b8 $str[92] = {91, 67, 79, 79, 77, 97, 116, 114, 105, 120, 32, 79, 70, 66, 32, 69, 114, 114, 111, 114, 93, 9, 105, 58, 32, 37, 100, 44, 32, 106, 58, 32, 37, 100, 44, 32, 98, 108, 111, 99, 107, 95, 105, 110, 100, 101, 120, 58, 32, 37, 100, 44, 32, 110, 95, 114, 111, 119, 115, 58, 32, 37, 100, 44, 32, 110, 95, 99, 111, 108, 115, 58, 32, 37, 100, 44, 32, 115, 105, 122, 101, 58, 32, 37, 100, 33, 33, 33, 33, 33, 10, 0};

.visible .entry negate_arr_vec12d_cuda_kernel_forward(
	.param .align 8 .b8 negate_arr_vec12d_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 negate_arr_vec12d_cuda_kernel_forward_param_1[56]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<47>;
	.reg .f64 	%fd<49>;
	.reg .b64 	%rd<60>;


	ld.param.v2.u32 	{%r16, %r17}, [negate_arr_vec12d_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r18, %r19}, [negate_arr_vec12d_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r24, %r25}, [negate_arr_vec12d_cuda_kernel_forward_param_1+32];
	ld.param.u64 	%rd30, [negate_arr_vec12d_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd29, [negate_arr_vec12d_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [negate_arr_vec12d_cuda_kernel_forward_param_0+16];
	mov.u32 	%r28, %ntid.x;
	cvt.u64.u32 	%rd1, %r28;
	mov.u32 	%r29, %ctaid.x;
	mul.wide.u32 	%rd32, %r28, %r29;
	mov.u32 	%r30, %tid.x;
	cvt.u64.u32 	%rd33, %r30;
	add.s64 	%rd53, %rd32, %rd33;
	setp.ge.u64 	%p1, %rd53, %rd29;
	@%p1 bra 	$L__BB0_25;

	cvta.to.global.u64 	%rd4, %rd30;
	cvt.s64.s32 	%rd5, %r19;
	cvt.s64.s32 	%rd6, %r18;
	cvt.s64.s32 	%rd7, %r17;
	cvt.s64.s32 	%rd8, %r24;
	mov.u32 	%r31, %nctaid.x;
	cvt.u64.u32 	%rd34, %r31;
	mul.lo.s64 	%rd9, %rd1, %rd34;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_2;

$L__BB0_12:
	cvt.u32.u64 	%r38, %rd5;
	cvt.u32.u64 	%r41, %rd6;
	cvt.u32.u64 	%r44, %rd7;

$L__BB0_13:
	or.b64  	%rd43, %rd53, %rd5;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p9, %rd44, 0;
	@%p9 bra 	$L__BB0_15;

	div.u64 	%rd58, %rd53, %rd5;
	bra.uni 	$L__BB0_16;

$L__BB0_15:
	cvt.u32.u64 	%r39, %rd53;
	div.u32 	%r40, %r39, %r38;
	cvt.u64.u32 	%rd58, %r40;

$L__BB0_16:
	setp.lt.s32 	%p10, %r6, 3;
	@%p10 bra 	$L__BB0_20;

	or.b64  	%rd45, %rd58, %rd6;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p11, %rd46, 0;
	@%p11 bra 	$L__BB0_19;

	div.u64 	%rd58, %rd58, %rd6;
	bra.uni 	$L__BB0_20;

$L__BB0_19:
	cvt.u32.u64 	%r42, %rd58;
	div.u32 	%r43, %r42, %r41;
	cvt.u64.u32 	%rd58, %r43;

$L__BB0_20:
	setp.lt.s32 	%p12, %r6, 2;
	@%p12 bra 	$L__BB0_24;

	or.b64  	%rd47, %rd58, %rd7;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p13, %rd48, 0;
	@%p13 bra 	$L__BB0_23;

	div.u64 	%rd58, %rd58, %rd7;
	bra.uni 	$L__BB0_24;

$L__BB0_23:
	cvt.u32.u64 	%r45, %rd58;
	div.u32 	%r46, %r45, %r44;
	cvt.u64.u32 	%rd58, %r46;

$L__BB0_24:
	cvt.s64.s32 	%rd49, %rd58;
	setp.gt.s32 	%p14, %r6, 0;
	selp.b64 	%rd50, %rd49, 0, %p14;
	mul.lo.s64 	%rd51, %rd50, %rd8;
	add.s64 	%rd52, %rd4, %rd51;
	ld.global.f64 	%fd25, [%rd52];
	neg.f64 	%fd26, %fd25;
	ld.global.f64 	%fd27, [%rd52+8];
	neg.f64 	%fd28, %fd27;
	ld.global.f64 	%fd29, [%rd52+16];
	neg.f64 	%fd30, %fd29;
	ld.global.f64 	%fd31, [%rd52+24];
	neg.f64 	%fd32, %fd31;
	ld.global.f64 	%fd33, [%rd52+32];
	neg.f64 	%fd34, %fd33;
	ld.global.f64 	%fd35, [%rd52+40];
	neg.f64 	%fd36, %fd35;
	ld.global.f64 	%fd37, [%rd52+48];
	neg.f64 	%fd38, %fd37;
	ld.global.f64 	%fd39, [%rd52+56];
	neg.f64 	%fd40, %fd39;
	ld.global.f64 	%fd41, [%rd52+64];
	neg.f64 	%fd42, %fd41;
	ld.global.f64 	%fd43, [%rd52+72];
	neg.f64 	%fd44, %fd43;
	ld.global.f64 	%fd45, [%rd52+80];
	neg.f64 	%fd46, %fd45;
	ld.global.f64 	%fd47, [%rd52+88];
	neg.f64 	%fd48, %fd47;
	st.global.f64 	[%rd52], %fd26;
	st.global.f64 	[%rd52+8], %fd28;
	st.global.f64 	[%rd52+16], %fd30;
	st.global.f64 	[%rd52+24], %fd32;
	st.global.f64 	[%rd52+32], %fd34;
	st.global.f64 	[%rd52+40], %fd36;
	st.global.f64 	[%rd52+48], %fd38;
	st.global.f64 	[%rd52+56], %fd40;
	st.global.f64 	[%rd52+64], %fd42;
	st.global.f64 	[%rd52+72], %fd44;
	st.global.f64 	[%rd52+80], %fd46;
	st.global.f64 	[%rd52+88], %fd48;
	add.s64 	%rd53, %rd53, %rd9;
	setp.lt.u64 	%p15, %rd53, %rd29;
	@%p15 bra 	$L__BB0_13;
	bra.uni 	$L__BB0_25;

$L__BB0_2:
	cvt.u32.u64 	%r32, %rd6;
	cvt.u32.u64 	%r35, %rd7;

$L__BB0_3:
	setp.lt.s32 	%p3, %r6, 3;
	mov.u64 	%rd54, %rd53;
	@%p3 bra 	$L__BB0_7;

	or.b64  	%rd35, %rd53, %rd6;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p4, %rd36, 0;
	@%p4 bra 	$L__BB0_6;

	div.u64 	%rd54, %rd53, %rd6;
	bra.uni 	$L__BB0_7;

$L__BB0_6:
	cvt.u32.u64 	%r33, %rd53;
	div.u32 	%r34, %r33, %r32;
	cvt.u64.u32 	%rd54, %r34;

$L__BB0_7:
	setp.lt.s32 	%p5, %r6, 2;
	@%p5 bra 	$L__BB0_11;

	or.b64  	%rd37, %rd54, %rd7;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.eq.s64 	%p6, %rd38, 0;
	@%p6 bra 	$L__BB0_10;

	div.u64 	%rd54, %rd54, %rd7;
	bra.uni 	$L__BB0_11;

$L__BB0_10:
	cvt.u32.u64 	%r36, %rd54;
	div.u32 	%r37, %r36, %r35;
	cvt.u64.u32 	%rd54, %r37;

$L__BB0_11:
	cvt.s64.s32 	%rd39, %rd54;
	setp.gt.s32 	%p7, %r6, 0;
	selp.b64 	%rd40, %rd39, 0, %p7;
	mul.lo.s64 	%rd41, %rd40, %rd8;
	add.s64 	%rd42, %rd4, %rd41;
	ld.global.f64 	%fd1, [%rd42];
	neg.f64 	%fd2, %fd1;
	ld.global.f64 	%fd3, [%rd42+8];
	neg.f64 	%fd4, %fd3;
	ld.global.f64 	%fd5, [%rd42+16];
	neg.f64 	%fd6, %fd5;
	ld.global.f64 	%fd7, [%rd42+24];
	neg.f64 	%fd8, %fd7;
	ld.global.f64 	%fd9, [%rd42+32];
	neg.f64 	%fd10, %fd9;
	ld.global.f64 	%fd11, [%rd42+40];
	neg.f64 	%fd12, %fd11;
	ld.global.f64 	%fd13, [%rd42+48];
	neg.f64 	%fd14, %fd13;
	ld.global.f64 	%fd15, [%rd42+56];
	neg.f64 	%fd16, %fd15;
	ld.global.f64 	%fd17, [%rd42+64];
	neg.f64 	%fd18, %fd17;
	ld.global.f64 	%fd19, [%rd42+72];
	neg.f64 	%fd20, %fd19;
	ld.global.f64 	%fd21, [%rd42+80];
	neg.f64 	%fd22, %fd21;
	ld.global.f64 	%fd23, [%rd42+88];
	neg.f64 	%fd24, %fd23;
	st.global.f64 	[%rd42], %fd2;
	st.global.f64 	[%rd42+8], %fd4;
	st.global.f64 	[%rd42+16], %fd6;
	st.global.f64 	[%rd42+24], %fd8;
	st.global.f64 	[%rd42+32], %fd10;
	st.global.f64 	[%rd42+40], %fd12;
	st.global.f64 	[%rd42+48], %fd14;
	st.global.f64 	[%rd42+56], %fd16;
	st.global.f64 	[%rd42+64], %fd18;
	st.global.f64 	[%rd42+72], %fd20;
	st.global.f64 	[%rd42+80], %fd22;
	st.global.f64 	[%rd42+88], %fd24;
	add.s64 	%rd53, %rd53, %rd9;
	setp.lt.u64 	%p8, %rd53, %rd29;
	@%p8 bra 	$L__BB0_3;

$L__BB0_25:
	ret;

}
	// .globl	negate_arr_vec12d_cuda_kernel_backward
.visible .entry negate_arr_vec12d_cuda_kernel_backward(
	.param .align 8 .b8 negate_arr_vec12d_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 negate_arr_vec12d_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 negate_arr_vec12d_cuda_kernel_backward_param_2[56]
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<58>;
	.reg .f64 	%fd<146>;
	.reg .b64 	%rd<74>;


	ld.param.v2.u32 	{%r25, %r26}, [negate_arr_vec12d_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r27, %r28}, [negate_arr_vec12d_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r33, %r34}, [negate_arr_vec12d_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r41, %r42}, [negate_arr_vec12d_cuda_kernel_backward_param_2+32];
	ld.param.u64 	%rd30, [negate_arr_vec12d_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd29, [negate_arr_vec12d_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd27, [negate_arr_vec12d_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [negate_arr_vec12d_cuda_kernel_backward_param_0+16];
	mov.u32 	%r45, %ntid.x;
	cvt.u64.u32 	%rd1, %r45;
	mov.u32 	%r46, %ctaid.x;
	mul.wide.u32 	%rd32, %r45, %r46;
	mov.u32 	%r47, %tid.x;
	cvt.u64.u32 	%rd33, %r47;
	add.s64 	%rd70, %rd32, %rd33;
	setp.ge.u64 	%p1, %rd70, %rd27;
	@%p1 bra 	$L__BB1_23;

	cvta.to.global.u64 	%rd6, %rd30;
	cvta.to.global.u64 	%rd7, %rd29;
	cvt.s64.s32 	%rd8, %r28;
	cvt.s64.s32 	%rd9, %r27;
	cvt.s64.s32 	%rd10, %r26;
	cvt.s64.s32 	%rd11, %r33;
	cvt.s64.s32 	%rd12, %r41;
	mov.u32 	%r48, %nctaid.x;
	cvt.u64.u32 	%rd34, %r48;
	mul.lo.s64 	%rd13, %rd1, %rd34;

$L__BB1_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd71, %rd70;
	@%p2 bra 	$L__BB1_6;

	or.b64  	%rd35, %rd70, %rd8;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p3, %rd36, 0;
	@%p3 bra 	$L__BB1_5;

	div.u64 	%rd71, %rd70, %rd8;
	bra.uni 	$L__BB1_6;

$L__BB1_5:
	cvt.u32.u64 	%r49, %rd8;
	cvt.u32.u64 	%r50, %rd70;
	div.u32 	%r51, %r50, %r49;
	cvt.u64.u32 	%rd71, %r51;

$L__BB1_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB1_10;

	or.b64  	%rd37, %rd71, %rd9;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.eq.s64 	%p5, %rd38, 0;
	@%p5 bra 	$L__BB1_9;

	div.u64 	%rd71, %rd71, %rd9;
	bra.uni 	$L__BB1_10;

$L__BB1_9:
	cvt.u32.u64 	%r52, %rd9;
	cvt.u32.u64 	%r53, %rd71;
	div.u32 	%r54, %r53, %r52;
	cvt.u64.u32 	%rd71, %r54;

$L__BB1_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB1_14;

	or.b64  	%rd39, %rd71, %rd10;
	and.b64  	%rd40, %rd39, -4294967296;
	setp.eq.s64 	%p7, %rd40, 0;
	@%p7 bra 	$L__BB1_13;

	div.u64 	%rd71, %rd71, %rd10;
	bra.uni 	$L__BB1_14;

$L__BB1_13:
	cvt.u32.u64 	%r55, %rd10;
	cvt.u32.u64 	%r56, %rd71;
	div.u32 	%r57, %r56, %r55;
	cvt.u64.u32 	%rd71, %r57;

$L__BB1_14:
	cvt.s64.s32 	%rd41, %rd71;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd24, %rd41, 0, %p8;
	mul.lo.s64 	%rd25, %rd24, %rd11;
	setp.eq.s64 	%p9, %rd30, 0;
	@%p9 bra 	$L__BB1_16;

	mul.lo.s64 	%rd42, %rd24, %rd12;
	add.s64 	%rd43, %rd6, %rd42;
	ld.global.f64 	%fd49, [%rd43];
	add.f64 	%fd145, %fd49, 0d0000000000000000;
	ld.global.f64 	%fd50, [%rd43+8];
	add.f64 	%fd144, %fd50, 0d0000000000000000;
	ld.global.f64 	%fd51, [%rd43+16];
	add.f64 	%fd143, %fd51, 0d0000000000000000;
	ld.global.f64 	%fd52, [%rd43+24];
	add.f64 	%fd142, %fd52, 0d0000000000000000;
	ld.global.f64 	%fd53, [%rd43+32];
	add.f64 	%fd141, %fd53, 0d0000000000000000;
	ld.global.f64 	%fd54, [%rd43+40];
	add.f64 	%fd140, %fd54, 0d0000000000000000;
	ld.global.f64 	%fd55, [%rd43+48];
	add.f64 	%fd139, %fd55, 0d0000000000000000;
	ld.global.f64 	%fd56, [%rd43+56];
	add.f64 	%fd138, %fd56, 0d0000000000000000;
	ld.global.f64 	%fd57, [%rd43+64];
	add.f64 	%fd137, %fd57, 0d0000000000000000;
	ld.global.f64 	%fd58, [%rd43+72];
	add.f64 	%fd136, %fd58, 0d0000000000000000;
	ld.global.f64 	%fd59, [%rd43+80];
	add.f64 	%fd135, %fd59, 0d0000000000000000;
	ld.global.f64 	%fd60, [%rd43+88];
	add.f64 	%fd134, %fd60, 0d0000000000000000;
	bra.uni 	$L__BB1_18;

$L__BB1_16:
	setp.eq.s64 	%p10, %rd29, 0;
	mov.f64 	%fd134, 0d0000000000000000;
	mov.f64 	%fd135, %fd134;
	mov.f64 	%fd136, %fd134;
	mov.f64 	%fd137, %fd134;
	mov.f64 	%fd138, %fd134;
	mov.f64 	%fd139, %fd134;
	mov.f64 	%fd140, %fd134;
	mov.f64 	%fd141, %fd134;
	mov.f64 	%fd142, %fd134;
	mov.f64 	%fd143, %fd134;
	mov.f64 	%fd144, %fd134;
	mov.f64 	%fd145, %fd134;
	@%p10 bra 	$L__BB1_18;

	add.s64 	%rd44, %rd7, %rd25;
	ld.global.f64 	%fd73, [%rd44];
	add.f64 	%fd145, %fd73, 0d0000000000000000;
	ld.global.f64 	%fd74, [%rd44+8];
	add.f64 	%fd144, %fd74, 0d0000000000000000;
	ld.global.f64 	%fd75, [%rd44+16];
	add.f64 	%fd143, %fd75, 0d0000000000000000;
	ld.global.f64 	%fd76, [%rd44+24];
	add.f64 	%fd142, %fd76, 0d0000000000000000;
	ld.global.f64 	%fd77, [%rd44+32];
	add.f64 	%fd141, %fd77, 0d0000000000000000;
	ld.global.f64 	%fd78, [%rd44+40];
	add.f64 	%fd140, %fd78, 0d0000000000000000;
	ld.global.f64 	%fd79, [%rd44+48];
	add.f64 	%fd139, %fd79, 0d0000000000000000;
	ld.global.f64 	%fd80, [%rd44+56];
	add.f64 	%fd138, %fd80, 0d0000000000000000;
	ld.global.f64 	%fd81, [%rd44+64];
	add.f64 	%fd137, %fd81, 0d0000000000000000;
	ld.global.f64 	%fd82, [%rd44+72];
	add.f64 	%fd136, %fd82, 0d0000000000000000;
	ld.global.f64 	%fd83, [%rd44+80];
	add.f64 	%fd135, %fd83, 0d0000000000000000;
	ld.global.f64 	%fd84, [%rd44+88];
	add.f64 	%fd134, %fd84, 0d0000000000000000;

$L__BB1_18:
	mov.f64 	%fd85, 0d0000000000000000;
	sub.f64 	%fd37, %fd85, %fd145;
	sub.f64 	%fd38, %fd85, %fd144;
	sub.f64 	%fd39, %fd85, %fd143;
	sub.f64 	%fd40, %fd85, %fd142;
	sub.f64 	%fd41, %fd85, %fd141;
	sub.f64 	%fd42, %fd85, %fd140;
	sub.f64 	%fd43, %fd85, %fd139;
	sub.f64 	%fd44, %fd85, %fd138;
	sub.f64 	%fd45, %fd85, %fd137;
	sub.f64 	%fd46, %fd85, %fd136;
	sub.f64 	%fd47, %fd85, %fd135;
	sub.f64 	%fd48, %fd85, %fd134;
	@%p9 bra 	$L__BB1_20;

	mul.lo.s64 	%rd57, %rd24, %rd12;
	add.s64 	%rd45, %rd30, %rd57;
	// begin inline asm
	{ atom.add.f64 %fd86,[%rd45],%fd37; }

	// end inline asm
	add.s64 	%rd46, %rd45, 8;
	// begin inline asm
	{ atom.add.f64 %fd88,[%rd46],%fd38; }

	// end inline asm
	add.s64 	%rd47, %rd45, 16;
	// begin inline asm
	{ atom.add.f64 %fd90,[%rd47],%fd39; }

	// end inline asm
	add.s64 	%rd48, %rd45, 24;
	// begin inline asm
	{ atom.add.f64 %fd92,[%rd48],%fd40; }

	// end inline asm
	add.s64 	%rd49, %rd45, 32;
	// begin inline asm
	{ atom.add.f64 %fd94,[%rd49],%fd41; }

	// end inline asm
	add.s64 	%rd50, %rd45, 40;
	// begin inline asm
	{ atom.add.f64 %fd96,[%rd50],%fd42; }

	// end inline asm
	add.s64 	%rd51, %rd45, 48;
	// begin inline asm
	{ atom.add.f64 %fd98,[%rd51],%fd43; }

	// end inline asm
	add.s64 	%rd52, %rd45, 56;
	// begin inline asm
	{ atom.add.f64 %fd100,[%rd52],%fd44; }

	// end inline asm
	add.s64 	%rd53, %rd45, 64;
	// begin inline asm
	{ atom.add.f64 %fd102,[%rd53],%fd45; }

	// end inline asm
	add.s64 	%rd54, %rd45, 72;
	// begin inline asm
	{ atom.add.f64 %fd104,[%rd54],%fd46; }

	// end inline asm
	add.s64 	%rd55, %rd45, 80;
	// begin inline asm
	{ atom.add.f64 %fd106,[%rd55],%fd47; }

	// end inline asm
	add.s64 	%rd56, %rd45, 88;
	// begin inline asm
	{ atom.add.f64 %fd108,[%rd56],%fd48; }

	// end inline asm
	bra.uni 	$L__BB1_22;

$L__BB1_20:
	setp.eq.s64 	%p12, %rd29, 0;
	@%p12 bra 	$L__BB1_22;

	add.s64 	%rd58, %rd29, %rd25;
	// begin inline asm
	{ atom.add.f64 %fd110,[%rd58],%fd37; }

	// end inline asm
	add.s64 	%rd59, %rd58, 8;
	// begin inline asm
	{ atom.add.f64 %fd112,[%rd59],%fd38; }

	// end inline asm
	add.s64 	%rd60, %rd58, 16;
	// begin inline asm
	{ atom.add.f64 %fd114,[%rd60],%fd39; }

	// end inline asm
	add.s64 	%rd61, %rd58, 24;
	// begin inline asm
	{ atom.add.f64 %fd116,[%rd61],%fd40; }

	// end inline asm
	add.s64 	%rd62, %rd58, 32;
	// begin inline asm
	{ atom.add.f64 %fd118,[%rd62],%fd41; }

	// end inline asm
	add.s64 	%rd63, %rd58, 40;
	// begin inline asm
	{ atom.add.f64 %fd120,[%rd63],%fd42; }

	// end inline asm
	add.s64 	%rd64, %rd58, 48;
	// begin inline asm
	{ atom.add.f64 %fd122,[%rd64],%fd43; }

	// end inline asm
	add.s64 	%rd65, %rd58, 56;
	// begin inline asm
	{ atom.add.f64 %fd124,[%rd65],%fd44; }

	// end inline asm
	add.s64 	%rd66, %rd58, 64;
	// begin inline asm
	{ atom.add.f64 %fd126,[%rd66],%fd45; }

	// end inline asm
	add.s64 	%rd67, %rd58, 72;
	// begin inline asm
	{ atom.add.f64 %fd128,[%rd67],%fd46; }

	// end inline asm
	add.s64 	%rd68, %rd58, 80;
	// begin inline asm
	{ atom.add.f64 %fd130,[%rd68],%fd47; }

	// end inline asm
	add.s64 	%rd69, %rd58, 88;
	// begin inline asm
	{ atom.add.f64 %fd132,[%rd69],%fd48; }

	// end inline asm

$L__BB1_22:
	add.s64 	%rd70, %rd70, %rd13;
	setp.lt.u64 	%p13, %rd70, %rd27;
	@%p13 bra 	$L__BB1_2;

$L__BB1_23:
	ret;

}
	// .globl	absolutize_arr_vec3d_cuda_kernel_forward
.visible .entry absolutize_arr_vec3d_cuda_kernel_forward(
	.param .align 8 .b8 absolutize_arr_vec3d_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 absolutize_arr_vec3d_cuda_kernel_forward_param_1[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<50>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<73>;


	ld.param.v2.u32 	{%r16, %r17}, [absolutize_arr_vec3d_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r18, %r19}, [absolutize_arr_vec3d_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r24, %r25}, [absolutize_arr_vec3d_cuda_kernel_forward_param_1+32];
	ld.param.u64 	%rd35, [absolutize_arr_vec3d_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd34, [absolutize_arr_vec3d_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [absolutize_arr_vec3d_cuda_kernel_forward_param_0+16];
	mov.u32 	%r28, %ntid.x;
	cvt.u64.u32 	%rd1, %r28;
	mov.u32 	%r29, %ctaid.x;
	mul.wide.u32 	%rd37, %r28, %r29;
	mov.u32 	%r30, %tid.x;
	cvt.u64.u32 	%rd38, %r30;
	add.s64 	%rd64, %rd37, %rd38;
	setp.ge.u64 	%p1, %rd64, %rd34;
	@%p1 bra 	$L__BB2_31;

	cvta.to.global.u64 	%rd4, %rd35;
	cvt.s64.s32 	%rd5, %r19;
	cvt.s64.s32 	%rd6, %r18;
	cvt.s64.s32 	%rd7, %r17;
	cvt.s64.s32 	%rd8, %r24;
	mov.u32 	%r31, %nctaid.x;
	cvt.u64.u32 	%rd39, %r31;
	mul.lo.s64 	%rd9, %rd1, %rd39;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB2_18;
	bra.uni 	$L__BB2_2;

$L__BB2_18:
	cvt.u32.u64 	%r41, %rd5;
	cvt.u32.u64 	%r44, %rd6;
	cvt.u32.u64 	%r47, %rd7;

$L__BB2_19:
	or.b64  	%rd54, %rd64, %rd5;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p13, %rd55, 0;
	@%p13 bra 	$L__BB2_21;

	div.u64 	%rd71, %rd64, %rd5;
	bra.uni 	$L__BB2_22;

$L__BB2_21:
	cvt.u32.u64 	%r42, %rd64;
	div.u32 	%r43, %r42, %r41;
	cvt.u64.u32 	%rd71, %r43;

$L__BB2_22:
	setp.lt.s32 	%p14, %r6, 3;
	@%p14 bra 	$L__BB2_26;

	or.b64  	%rd56, %rd71, %rd6;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p15, %rd57, 0;
	@%p15 bra 	$L__BB2_25;

	div.u64 	%rd71, %rd71, %rd6;
	bra.uni 	$L__BB2_26;

$L__BB2_25:
	cvt.u32.u64 	%r45, %rd71;
	div.u32 	%r46, %r45, %r44;
	cvt.u64.u32 	%rd71, %r46;

$L__BB2_26:
	setp.lt.s32 	%p16, %r6, 2;
	@%p16 bra 	$L__BB2_30;

	or.b64  	%rd58, %rd71, %rd7;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p17, %rd59, 0;
	@%p17 bra 	$L__BB2_29;

	div.u64 	%rd71, %rd71, %rd7;
	bra.uni 	$L__BB2_30;

$L__BB2_29:
	cvt.u32.u64 	%r48, %rd71;
	div.u32 	%r49, %r48, %r47;
	cvt.u64.u32 	%rd71, %r49;

$L__BB2_30:
	cvt.s64.s32 	%rd60, %rd71;
	setp.gt.s32 	%p18, %r6, 0;
	selp.b64 	%rd61, %rd60, 0, %p18;
	mul.lo.s64 	%rd62, %rd61, %rd8;
	add.s64 	%rd63, %rd4, %rd62;
	ld.global.f64 	%fd13, [%rd63];
	abs.f64 	%fd14, %fd13;
	st.global.f64 	[%rd63], %fd14;
	ld.global.f64 	%fd15, [%rd63+8];
	abs.f64 	%fd16, %fd15;
	st.global.f64 	[%rd63+8], %fd16;
	ld.global.f64 	%fd17, [%rd63+16];
	abs.f64 	%fd18, %fd17;
	st.global.f64 	[%rd63+16], %fd18;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p19, %rd64, %rd34;
	@%p19 bra 	$L__BB2_19;
	bra.uni 	$L__BB2_31;

$L__BB2_2:
	setp.gt.s32 	%p3, %r6, 2;
	@%p3 bra 	$L__BB2_9;
	bra.uni 	$L__BB2_3;

$L__BB2_9:
	cvt.u32.u64 	%r35, %rd6;
	cvt.u32.u64 	%r38, %rd7;

$L__BB2_10:
	or.b64  	%rd46, %rd64, %rd6;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p8, %rd47, 0;
	@%p8 bra 	$L__BB2_12;

	div.u64 	%rd68, %rd64, %rd6;
	bra.uni 	$L__BB2_13;

$L__BB2_12:
	cvt.u32.u64 	%r36, %rd64;
	div.u32 	%r37, %r36, %r35;
	cvt.u64.u32 	%rd68, %r37;

$L__BB2_13:
	setp.lt.s32 	%p9, %r6, 2;
	@%p9 bra 	$L__BB2_17;

	or.b64  	%rd48, %rd68, %rd7;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p10, %rd49, 0;
	@%p10 bra 	$L__BB2_16;

	div.u64 	%rd68, %rd68, %rd7;
	bra.uni 	$L__BB2_17;

$L__BB2_16:
	cvt.u32.u64 	%r39, %rd68;
	div.u32 	%r40, %r39, %r38;
	cvt.u64.u32 	%rd68, %r40;

$L__BB2_17:
	cvt.s64.s32 	%rd50, %rd68;
	setp.gt.s32 	%p11, %r6, 0;
	selp.b64 	%rd51, %rd50, 0, %p11;
	mul.lo.s64 	%rd52, %rd51, %rd8;
	add.s64 	%rd53, %rd4, %rd52;
	ld.global.f64 	%fd7, [%rd53];
	abs.f64 	%fd8, %fd7;
	st.global.f64 	[%rd53], %fd8;
	ld.global.f64 	%fd9, [%rd53+8];
	abs.f64 	%fd10, %fd9;
	st.global.f64 	[%rd53+8], %fd10;
	ld.global.f64 	%fd11, [%rd53+16];
	abs.f64 	%fd12, %fd11;
	st.global.f64 	[%rd53+16], %fd12;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p12, %rd64, %rd34;
	@%p12 bra 	$L__BB2_10;
	bra.uni 	$L__BB2_31;

$L__BB2_3:
	cvt.u32.u64 	%r32, %rd7;

$L__BB2_4:
	setp.lt.s32 	%p4, %r6, 2;
	mov.u64 	%rd65, %rd64;
	@%p4 bra 	$L__BB2_8;

	or.b64  	%rd40, %rd64, %rd7;
	and.b64  	%rd41, %rd40, -4294967296;
	setp.eq.s64 	%p5, %rd41, 0;
	@%p5 bra 	$L__BB2_7;

	div.u64 	%rd65, %rd64, %rd7;
	bra.uni 	$L__BB2_8;

$L__BB2_7:
	cvt.u32.u64 	%r33, %rd64;
	div.u32 	%r34, %r33, %r32;
	cvt.u64.u32 	%rd65, %r34;

$L__BB2_8:
	cvt.s64.s32 	%rd42, %rd65;
	setp.gt.s32 	%p6, %r6, 0;
	selp.b64 	%rd43, %rd42, 0, %p6;
	mul.lo.s64 	%rd44, %rd43, %rd8;
	add.s64 	%rd45, %rd4, %rd44;
	ld.global.f64 	%fd1, [%rd45];
	abs.f64 	%fd2, %fd1;
	st.global.f64 	[%rd45], %fd2;
	ld.global.f64 	%fd3, [%rd45+8];
	abs.f64 	%fd4, %fd3;
	st.global.f64 	[%rd45+8], %fd4;
	ld.global.f64 	%fd5, [%rd45+16];
	abs.f64 	%fd6, %fd5;
	st.global.f64 	[%rd45+16], %fd6;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p7, %rd64, %rd34;
	@%p7 bra 	$L__BB2_4;

$L__BB2_31:
	ret;

}
	// .globl	absolutize_arr_vec3d_cuda_kernel_backward
.visible .entry absolutize_arr_vec3d_cuda_kernel_backward(
	.param .align 8 .b8 absolutize_arr_vec3d_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 absolutize_arr_vec3d_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 absolutize_arr_vec3d_cuda_kernel_backward_param_2[56]
)
{
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<60>;
	.reg .f64 	%fd<73>;
	.reg .b64 	%rd<101>;


	ld.param.v2.u32 	{%r26, %r27}, [absolutize_arr_vec3d_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r28, %r29}, [absolutize_arr_vec3d_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r34, %r35}, [absolutize_arr_vec3d_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r42, %r43}, [absolutize_arr_vec3d_cuda_kernel_backward_param_2+32];
	ld.param.u64 	%rd26, [absolutize_arr_vec3d_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd25, [absolutize_arr_vec3d_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd23, [absolutize_arr_vec3d_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [absolutize_arr_vec3d_cuda_kernel_backward_param_0+16];
	mov.u32 	%r46, %ntid.x;
	cvt.u64.u32 	%rd1, %r46;
	mov.u32 	%r47, %ctaid.x;
	mul.wide.u32 	%rd28, %r46, %r47;
	mov.u32 	%r48, %tid.x;
	cvt.u64.u32 	%rd29, %r48;
	add.s64 	%rd97, %rd28, %rd29;
	setp.ge.u64 	%p1, %rd97, %rd23;
	@%p1 bra 	$L__BB3_39;

	cvt.s64.s32 	%rd6, %r29;
	cvt.s64.s32 	%rd7, %r28;
	cvt.s64.s32 	%rd8, %r27;
	cvt.s64.s32 	%rd9, %r42;
	cvt.s64.s32 	%rd10, %r34;
	mov.u32 	%r49, %nctaid.x;
	cvt.u64.u32 	%rd30, %r49;
	mul.lo.s64 	%rd11, %rd1, %rd30;

$L__BB3_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd98, %rd97;
	@%p2 bra 	$L__BB3_6;

	or.b64  	%rd31, %rd97, %rd6;
	and.b64  	%rd32, %rd31, -4294967296;
	setp.eq.s64 	%p3, %rd32, 0;
	@%p3 bra 	$L__BB3_5;

	div.u64 	%rd98, %rd97, %rd6;
	bra.uni 	$L__BB3_6;

$L__BB3_5:
	cvt.u32.u64 	%r50, %rd6;
	cvt.u32.u64 	%r51, %rd97;
	div.u32 	%r52, %r51, %r50;
	cvt.u64.u32 	%rd98, %r52;

$L__BB3_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB3_10;

	or.b64  	%rd33, %rd98, %rd7;
	and.b64  	%rd34, %rd33, -4294967296;
	setp.eq.s64 	%p5, %rd34, 0;
	@%p5 bra 	$L__BB3_9;

	div.u64 	%rd98, %rd98, %rd7;
	bra.uni 	$L__BB3_10;

$L__BB3_9:
	cvt.u32.u64 	%r53, %rd7;
	cvt.u32.u64 	%r54, %rd98;
	div.u32 	%r55, %r54, %r53;
	cvt.u64.u32 	%rd98, %r55;

$L__BB3_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB3_14;

	or.b64  	%rd35, %rd98, %rd8;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p7, %rd36, 0;
	@%p7 bra 	$L__BB3_13;

	div.u64 	%rd98, %rd98, %rd8;
	bra.uni 	$L__BB3_14;

$L__BB3_13:
	cvt.u32.u64 	%r56, %rd8;
	cvt.u32.u64 	%r57, %rd98;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd98, %r58;

$L__BB3_14:
	cvt.u32.u64 	%r59, %rd98;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r59, 0, %p8;
	setp.eq.s64 	%p9, %rd26, 0;
	@%p9 bra 	$L__BB3_16;

	cvt.s64.s32 	%rd40, %r2;
	mul.lo.s64 	%rd41, %rd40, %rd9;
	add.s64 	%rd37, %rd26, %rd41;
	mov.f64 	%fd6, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1,[%rd37],%fd6; }

	// end inline asm
	add.s64 	%rd38, %rd37, 8;
	// begin inline asm
	{ atom.add.f64 %fd3,[%rd38],%fd6; }

	// end inline asm
	add.s64 	%rd39, %rd37, 16;
	// begin inline asm
	{ atom.add.f64 %fd5,[%rd39],%fd6; }

	// end inline asm
	bra.uni 	$L__BB3_18;

$L__BB3_16:
	setp.eq.s64 	%p10, %rd25, 0;
	@%p10 bra 	$L__BB3_18;

	cvt.s64.s32 	%rd45, %r2;
	mul.lo.s64 	%rd46, %rd45, %rd10;
	add.s64 	%rd42, %rd25, %rd46;
	mov.f64 	%fd12, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd7,[%rd42],%fd12; }

	// end inline asm
	add.s64 	%rd43, %rd42, 8;
	// begin inline asm
	{ atom.add.f64 %fd9,[%rd43],%fd12; }

	// end inline asm
	add.s64 	%rd44, %rd42, 16;
	// begin inline asm
	{ atom.add.f64 %fd11,[%rd44],%fd12; }

	// end inline asm

$L__BB3_18:
	@%p9 bra 	$L__BB3_20;

	cvt.s64.s32 	%rd50, %r2;
	mul.lo.s64 	%rd51, %rd50, %rd9;
	add.s64 	%rd47, %rd26, %rd51;
	mov.f64 	%fd18, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd13,[%rd47],%fd18; }

	// end inline asm
	add.s64 	%rd48, %rd47, 8;
	// begin inline asm
	{ atom.add.f64 %fd15,[%rd48],%fd18; }

	// end inline asm
	add.s64 	%rd49, %rd47, 16;
	// begin inline asm
	{ atom.add.f64 %fd17,[%rd49],%fd18; }

	// end inline asm
	bra.uni 	$L__BB3_22;

$L__BB3_20:
	setp.eq.s64 	%p12, %rd25, 0;
	@%p12 bra 	$L__BB3_22;

	cvt.s64.s32 	%rd55, %r2;
	mul.lo.s64 	%rd56, %rd55, %rd10;
	add.s64 	%rd52, %rd25, %rd56;
	mov.f64 	%fd24, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd52],%fd24; }

	// end inline asm
	add.s64 	%rd53, %rd52, 8;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd53],%fd24; }

	// end inline asm
	add.s64 	%rd54, %rd52, 16;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd54],%fd24; }

	// end inline asm

$L__BB3_22:
	@%p9 bra 	$L__BB3_24;

	cvt.s64.s32 	%rd60, %r2;
	mul.lo.s64 	%rd61, %rd60, %rd9;
	add.s64 	%rd57, %rd26, %rd61;
	mov.f64 	%fd30, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd57],%fd30; }

	// end inline asm
	add.s64 	%rd58, %rd57, 8;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd58],%fd30; }

	// end inline asm
	add.s64 	%rd59, %rd57, 16;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd59],%fd30; }

	// end inline asm
	bra.uni 	$L__BB3_26;

$L__BB3_24:
	setp.eq.s64 	%p14, %rd25, 0;
	@%p14 bra 	$L__BB3_26;

	cvt.s64.s32 	%rd65, %r2;
	mul.lo.s64 	%rd66, %rd65, %rd10;
	add.s64 	%rd62, %rd25, %rd66;
	mov.f64 	%fd36, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd31,[%rd62],%fd36; }

	// end inline asm
	add.s64 	%rd63, %rd62, 8;
	// begin inline asm
	{ atom.add.f64 %fd33,[%rd63],%fd36; }

	// end inline asm
	add.s64 	%rd64, %rd62, 16;
	// begin inline asm
	{ atom.add.f64 %fd35,[%rd64],%fd36; }

	// end inline asm

$L__BB3_26:
	@%p9 bra 	$L__BB3_28;

	cvt.s64.s32 	%rd70, %r2;
	mul.lo.s64 	%rd71, %rd70, %rd9;
	add.s64 	%rd67, %rd26, %rd71;
	mov.f64 	%fd42, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd37,[%rd67],%fd42; }

	// end inline asm
	add.s64 	%rd68, %rd67, 8;
	// begin inline asm
	{ atom.add.f64 %fd39,[%rd68],%fd42; }

	// end inline asm
	add.s64 	%rd69, %rd67, 16;
	// begin inline asm
	{ atom.add.f64 %fd41,[%rd69],%fd42; }

	// end inline asm
	bra.uni 	$L__BB3_30;

$L__BB3_28:
	setp.eq.s64 	%p16, %rd25, 0;
	@%p16 bra 	$L__BB3_30;

	cvt.s64.s32 	%rd75, %r2;
	mul.lo.s64 	%rd76, %rd75, %rd10;
	add.s64 	%rd72, %rd25, %rd76;
	mov.f64 	%fd48, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd43,[%rd72],%fd48; }

	// end inline asm
	add.s64 	%rd73, %rd72, 8;
	// begin inline asm
	{ atom.add.f64 %fd45,[%rd73],%fd48; }

	// end inline asm
	add.s64 	%rd74, %rd72, 16;
	// begin inline asm
	{ atom.add.f64 %fd47,[%rd74],%fd48; }

	// end inline asm

$L__BB3_30:
	@%p9 bra 	$L__BB3_32;

	cvt.s64.s32 	%rd80, %r2;
	mul.lo.s64 	%rd81, %rd80, %rd9;
	add.s64 	%rd77, %rd26, %rd81;
	mov.f64 	%fd54, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd49,[%rd77],%fd54; }

	// end inline asm
	add.s64 	%rd78, %rd77, 8;
	// begin inline asm
	{ atom.add.f64 %fd51,[%rd78],%fd54; }

	// end inline asm
	add.s64 	%rd79, %rd77, 16;
	// begin inline asm
	{ atom.add.f64 %fd53,[%rd79],%fd54; }

	// end inline asm
	bra.uni 	$L__BB3_34;

$L__BB3_32:
	setp.eq.s64 	%p18, %rd25, 0;
	@%p18 bra 	$L__BB3_34;

	cvt.s64.s32 	%rd85, %r2;
	mul.lo.s64 	%rd86, %rd85, %rd10;
	add.s64 	%rd82, %rd25, %rd86;
	mov.f64 	%fd60, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd55,[%rd82],%fd60; }

	// end inline asm
	add.s64 	%rd83, %rd82, 8;
	// begin inline asm
	{ atom.add.f64 %fd57,[%rd83],%fd60; }

	// end inline asm
	add.s64 	%rd84, %rd82, 16;
	// begin inline asm
	{ atom.add.f64 %fd59,[%rd84],%fd60; }

	// end inline asm

$L__BB3_34:
	@%p9 bra 	$L__BB3_36;

	cvt.s64.s32 	%rd90, %r2;
	mul.lo.s64 	%rd91, %rd90, %rd9;
	add.s64 	%rd87, %rd26, %rd91;
	mov.f64 	%fd66, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd61,[%rd87],%fd66; }

	// end inline asm
	add.s64 	%rd88, %rd87, 8;
	// begin inline asm
	{ atom.add.f64 %fd63,[%rd88],%fd66; }

	// end inline asm
	add.s64 	%rd89, %rd87, 16;
	// begin inline asm
	{ atom.add.f64 %fd65,[%rd89],%fd66; }

	// end inline asm
	bra.uni 	$L__BB3_38;

$L__BB3_36:
	setp.eq.s64 	%p20, %rd25, 0;
	@%p20 bra 	$L__BB3_38;

	cvt.s64.s32 	%rd95, %r2;
	mul.lo.s64 	%rd96, %rd95, %rd10;
	add.s64 	%rd92, %rd25, %rd96;
	mov.f64 	%fd72, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd67,[%rd92],%fd72; }

	// end inline asm
	add.s64 	%rd93, %rd92, 8;
	// begin inline asm
	{ atom.add.f64 %fd69,[%rd93],%fd72; }

	// end inline asm
	add.s64 	%rd94, %rd92, 16;
	// begin inline asm
	{ atom.add.f64 %fd71,[%rd94],%fd72; }

	// end inline asm

$L__BB3_38:
	add.s64 	%rd97, %rd97, %rd11;
	setp.lt.u64 	%p21, %rd97, %rd23;
	@%p21 bra 	$L__BB3_2;

$L__BB3_39:
	ret;

}
	// .globl	sys_to_x_soft_cuda_kernel_forward
.visible .entry sys_to_x_soft_cuda_kernel_forward(
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_forward_param_2[56],
	.param .u32 sys_to_x_soft_cuda_kernel_forward_param_3,
	.param .u32 sys_to_x_soft_cuda_kernel_forward_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<82>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<83>;


	ld.param.v2.u32 	{%r28, %r29}, [sys_to_x_soft_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r30, %r31}, [sys_to_x_soft_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r36, %r37}, [sys_to_x_soft_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r44, %r45}, [sys_to_x_soft_cuda_kernel_forward_param_2+32];
	ld.param.u32 	%r26, [sys_to_x_soft_cuda_kernel_forward_param_3];
	ld.param.u32 	%r27, [sys_to_x_soft_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd39, [sys_to_x_soft_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd37, [sys_to_x_soft_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd36, [sys_to_x_soft_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r7, [sys_to_x_soft_cuda_kernel_forward_param_0+16];
	mov.u32 	%r48, %ntid.x;
	cvt.u64.u32 	%rd1, %r48;
	mov.u32 	%r49, %ctaid.x;
	mul.wide.u32 	%rd41, %r48, %r49;
	mov.u32 	%r50, %tid.x;
	cvt.u64.u32 	%rd42, %r50;
	add.s64 	%rd74, %rd41, %rd42;
	setp.ge.u64 	%p1, %rd74, %rd36;
	@%p1 bra 	$L__BB4_31;

	cvta.to.global.u64 	%rd4, %rd39;
	cvta.to.global.u64 	%rd5, %rd37;
	cvt.s64.s32 	%rd6, %r31;
	cvt.s64.s32 	%rd7, %r30;
	cvt.s64.s32 	%rd8, %r29;
	shl.b32 	%r2, %r27, 2;
	cvt.s64.s32 	%rd9, %r36;
	cvt.s64.s32 	%rd10, %r44;
	mov.u32 	%r51, %nctaid.x;
	cvt.u64.u32 	%rd43, %r51;
	mul.lo.s64 	%rd11, %rd1, %rd43;
	setp.gt.s32 	%p2, %r7, 3;
	@%p2 bra 	$L__BB4_18;
	bra.uni 	$L__BB4_2;

$L__BB4_18:
	cvt.u32.u64 	%r69, %rd6;
	cvt.u32.u64 	%r72, %rd7;

$L__BB4_19:
	or.b64  	%rd62, %rd74, %rd6;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p13, %rd63, 0;
	@%p13 bra 	$L__BB4_21;

	div.u64 	%rd81, %rd74, %rd6;
	bra.uni 	$L__BB4_22;

$L__BB4_21:
	cvt.u32.u64 	%r70, %rd74;
	div.u32 	%r71, %r70, %r69;
	cvt.u64.u32 	%rd81, %r71;

$L__BB4_22:
	setp.lt.s32 	%p14, %r7, 3;
	@%p14 bra 	$L__BB4_26;

	or.b64  	%rd64, %rd81, %rd7;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p15, %rd65, 0;
	@%p15 bra 	$L__BB4_25;

	div.u64 	%rd81, %rd81, %rd7;
	bra.uni 	$L__BB4_26;

$L__BB4_25:
	cvt.u32.u64 	%r73, %rd81;
	div.u32 	%r74, %r73, %r72;
	cvt.u64.u32 	%rd81, %r74;

$L__BB4_26:
	setp.lt.s32 	%p16, %r7, 2;
	@%p16 bra 	$L__BB4_30;

	or.b64  	%rd66, %rd81, %rd8;
	and.b64  	%rd67, %rd66, -4294967296;
	setp.eq.s64 	%p17, %rd67, 0;
	@%p17 bra 	$L__BB4_29;

	div.u64 	%rd81, %rd81, %rd8;
	bra.uni 	$L__BB4_30;

$L__BB4_29:
	cvt.u32.u64 	%r75, %rd8;
	cvt.u32.u64 	%r76, %rd81;
	div.u32 	%r77, %r76, %r75;
	cvt.u64.u32 	%rd81, %r77;

$L__BB4_30:
	cvt.u32.u64 	%r78, %rd81;
	setp.gt.s32 	%p18, %r7, 0;
	selp.b32 	%r79, %r78, 0, %p18;
	add.s32 	%r80, %r79, %r2;
	cvt.s64.s32 	%rd68, %r80;
	mul.lo.s64 	%rd69, %rd68, %rd9;
	add.s64 	%rd70, %rd5, %rd69;
	add.s32 	%r81, %r79, %r26;
	ld.global.f64 	%fd7, [%rd70];
	ld.global.f64 	%fd8, [%rd70+8];
	ld.global.f64 	%fd9, [%rd70+16];
	cvt.s64.s32 	%rd71, %r81;
	mul.lo.s64 	%rd72, %rd71, %rd10;
	add.s64 	%rd73, %rd4, %rd72;
	st.global.f64 	[%rd73], %fd7;
	st.global.f64 	[%rd73+8], %fd8;
	st.global.f64 	[%rd73+16], %fd9;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p19, %rd74, %rd36;
	@%p19 bra 	$L__BB4_19;
	bra.uni 	$L__BB4_31;

$L__BB4_2:
	setp.gt.s32 	%p3, %r7, 2;
	@%p3 bra 	$L__BB4_9;
	bra.uni 	$L__BB4_3;

$L__BB4_9:
	cvt.u32.u64 	%r59, %rd7;
	cvt.u32.u64 	%r62, %rd8;

$L__BB4_10:
	or.b64  	%rd52, %rd74, %rd7;
	and.b64  	%rd53, %rd52, -4294967296;
	setp.eq.s64 	%p8, %rd53, 0;
	@%p8 bra 	$L__BB4_12;

	div.u64 	%rd78, %rd74, %rd7;
	bra.uni 	$L__BB4_13;

$L__BB4_12:
	cvt.u32.u64 	%r60, %rd74;
	div.u32 	%r61, %r60, %r59;
	cvt.u64.u32 	%rd78, %r61;

$L__BB4_13:
	setp.lt.s32 	%p9, %r7, 2;
	@%p9 bra 	$L__BB4_17;

	or.b64  	%rd54, %rd78, %rd8;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p10, %rd55, 0;
	@%p10 bra 	$L__BB4_16;

	div.u64 	%rd78, %rd78, %rd8;
	bra.uni 	$L__BB4_17;

$L__BB4_16:
	cvt.u32.u64 	%r63, %rd78;
	div.u32 	%r64, %r63, %r62;
	cvt.u64.u32 	%rd78, %r64;

$L__BB4_17:
	cvt.u32.u64 	%r65, %rd78;
	setp.gt.s32 	%p11, %r7, 0;
	selp.b32 	%r66, %r65, 0, %p11;
	add.s32 	%r67, %r66, %r2;
	cvt.s64.s32 	%rd56, %r67;
	mul.lo.s64 	%rd57, %rd56, %rd9;
	add.s64 	%rd58, %rd5, %rd57;
	add.s32 	%r68, %r66, %r26;
	ld.global.f64 	%fd4, [%rd58];
	ld.global.f64 	%fd5, [%rd58+8];
	ld.global.f64 	%fd6, [%rd58+16];
	cvt.s64.s32 	%rd59, %r68;
	mul.lo.s64 	%rd60, %rd59, %rd10;
	add.s64 	%rd61, %rd4, %rd60;
	st.global.f64 	[%rd61], %fd4;
	st.global.f64 	[%rd61+8], %fd5;
	st.global.f64 	[%rd61+16], %fd6;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p12, %rd74, %rd36;
	@%p12 bra 	$L__BB4_10;
	bra.uni 	$L__BB4_31;

$L__BB4_3:
	cvt.u32.u64 	%r52, %rd8;

$L__BB4_4:
	setp.lt.s32 	%p4, %r7, 2;
	mov.u64 	%rd75, %rd74;
	@%p4 bra 	$L__BB4_8;

	or.b64  	%rd44, %rd74, %rd8;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p5, %rd45, 0;
	@%p5 bra 	$L__BB4_7;

	div.u64 	%rd75, %rd74, %rd8;
	bra.uni 	$L__BB4_8;

$L__BB4_7:
	cvt.u32.u64 	%r53, %rd74;
	div.u32 	%r54, %r53, %r52;
	cvt.u64.u32 	%rd75, %r54;

$L__BB4_8:
	cvt.u32.u64 	%r55, %rd75;
	setp.gt.s32 	%p6, %r7, 0;
	selp.b32 	%r56, %r55, 0, %p6;
	add.s32 	%r57, %r56, %r2;
	cvt.s64.s32 	%rd46, %r57;
	mul.lo.s64 	%rd47, %rd46, %rd9;
	add.s64 	%rd48, %rd5, %rd47;
	add.s32 	%r58, %r56, %r26;
	ld.global.f64 	%fd1, [%rd48];
	ld.global.f64 	%fd2, [%rd48+8];
	ld.global.f64 	%fd3, [%rd48+16];
	cvt.s64.s32 	%rd49, %r58;
	mul.lo.s64 	%rd50, %rd49, %rd10;
	add.s64 	%rd51, %rd4, %rd50;
	st.global.f64 	[%rd51], %fd1;
	st.global.f64 	[%rd51+8], %fd2;
	st.global.f64 	[%rd51+16], %fd3;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p7, %rd74, %rd36;
	@%p7 bra 	$L__BB4_4;

$L__BB4_31:
	ret;

}
	// .globl	sys_to_x_soft_cuda_kernel_backward
.visible .entry sys_to_x_soft_cuda_kernel_backward(
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_backward_param_2[56],
	.param .u32 sys_to_x_soft_cuda_kernel_backward_param_3,
	.param .u32 sys_to_x_soft_cuda_kernel_backward_param_4,
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_backward_param_6[56],
	.param .u32 sys_to_x_soft_cuda_kernel_backward_param_7,
	.param .u32 sys_to_x_soft_cuda_kernel_backward_param_8
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<99>;
	.reg .f64 	%fd<34>;
	.reg .b64 	%rd<67>;


	ld.param.v2.u32 	{%r48, %r49}, [sys_to_x_soft_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r50, %r51}, [sys_to_x_soft_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r56, %r57}, [sys_to_x_soft_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r64, %r65}, [sys_to_x_soft_cuda_kernel_backward_param_2+32];
	ld.param.u32 	%r28, [sys_to_x_soft_cuda_kernel_backward_param_3];
	ld.param.u32 	%r29, [sys_to_x_soft_cuda_kernel_backward_param_4];
	ld.param.v2.u32 	{%r72, %r73}, [sys_to_x_soft_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r80, %r81}, [sys_to_x_soft_cuda_kernel_backward_param_6+32];
	ld.param.u64 	%rd36, [sys_to_x_soft_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd34, [sys_to_x_soft_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd33, [sys_to_x_soft_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd31, [sys_to_x_soft_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd29, [sys_to_x_soft_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r9, [sys_to_x_soft_cuda_kernel_backward_param_0+16];
	mov.u32 	%r84, %ntid.x;
	cvt.u64.u32 	%rd1, %r84;
	mov.u32 	%r85, %ctaid.x;
	mul.wide.u32 	%rd38, %r84, %r85;
	mov.u32 	%r86, %tid.x;
	cvt.u64.u32 	%rd39, %r86;
	add.s64 	%rd63, %rd38, %rd39;
	setp.ge.u64 	%p1, %rd63, %rd29;
	@%p1 bra 	$L__BB5_23;

	cvta.to.global.u64 	%rd8, %rd36;
	cvta.to.global.u64 	%rd9, %rd33;
	cvt.s64.s32 	%rd10, %r51;
	cvt.s64.s32 	%rd11, %r50;
	cvt.s64.s32 	%rd12, %r49;
	shl.b32 	%r2, %r29, 2;
	cvt.s64.s32 	%rd13, %r80;
	cvt.s64.s32 	%rd14, %r64;
	mov.u32 	%r87, %nctaid.x;
	cvt.u64.u32 	%rd40, %r87;
	mul.lo.s64 	%rd15, %rd1, %rd40;
	cvt.s64.s32 	%rd16, %r72;
	cvt.s64.s32 	%rd17, %r56;

$L__BB5_2:
	setp.lt.s32 	%p2, %r9, 4;
	mov.u64 	%rd64, %rd63;
	@%p2 bra 	$L__BB5_6;

	or.b64  	%rd41, %rd63, %rd10;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p3, %rd42, 0;
	@%p3 bra 	$L__BB5_5;

	div.u64 	%rd64, %rd63, %rd10;
	bra.uni 	$L__BB5_6;

$L__BB5_5:
	cvt.u32.u64 	%r88, %rd10;
	cvt.u32.u64 	%r89, %rd63;
	div.u32 	%r90, %r89, %r88;
	cvt.u64.u32 	%rd64, %r90;

$L__BB5_6:
	setp.lt.s32 	%p4, %r9, 3;
	@%p4 bra 	$L__BB5_10;

	or.b64  	%rd43, %rd64, %rd11;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p5, %rd44, 0;
	@%p5 bra 	$L__BB5_9;

	div.u64 	%rd64, %rd64, %rd11;
	bra.uni 	$L__BB5_10;

$L__BB5_9:
	cvt.u32.u64 	%r91, %rd11;
	cvt.u32.u64 	%r92, %rd64;
	div.u32 	%r93, %r92, %r91;
	cvt.u64.u32 	%rd64, %r93;

$L__BB5_10:
	setp.lt.s32 	%p6, %r9, 2;
	@%p6 bra 	$L__BB5_14;

	or.b64  	%rd45, %rd64, %rd12;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p7, %rd46, 0;
	@%p7 bra 	$L__BB5_13;

	div.u64 	%rd64, %rd64, %rd12;
	bra.uni 	$L__BB5_14;

$L__BB5_13:
	cvt.u32.u64 	%r94, %rd12;
	cvt.u32.u64 	%r95, %rd64;
	div.u32 	%r96, %r95, %r94;
	cvt.u64.u32 	%rd64, %r96;

$L__BB5_14:
	cvt.u32.u64 	%r97, %rd64;
	setp.gt.s32 	%p8, %r9, 0;
	selp.b32 	%r98, %r97, 0, %p8;
	add.s32 	%r3, %r98, %r2;
	add.s32 	%r4, %r98, %r28;
	setp.eq.s64 	%p9, %rd36, 0;
	@%p9 bra 	$L__BB5_16;

	cvt.s64.s32 	%rd47, %r4;
	mul.lo.s64 	%rd48, %rd47, %rd13;
	add.s64 	%rd49, %rd8, %rd48;
	ld.global.f64 	%fd10, [%rd49];
	add.f64 	%fd33, %fd10, 0d0000000000000000;
	ld.global.f64 	%fd11, [%rd49+8];
	add.f64 	%fd32, %fd11, 0d0000000000000000;
	ld.global.f64 	%fd12, [%rd49+16];
	add.f64 	%fd31, %fd12, 0d0000000000000000;
	bra.uni 	$L__BB5_18;

$L__BB5_16:
	setp.eq.s64 	%p10, %rd33, 0;
	mov.f64 	%fd31, 0d0000000000000000;
	mov.f64 	%fd32, %fd31;
	mov.f64 	%fd33, %fd31;
	@%p10 bra 	$L__BB5_18;

	cvt.s64.s32 	%rd50, %r4;
	mul.lo.s64 	%rd51, %rd50, %rd14;
	add.s64 	%rd52, %rd9, %rd51;
	ld.global.f64 	%fd16, [%rd52];
	add.f64 	%fd33, %fd16, 0d0000000000000000;
	ld.global.f64 	%fd17, [%rd52+8];
	add.f64 	%fd32, %fd17, 0d0000000000000000;
	ld.global.f64 	%fd18, [%rd52+16];
	add.f64 	%fd31, %fd18, 0d0000000000000000;

$L__BB5_18:
	setp.eq.s64 	%p11, %rd34, 0;
	@%p11 bra 	$L__BB5_20;

	cvt.s64.s32 	%rd56, %r3;
	mul.lo.s64 	%rd57, %rd56, %rd16;
	add.s64 	%rd53, %rd34, %rd57;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd53],%fd33; }

	// end inline asm
	add.s64 	%rd54, %rd53, 8;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd54],%fd32; }

	// end inline asm
	add.s64 	%rd55, %rd53, 16;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd55],%fd31; }

	// end inline asm
	bra.uni 	$L__BB5_22;

$L__BB5_20:
	setp.eq.s64 	%p12, %rd31, 0;
	@%p12 bra 	$L__BB5_22;

	cvt.s64.s32 	%rd61, %r3;
	mul.lo.s64 	%rd62, %rd61, %rd17;
	add.s64 	%rd58, %rd31, %rd62;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd58],%fd33; }

	// end inline asm
	add.s64 	%rd59, %rd58, 8;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd59],%fd32; }

	// end inline asm
	add.s64 	%rd60, %rd58, 16;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd60],%fd31; }

	// end inline asm

$L__BB5_22:
	add.s64 	%rd63, %rd63, %rd15;
	setp.lt.u64 	%p13, %rd63, %rd29;
	@%p13 bra 	$L__BB5_2;

$L__BB5_23:
	ret;

}
	// .globl	initialize_soft_tilde_x_cuda_kernel_forward
.visible .entry initialize_soft_tilde_x_cuda_kernel_forward(
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_forward_param_3[56],
	.param .f64 initialize_soft_tilde_x_cuda_kernel_forward_param_4,
	.param .u32 initialize_soft_tilde_x_cuda_kernel_forward_param_5,
	.param .u32 initialize_soft_tilde_x_cuda_kernel_forward_param_6
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<25>;
	.reg .b32 	%r<91>;
	.reg .f64 	%fd<26>;
	.reg .b64 	%rd<79>;


	ld.param.v2.u32 	{%r38, %r39}, [initialize_soft_tilde_x_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r40, %r41}, [initialize_soft_tilde_x_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r46, %r47}, [initialize_soft_tilde_x_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r54, %r55}, [initialize_soft_tilde_x_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r62, %r63}, [initialize_soft_tilde_x_cuda_kernel_forward_param_3+32];
	ld.param.f64 	%fd1, [initialize_soft_tilde_x_cuda_kernel_forward_param_4];
	ld.param.u32 	%r36, [initialize_soft_tilde_x_cuda_kernel_forward_param_5];
	ld.param.u32 	%r37, [initialize_soft_tilde_x_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd42, [initialize_soft_tilde_x_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd40, [initialize_soft_tilde_x_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd38, [initialize_soft_tilde_x_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd37, [initialize_soft_tilde_x_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r8, [initialize_soft_tilde_x_cuda_kernel_forward_param_0+16];
	mov.u32 	%r66, %ntid.x;
	cvt.u64.u32 	%rd1, %r66;
	mov.u32 	%r67, %ctaid.x;
	mul.wide.u32 	%rd44, %r66, %r67;
	mov.u32 	%r68, %tid.x;
	cvt.u64.u32 	%rd45, %r68;
	add.s64 	%rd72, %rd44, %rd45;
	setp.ge.u64 	%p1, %rd72, %rd37;
	@%p1 bra 	$L__BB6_33;

	cvta.to.global.u64 	%rd4, %rd40;
	cvta.to.global.u64 	%rd5, %rd38;
	cvta.to.global.u64 	%rd6, %rd42;
	cvt.s64.s32 	%rd7, %r41;
	cvt.s64.s32 	%rd8, %r40;
	cvt.s64.s32 	%rd9, %r39;
	cvt.s64.s32 	%rd10, %r54;
	cvt.s64.s32 	%rd11, %r46;
	cvt.s64.s32 	%rd12, %r62;
	setp.gt.s32 	%p2, %r8, 3;
	@%p2 bra 	$L__BB6_16;
	bra.uni 	$L__BB6_2;

$L__BB6_16:
	cvt.u32.u64 	%r78, %rd7;
	cvt.u32.u64 	%r81, %rd8;
	cvt.u32.u64 	%r84, %rd9;

$L__BB6_17:
	or.b64  	%rd58, %rd72, %rd7;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p11, %rd59, 0;
	@%p11 bra 	$L__BB6_19;

	div.u64 	%rd77, %rd72, %rd7;
	bra.uni 	$L__BB6_20;

$L__BB6_19:
	cvt.u32.u64 	%r79, %rd72;
	div.u32 	%r80, %r79, %r78;
	cvt.u64.u32 	%rd77, %r80;

$L__BB6_20:
	setp.lt.s32 	%p12, %r8, 3;
	@%p12 bra 	$L__BB6_24;

	or.b64  	%rd60, %rd77, %rd8;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.eq.s64 	%p13, %rd61, 0;
	@%p13 bra 	$L__BB6_23;

	div.u64 	%rd77, %rd77, %rd8;
	bra.uni 	$L__BB6_24;

$L__BB6_23:
	cvt.u32.u64 	%r82, %rd77;
	div.u32 	%r83, %r82, %r81;
	cvt.u64.u32 	%rd77, %r83;

$L__BB6_24:
	setp.lt.s32 	%p14, %r8, 2;
	@%p14 bra 	$L__BB6_28;

	or.b64  	%rd62, %rd77, %rd9;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p15, %rd63, 0;
	@%p15 bra 	$L__BB6_27;

	div.u64 	%rd77, %rd77, %rd9;
	bra.uni 	$L__BB6_28;

$L__BB6_27:
	cvt.u32.u64 	%r85, %rd77;
	div.u32 	%r86, %r85, %r84;
	cvt.u64.u32 	%rd77, %r86;

$L__BB6_28:
	cvt.u32.u64 	%r87, %rd77;
	setp.gt.s32 	%p16, %r8, 0;
	selp.b32 	%r88, %r87, 0, %p16;
	add.s32 	%r3, %r88, %r36;
	cvt.s64.s32 	%rd64, %r3;
	mul.lo.s64 	%rd65, %rd64, %rd10;
	add.s64 	%rd34, %rd4, %rd65;
	cvt.s64.s32 	%rd66, %r88;
	mul.lo.s64 	%rd67, %rd66, %rd11;
	add.s64 	%rd35, %rd5, %rd67;
	setp.eq.s32 	%p17, %r37, 0;
	@%p17 bra 	$L__BB6_31;

	setp.ne.s32 	%p18, %r37, 1;
	@%p18 bra 	$L__BB6_32;

	mul.lo.s64 	%rd69, %rd64, %rd12;
	add.s64 	%rd70, %rd6, %rd69;
	ld.global.f64 	%fd14, [%rd70];
	ld.global.f64 	%fd15, [%rd70+8];
	ld.global.f64 	%fd16, [%rd70+16];
	ld.global.f64 	%fd17, [%rd34];
	fma.rn.f64 	%fd18, %fd14, %fd1, %fd17;
	ld.global.f64 	%fd19, [%rd34+8];
	fma.rn.f64 	%fd20, %fd15, %fd1, %fd19;
	ld.global.f64 	%fd21, [%rd34+16];
	fma.rn.f64 	%fd22, %fd16, %fd1, %fd21;
	st.global.f64 	[%rd35], %fd18;
	st.global.f64 	[%rd35+8], %fd20;
	st.global.f64 	[%rd35+16], %fd22;
	bra.uni 	$L__BB6_32;

$L__BB6_31:
	ld.global.f64 	%fd23, [%rd34];
	ld.global.f64 	%fd24, [%rd34+8];
	ld.global.f64 	%fd25, [%rd34+16];
	st.global.f64 	[%rd35], %fd23;
	st.global.f64 	[%rd35+8], %fd24;
	st.global.f64 	[%rd35+16], %fd25;

$L__BB6_32:
	mov.u32 	%r90, %nctaid.x;
	mul.wide.u32 	%rd71, %r66, %r90;
	add.s64 	%rd72, %rd72, %rd71;
	setp.lt.u64 	%p19, %rd72, %rd37;
	@%p19 bra 	$L__BB6_17;
	bra.uni 	$L__BB6_33;

$L__BB6_2:
	mov.u32 	%r69, %nctaid.x;
	cvt.u64.u32 	%rd46, %r69;
	mul.lo.s64 	%rd13, %rd1, %rd46;
	cvt.u32.u64 	%r70, %rd8;
	cvt.u32.u64 	%r73, %rd9;

$L__BB6_3:
	setp.lt.s32 	%p3, %r8, 3;
	mov.u64 	%rd73, %rd72;
	@%p3 bra 	$L__BB6_7;

	or.b64  	%rd47, %rd72, %rd8;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p4, %rd48, 0;
	@%p4 bra 	$L__BB6_6;

	div.u64 	%rd73, %rd72, %rd8;
	bra.uni 	$L__BB6_7;

$L__BB6_6:
	cvt.u32.u64 	%r71, %rd72;
	div.u32 	%r72, %r71, %r70;
	cvt.u64.u32 	%rd73, %r72;

$L__BB6_7:
	setp.lt.s32 	%p5, %r8, 2;
	@%p5 bra 	$L__BB6_11;

	or.b64  	%rd49, %rd73, %rd9;
	and.b64  	%rd50, %rd49, -4294967296;
	setp.eq.s64 	%p6, %rd50, 0;
	@%p6 bra 	$L__BB6_10;

	div.u64 	%rd73, %rd73, %rd9;
	bra.uni 	$L__BB6_11;

$L__BB6_10:
	cvt.u32.u64 	%r74, %rd73;
	div.u32 	%r75, %r74, %r73;
	cvt.u64.u32 	%rd73, %r75;

$L__BB6_11:
	cvt.u32.u64 	%r76, %rd73;
	setp.gt.s32 	%p7, %r8, 0;
	selp.b32 	%r77, %r76, 0, %p7;
	add.s32 	%r2, %r77, %r36;
	cvt.s64.s32 	%rd51, %r2;
	mul.lo.s64 	%rd52, %rd51, %rd10;
	add.s64 	%rd21, %rd4, %rd52;
	cvt.s64.s32 	%rd53, %r77;
	mul.lo.s64 	%rd54, %rd53, %rd11;
	add.s64 	%rd22, %rd5, %rd54;
	setp.eq.s32 	%p8, %r37, 0;
	@%p8 bra 	$L__BB6_14;

	setp.ne.s32 	%p9, %r37, 1;
	@%p9 bra 	$L__BB6_15;

	mul.lo.s64 	%rd56, %rd51, %rd12;
	add.s64 	%rd57, %rd6, %rd56;
	ld.global.f64 	%fd2, [%rd57];
	ld.global.f64 	%fd3, [%rd57+8];
	ld.global.f64 	%fd4, [%rd57+16];
	ld.global.f64 	%fd5, [%rd21];
	fma.rn.f64 	%fd6, %fd2, %fd1, %fd5;
	ld.global.f64 	%fd7, [%rd21+8];
	fma.rn.f64 	%fd8, %fd3, %fd1, %fd7;
	ld.global.f64 	%fd9, [%rd21+16];
	fma.rn.f64 	%fd10, %fd4, %fd1, %fd9;
	st.global.f64 	[%rd22], %fd6;
	st.global.f64 	[%rd22+8], %fd8;
	st.global.f64 	[%rd22+16], %fd10;
	bra.uni 	$L__BB6_15;

$L__BB6_14:
	ld.global.f64 	%fd11, [%rd21];
	ld.global.f64 	%fd12, [%rd21+8];
	ld.global.f64 	%fd13, [%rd21+16];
	st.global.f64 	[%rd22], %fd11;
	st.global.f64 	[%rd22+8], %fd12;
	st.global.f64 	[%rd22+16], %fd13;

$L__BB6_15:
	add.s64 	%rd72, %rd72, %rd13;
	setp.lt.u64 	%p10, %rd72, %rd37;
	@%p10 bra 	$L__BB6_3;

$L__BB6_33:
	ret;

}
	// .globl	initialize_soft_tilde_x_cuda_kernel_backward
.visible .entry initialize_soft_tilde_x_cuda_kernel_backward(
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_backward_param_3[56],
	.param .f64 initialize_soft_tilde_x_cuda_kernel_backward_param_4,
	.param .u32 initialize_soft_tilde_x_cuda_kernel_backward_param_5,
	.param .u32 initialize_soft_tilde_x_cuda_kernel_backward_param_6,
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_backward_param_9[56],
	.param .f64 initialize_soft_tilde_x_cuda_kernel_backward_param_10,
	.param .u32 initialize_soft_tilde_x_cuda_kernel_backward_param_11,
	.param .u32 initialize_soft_tilde_x_cuda_kernel_backward_param_12
)
{
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<55>;
	.reg .b32 	%r<133>;
	.reg .f64 	%fd<87>;
	.reg .b64 	%rd<96>;


	ld.param.v2.u32 	{%r64, %r65}, [initialize_soft_tilde_x_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r66, %r67}, [initialize_soft_tilde_x_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r72, %r73}, [initialize_soft_tilde_x_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r80, %r81}, [initialize_soft_tilde_x_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r88, %r89}, [initialize_soft_tilde_x_cuda_kernel_backward_param_3+32];
	ld.param.u32 	%r36, [initialize_soft_tilde_x_cuda_kernel_backward_param_6];
	ld.param.v2.u32 	{%r96, %r97}, [initialize_soft_tilde_x_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r104, %r105}, [initialize_soft_tilde_x_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r112, %r113}, [initialize_soft_tilde_x_cuda_kernel_backward_param_9+32];
	ld.param.u64 	%rd46, [initialize_soft_tilde_x_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd44, [initialize_soft_tilde_x_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd42, [initialize_soft_tilde_x_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd41, [initialize_soft_tilde_x_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd39, [initialize_soft_tilde_x_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd37, [initialize_soft_tilde_x_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd35, [initialize_soft_tilde_x_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [initialize_soft_tilde_x_cuda_kernel_backward_param_0+16];
	mov.u32 	%r116, %ntid.x;
	cvt.u64.u32 	%rd1, %r116;
	mov.u32 	%r117, %ctaid.x;
	mul.wide.u32 	%rd48, %r116, %r117;
	mov.u32 	%r118, %tid.x;
	cvt.u64.u32 	%rd49, %r118;
	add.s64 	%rd92, %rd48, %rd49;
	setp.ge.u64 	%p2, %rd92, %rd35;
	@%p2 bra 	$L__BB7_37;

	cvt.s64.s32 	%rd12, %r67;
	cvt.s64.s32 	%rd13, %r66;
	cvt.s64.s32 	%rd14, %r65;
	setp.ne.s32 	%p1, %r36, 0;
	cvt.s64.s32 	%rd15, %r80;
	cvt.s64.s32 	%rd16, %r88;
	cvt.s64.s32 	%rd17, %r96;
	mov.u32 	%r119, %nctaid.x;
	cvt.u64.u32 	%rd50, %r119;
	mul.lo.s64 	%rd18, %rd1, %rd50;
	cvt.s64.s32 	%rd19, %r72;
	cvt.s64.s32 	%rd20, %r104;
	cvt.s64.s32 	%rd21, %r112;
	not.pred 	%p13, %p1;

$L__BB7_2:
	setp.lt.s32 	%p3, %r7, 4;
	mov.u64 	%rd93, %rd92;
	@%p3 bra 	$L__BB7_6;

	or.b64  	%rd51, %rd92, %rd12;
	and.b64  	%rd52, %rd51, -4294967296;
	setp.eq.s64 	%p4, %rd52, 0;
	@%p4 bra 	$L__BB7_5;

	div.u64 	%rd93, %rd92, %rd12;
	bra.uni 	$L__BB7_6;

$L__BB7_5:
	cvt.u32.u64 	%r120, %rd12;
	cvt.u32.u64 	%r121, %rd92;
	div.u32 	%r122, %r121, %r120;
	cvt.u64.u32 	%rd93, %r122;

$L__BB7_6:
	setp.lt.s32 	%p5, %r7, 3;
	@%p5 bra 	$L__BB7_10;

	or.b64  	%rd53, %rd93, %rd13;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p6, %rd54, 0;
	@%p6 bra 	$L__BB7_9;

	div.u64 	%rd93, %rd93, %rd13;
	bra.uni 	$L__BB7_10;

$L__BB7_9:
	cvt.u32.u64 	%r123, %rd13;
	cvt.u32.u64 	%r124, %rd93;
	div.u32 	%r125, %r124, %r123;
	cvt.u64.u32 	%rd93, %r125;

$L__BB7_10:
	setp.lt.s32 	%p7, %r7, 2;
	@%p7 bra 	$L__BB7_14;

	or.b64  	%rd55, %rd93, %rd14;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p8, %rd56, 0;
	@%p8 bra 	$L__BB7_13;

	div.u64 	%rd93, %rd93, %rd14;
	bra.uni 	$L__BB7_14;

$L__BB7_13:
	cvt.u32.u64 	%r126, %rd14;
	cvt.u32.u64 	%r127, %rd93;
	div.u32 	%r128, %r127, %r126;
	cvt.u64.u32 	%rd93, %r128;

$L__BB7_14:
	cvta.to.global.u64 	%rd91, %rd37;
	cvta.to.global.u64 	%rd90, %rd42;
	ld.param.u32 	%r132, [initialize_soft_tilde_x_cuda_kernel_backward_param_5];
	ld.param.u32 	%r131, [initialize_soft_tilde_x_cuda_kernel_backward_param_6];
	cvt.u32.u64 	%r129, %rd93;
	setp.gt.s32 	%p9, %r7, 0;
	selp.b32 	%r130, %r129, 0, %p9;
	add.s32 	%r2, %r130, %r132;
	setp.eq.s32 	%p10, %r131, 1;
	selp.u16 	%rs52, 1, 0, %p10;
	setp.eq.s32 	%p11, %r131, 0;
	selp.b16 	%rs54, %rs54, %rs52, %p11;
	and.b16  	%rs53, %rs54, 255;
	setp.eq.s16 	%p12, %rs53, 0;
	cvt.s64.s32 	%rd57, %r130;
	mul.lo.s64 	%rd58, %rd57, %rd17;
	add.s64 	%rd32, %rd90, %rd58;
	mul.lo.s64 	%rd59, %rd57, %rd19;
	add.s64 	%rd33, %rd91, %rd59;
	or.pred  	%p14, %p12, %p13;
	@%p14 bra 	$L__BB7_27;

	setp.eq.s64 	%p15, %rd42, 0;
	@%p15 bra 	$L__BB7_17;

	ld.global.f64 	%fd26, [%rd32];
	add.f64 	%fd83, %fd26, 0d0000000000000000;
	ld.global.f64 	%fd27, [%rd32+8];
	add.f64 	%fd82, %fd27, 0d0000000000000000;
	ld.global.f64 	%fd28, [%rd32+16];
	add.f64 	%fd81, %fd28, 0d0000000000000000;
	bra.uni 	$L__BB7_19;

$L__BB7_17:
	setp.eq.s64 	%p16, %rd37, 0;
	mov.f64 	%fd81, 0d0000000000000000;
	mov.f64 	%fd82, %fd81;
	mov.f64 	%fd83, %fd81;
	@%p16 bra 	$L__BB7_19;

	ld.global.f64 	%fd32, [%rd33];
	add.f64 	%fd83, %fd32, 0d0000000000000000;
	ld.global.f64 	%fd33, [%rd33+8];
	add.f64 	%fd82, %fd33, 0d0000000000000000;
	ld.global.f64 	%fd34, [%rd33+16];
	add.f64 	%fd81, %fd34, 0d0000000000000000;

$L__BB7_19:
	ld.param.f64 	%fd80, [initialize_soft_tilde_x_cuda_kernel_backward_param_4];
	add.f64 	%fd10, %fd83, 0d0000000000000000;
	fma.rn.f64 	%fd11, %fd10, %fd80, 0d0000000000000000;
	add.f64 	%fd12, %fd82, 0d0000000000000000;
	fma.rn.f64 	%fd13, %fd12, %fd80, 0d0000000000000000;
	add.f64 	%fd14, %fd81, 0d0000000000000000;
	fma.rn.f64 	%fd15, %fd14, %fd80, 0d0000000000000000;
	setp.eq.s64 	%p17, %rd46, 0;
	@%p17 bra 	$L__BB7_21;

	cvt.s64.s32 	%rd63, %r2;
	mul.lo.s64 	%rd64, %rd63, %rd21;
	add.s64 	%rd60, %rd46, %rd64;
	// begin inline asm
	{ atom.add.f64 %fd35,[%rd60],%fd11; }

	// end inline asm
	add.s64 	%rd61, %rd60, 8;
	// begin inline asm
	{ atom.add.f64 %fd37,[%rd61],%fd13; }

	// end inline asm
	add.s64 	%rd62, %rd60, 16;
	// begin inline asm
	{ atom.add.f64 %fd39,[%rd62],%fd15; }

	// end inline asm
	bra.uni 	$L__BB7_23;

$L__BB7_21:
	setp.eq.s64 	%p18, %rd41, 0;
	@%p18 bra 	$L__BB7_23;

	cvt.s64.s32 	%rd68, %r2;
	mul.lo.s64 	%rd69, %rd68, %rd16;
	add.s64 	%rd65, %rd41, %rd69;
	// begin inline asm
	{ atom.add.f64 %fd41,[%rd65],%fd11; }

	// end inline asm
	add.s64 	%rd66, %rd65, 8;
	// begin inline asm
	{ atom.add.f64 %fd43,[%rd66],%fd13; }

	// end inline asm
	add.s64 	%rd67, %rd65, 16;
	// begin inline asm
	{ atom.add.f64 %fd45,[%rd67],%fd15; }

	// end inline asm

$L__BB7_23:
	setp.eq.s64 	%p19, %rd44, 0;
	@%p19 bra 	$L__BB7_25;

	cvt.s64.s32 	%rd73, %r2;
	mul.lo.s64 	%rd74, %rd73, %rd20;
	add.s64 	%rd70, %rd44, %rd74;
	// begin inline asm
	{ atom.add.f64 %fd47,[%rd70],%fd10; }

	// end inline asm
	add.s64 	%rd71, %rd70, 8;
	// begin inline asm
	{ atom.add.f64 %fd49,[%rd71],%fd12; }

	// end inline asm
	add.s64 	%rd72, %rd70, 16;
	// begin inline asm
	{ atom.add.f64 %fd51,[%rd72],%fd14; }

	// end inline asm
	bra.uni 	$L__BB7_27;

$L__BB7_25:
	setp.eq.s64 	%p20, %rd39, 0;
	@%p20 bra 	$L__BB7_27;

	cvt.s64.s32 	%rd78, %r2;
	mul.lo.s64 	%rd79, %rd78, %rd15;
	add.s64 	%rd75, %rd39, %rd79;
	// begin inline asm
	{ atom.add.f64 %fd53,[%rd75],%fd10; }

	// end inline asm
	add.s64 	%rd76, %rd75, 8;
	// begin inline asm
	{ atom.add.f64 %fd55,[%rd76],%fd12; }

	// end inline asm
	add.s64 	%rd77, %rd75, 16;
	// begin inline asm
	{ atom.add.f64 %fd57,[%rd77],%fd14; }

	// end inline asm

$L__BB7_27:
	@%p1 bra 	$L__BB7_36;

	setp.eq.s64 	%p22, %rd42, 0;
	@%p22 bra 	$L__BB7_30;

	ld.global.f64 	%fd59, [%rd32];
	add.f64 	%fd86, %fd59, 0d0000000000000000;
	ld.global.f64 	%fd60, [%rd32+8];
	add.f64 	%fd85, %fd60, 0d0000000000000000;
	ld.global.f64 	%fd61, [%rd32+16];
	add.f64 	%fd84, %fd61, 0d0000000000000000;
	bra.uni 	$L__BB7_32;

$L__BB7_30:
	setp.eq.s64 	%p23, %rd37, 0;
	mov.f64 	%fd84, 0d0000000000000000;
	mov.f64 	%fd85, %fd84;
	mov.f64 	%fd86, %fd84;
	@%p23 bra 	$L__BB7_32;

	ld.global.f64 	%fd65, [%rd33];
	add.f64 	%fd86, %fd65, 0d0000000000000000;
	ld.global.f64 	%fd66, [%rd33+8];
	add.f64 	%fd85, %fd66, 0d0000000000000000;
	ld.global.f64 	%fd67, [%rd33+16];
	add.f64 	%fd84, %fd67, 0d0000000000000000;

$L__BB7_32:
	setp.eq.s64 	%p24, %rd44, 0;
	@%p24 bra 	$L__BB7_34;

	cvt.s64.s32 	%rd83, %r2;
	mul.lo.s64 	%rd84, %rd83, %rd20;
	add.s64 	%rd80, %rd44, %rd84;
	// begin inline asm
	{ atom.add.f64 %fd68,[%rd80],%fd86; }

	// end inline asm
	add.s64 	%rd81, %rd80, 8;
	// begin inline asm
	{ atom.add.f64 %fd70,[%rd81],%fd85; }

	// end inline asm
	add.s64 	%rd82, %rd80, 16;
	// begin inline asm
	{ atom.add.f64 %fd72,[%rd82],%fd84; }

	// end inline asm
	bra.uni 	$L__BB7_36;

$L__BB7_34:
	setp.eq.s64 	%p25, %rd39, 0;
	@%p25 bra 	$L__BB7_36;

	cvt.s64.s32 	%rd88, %r2;
	mul.lo.s64 	%rd89, %rd88, %rd15;
	add.s64 	%rd85, %rd39, %rd89;
	// begin inline asm
	{ atom.add.f64 %fd74,[%rd85],%fd86; }

	// end inline asm
	add.s64 	%rd86, %rd85, 8;
	// begin inline asm
	{ atom.add.f64 %fd76,[%rd86],%fd85; }

	// end inline asm
	add.s64 	%rd87, %rd85, 16;
	// begin inline asm
	{ atom.add.f64 %fd78,[%rd87],%fd84; }

	// end inline asm

$L__BB7_36:
	add.s64 	%rd92, %rd92, %rd18;
	setp.lt.u64 	%p26, %rd92, %rd35;
	@%p26 bra 	$L__BB7_2;

$L__BB7_37:
	ret;

}
	// .globl	advection_x_cuda_kernel_forward
.visible .entry advection_x_cuda_kernel_forward(
	.param .align 8 .b8 advection_x_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 advection_x_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 advection_x_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 advection_x_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 advection_x_cuda_kernel_forward_param_4[56],
	.param .f64 advection_x_cuda_kernel_forward_param_5,
	.param .u32 advection_x_cuda_kernel_forward_param_6
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<103>;
	.reg .f64 	%fd<69>;
	.reg .b64 	%rd<84>;


	ld.param.v2.u32 	{%r46, %r47}, [advection_x_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r48, %r49}, [advection_x_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r54, %r55}, [advection_x_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r62, %r63}, [advection_x_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r70, %r71}, [advection_x_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r78, %r79}, [advection_x_cuda_kernel_forward_param_4+32];
	ld.param.f64 	%fd2, [advection_x_cuda_kernel_forward_param_5];
	ld.param.u32 	%r45, [advection_x_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd48, [advection_x_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd46, [advection_x_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd44, [advection_x_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd42, [advection_x_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd41, [advection_x_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r8, [advection_x_cuda_kernel_forward_param_0+16];
	mov.u32 	%r82, %ntid.x;
	cvt.u64.u32 	%rd1, %r82;
	mov.u32 	%r83, %ctaid.x;
	mul.wide.u32 	%rd50, %r82, %r83;
	mov.u32 	%r84, %tid.x;
	cvt.u64.u32 	%rd51, %r84;
	add.s64 	%rd77, %rd50, %rd51;
	setp.ge.u64 	%p1, %rd77, %rd41;
	@%p1 bra 	$L__BB8_33;

	cvta.to.global.u64 	%rd4, %rd48;
	cvta.to.global.u64 	%rd5, %rd46;
	cvta.to.global.u64 	%rd6, %rd42;
	cvta.to.global.u64 	%rd7, %rd44;
	cvt.s64.s32 	%rd8, %r49;
	cvt.s64.s32 	%rd9, %r48;
	cvt.s64.s32 	%rd10, %r47;
	mov.u32 	%r85, %nctaid.x;
	cvt.u64.u32 	%rd52, %r85;
	mul.lo.s64 	%rd11, %rd1, %rd52;
	cvt.s64.s32 	%rd12, %r70;
	cvt.s64.s32 	%rd13, %r54;
	mul.f64 	%fd1, %fd2, %fd2;
	cvt.s64.s32 	%rd14, %r78;
	cvt.s64.s32 	%rd15, %r62;
	setp.gt.s32 	%p2, %r8, 3;
	@%p2 bra 	$L__BB8_16;
	bra.uni 	$L__BB8_2;

$L__BB8_16:
	cvt.u32.u64 	%r93, %rd8;
	cvt.u32.u64 	%r96, %rd9;
	cvt.u32.u64 	%r99, %rd10;

$L__BB8_17:
	or.b64  	%rd64, %rd77, %rd8;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p11, %rd65, 0;
	@%p11 bra 	$L__BB8_19;

	div.u64 	%rd82, %rd77, %rd8;
	bra.uni 	$L__BB8_20;

$L__BB8_19:
	cvt.u32.u64 	%r94, %rd77;
	div.u32 	%r95, %r94, %r93;
	cvt.u64.u32 	%rd82, %r95;

$L__BB8_20:
	setp.lt.s32 	%p12, %r8, 3;
	@%p12 bra 	$L__BB8_24;

	or.b64  	%rd66, %rd82, %rd9;
	and.b64  	%rd67, %rd66, -4294967296;
	setp.eq.s64 	%p13, %rd67, 0;
	@%p13 bra 	$L__BB8_23;

	div.u64 	%rd82, %rd82, %rd9;
	bra.uni 	$L__BB8_24;

$L__BB8_23:
	cvt.u32.u64 	%r97, %rd82;
	div.u32 	%r98, %r97, %r96;
	cvt.u64.u32 	%rd82, %r98;

$L__BB8_24:
	setp.lt.s32 	%p14, %r8, 2;
	@%p14 bra 	$L__BB8_28;

	or.b64  	%rd68, %rd82, %rd10;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.eq.s64 	%p15, %rd69, 0;
	@%p15 bra 	$L__BB8_27;

	div.u64 	%rd82, %rd82, %rd10;
	bra.uni 	$L__BB8_28;

$L__BB8_27:
	cvt.u32.u64 	%r100, %rd82;
	div.u32 	%r101, %r100, %r99;
	cvt.u64.u32 	%rd82, %r101;

$L__BB8_28:
	cvt.u32.u64 	%r102, %rd82;
	setp.gt.s32 	%p16, %r8, 0;
	selp.b32 	%r3, %r102, 0, %p16;
	cvt.s64.s32 	%rd70, %r3;
	mul.lo.s64 	%rd71, %rd70, %rd12;
	add.s64 	%rd37, %rd5, %rd71;
	mul.lo.s64 	%rd72, %rd70, %rd13;
	add.s64 	%rd38, %rd6, %rd72;
	mul.lo.s64 	%rd73, %rd70, %rd14;
	add.s64 	%rd39, %rd4, %rd73;
	setp.eq.s32 	%p17, %r45, 0;
	@%p17 bra 	$L__BB8_31;

	setp.ne.s32 	%p18, %r45, 1;
	@%p18 bra 	$L__BB8_32;

	ld.global.f64 	%fd36, [%rd38];
	ld.global.f64 	%fd37, [%rd37];
	sub.f64 	%fd38, %fd37, %fd36;
	ld.global.f64 	%fd39, [%rd38+8];
	ld.global.f64 	%fd40, [%rd37+8];
	sub.f64 	%fd41, %fd40, %fd39;
	ld.global.f64 	%fd42, [%rd38+16];
	ld.global.f64 	%fd43, [%rd37+16];
	sub.f64 	%fd44, %fd43, %fd42;
	div.rn.f64 	%fd45, %fd38, %fd2;
	div.rn.f64 	%fd46, %fd41, %fd2;
	div.rn.f64 	%fd47, %fd44, %fd2;
	mul.lo.s64 	%rd75, %rd70, %rd15;
	add.s64 	%rd76, %rd7, %rd75;
	ld.global.f64 	%fd48, [%rd76];
	sub.f64 	%fd49, %fd45, %fd48;
	ld.global.f64 	%fd50, [%rd76+8];
	sub.f64 	%fd51, %fd46, %fd50;
	ld.global.f64 	%fd52, [%rd76+16];
	sub.f64 	%fd53, %fd47, %fd52;
	div.rn.f64 	%fd54, %fd49, %fd2;
	div.rn.f64 	%fd55, %fd51, %fd2;
	div.rn.f64 	%fd56, %fd53, %fd2;
	st.global.f64 	[%rd39], %fd54;
	st.global.f64 	[%rd39+8], %fd55;
	st.global.f64 	[%rd39+16], %fd56;
	st.global.f64 	[%rd76], %fd45;
	st.global.f64 	[%rd76+8], %fd46;
	st.global.f64 	[%rd76+16], %fd47;
	bra.uni 	$L__BB8_32;

$L__BB8_31:
	ld.global.f64 	%fd57, [%rd37];
	ld.global.f64 	%fd58, [%rd38];
	sub.f64 	%fd59, %fd57, %fd58;
	ld.global.f64 	%fd60, [%rd38+8];
	ld.global.f64 	%fd61, [%rd37+8];
	sub.f64 	%fd62, %fd61, %fd60;
	ld.global.f64 	%fd63, [%rd38+16];
	ld.global.f64 	%fd64, [%rd37+16];
	sub.f64 	%fd65, %fd64, %fd63;
	div.rn.f64 	%fd66, %fd59, %fd1;
	div.rn.f64 	%fd67, %fd62, %fd1;
	div.rn.f64 	%fd68, %fd65, %fd1;
	st.global.f64 	[%rd39], %fd66;
	st.global.f64 	[%rd39+8], %fd67;
	st.global.f64 	[%rd39+16], %fd68;

$L__BB8_32:
	add.s64 	%rd77, %rd77, %rd11;
	setp.lt.u64 	%p19, %rd77, %rd41;
	@%p19 bra 	$L__BB8_17;
	bra.uni 	$L__BB8_33;

$L__BB8_2:
	cvt.u32.u64 	%r86, %rd9;
	cvt.u32.u64 	%r89, %rd10;

$L__BB8_3:
	setp.lt.s32 	%p3, %r8, 3;
	mov.u64 	%rd78, %rd77;
	@%p3 bra 	$L__BB8_7;

	or.b64  	%rd53, %rd77, %rd9;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p4, %rd54, 0;
	@%p4 bra 	$L__BB8_6;

	div.u64 	%rd78, %rd77, %rd9;
	bra.uni 	$L__BB8_7;

$L__BB8_6:
	cvt.u32.u64 	%r87, %rd77;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd78, %r88;

$L__BB8_7:
	setp.lt.s32 	%p5, %r8, 2;
	@%p5 bra 	$L__BB8_11;

	or.b64  	%rd55, %rd78, %rd10;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p6, %rd56, 0;
	@%p6 bra 	$L__BB8_10;

	div.u64 	%rd78, %rd78, %rd10;
	bra.uni 	$L__BB8_11;

$L__BB8_10:
	cvt.u32.u64 	%r90, %rd78;
	div.u32 	%r91, %r90, %r89;
	cvt.u64.u32 	%rd78, %r91;

$L__BB8_11:
	cvt.u32.u64 	%r92, %rd78;
	setp.gt.s32 	%p7, %r8, 0;
	selp.b32 	%r2, %r92, 0, %p7;
	cvt.s64.s32 	%rd57, %r2;
	mul.lo.s64 	%rd58, %rd57, %rd12;
	add.s64 	%rd23, %rd5, %rd58;
	mul.lo.s64 	%rd59, %rd57, %rd13;
	add.s64 	%rd24, %rd6, %rd59;
	mul.lo.s64 	%rd60, %rd57, %rd14;
	add.s64 	%rd25, %rd4, %rd60;
	setp.eq.s32 	%p8, %r45, 0;
	@%p8 bra 	$L__BB8_14;

	setp.ne.s32 	%p9, %r45, 1;
	@%p9 bra 	$L__BB8_15;

	ld.global.f64 	%fd3, [%rd24];
	ld.global.f64 	%fd4, [%rd23];
	sub.f64 	%fd5, %fd4, %fd3;
	ld.global.f64 	%fd6, [%rd24+8];
	ld.global.f64 	%fd7, [%rd23+8];
	sub.f64 	%fd8, %fd7, %fd6;
	ld.global.f64 	%fd9, [%rd24+16];
	ld.global.f64 	%fd10, [%rd23+16];
	sub.f64 	%fd11, %fd10, %fd9;
	div.rn.f64 	%fd12, %fd5, %fd2;
	div.rn.f64 	%fd13, %fd8, %fd2;
	div.rn.f64 	%fd14, %fd11, %fd2;
	mul.lo.s64 	%rd62, %rd57, %rd15;
	add.s64 	%rd63, %rd7, %rd62;
	ld.global.f64 	%fd15, [%rd63];
	sub.f64 	%fd16, %fd12, %fd15;
	ld.global.f64 	%fd17, [%rd63+8];
	sub.f64 	%fd18, %fd13, %fd17;
	ld.global.f64 	%fd19, [%rd63+16];
	sub.f64 	%fd20, %fd14, %fd19;
	div.rn.f64 	%fd21, %fd16, %fd2;
	div.rn.f64 	%fd22, %fd18, %fd2;
	div.rn.f64 	%fd23, %fd20, %fd2;
	st.global.f64 	[%rd25], %fd21;
	st.global.f64 	[%rd25+8], %fd22;
	st.global.f64 	[%rd25+16], %fd23;
	st.global.f64 	[%rd63], %fd12;
	st.global.f64 	[%rd63+8], %fd13;
	st.global.f64 	[%rd63+16], %fd14;
	bra.uni 	$L__BB8_15;

$L__BB8_14:
	ld.global.f64 	%fd24, [%rd23];
	ld.global.f64 	%fd25, [%rd24];
	sub.f64 	%fd26, %fd24, %fd25;
	ld.global.f64 	%fd27, [%rd24+8];
	ld.global.f64 	%fd28, [%rd23+8];
	sub.f64 	%fd29, %fd28, %fd27;
	ld.global.f64 	%fd30, [%rd24+16];
	ld.global.f64 	%fd31, [%rd23+16];
	sub.f64 	%fd32, %fd31, %fd30;
	div.rn.f64 	%fd33, %fd26, %fd1;
	div.rn.f64 	%fd34, %fd29, %fd1;
	div.rn.f64 	%fd35, %fd32, %fd1;
	st.global.f64 	[%rd25], %fd33;
	st.global.f64 	[%rd25+8], %fd34;
	st.global.f64 	[%rd25+16], %fd35;

$L__BB8_15:
	add.s64 	%rd77, %rd77, %rd11;
	setp.lt.u64 	%p10, %rd77, %rd41;
	@%p10 bra 	$L__BB8_3;

$L__BB8_33:
	ret;

}
	// .globl	advection_x_cuda_kernel_backward
.visible .entry advection_x_cuda_kernel_backward(
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_4[56],
	.param .f64 advection_x_cuda_kernel_backward_param_5,
	.param .u32 advection_x_cuda_kernel_backward_param_6,
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_10[56],
	.param .f64 advection_x_cuda_kernel_backward_param_11,
	.param .u32 advection_x_cuda_kernel_backward_param_12
)
{
	.reg .pred 	%p<34>;
	.reg .b16 	%rs<71>;
	.reg .b32 	%r<164>;
	.reg .f64 	%fd<195>;
	.reg .b64 	%rd<135>;


	ld.param.v2.u32 	{%r81, %r82}, [advection_x_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r83, %r84}, [advection_x_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r89, %r90}, [advection_x_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r97, %r98}, [advection_x_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r105, %r106}, [advection_x_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r113, %r114}, [advection_x_cuda_kernel_backward_param_4+32];
	ld.param.f64 	%fd49, [advection_x_cuda_kernel_backward_param_5];
	ld.param.u32 	%r44, [advection_x_cuda_kernel_backward_param_6];
	ld.param.v2.u32 	{%r121, %r122}, [advection_x_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r129, %r130}, [advection_x_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r137, %r138}, [advection_x_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r145, %r146}, [advection_x_cuda_kernel_backward_param_10+32];
	ld.param.u64 	%rd56, [advection_x_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd54, [advection_x_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd52, [advection_x_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd50, [advection_x_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd49, [advection_x_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd47, [advection_x_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd45, [advection_x_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd43, [advection_x_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd41, [advection_x_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [advection_x_cuda_kernel_backward_param_0+16];
	mov.u32 	%r149, %ntid.x;
	cvt.u64.u32 	%rd1, %r149;
	mov.u32 	%r150, %ctaid.x;
	mul.wide.u32 	%rd58, %r149, %r150;
	mov.u32 	%r151, %tid.x;
	cvt.u64.u32 	%rd59, %r151;
	add.s64 	%rd131, %rd58, %rd59;
	setp.ge.u64 	%p2, %rd131, %rd41;
	@%p2 bra 	$L__BB9_49;

	cvt.s64.s32 	%rd16, %r84;
	cvt.s64.s32 	%rd17, %r83;
	cvt.s64.s32 	%rd18, %r82;
	cvt.s64.s32 	%rd19, %r105;
	cvt.s64.s32 	%rd20, %r89;
	mul.f64 	%fd1, %fd49, %fd49;
	setp.ne.s32 	%p1, %r44, 0;
	cvt.s64.s32 	%rd21, %r97;
	cvt.s64.s32 	%rd22, %r129;
	mov.u32 	%r152, %nctaid.x;
	cvt.u64.u32 	%rd60, %r152;
	mul.lo.s64 	%rd23, %rd1, %rd60;
	cvt.s64.s32 	%rd24, %r145;
	cvt.s64.s32 	%rd25, %r113;
	cvt.s64.s32 	%rd26, %r121;
	cvt.s64.s32 	%rd27, %r137;
	not.pred 	%p13, %p1;

$L__BB9_2:
	setp.lt.s32 	%p3, %r7, 4;
	mov.u64 	%rd132, %rd131;
	@%p3 bra 	$L__BB9_6;

	or.b64  	%rd61, %rd131, %rd16;
	and.b64  	%rd62, %rd61, -4294967296;
	setp.eq.s64 	%p4, %rd62, 0;
	@%p4 bra 	$L__BB9_5;

	div.u64 	%rd132, %rd131, %rd16;
	bra.uni 	$L__BB9_6;

$L__BB9_5:
	cvt.u32.u64 	%r153, %rd16;
	cvt.u32.u64 	%r154, %rd131;
	div.u32 	%r155, %r154, %r153;
	cvt.u64.u32 	%rd132, %r155;

$L__BB9_6:
	setp.lt.s32 	%p5, %r7, 3;
	@%p5 bra 	$L__BB9_10;

	or.b64  	%rd63, %rd132, %rd17;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.eq.s64 	%p6, %rd64, 0;
	@%p6 bra 	$L__BB9_9;

	div.u64 	%rd132, %rd132, %rd17;
	bra.uni 	$L__BB9_10;

$L__BB9_9:
	cvt.u32.u64 	%r156, %rd17;
	cvt.u32.u64 	%r157, %rd132;
	div.u32 	%r158, %r157, %r156;
	cvt.u64.u32 	%rd132, %r158;

$L__BB9_10:
	setp.lt.s32 	%p7, %r7, 2;
	@%p7 bra 	$L__BB9_14;

	or.b64  	%rd65, %rd132, %rd18;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p8, %rd66, 0;
	@%p8 bra 	$L__BB9_13;

	div.u64 	%rd132, %rd132, %rd18;
	bra.uni 	$L__BB9_14;

$L__BB9_13:
	cvt.u32.u64 	%r159, %rd18;
	cvt.u32.u64 	%r160, %rd132;
	div.u32 	%r161, %r160, %r159;
	cvt.u64.u32 	%rd132, %r161;

$L__BB9_14:
	cvta.to.global.u64 	%rd128, %rd49;
	cvta.to.global.u64 	%rd127, %rd56;
	ld.param.u32 	%r163, [advection_x_cuda_kernel_backward_param_6];
	cvt.u32.u64 	%r162, %rd132;
	setp.gt.s32 	%p9, %r7, 0;
	selp.b32 	%r2, %r162, 0, %p9;
	setp.eq.s32 	%p10, %r163, 0;
	selp.f64 	%fd185, %fd1, %fd185, %p10;
	setp.eq.s32 	%p11, %r163, 1;
	selp.u16 	%rs68, 1, 0, %p11;
	selp.b16 	%rs70, %rs70, %rs68, %p10;
	and.b16  	%rs69, %rs70, 255;
	setp.eq.s16 	%p12, %rs69, 0;
	cvt.s64.s32 	%rd67, %r2;
	mul.lo.s64 	%rd68, %rd67, %rd24;
	add.s64 	%rd38, %rd127, %rd68;
	mul.lo.s64 	%rd69, %rd67, %rd25;
	add.s64 	%rd39, %rd128, %rd69;
	or.pred  	%p14, %p12, %p13;
	@%p14 bra 	$L__BB9_35;

	setp.eq.s64 	%p15, %rd52, 0;
	@%p15 bra 	$L__BB9_17;

	cvta.to.global.u64 	%rd129, %rd52;
	mul.lo.s64 	%rd71, %rd67, %rd22;
	add.s64 	%rd72, %rd129, %rd71;
	ld.global.f64 	%fd51, [%rd72];
	add.f64 	%fd188, %fd51, 0d0000000000000000;
	ld.global.f64 	%fd52, [%rd72+8];
	add.f64 	%fd187, %fd52, 0d0000000000000000;
	ld.global.f64 	%fd53, [%rd72+16];
	add.f64 	%fd186, %fd53, 0d0000000000000000;
	bra.uni 	$L__BB9_19;

$L__BB9_17:
	setp.eq.s64 	%p16, %rd45, 0;
	mov.f64 	%fd186, 0d0000000000000000;
	mov.f64 	%fd187, %fd186;
	mov.f64 	%fd188, %fd186;
	@%p16 bra 	$L__BB9_19;

	cvta.to.global.u64 	%rd130, %rd45;
	mul.lo.s64 	%rd74, %rd67, %rd21;
	add.s64 	%rd75, %rd130, %rd74;
	ld.global.f64 	%fd57, [%rd75];
	add.f64 	%fd188, %fd57, 0d0000000000000000;
	ld.global.f64 	%fd58, [%rd75+8];
	add.f64 	%fd187, %fd58, 0d0000000000000000;
	ld.global.f64 	%fd59, [%rd75+16];
	add.f64 	%fd186, %fd59, 0d0000000000000000;

$L__BB9_19:
	setp.eq.s64 	%p17, %rd56, 0;
	@%p17 bra 	$L__BB9_21;

	ld.global.f64 	%fd60, [%rd38];
	add.f64 	%fd191, %fd60, 0d0000000000000000;
	ld.global.f64 	%fd61, [%rd38+8];
	add.f64 	%fd190, %fd61, 0d0000000000000000;
	ld.global.f64 	%fd62, [%rd38+16];
	add.f64 	%fd189, %fd62, 0d0000000000000000;
	bra.uni 	$L__BB9_23;

$L__BB9_21:
	setp.eq.s64 	%p18, %rd49, 0;
	mov.f64 	%fd189, 0d0000000000000000;
	mov.f64 	%fd190, %fd189;
	mov.f64 	%fd191, %fd189;
	@%p18 bra 	$L__BB9_23;

	ld.global.f64 	%fd66, [%rd39];
	add.f64 	%fd191, %fd66, 0d0000000000000000;
	ld.global.f64 	%fd67, [%rd39+8];
	add.f64 	%fd190, %fd67, 0d0000000000000000;
	ld.global.f64 	%fd68, [%rd39+16];
	add.f64 	%fd189, %fd68, 0d0000000000000000;

$L__BB9_23:
	setp.eq.s64 	%p33, %rd52, 0;
	div.rn.f64 	%fd69, %fd191, %fd49;
	add.f64 	%fd70, %fd69, 0d0000000000000000;
	mov.f64 	%fd71, 0d0000000000000000;
	div.rn.f64 	%fd72, %fd190, %fd49;
	add.f64 	%fd73, %fd72, 0d0000000000000000;
	div.rn.f64 	%fd74, %fd189, %fd49;
	add.f64 	%fd75, %fd74, 0d0000000000000000;
	add.f64 	%fd22, %fd70, %fd188;
	add.f64 	%fd23, %fd73, %fd187;
	add.f64 	%fd24, %fd75, %fd186;
	sub.f64 	%fd25, %fd71, %fd70;
	sub.f64 	%fd26, %fd71, %fd73;
	sub.f64 	%fd27, %fd71, %fd75;
	@%p33 bra 	$L__BB9_25;

	mul.lo.s64 	%rd80, %rd67, %rd22;
	add.s64 	%rd76, %rd52, %rd80;
	// begin inline asm
	{ atom.add.f64 %fd76,[%rd76],%fd25; }

	// end inline asm
	add.s64 	%rd77, %rd76, 8;
	// begin inline asm
	{ atom.add.f64 %fd78,[%rd77],%fd26; }

	// end inline asm
	add.s64 	%rd78, %rd76, 16;
	// begin inline asm
	{ atom.add.f64 %fd80,[%rd78],%fd27; }

	// end inline asm
	bra.uni 	$L__BB9_27;

$L__BB9_25:
	setp.eq.s64 	%p20, %rd45, 0;
	@%p20 bra 	$L__BB9_27;

	mul.lo.s64 	%rd85, %rd67, %rd21;
	add.s64 	%rd81, %rd45, %rd85;
	// begin inline asm
	{ atom.add.f64 %fd82,[%rd81],%fd25; }

	// end inline asm
	add.s64 	%rd82, %rd81, 8;
	// begin inline asm
	{ atom.add.f64 %fd84,[%rd82],%fd26; }

	// end inline asm
	add.s64 	%rd83, %rd81, 16;
	// begin inline asm
	{ atom.add.f64 %fd86,[%rd83],%fd27; }

	// end inline asm

$L__BB9_27:
	setp.eq.s64 	%p21, %rd50, 0;
	div.rn.f64 	%fd88, %fd22, %fd49;
	mov.f64 	%fd89, 0d0000000000000000;
	div.rn.f64 	%fd90, %fd23, %fd49;
	div.rn.f64 	%fd91, %fd24, %fd49;
	add.f64 	%fd30, %fd91, 0d0000000000000000;
	sub.f64 	%fd33, %fd89, %fd30;
	@%p21 bra 	$L__BB9_29;

	mov.f64 	%fd157, 0d0000000000000000;
	add.f64 	%fd156, %fd90, 0d0000000000000000;
	sub.f64 	%fd155, %fd157, %fd156;
	add.f64 	%fd154, %fd88, 0d0000000000000000;
	sub.f64 	%fd153, %fd157, %fd154;
	mul.lo.s64 	%rd90, %rd67, %rd26;
	add.s64 	%rd86, %rd50, %rd90;
	// begin inline asm
	{ atom.add.f64 %fd92,[%rd86],%fd153; }

	// end inline asm
	add.s64 	%rd87, %rd86, 8;
	// begin inline asm
	{ atom.add.f64 %fd94,[%rd87],%fd155; }

	// end inline asm
	add.s64 	%rd88, %rd86, 16;
	// begin inline asm
	{ atom.add.f64 %fd96,[%rd88],%fd33; }

	// end inline asm
	bra.uni 	$L__BB9_31;

$L__BB9_29:
	setp.eq.s64 	%p22, %rd43, 0;
	@%p22 bra 	$L__BB9_31;

	mov.f64 	%fd168, 0d0000000000000000;
	add.f64 	%fd167, %fd90, 0d0000000000000000;
	sub.f64 	%fd166, %fd168, %fd167;
	add.f64 	%fd165, %fd88, 0d0000000000000000;
	sub.f64 	%fd164, %fd168, %fd165;
	mul.lo.s64 	%rd95, %rd67, %rd20;
	add.s64 	%rd91, %rd43, %rd95;
	// begin inline asm
	{ atom.add.f64 %fd98,[%rd91],%fd164; }

	// end inline asm
	add.s64 	%rd92, %rd91, 8;
	// begin inline asm
	{ atom.add.f64 %fd100,[%rd92],%fd166; }

	// end inline asm
	add.s64 	%rd93, %rd91, 16;
	// begin inline asm
	{ atom.add.f64 %fd102,[%rd93],%fd33; }

	// end inline asm

$L__BB9_31:
	setp.eq.s64 	%p23, %rd54, 0;
	@%p23 bra 	$L__BB9_33;

	add.f64 	%fd160, %fd91, 0d0000000000000000;
	add.f64 	%fd159, %fd90, 0d0000000000000000;
	add.f64 	%fd158, %fd88, 0d0000000000000000;
	mul.lo.s64 	%rd100, %rd67, %rd27;
	add.s64 	%rd96, %rd54, %rd100;
	// begin inline asm
	{ atom.add.f64 %fd104,[%rd96],%fd158; }

	// end inline asm
	add.s64 	%rd97, %rd96, 8;
	// begin inline asm
	{ atom.add.f64 %fd106,[%rd97],%fd159; }

	// end inline asm
	add.s64 	%rd98, %rd96, 16;
	// begin inline asm
	{ atom.add.f64 %fd108,[%rd98],%fd160; }

	// end inline asm
	bra.uni 	$L__BB9_35;

$L__BB9_33:
	setp.eq.s64 	%p24, %rd47, 0;
	@%p24 bra 	$L__BB9_35;

	add.f64 	%fd163, %fd91, 0d0000000000000000;
	add.f64 	%fd162, %fd90, 0d0000000000000000;
	add.f64 	%fd161, %fd88, 0d0000000000000000;
	mul.lo.s64 	%rd105, %rd67, %rd19;
	add.s64 	%rd101, %rd47, %rd105;
	// begin inline asm
	{ atom.add.f64 %fd110,[%rd101],%fd161; }

	// end inline asm
	add.s64 	%rd102, %rd101, 8;
	// begin inline asm
	{ atom.add.f64 %fd112,[%rd102],%fd162; }

	// end inline asm
	add.s64 	%rd103, %rd101, 16;
	// begin inline asm
	{ atom.add.f64 %fd114,[%rd103],%fd163; }

	// end inline asm

$L__BB9_35:
	@%p1 bra 	$L__BB9_48;

	setp.eq.s64 	%p26, %rd56, 0;
	@%p26 bra 	$L__BB9_38;

	ld.global.f64 	%fd116, [%rd38];
	add.f64 	%fd194, %fd116, 0d0000000000000000;
	ld.global.f64 	%fd117, [%rd38+8];
	add.f64 	%fd193, %fd117, 0d0000000000000000;
	ld.global.f64 	%fd118, [%rd38+16];
	add.f64 	%fd192, %fd118, 0d0000000000000000;
	bra.uni 	$L__BB9_40;

$L__BB9_38:
	setp.eq.s64 	%p27, %rd49, 0;
	mov.f64 	%fd192, 0d0000000000000000;
	mov.f64 	%fd193, %fd192;
	mov.f64 	%fd194, %fd192;
	@%p27 bra 	$L__BB9_40;

	ld.global.f64 	%fd122, [%rd39];
	add.f64 	%fd194, %fd122, 0d0000000000000000;
	ld.global.f64 	%fd123, [%rd39+8];
	add.f64 	%fd193, %fd123, 0d0000000000000000;
	ld.global.f64 	%fd124, [%rd39+16];
	add.f64 	%fd192, %fd124, 0d0000000000000000;

$L__BB9_40:
	div.rn.f64 	%fd125, %fd194, %fd185;
	mov.f64 	%fd126, 0d0000000000000000;
	div.rn.f64 	%fd127, %fd193, %fd185;
	div.rn.f64 	%fd128, %fd192, %fd185;
	add.f64 	%fd45, %fd128, 0d0000000000000000;
	sub.f64 	%fd48, %fd126, %fd45;
	setp.eq.s64 	%p28, %rd50, 0;
	@%p28 bra 	$L__BB9_42;

	mov.f64 	%fd173, 0d0000000000000000;
	add.f64 	%fd172, %fd127, 0d0000000000000000;
	sub.f64 	%fd171, %fd173, %fd172;
	add.f64 	%fd170, %fd125, 0d0000000000000000;
	sub.f64 	%fd169, %fd173, %fd170;
	mul.lo.s64 	%rd110, %rd67, %rd26;
	add.s64 	%rd106, %rd50, %rd110;
	// begin inline asm
	{ atom.add.f64 %fd129,[%rd106],%fd169; }

	// end inline asm
	add.s64 	%rd107, %rd106, 8;
	// begin inline asm
	{ atom.add.f64 %fd131,[%rd107],%fd171; }

	// end inline asm
	add.s64 	%rd108, %rd106, 16;
	// begin inline asm
	{ atom.add.f64 %fd133,[%rd108],%fd48; }

	// end inline asm
	bra.uni 	$L__BB9_44;

$L__BB9_42:
	setp.eq.s64 	%p29, %rd43, 0;
	@%p29 bra 	$L__BB9_44;

	mov.f64 	%fd184, 0d0000000000000000;
	add.f64 	%fd183, %fd127, 0d0000000000000000;
	sub.f64 	%fd182, %fd184, %fd183;
	add.f64 	%fd181, %fd125, 0d0000000000000000;
	sub.f64 	%fd180, %fd184, %fd181;
	mul.lo.s64 	%rd115, %rd67, %rd20;
	add.s64 	%rd111, %rd43, %rd115;
	// begin inline asm
	{ atom.add.f64 %fd135,[%rd111],%fd180; }

	// end inline asm
	add.s64 	%rd112, %rd111, 8;
	// begin inline asm
	{ atom.add.f64 %fd137,[%rd112],%fd182; }

	// end inline asm
	add.s64 	%rd113, %rd111, 16;
	// begin inline asm
	{ atom.add.f64 %fd139,[%rd113],%fd48; }

	// end inline asm

$L__BB9_44:
	setp.eq.s64 	%p30, %rd54, 0;
	@%p30 bra 	$L__BB9_46;

	add.f64 	%fd176, %fd128, 0d0000000000000000;
	add.f64 	%fd175, %fd127, 0d0000000000000000;
	add.f64 	%fd174, %fd125, 0d0000000000000000;
	mul.lo.s64 	%rd120, %rd67, %rd27;
	add.s64 	%rd116, %rd54, %rd120;
	// begin inline asm
	{ atom.add.f64 %fd141,[%rd116],%fd174; }

	// end inline asm
	add.s64 	%rd117, %rd116, 8;
	// begin inline asm
	{ atom.add.f64 %fd143,[%rd117],%fd175; }

	// end inline asm
	add.s64 	%rd118, %rd116, 16;
	// begin inline asm
	{ atom.add.f64 %fd145,[%rd118],%fd176; }

	// end inline asm
	bra.uni 	$L__BB9_48;

$L__BB9_46:
	setp.eq.s64 	%p31, %rd47, 0;
	@%p31 bra 	$L__BB9_48;

	add.f64 	%fd179, %fd128, 0d0000000000000000;
	add.f64 	%fd178, %fd127, 0d0000000000000000;
	add.f64 	%fd177, %fd125, 0d0000000000000000;
	mul.lo.s64 	%rd125, %rd67, %rd19;
	add.s64 	%rd121, %rd47, %rd125;
	// begin inline asm
	{ atom.add.f64 %fd147,[%rd121],%fd177; }

	// end inline asm
	add.s64 	%rd122, %rd121, 8;
	// begin inline asm
	{ atom.add.f64 %fd149,[%rd122],%fd178; }

	// end inline asm
	add.s64 	%rd123, %rd121, 16;
	// begin inline asm
	{ atom.add.f64 %fd151,[%rd123],%fd179; }

	// end inline asm

$L__BB9_48:
	ld.param.u64 	%rd126, [advection_x_cuda_kernel_backward_param_0+24];
	add.s64 	%rd131, %rd131, %rd23;
	setp.lt.u64 	%p32, %rd131, %rd126;
	@%p32 bra 	$L__BB9_2;

$L__BB9_49:
	ret;

}
	// .globl	sys_to_x_affine_cuda_kernel_forward
.visible .entry sys_to_x_affine_cuda_kernel_forward(
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_forward_param_4[56]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<108>;
	.reg .f64 	%fd<63>;
	.reg .b64 	%rd<104>;


	ld.param.v2.u32 	{%r43, %r44}, [sys_to_x_affine_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r45, %r46}, [sys_to_x_affine_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r51, %r52}, [sys_to_x_affine_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r59, %r60}, [sys_to_x_affine_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r67, %r68}, [sys_to_x_affine_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r75, %r76}, [sys_to_x_affine_cuda_kernel_forward_param_4+32];
	ld.param.u64 	%rd42, [sys_to_x_affine_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd40, [sys_to_x_affine_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd38, [sys_to_x_affine_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd36, [sys_to_x_affine_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd35, [sys_to_x_affine_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [sys_to_x_affine_cuda_kernel_forward_param_0+16];
	mov.u32 	%r79, %ntid.x;
	cvt.u64.u32 	%rd1, %r79;
	mov.u32 	%r80, %ctaid.x;
	mul.wide.u32 	%rd44, %r79, %r80;
	mov.u32 	%r81, %tid.x;
	cvt.u64.u32 	%rd45, %r81;
	add.s64 	%rd97, %rd44, %rd45;
	setp.ge.u64 	%p1, %rd97, %rd35;
	@%p1 bra 	$L__BB10_25;

	cvta.to.global.u64 	%rd4, %rd42;
	cvta.to.global.u64 	%rd5, %rd40;
	cvta.to.global.u64 	%rd6, %rd38;
	cvta.to.global.u64 	%rd7, %rd36;
	cvt.s64.s32 	%rd8, %r46;
	cvt.s64.s32 	%rd9, %r45;
	cvt.s64.s32 	%rd10, %r44;
	cvt.s64.s32 	%rd11, %r75;
	cvt.s64.s32 	%rd12, %r51;
	cvt.s64.s32 	%rd13, %r67;
	cvt.s64.s32 	%rd14, %r59;
	mov.u32 	%r82, %nctaid.x;
	cvt.u64.u32 	%rd46, %r82;
	mul.lo.s64 	%rd15, %rd1, %rd46;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB10_12;
	bra.uni 	$L__BB10_2;

$L__BB10_12:
	cvt.u32.u64 	%r94, %rd8;
	cvt.u32.u64 	%r97, %rd9;
	cvt.u32.u64 	%r100, %rd10;

$L__BB10_13:
	or.b64  	%rd71, %rd97, %rd8;
	and.b64  	%rd72, %rd71, -4294967296;
	setp.eq.s64 	%p9, %rd72, 0;
	@%p9 bra 	$L__BB10_15;

	div.u64 	%rd102, %rd97, %rd8;
	bra.uni 	$L__BB10_16;

$L__BB10_15:
	cvt.u32.u64 	%r95, %rd97;
	div.u32 	%r96, %r95, %r94;
	cvt.u64.u32 	%rd102, %r96;

$L__BB10_16:
	setp.lt.s32 	%p10, %r6, 3;
	@%p10 bra 	$L__BB10_20;

	or.b64  	%rd73, %rd102, %rd9;
	and.b64  	%rd74, %rd73, -4294967296;
	setp.eq.s64 	%p11, %rd74, 0;
	@%p11 bra 	$L__BB10_19;

	div.u64 	%rd102, %rd102, %rd9;
	bra.uni 	$L__BB10_20;

$L__BB10_19:
	cvt.u32.u64 	%r98, %rd102;
	div.u32 	%r99, %r98, %r97;
	cvt.u64.u32 	%rd102, %r99;

$L__BB10_20:
	setp.lt.s32 	%p12, %r6, 2;
	@%p12 bra 	$L__BB10_24;

	or.b64  	%rd75, %rd102, %rd10;
	and.b64  	%rd76, %rd75, -4294967296;
	setp.eq.s64 	%p13, %rd76, 0;
	@%p13 bra 	$L__BB10_23;

	div.u64 	%rd102, %rd102, %rd10;
	bra.uni 	$L__BB10_24;

$L__BB10_23:
	cvt.u32.u64 	%r101, %rd102;
	div.u32 	%r102, %r101, %r100;
	cvt.u64.u32 	%rd102, %r102;

$L__BB10_24:
	cvt.s64.s32 	%rd77, %rd102;
	setp.gt.s32 	%p14, %r6, 0;
	selp.b64 	%rd78, %rd77, 0, %p14;
	mul.lo.s64 	%rd79, %rd78, %rd11;
	add.s64 	%rd80, %rd4, %rd79;
	ld.global.u32 	%r103, [%rd80];
	shl.b32 	%r104, %r103, 2;
	cvt.s64.s32 	%rd81, %r104;
	mul.lo.s64 	%rd82, %rd81, %rd12;
	add.s64 	%rd83, %rd7, %rd82;
	or.b32  	%r105, %r104, 1;
	cvt.s64.s32 	%rd84, %r105;
	mul.lo.s64 	%rd85, %rd84, %rd12;
	add.s64 	%rd86, %rd7, %rd85;
	or.b32  	%r106, %r104, 2;
	cvt.s64.s32 	%rd87, %r106;
	mul.lo.s64 	%rd88, %rd87, %rd12;
	add.s64 	%rd89, %rd7, %rd88;
	or.b32  	%r107, %r104, 3;
	cvt.s64.s32 	%rd90, %r107;
	mul.lo.s64 	%rd91, %rd90, %rd12;
	add.s64 	%rd92, %rd7, %rd91;
	mul.lo.s64 	%rd93, %rd78, %rd13;
	add.s64 	%rd94, %rd5, %rd93;
	ld.global.f64 	%fd32, [%rd94];
	mov.f64 	%fd33, 0d3FF0000000000000;
	sub.f64 	%fd34, %fd33, %fd32;
	ld.global.f64 	%fd35, [%rd94+8];
	sub.f64 	%fd36, %fd34, %fd35;
	ld.global.f64 	%fd37, [%rd94+16];
	sub.f64 	%fd38, %fd36, %fd37;
	ld.global.f64 	%fd39, [%rd83];
	mul.f64 	%fd40, %fd39, %fd38;
	ld.global.f64 	%fd41, [%rd83+8];
	mul.f64 	%fd42, %fd41, %fd38;
	ld.global.f64 	%fd43, [%rd83+16];
	mul.f64 	%fd44, %fd43, %fd38;
	ld.global.f64 	%fd45, [%rd86];
	ld.global.f64 	%fd46, [%rd86+8];
	ld.global.f64 	%fd47, [%rd86+16];
	fma.rn.f64 	%fd48, %fd45, %fd32, %fd40;
	fma.rn.f64 	%fd49, %fd46, %fd32, %fd42;
	fma.rn.f64 	%fd50, %fd47, %fd32, %fd44;
	ld.global.f64 	%fd51, [%rd89];
	ld.global.f64 	%fd52, [%rd89+8];
	ld.global.f64 	%fd53, [%rd89+16];
	fma.rn.f64 	%fd54, %fd51, %fd35, %fd48;
	fma.rn.f64 	%fd55, %fd52, %fd35, %fd49;
	fma.rn.f64 	%fd56, %fd53, %fd35, %fd50;
	ld.global.f64 	%fd57, [%rd92];
	ld.global.f64 	%fd58, [%rd92+8];
	ld.global.f64 	%fd59, [%rd92+16];
	fma.rn.f64 	%fd60, %fd57, %fd37, %fd54;
	fma.rn.f64 	%fd61, %fd58, %fd37, %fd55;
	fma.rn.f64 	%fd62, %fd59, %fd37, %fd56;
	mul.lo.s64 	%rd95, %rd78, %rd14;
	add.s64 	%rd96, %rd6, %rd95;
	st.global.f64 	[%rd96], %fd60;
	st.global.f64 	[%rd96+8], %fd61;
	st.global.f64 	[%rd96+16], %fd62;
	add.s64 	%rd97, %rd97, %rd15;
	setp.lt.u64 	%p15, %rd97, %rd35;
	@%p15 bra 	$L__BB10_13;
	bra.uni 	$L__BB10_25;

$L__BB10_2:
	cvt.u32.u64 	%r83, %rd9;
	cvt.u32.u64 	%r86, %rd10;

$L__BB10_3:
	setp.lt.s32 	%p3, %r6, 3;
	mov.u64 	%rd98, %rd97;
	@%p3 bra 	$L__BB10_7;

	or.b64  	%rd47, %rd97, %rd9;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p4, %rd48, 0;
	@%p4 bra 	$L__BB10_6;

	div.u64 	%rd98, %rd97, %rd9;
	bra.uni 	$L__BB10_7;

$L__BB10_6:
	cvt.u32.u64 	%r84, %rd97;
	div.u32 	%r85, %r84, %r83;
	cvt.u64.u32 	%rd98, %r85;

$L__BB10_7:
	setp.lt.s32 	%p5, %r6, 2;
	@%p5 bra 	$L__BB10_11;

	or.b64  	%rd49, %rd98, %rd10;
	and.b64  	%rd50, %rd49, -4294967296;
	setp.eq.s64 	%p6, %rd50, 0;
	@%p6 bra 	$L__BB10_10;

	div.u64 	%rd98, %rd98, %rd10;
	bra.uni 	$L__BB10_11;

$L__BB10_10:
	cvt.u32.u64 	%r87, %rd98;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd98, %r88;

$L__BB10_11:
	cvt.s64.s32 	%rd51, %rd98;
	setp.gt.s32 	%p7, %r6, 0;
	selp.b64 	%rd52, %rd51, 0, %p7;
	mul.lo.s64 	%rd53, %rd52, %rd11;
	add.s64 	%rd54, %rd4, %rd53;
	ld.global.u32 	%r89, [%rd54];
	shl.b32 	%r90, %r89, 2;
	cvt.s64.s32 	%rd55, %r90;
	mul.lo.s64 	%rd56, %rd55, %rd12;
	add.s64 	%rd57, %rd7, %rd56;
	or.b32  	%r91, %r90, 1;
	cvt.s64.s32 	%rd58, %r91;
	mul.lo.s64 	%rd59, %rd58, %rd12;
	add.s64 	%rd60, %rd7, %rd59;
	or.b32  	%r92, %r90, 2;
	cvt.s64.s32 	%rd61, %r92;
	mul.lo.s64 	%rd62, %rd61, %rd12;
	add.s64 	%rd63, %rd7, %rd62;
	or.b32  	%r93, %r90, 3;
	cvt.s64.s32 	%rd64, %r93;
	mul.lo.s64 	%rd65, %rd64, %rd12;
	add.s64 	%rd66, %rd7, %rd65;
	mul.lo.s64 	%rd67, %rd52, %rd13;
	add.s64 	%rd68, %rd5, %rd67;
	ld.global.f64 	%fd1, [%rd68];
	mov.f64 	%fd2, 0d3FF0000000000000;
	sub.f64 	%fd3, %fd2, %fd1;
	ld.global.f64 	%fd4, [%rd68+8];
	sub.f64 	%fd5, %fd3, %fd4;
	ld.global.f64 	%fd6, [%rd68+16];
	sub.f64 	%fd7, %fd5, %fd6;
	ld.global.f64 	%fd8, [%rd57];
	mul.f64 	%fd9, %fd8, %fd7;
	ld.global.f64 	%fd10, [%rd57+8];
	mul.f64 	%fd11, %fd10, %fd7;
	ld.global.f64 	%fd12, [%rd57+16];
	mul.f64 	%fd13, %fd12, %fd7;
	ld.global.f64 	%fd14, [%rd60];
	ld.global.f64 	%fd15, [%rd60+8];
	ld.global.f64 	%fd16, [%rd60+16];
	fma.rn.f64 	%fd17, %fd14, %fd1, %fd9;
	fma.rn.f64 	%fd18, %fd15, %fd1, %fd11;
	fma.rn.f64 	%fd19, %fd16, %fd1, %fd13;
	ld.global.f64 	%fd20, [%rd63];
	ld.global.f64 	%fd21, [%rd63+8];
	ld.global.f64 	%fd22, [%rd63+16];
	fma.rn.f64 	%fd23, %fd20, %fd4, %fd17;
	fma.rn.f64 	%fd24, %fd21, %fd4, %fd18;
	fma.rn.f64 	%fd25, %fd22, %fd4, %fd19;
	ld.global.f64 	%fd26, [%rd66];
	ld.global.f64 	%fd27, [%rd66+8];
	ld.global.f64 	%fd28, [%rd66+16];
	fma.rn.f64 	%fd29, %fd26, %fd6, %fd23;
	fma.rn.f64 	%fd30, %fd27, %fd6, %fd24;
	fma.rn.f64 	%fd31, %fd28, %fd6, %fd25;
	mul.lo.s64 	%rd69, %rd52, %rd14;
	add.s64 	%rd70, %rd6, %rd69;
	st.global.f64 	[%rd70], %fd29;
	st.global.f64 	[%rd70+8], %fd30;
	st.global.f64 	[%rd70+16], %fd31;
	add.s64 	%rd97, %rd97, %rd15;
	setp.lt.u64 	%p8, %rd97, %rd35;
	@%p8 bra 	$L__BB10_3;

$L__BB10_25:
	ret;

}
	// .globl	sys_to_x_affine_cuda_kernel_backward
.visible .entry sys_to_x_affine_cuda_kernel_backward(
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_8[56]
)
{
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<57>;
	.reg .b32 	%r<145>;
	.reg .f64 	%fd<140>;
	.reg .b64 	%rd<128>;


	ld.param.v2.u32 	{%r70, %r71}, [sys_to_x_affine_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r72, %r73}, [sys_to_x_affine_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r78, %r79}, [sys_to_x_affine_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r86, %r87}, [sys_to_x_affine_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r94, %r95}, [sys_to_x_affine_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r102, %r103}, [sys_to_x_affine_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r110, %r111}, [sys_to_x_affine_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r118, %r119}, [sys_to_x_affine_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r126, %r127}, [sys_to_x_affine_cuda_kernel_backward_param_7+32];
	ld.param.u64 	%rd57, [sys_to_x_affine_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd55, [sys_to_x_affine_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd53, [sys_to_x_affine_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd51, [sys_to_x_affine_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd50, [sys_to_x_affine_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd49, [sys_to_x_affine_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd48, [sys_to_x_affine_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd46, [sys_to_x_affine_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd45, [sys_to_x_affine_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd44, [sys_to_x_affine_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [sys_to_x_affine_cuda_kernel_backward_param_0+16];
	mov.u32 	%r130, %ntid.x;
	cvt.u64.u32 	%rd1, %r130;
	mov.u32 	%r131, %ctaid.x;
	mul.wide.u32 	%rd59, %r130, %r131;
	mov.u32 	%r132, %tid.x;
	cvt.u64.u32 	%rd60, %r132;
	add.s64 	%rd124, %rd59, %rd60;
	setp.ge.u64 	%p1, %rd124, %rd44;
	@%p1 bra 	$L__BB11_39;

	cvta.to.global.u64 	%rd10, %rd55;
	cvta.to.global.u64 	%rd11, %rd51;
	cvta.to.global.u64 	%rd12, %rd49;
	cvta.to.global.u64 	%rd13, %rd48;
	cvta.to.global.u64 	%rd14, %rd45;
	cvt.s64.s32 	%rd15, %r73;
	cvt.s64.s32 	%rd16, %r72;
	cvt.s64.s32 	%rd17, %r71;
	cvt.s64.s32 	%rd18, %r102;
	cvt.s64.s32 	%rd19, %r78;
	cvt.s64.s32 	%rd20, %r94;
	cvt.s64.s32 	%rd21, %r118;
	cvt.s64.s32 	%rd22, %r86;
	cvt.s64.s32 	%rd23, %r126;
	cvt.s64.s32 	%rd24, %r110;
	mov.u32 	%r133, %nctaid.x;
	cvt.u64.u32 	%rd61, %r133;
	mul.lo.s64 	%rd25, %rd1, %rd61;

$L__BB11_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd125, %rd124;
	@%p2 bra 	$L__BB11_6;

	or.b64  	%rd62, %rd124, %rd15;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p3, %rd63, 0;
	@%p3 bra 	$L__BB11_5;

	div.u64 	%rd125, %rd124, %rd15;
	bra.uni 	$L__BB11_6;

$L__BB11_5:
	cvt.u32.u64 	%r134, %rd15;
	cvt.u32.u64 	%r135, %rd124;
	div.u32 	%r136, %r135, %r134;
	cvt.u64.u32 	%rd125, %r136;

$L__BB11_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB11_10;

	or.b64  	%rd64, %rd125, %rd16;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p5, %rd65, 0;
	@%p5 bra 	$L__BB11_9;

	div.u64 	%rd125, %rd125, %rd16;
	bra.uni 	$L__BB11_10;

$L__BB11_9:
	cvt.u32.u64 	%r137, %rd16;
	cvt.u32.u64 	%r138, %rd125;
	div.u32 	%r139, %r138, %r137;
	cvt.u64.u32 	%rd125, %r139;

$L__BB11_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB11_14;

	or.b64  	%rd66, %rd125, %rd17;
	and.b64  	%rd67, %rd66, -4294967296;
	setp.eq.s64 	%p7, %rd67, 0;
	@%p7 bra 	$L__BB11_13;

	div.u64 	%rd125, %rd125, %rd17;
	bra.uni 	$L__BB11_14;

$L__BB11_13:
	cvt.u32.u64 	%r140, %rd17;
	cvt.u32.u64 	%r141, %rd125;
	div.u32 	%r142, %r141, %r140;
	cvt.u64.u32 	%rd125, %r142;

$L__BB11_14:
	ld.param.u64 	%rd122, [sys_to_x_affine_cuda_kernel_backward_param_6];
	cvt.s64.s32 	%rd68, %rd125;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd36, %rd68, 0, %p8;
	mul.lo.s64 	%rd69, %rd36, %rd18;
	add.s64 	%rd70, %rd11, %rd69;
	ld.global.u32 	%r143, [%rd70];
	shl.b32 	%r144, %r143, 2;
	cvt.s64.s32 	%rd37, %r144;
	mul.lo.s64 	%rd38, %rd37, %rd19;
	add.s64 	%rd71, %rd14, %rd38;
	ld.global.f64 	%fd1, [%rd71];
	ld.global.f64 	%fd2, [%rd71+8];
	ld.global.f64 	%fd3, [%rd71+16];
	or.b64  	%rd72, %rd37, 1;
	mul.lo.s64 	%rd39, %rd72, %rd19;
	add.s64 	%rd73, %rd14, %rd39;
	ld.global.f64 	%fd4, [%rd73];
	ld.global.f64 	%fd5, [%rd73+8];
	ld.global.f64 	%fd6, [%rd73+16];
	or.b64  	%rd74, %rd37, 2;
	mul.lo.s64 	%rd40, %rd74, %rd19;
	add.s64 	%rd75, %rd14, %rd40;
	ld.global.f64 	%fd7, [%rd75];
	ld.global.f64 	%fd8, [%rd75+8];
	ld.global.f64 	%fd9, [%rd75+16];
	or.b64  	%rd76, %rd37, 3;
	mul.lo.s64 	%rd41, %rd76, %rd19;
	add.s64 	%rd77, %rd14, %rd41;
	ld.global.f64 	%fd10, [%rd77];
	ld.global.f64 	%fd11, [%rd77+8];
	ld.global.f64 	%fd12, [%rd77+16];
	mul.lo.s64 	%rd42, %rd36, %rd20;
	add.s64 	%rd78, %rd12, %rd42;
	ld.global.f64 	%fd13, [%rd78];
	ld.global.f64 	%fd14, [%rd78+8];
	ld.global.f64 	%fd15, [%rd78+16];
	setp.eq.s64 	%p9, %rd122, 0;
	@%p9 bra 	$L__BB11_16;

	mul.lo.s64 	%rd79, %rd36, %rd21;
	add.s64 	%rd80, %rd10, %rd79;
	ld.global.f64 	%fd40, [%rd80];
	add.f64 	%fd139, %fd40, 0d0000000000000000;
	ld.global.f64 	%fd41, [%rd80+8];
	add.f64 	%fd138, %fd41, 0d0000000000000000;
	ld.global.f64 	%fd42, [%rd80+16];
	add.f64 	%fd137, %fd42, 0d0000000000000000;
	bra.uni 	$L__BB11_18;

$L__BB11_16:
	ld.param.u64 	%rd123, [sys_to_x_affine_cuda_kernel_backward_param_2+8];
	setp.eq.s64 	%p10, %rd123, 0;
	mov.f64 	%fd137, 0d0000000000000000;
	mov.f64 	%fd138, %fd137;
	mov.f64 	%fd139, %fd137;
	@%p10 bra 	$L__BB11_18;

	mul.lo.s64 	%rd81, %rd36, %rd22;
	add.s64 	%rd82, %rd13, %rd81;
	ld.global.f64 	%fd46, [%rd82];
	add.f64 	%fd139, %fd46, 0d0000000000000000;
	ld.global.f64 	%fd47, [%rd82+8];
	add.f64 	%fd138, %fd47, 0d0000000000000000;
	ld.global.f64 	%fd48, [%rd82+16];
	add.f64 	%fd137, %fd48, 0d0000000000000000;

$L__BB11_18:
	mov.f64 	%fd49, 0d3FF0000000000000;
	sub.f64 	%fd50, %fd49, %fd13;
	sub.f64 	%fd51, %fd50, %fd14;
	sub.f64 	%fd52, %fd51, %fd15;
	add.f64 	%fd53, %fd139, 0d0000000000000000;
	mov.f64 	%fd54, 0d0000000000000000;
	fma.rn.f64 	%fd25, %fd15, %fd53, 0d0000000000000000;
	add.f64 	%fd55, %fd138, 0d0000000000000000;
	fma.rn.f64 	%fd26, %fd15, %fd55, 0d0000000000000000;
	add.f64 	%fd56, %fd137, 0d0000000000000000;
	fma.rn.f64 	%fd27, %fd15, %fd56, 0d0000000000000000;
	mul.f64 	%fd57, %fd11, %fd55;
	fma.rn.f64 	%fd58, %fd10, %fd53, %fd57;
	fma.rn.f64 	%fd59, %fd12, %fd56, %fd58;
	fma.rn.f64 	%fd28, %fd14, %fd53, 0d0000000000000000;
	fma.rn.f64 	%fd29, %fd14, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd30, %fd14, %fd56, 0d0000000000000000;
	mul.f64 	%fd60, %fd8, %fd55;
	fma.rn.f64 	%fd61, %fd7, %fd53, %fd60;
	fma.rn.f64 	%fd62, %fd9, %fd56, %fd61;
	fma.rn.f64 	%fd31, %fd13, %fd53, 0d0000000000000000;
	fma.rn.f64 	%fd32, %fd13, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd33, %fd13, %fd56, 0d0000000000000000;
	mul.f64 	%fd63, %fd5, %fd55;
	fma.rn.f64 	%fd64, %fd4, %fd53, %fd63;
	fma.rn.f64 	%fd65, %fd6, %fd56, %fd64;
	add.f64 	%fd66, %fd59, 0d0000000000000000;
	fma.rn.f64 	%fd34, %fd52, %fd53, 0d0000000000000000;
	fma.rn.f64 	%fd35, %fd52, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd36, %fd52, %fd56, 0d0000000000000000;
	add.f64 	%fd67, %fd62, 0d0000000000000000;
	add.f64 	%fd68, %fd65, 0d0000000000000000;
	mul.f64 	%fd69, %fd2, %fd55;
	fma.rn.f64 	%fd70, %fd1, %fd53, %fd69;
	fma.rn.f64 	%fd71, %fd3, %fd56, %fd70;
	add.f64 	%fd72, %fd71, 0d0000000000000000;
	sub.f64 	%fd73, %fd54, %fd72;
	add.f64 	%fd74, %fd66, %fd73;
	add.f64 	%fd75, %fd67, %fd73;
	add.f64 	%fd76, %fd68, %fd73;
	add.f64 	%fd37, %fd76, 0d0000000000000000;
	add.f64 	%fd38, %fd75, 0d0000000000000000;
	add.f64 	%fd39, %fd74, 0d0000000000000000;
	setp.eq.s64 	%p11, %rd57, 0;
	@%p11 bra 	$L__BB11_20;

	mul.lo.s64 	%rd86, %rd36, %rd23;
	add.s64 	%rd83, %rd57, %rd86;
	// begin inline asm
	{ atom.add.f64 %fd77,[%rd83],%fd37; }

	// end inline asm
	add.s64 	%rd84, %rd83, 8;
	// begin inline asm
	{ atom.add.f64 %fd79,[%rd84],%fd38; }

	// end inline asm
	add.s64 	%rd85, %rd83, 16;
	// begin inline asm
	{ atom.add.f64 %fd81,[%rd85],%fd39; }

	// end inline asm
	bra.uni 	$L__BB11_22;

$L__BB11_20:
	setp.eq.s64 	%p12, %rd50, 0;
	@%p12 bra 	$L__BB11_22;

	add.s64 	%rd87, %rd50, %rd42;
	// begin inline asm
	{ atom.add.f64 %fd83,[%rd87],%fd37; }

	// end inline asm
	add.s64 	%rd88, %rd87, 8;
	// begin inline asm
	{ atom.add.f64 %fd85,[%rd88],%fd38; }

	// end inline asm
	add.s64 	%rd89, %rd87, 16;
	// begin inline asm
	{ atom.add.f64 %fd87,[%rd89],%fd39; }

	// end inline asm

$L__BB11_22:
	setp.eq.s64 	%p13, %rd53, 0;
	@%p13 bra 	$L__BB11_24;

	add.s64 	%rd93, %rd37, 3;
	mul.lo.s64 	%rd94, %rd93, %rd24;
	add.s64 	%rd90, %rd53, %rd94;
	// begin inline asm
	{ atom.add.f64 %fd89,[%rd90],%fd25; }

	// end inline asm
	add.s64 	%rd91, %rd90, 8;
	// begin inline asm
	{ atom.add.f64 %fd91,[%rd91],%fd26; }

	// end inline asm
	add.s64 	%rd92, %rd90, 16;
	// begin inline asm
	{ atom.add.f64 %fd93,[%rd92],%fd27; }

	// end inline asm
	bra.uni 	$L__BB11_26;

$L__BB11_24:
	setp.eq.s64 	%p14, %rd46, 0;
	@%p14 bra 	$L__BB11_26;

	add.s64 	%rd95, %rd46, %rd41;
	// begin inline asm
	{ atom.add.f64 %fd95,[%rd95],%fd25; }

	// end inline asm
	add.s64 	%rd96, %rd95, 8;
	// begin inline asm
	{ atom.add.f64 %fd97,[%rd96],%fd26; }

	// end inline asm
	add.s64 	%rd97, %rd95, 16;
	// begin inline asm
	{ atom.add.f64 %fd99,[%rd97],%fd27; }

	// end inline asm

$L__BB11_26:
	@%p13 bra 	$L__BB11_28;

	add.s64 	%rd101, %rd37, 2;
	mul.lo.s64 	%rd102, %rd101, %rd24;
	add.s64 	%rd98, %rd53, %rd102;
	// begin inline asm
	{ atom.add.f64 %fd101,[%rd98],%fd28; }

	// end inline asm
	add.s64 	%rd99, %rd98, 8;
	// begin inline asm
	{ atom.add.f64 %fd103,[%rd99],%fd29; }

	// end inline asm
	add.s64 	%rd100, %rd98, 16;
	// begin inline asm
	{ atom.add.f64 %fd105,[%rd100],%fd30; }

	// end inline asm
	bra.uni 	$L__BB11_30;

$L__BB11_28:
	setp.eq.s64 	%p16, %rd46, 0;
	@%p16 bra 	$L__BB11_30;

	add.s64 	%rd103, %rd46, %rd40;
	// begin inline asm
	{ atom.add.f64 %fd107,[%rd103],%fd28; }

	// end inline asm
	add.s64 	%rd104, %rd103, 8;
	// begin inline asm
	{ atom.add.f64 %fd109,[%rd104],%fd29; }

	// end inline asm
	add.s64 	%rd105, %rd103, 16;
	// begin inline asm
	{ atom.add.f64 %fd111,[%rd105],%fd30; }

	// end inline asm

$L__BB11_30:
	@%p13 bra 	$L__BB11_32;

	add.s64 	%rd109, %rd37, 1;
	mul.lo.s64 	%rd110, %rd109, %rd24;
	add.s64 	%rd106, %rd53, %rd110;
	// begin inline asm
	{ atom.add.f64 %fd113,[%rd106],%fd31; }

	// end inline asm
	add.s64 	%rd107, %rd106, 8;
	// begin inline asm
	{ atom.add.f64 %fd115,[%rd107],%fd32; }

	// end inline asm
	add.s64 	%rd108, %rd106, 16;
	// begin inline asm
	{ atom.add.f64 %fd117,[%rd108],%fd33; }

	// end inline asm
	bra.uni 	$L__BB11_34;

$L__BB11_32:
	setp.eq.s64 	%p18, %rd46, 0;
	@%p18 bra 	$L__BB11_34;

	add.s64 	%rd111, %rd46, %rd39;
	// begin inline asm
	{ atom.add.f64 %fd119,[%rd111],%fd31; }

	// end inline asm
	add.s64 	%rd112, %rd111, 8;
	// begin inline asm
	{ atom.add.f64 %fd121,[%rd112],%fd32; }

	// end inline asm
	add.s64 	%rd113, %rd111, 16;
	// begin inline asm
	{ atom.add.f64 %fd123,[%rd113],%fd33; }

	// end inline asm

$L__BB11_34:
	@%p13 bra 	$L__BB11_36;

	mul.lo.s64 	%rd117, %rd37, %rd24;
	add.s64 	%rd114, %rd53, %rd117;
	// begin inline asm
	{ atom.add.f64 %fd125,[%rd114],%fd34; }

	// end inline asm
	add.s64 	%rd115, %rd114, 8;
	// begin inline asm
	{ atom.add.f64 %fd127,[%rd115],%fd35; }

	// end inline asm
	add.s64 	%rd116, %rd114, 16;
	// begin inline asm
	{ atom.add.f64 %fd129,[%rd116],%fd36; }

	// end inline asm
	bra.uni 	$L__BB11_38;

$L__BB11_36:
	setp.eq.s64 	%p20, %rd46, 0;
	@%p20 bra 	$L__BB11_38;

	add.s64 	%rd118, %rd46, %rd38;
	// begin inline asm
	{ atom.add.f64 %fd131,[%rd118],%fd34; }

	// end inline asm
	add.s64 	%rd119, %rd118, 8;
	// begin inline asm
	{ atom.add.f64 %fd133,[%rd119],%fd35; }

	// end inline asm
	add.s64 	%rd120, %rd118, 16;
	// begin inline asm
	{ atom.add.f64 %fd135,[%rd120],%fd36; }

	// end inline asm

$L__BB11_38:
	ld.param.u64 	%rd121, [sys_to_x_affine_cuda_kernel_backward_param_0+24];
	add.s64 	%rd124, %rd124, %rd25;
	setp.lt.u64 	%p21, %rd124, %rd121;
	@%p21 bra 	$L__BB11_2;

$L__BB11_39:
	ret;

}
	// .globl	init_soft_diag_hess_inds_kernel_cuda_kernel_forward
.visible .entry init_soft_diag_hess_inds_kernel_cuda_kernel_forward(
	.param .align 8 .b8 init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_1[184],
	.param .u32 init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_2
)
{
	.local .align 8 .b8 	__local_depot12[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b32 	%r<57>;
	.reg .b64 	%rd<91>;


	mov.u64 	%SPL, __local_depot12;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r20, %r21}, [init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r22, %r23}, [init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_0+8];
	mov.b64 	%rd38, init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_1;
	ld.param.u32 	%r19, [init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd37, [init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r18, [init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_0+16];
	mov.u32 	%r24, %ntid.x;
	cvt.u64.u32 	%rd1, %r24;
	mov.u32 	%r25, %ctaid.x;
	mul.wide.u32 	%rd39, %r24, %r25;
	mov.u32 	%r26, %tid.x;
	cvt.u64.u32 	%rd40, %r26;
	add.s64 	%rd82, %rd39, %rd40;
	setp.ge.u64 	%p1, %rd82, %rd37;
	@%p1 bra 	$L__BB12_40;

	cvt.s64.s32 	%rd4, %r23;
	cvt.s64.s32 	%rd5, %r22;
	cvt.s64.s32 	%rd6, %r21;
	shl.b32 	%r2, %r19, 2;
	mov.u64 	%rd41, %rd38;
	ld.param.v2.u32 	{%r27, %r28}, [%rd41+176];
	ld.param.u32 	%r7, [%rd41+172];
	mov.u32 	%r29, %nctaid.x;
	cvt.u64.u32 	%rd42, %r29;
	mul.lo.s64 	%rd7, %rd1, %rd42;
	ld.param.u64 	%rd43, [%rd41];
	cvta.to.global.u64 	%rd8, %rd43;
	ld.param.s32 	%rd9, [%rd41+32];
	ld.param.u64 	%rd44, [%rd41+56];
	cvta.to.global.u64 	%rd10, %rd44;
	ld.param.s32 	%rd11, [%rd41+88];
	add.u64 	%rd45, %SP, 0;
	add.u64 	%rd12, %SPL, 0;
	setp.gt.s32 	%p2, %r18, 3;
	@%p2 bra 	$L__BB12_25;
	bra.uni 	$L__BB12_2;

$L__BB12_25:
	or.b64  	%rd68, %rd82, %rd4;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.eq.s64 	%p27, %rd69, 0;
	@%p27 bra 	$L__BB12_27;

	div.u64 	%rd89, %rd82, %rd4;
	bra.uni 	$L__BB12_28;

$L__BB12_27:
	cvt.u32.u64 	%r45, %rd4;
	cvt.u32.u64 	%r46, %rd82;
	div.u32 	%r47, %r46, %r45;
	cvt.u64.u32 	%rd89, %r47;

$L__BB12_28:
	setp.lt.s32 	%p28, %r18, 3;
	@%p28 bra 	$L__BB12_32;

	or.b64  	%rd70, %rd89, %rd5;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.eq.s64 	%p29, %rd71, 0;
	@%p29 bra 	$L__BB12_31;

	div.u64 	%rd89, %rd89, %rd5;
	bra.uni 	$L__BB12_32;

$L__BB12_31:
	cvt.u32.u64 	%r48, %rd5;
	cvt.u32.u64 	%r49, %rd89;
	div.u32 	%r50, %r49, %r48;
	cvt.u64.u32 	%rd89, %r50;

$L__BB12_32:
	setp.lt.s32 	%p30, %r18, 2;
	@%p30 bra 	$L__BB12_36;

	or.b64  	%rd72, %rd89, %rd6;
	and.b64  	%rd73, %rd72, -4294967296;
	setp.eq.s64 	%p31, %rd73, 0;
	@%p31 bra 	$L__BB12_35;

	div.u64 	%rd89, %rd89, %rd6;
	bra.uni 	$L__BB12_36;

$L__BB12_35:
	cvt.u32.u64 	%r51, %rd6;
	cvt.u32.u64 	%r52, %rd89;
	div.u32 	%r53, %r52, %r51;
	cvt.u64.u32 	%rd89, %r53;

$L__BB12_36:
	cvt.u32.u64 	%r54, %rd89;
	setp.gt.s32 	%p32, %r18, 0;
	selp.b32 	%r12, %r54, 0, %p32;
	add.s32 	%r13, %r12, %r2;
	setp.le.s32 	%p33, %r27, %r13;
	setp.le.s32 	%p34, %r28, %r13;
	setp.le.s32 	%p35, %r7, %r12;
	or.pred  	%p36, %p33, %p34;
	or.b32  	%r55, %r13, %r12;
	setp.lt.s32 	%p37, %r55, 0;
	or.pred  	%p38, %p37, %p36;
	or.pred  	%p39, %p35, %p38;
	@%p39 bra 	$L__BB12_38;
	bra.uni 	$L__BB12_37;

$L__BB12_38:
	st.local.v2.u32 	[%rd12], {%r13, %r13};
	st.local.v2.u32 	[%rd12+8], {%r12, %r27};
	st.local.v2.u32 	[%rd12+16], {%r28, %r7};
	mov.u64 	%rd79, $str;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd80;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd45;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r56, [retval0+0];
	} // callseq 2
	bra.uni 	$L__BB12_39;

$L__BB12_37:
	cvt.s64.s32 	%rd74, %r12;
	mul.lo.s64 	%rd75, %rd9, %rd74;
	add.s64 	%rd76, %rd8, %rd75;
	st.global.u32 	[%rd76], %r13;
	mul.lo.s64 	%rd77, %rd11, %rd74;
	add.s64 	%rd78, %rd10, %rd77;
	st.global.u32 	[%rd78], %r13;

$L__BB12_39:
	add.s64 	%rd82, %rd82, %rd7;
	setp.lt.u64 	%p40, %rd82, %rd37;
	@%p40 bra 	$L__BB12_25;
	bra.uni 	$L__BB12_40;

$L__BB12_2:
	setp.gt.s32 	%p3, %r18, 2;
	@%p3 bra 	$L__BB12_12;
	bra.uni 	$L__BB12_3;

$L__BB12_12:
	cvt.u32.u64 	%r36, %rd5;
	cvt.u32.u64 	%r39, %rd6;

$L__BB12_13:
	or.b64  	%rd56, %rd82, %rd5;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p15, %rd57, 0;
	@%p15 bra 	$L__BB12_15;

	div.u64 	%rd86, %rd82, %rd5;
	bra.uni 	$L__BB12_16;

$L__BB12_15:
	cvt.u32.u64 	%r37, %rd82;
	div.u32 	%r38, %r37, %r36;
	cvt.u64.u32 	%rd86, %r38;

$L__BB12_16:
	setp.lt.s32 	%p16, %r18, 2;
	@%p16 bra 	$L__BB12_20;

	or.b64  	%rd58, %rd86, %rd6;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p17, %rd59, 0;
	@%p17 bra 	$L__BB12_19;

	div.u64 	%rd86, %rd86, %rd6;
	bra.uni 	$L__BB12_20;

$L__BB12_19:
	cvt.u32.u64 	%r40, %rd86;
	div.u32 	%r41, %r40, %r39;
	cvt.u64.u32 	%rd86, %r41;

$L__BB12_20:
	cvt.u32.u64 	%r42, %rd86;
	setp.gt.s32 	%p18, %r18, 0;
	selp.b32 	%r10, %r42, 0, %p18;
	add.s32 	%r11, %r10, %r2;
	setp.le.s32 	%p19, %r27, %r11;
	setp.le.s32 	%p20, %r28, %r11;
	setp.le.s32 	%p21, %r7, %r10;
	or.pred  	%p22, %p19, %p20;
	or.b32  	%r43, %r11, %r10;
	setp.lt.s32 	%p23, %r43, 0;
	or.pred  	%p24, %p23, %p22;
	or.pred  	%p25, %p21, %p24;
	@%p25 bra 	$L__BB12_22;
	bra.uni 	$L__BB12_21;

$L__BB12_22:
	st.local.v2.u32 	[%rd12], {%r11, %r11};
	st.local.v2.u32 	[%rd12+8], {%r10, %r27};
	st.local.v2.u32 	[%rd12+16], {%r28, %r7};
	mov.u64 	%rd65, $str;
	cvta.global.u64 	%rd66, %rd65;
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd66;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd45;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r44, [retval0+0];
	} // callseq 1
	bra.uni 	$L__BB12_23;

$L__BB12_21:
	cvt.s64.s32 	%rd60, %r10;
	mul.lo.s64 	%rd61, %rd9, %rd60;
	add.s64 	%rd62, %rd8, %rd61;
	st.global.u32 	[%rd62], %r11;
	mul.lo.s64 	%rd63, %rd11, %rd60;
	add.s64 	%rd64, %rd10, %rd63;
	st.global.u32 	[%rd64], %r11;

$L__BB12_23:
	add.s64 	%rd82, %rd82, %rd7;
	setp.lt.u64 	%p26, %rd82, %rd37;
	@%p26 bra 	$L__BB12_13;
	bra.uni 	$L__BB12_40;

$L__BB12_3:
	cvt.u32.u64 	%r30, %rd6;

$L__BB12_4:
	setp.lt.s32 	%p4, %r18, 2;
	mov.u64 	%rd83, %rd82;
	@%p4 bra 	$L__BB12_8;

	or.b64  	%rd46, %rd82, %rd6;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p5, %rd47, 0;
	@%p5 bra 	$L__BB12_7;

	div.u64 	%rd83, %rd82, %rd6;
	bra.uni 	$L__BB12_8;

$L__BB12_7:
	cvt.u32.u64 	%r31, %rd82;
	div.u32 	%r32, %r31, %r30;
	cvt.u64.u32 	%rd83, %r32;

$L__BB12_8:
	cvt.u32.u64 	%r33, %rd83;
	setp.gt.s32 	%p6, %r18, 0;
	selp.b32 	%r8, %r33, 0, %p6;
	add.s32 	%r9, %r8, %r2;
	setp.le.s32 	%p7, %r27, %r9;
	setp.le.s32 	%p8, %r28, %r9;
	setp.le.s32 	%p9, %r7, %r8;
	or.pred  	%p10, %p7, %p8;
	or.b32  	%r34, %r9, %r8;
	setp.lt.s32 	%p11, %r34, 0;
	or.pred  	%p12, %p11, %p10;
	or.pred  	%p13, %p9, %p12;
	@%p13 bra 	$L__BB12_10;
	bra.uni 	$L__BB12_9;

$L__BB12_10:
	st.local.v2.u32 	[%rd12], {%r9, %r9};
	st.local.v2.u32 	[%rd12+8], {%r8, %r27};
	st.local.v2.u32 	[%rd12+16], {%r28, %r7};
	mov.u64 	%rd53, $str;
	cvta.global.u64 	%rd54, %rd53;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd54;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd45;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r35, [retval0+0];
	} // callseq 0
	bra.uni 	$L__BB12_11;

$L__BB12_9:
	cvt.s64.s32 	%rd48, %r8;
	mul.lo.s64 	%rd49, %rd9, %rd48;
	add.s64 	%rd50, %rd8, %rd49;
	st.global.u32 	[%rd50], %r9;
	mul.lo.s64 	%rd51, %rd11, %rd48;
	add.s64 	%rd52, %rd10, %rd51;
	st.global.u32 	[%rd52], %r9;

$L__BB12_11:
	add.s64 	%rd82, %rd82, %rd7;
	setp.lt.u64 	%p14, %rd82, %rd37;
	@%p14 bra 	$L__BB12_4;

$L__BB12_40:
	ret;

}
	// .globl	init_soft_diag_hess_inds_kernel_cuda_kernel_backward
.visible .entry init_soft_diag_hess_inds_kernel_cuda_kernel_backward(
	.param .align 8 .b8 init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_1[184],
	.param .u32 init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_2,
	.param .align 8 .b8 init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_3[184],
	.param .u32 init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_4
)
{
	.local .align 8 .b8 	__local_depot13[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<32>;
	.reg .b32 	%r<51>;
	.reg .b64 	%rd<80>;


	mov.u64 	%SPL, __local_depot13;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r18, %r19}, [init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r20, %r21}, [init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_0+8];
	mov.b64 	%rd33, init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_1;
	ld.param.u32 	%r17, [init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd32, [init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r16, [init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_0+16];
	mov.u32 	%r22, %ntid.x;
	cvt.u64.u32 	%rd1, %r22;
	mov.u32 	%r23, %ctaid.x;
	mul.wide.u32 	%rd34, %r22, %r23;
	mov.u32 	%r24, %tid.x;
	cvt.u64.u32 	%rd35, %r24;
	add.s64 	%rd73, %rd34, %rd35;
	setp.ge.u64 	%p3, %rd73, %rd32;
	@%p3 bra 	$L__BB13_35;

	cvt.s64.s32 	%rd4, %r21;
	cvt.s64.s32 	%rd5, %r20;
	cvt.s64.s32 	%rd6, %r19;
	shl.b32 	%r2, %r17, 2;
	mov.u64 	%rd36, %rd33;
	ld.param.v2.u32 	{%r25, %r26}, [%rd36+176];
	ld.param.u32 	%r7, [%rd36+172];
	ld.param.u64 	%rd37, [%rd36];
	cvta.to.global.u64 	%rd7, %rd37;
	ld.param.s32 	%rd8, [%rd36+32];
	ld.param.u64 	%rd38, [%rd36+56];
	cvta.to.global.u64 	%rd9, %rd38;
	ld.param.s32 	%rd10, [%rd36+88];
	mov.u32 	%r27, %nctaid.x;
	cvt.u64.u32 	%rd39, %r27;
	mul.lo.s64 	%rd11, %rd1, %rd39;
	add.u64 	%rd40, %SP, 0;
	add.u64 	%rd12, %SPL, 0;
	setp.gt.s32 	%p4, %r16, 3;
	@%p4 bra 	$L__BB13_18;
	bra.uni 	$L__BB13_3;

$L__BB13_18:
	or.b64  	%rd56, %rd73, %rd4;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p18, %rd57, 0;
	@%p18 bra 	$L__BB13_20;

	div.u64 	%rd78, %rd73, %rd4;
	bra.uni 	$L__BB13_21;

$L__BB13_20:
	cvt.u32.u64 	%r38, %rd4;
	cvt.u32.u64 	%r39, %rd73;
	div.u32 	%r40, %r39, %r38;
	cvt.u64.u32 	%rd78, %r40;

$L__BB13_21:
	setp.lt.s32 	%p19, %r16, 3;
	@%p19 bra 	$L__BB13_25;

	or.b64  	%rd58, %rd78, %rd5;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p20, %rd59, 0;
	@%p20 bra 	$L__BB13_24;

	div.u64 	%rd78, %rd78, %rd5;
	bra.uni 	$L__BB13_25;

$L__BB13_24:
	cvt.u32.u64 	%r41, %rd5;
	cvt.u32.u64 	%r42, %rd78;
	div.u32 	%r43, %r42, %r41;
	cvt.u64.u32 	%rd78, %r43;

$L__BB13_25:
	setp.lt.s32 	%p21, %r16, 2;
	@%p21 bra 	$L__BB13_29;

	or.b64  	%rd60, %rd78, %rd6;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.eq.s64 	%p22, %rd61, 0;
	@%p22 bra 	$L__BB13_28;

	div.u64 	%rd78, %rd78, %rd6;
	bra.uni 	$L__BB13_29;

$L__BB13_28:
	cvt.u32.u64 	%r44, %rd6;
	cvt.u32.u64 	%r45, %rd78;
	div.u32 	%r46, %r45, %r44;
	cvt.u64.u32 	%rd78, %r46;

$L__BB13_29:
	cvt.u32.u64 	%r47, %rd78;
	setp.gt.s32 	%p23, %r16, 0;
	selp.b32 	%r10, %r47, 0, %p23;
	add.s32 	%r11, %r10, %r2;
	setp.le.s32 	%p24, %r25, %r11;
	setp.le.s32 	%p25, %r26, %r11;
	setp.le.s32 	%p26, %r7, %r10;
	or.pred  	%p27, %p24, %p25;
	or.b32  	%r48, %r11, %r10;
	setp.lt.s32 	%p28, %r48, 0;
	or.pred  	%p29, %p28, %p27;
	or.pred  	%p2, %p26, %p29;
	@%p2 bra 	$L__BB13_31;
	bra.uni 	$L__BB13_30;

$L__BB13_31:
	st.local.v2.u32 	[%rd12], {%r11, %r11};
	st.local.v2.u32 	[%rd12+8], {%r10, %r25};
	st.local.v2.u32 	[%rd12+16], {%r26, %r7};
	mov.u64 	%rd67, $str;
	cvta.global.u64 	%rd68, %rd67;
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd68;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd40;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r49, [retval0+0];
	} // callseq 5
	bra.uni 	$L__BB13_32;

$L__BB13_30:
	cvt.s64.s32 	%rd62, %r10;
	mul.lo.s64 	%rd63, %rd8, %rd62;
	add.s64 	%rd64, %rd7, %rd63;
	st.global.u32 	[%rd64], %r11;
	mul.lo.s64 	%rd65, %rd10, %rd62;
	add.s64 	%rd66, %rd9, %rd65;
	st.global.u32 	[%rd66], %r11;

$L__BB13_32:
	not.pred 	%p30, %p2;
	@%p30 bra 	$L__BB13_34;

	st.local.v2.u32 	[%rd12], {%r11, %r11};
	st.local.v2.u32 	[%rd12+8], {%r10, %r25};
	st.local.v2.u32 	[%rd12+16], {%r26, %r7};
	mov.u64 	%rd70, $str;
	cvta.global.u64 	%rd71, %rd70;
	{ // callseq 6, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd71;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd40;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r50, [retval0+0];
	} // callseq 6

$L__BB13_34:
	add.s64 	%rd73, %rd73, %rd11;
	setp.lt.u64 	%p31, %rd73, %rd32;
	@%p31 bra 	$L__BB13_18;
	bra.uni 	$L__BB13_35;

$L__BB13_3:
	setp.lt.s32 	%p5, %r16, 3;
	mov.u64 	%rd74, %rd73;
	@%p5 bra 	$L__BB13_7;

	or.b64  	%rd41, %rd73, %rd5;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p6, %rd42, 0;
	@%p6 bra 	$L__BB13_6;

	div.u64 	%rd74, %rd73, %rd5;
	bra.uni 	$L__BB13_7;

$L__BB13_6:
	cvt.u32.u64 	%r28, %rd5;
	cvt.u32.u64 	%r29, %rd73;
	div.u32 	%r30, %r29, %r28;
	cvt.u64.u32 	%rd74, %r30;

$L__BB13_7:
	setp.lt.s32 	%p7, %r16, 2;
	@%p7 bra 	$L__BB13_11;

	or.b64  	%rd43, %rd74, %rd6;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p8, %rd44, 0;
	@%p8 bra 	$L__BB13_10;

	div.u64 	%rd74, %rd74, %rd6;
	bra.uni 	$L__BB13_11;

$L__BB13_10:
	cvt.u32.u64 	%r31, %rd6;
	cvt.u32.u64 	%r32, %rd74;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd74, %r33;

$L__BB13_11:
	cvt.u32.u64 	%r34, %rd74;
	setp.gt.s32 	%p9, %r16, 0;
	selp.b32 	%r8, %r34, 0, %p9;
	add.s32 	%r9, %r8, %r2;
	setp.le.s32 	%p10, %r25, %r9;
	setp.le.s32 	%p11, %r26, %r9;
	setp.le.s32 	%p12, %r7, %r8;
	or.pred  	%p13, %p10, %p11;
	or.b32  	%r35, %r9, %r8;
	setp.lt.s32 	%p14, %r35, 0;
	or.pred  	%p15, %p14, %p13;
	or.pred  	%p1, %p12, %p15;
	@%p1 bra 	$L__BB13_13;
	bra.uni 	$L__BB13_12;

$L__BB13_13:
	st.local.v2.u32 	[%rd12], {%r9, %r9};
	st.local.v2.u32 	[%rd12+8], {%r8, %r25};
	st.local.v2.u32 	[%rd12+16], {%r26, %r7};
	mov.u64 	%rd50, $str;
	cvta.global.u64 	%rd51, %rd50;
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd51;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd40;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r36, [retval0+0];
	} // callseq 3
	bra.uni 	$L__BB13_14;

$L__BB13_12:
	cvt.s64.s32 	%rd45, %r8;
	mul.lo.s64 	%rd46, %rd8, %rd45;
	add.s64 	%rd47, %rd7, %rd46;
	st.global.u32 	[%rd47], %r9;
	mul.lo.s64 	%rd48, %rd10, %rd45;
	add.s64 	%rd49, %rd9, %rd48;
	st.global.u32 	[%rd49], %r9;

$L__BB13_14:
	not.pred 	%p16, %p1;
	@%p16 bra 	$L__BB13_16;

	st.local.v2.u32 	[%rd12], {%r9, %r9};
	st.local.v2.u32 	[%rd12+8], {%r8, %r25};
	st.local.v2.u32 	[%rd12+16], {%r26, %r7};
	mov.u64 	%rd53, $str;
	cvta.global.u64 	%rd54, %rd53;
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd54;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd40;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r37, [retval0+0];
	} // callseq 4

$L__BB13_16:
	add.s64 	%rd73, %rd73, %rd11;
	setp.lt.u64 	%p17, %rd73, %rd32;
	@%p17 bra 	$L__BB13_3;

$L__BB13_35:
	ret;

}
	// .globl	soft_to_sys_grad_cuda_kernel_forward
.visible .entry soft_to_sys_grad_cuda_kernel_forward(
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_forward_param_2[56],
	.param .u32 soft_to_sys_grad_cuda_kernel_forward_param_3
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<77>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<83>;


	ld.param.v2.u32 	{%r26, %r27}, [soft_to_sys_grad_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r28, %r29}, [soft_to_sys_grad_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r34, %r35}, [soft_to_sys_grad_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r42, %r43}, [soft_to_sys_grad_cuda_kernel_forward_param_2+32];
	ld.param.u32 	%r25, [soft_to_sys_grad_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd39, [soft_to_sys_grad_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd37, [soft_to_sys_grad_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd36, [soft_to_sys_grad_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [soft_to_sys_grad_cuda_kernel_forward_param_0+16];
	mov.u32 	%r46, %ntid.x;
	cvt.u64.u32 	%rd1, %r46;
	mov.u32 	%r47, %ctaid.x;
	mul.wide.u32 	%rd41, %r46, %r47;
	mov.u32 	%r48, %tid.x;
	cvt.u64.u32 	%rd42, %r48;
	add.s64 	%rd74, %rd41, %rd42;
	setp.ge.u64 	%p1, %rd74, %rd36;
	@%p1 bra 	$L__BB14_31;

	cvta.to.global.u64 	%rd4, %rd39;
	cvta.to.global.u64 	%rd5, %rd37;
	cvt.s64.s32 	%rd6, %r29;
	cvt.s64.s32 	%rd7, %r28;
	cvt.s64.s32 	%rd8, %r27;
	cvt.s64.s32 	%rd9, %r34;
	cvt.s64.s32 	%rd10, %r42;
	mov.u32 	%r49, %nctaid.x;
	cvt.u64.u32 	%rd43, %r49;
	mul.lo.s64 	%rd11, %rd1, %rd43;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB14_18;
	bra.uni 	$L__BB14_2;

$L__BB14_18:
	cvt.u32.u64 	%r65, %rd6;
	cvt.u32.u64 	%r68, %rd7;
	cvt.u32.u64 	%r71, %rd8;

$L__BB14_19:
	or.b64  	%rd62, %rd74, %rd6;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p13, %rd63, 0;
	@%p13 bra 	$L__BB14_21;

	div.u64 	%rd81, %rd74, %rd6;
	bra.uni 	$L__BB14_22;

$L__BB14_21:
	cvt.u32.u64 	%r66, %rd74;
	div.u32 	%r67, %r66, %r65;
	cvt.u64.u32 	%rd81, %r67;

$L__BB14_22:
	setp.lt.s32 	%p14, %r6, 3;
	@%p14 bra 	$L__BB14_26;

	or.b64  	%rd64, %rd81, %rd7;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p15, %rd65, 0;
	@%p15 bra 	$L__BB14_25;

	div.u64 	%rd81, %rd81, %rd7;
	bra.uni 	$L__BB14_26;

$L__BB14_25:
	cvt.u32.u64 	%r69, %rd81;
	div.u32 	%r70, %r69, %r68;
	cvt.u64.u32 	%rd81, %r70;

$L__BB14_26:
	setp.lt.s32 	%p16, %r6, 2;
	@%p16 bra 	$L__BB14_30;

	or.b64  	%rd66, %rd81, %rd8;
	and.b64  	%rd67, %rd66, -4294967296;
	setp.eq.s64 	%p17, %rd67, 0;
	@%p17 bra 	$L__BB14_29;

	div.u64 	%rd81, %rd81, %rd8;
	bra.uni 	$L__BB14_30;

$L__BB14_29:
	cvt.u32.u64 	%r72, %rd81;
	div.u32 	%r73, %r72, %r71;
	cvt.u64.u32 	%rd81, %r73;

$L__BB14_30:
	cvt.u32.u64 	%r74, %rd81;
	setp.gt.s32 	%p18, %r6, 0;
	selp.b32 	%r75, %r74, 0, %p18;
	cvt.s64.s32 	%rd68, %r75;
	mul.lo.s64 	%rd69, %rd68, %rd9;
	add.s64 	%rd70, %rd5, %rd69;
	add.s32 	%r76, %r75, %r25;
	ld.global.f64 	%fd7, [%rd70];
	ld.global.f64 	%fd8, [%rd70+8];
	ld.global.f64 	%fd9, [%rd70+16];
	cvt.s64.s32 	%rd71, %r76;
	mul.lo.s64 	%rd72, %rd71, %rd10;
	add.s64 	%rd73, %rd4, %rd72;
	st.global.f64 	[%rd73], %fd7;
	st.global.f64 	[%rd73+8], %fd8;
	st.global.f64 	[%rd73+16], %fd9;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p19, %rd74, %rd36;
	@%p19 bra 	$L__BB14_19;
	bra.uni 	$L__BB14_31;

$L__BB14_2:
	setp.gt.s32 	%p3, %r6, 2;
	@%p3 bra 	$L__BB14_9;
	bra.uni 	$L__BB14_3;

$L__BB14_9:
	cvt.u32.u64 	%r56, %rd7;
	cvt.u32.u64 	%r59, %rd8;

$L__BB14_10:
	or.b64  	%rd52, %rd74, %rd7;
	and.b64  	%rd53, %rd52, -4294967296;
	setp.eq.s64 	%p8, %rd53, 0;
	@%p8 bra 	$L__BB14_12;

	div.u64 	%rd78, %rd74, %rd7;
	bra.uni 	$L__BB14_13;

$L__BB14_12:
	cvt.u32.u64 	%r57, %rd74;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd78, %r58;

$L__BB14_13:
	setp.lt.s32 	%p9, %r6, 2;
	@%p9 bra 	$L__BB14_17;

	or.b64  	%rd54, %rd78, %rd8;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p10, %rd55, 0;
	@%p10 bra 	$L__BB14_16;

	div.u64 	%rd78, %rd78, %rd8;
	bra.uni 	$L__BB14_17;

$L__BB14_16:
	cvt.u32.u64 	%r60, %rd78;
	div.u32 	%r61, %r60, %r59;
	cvt.u64.u32 	%rd78, %r61;

$L__BB14_17:
	cvt.u32.u64 	%r62, %rd78;
	setp.gt.s32 	%p11, %r6, 0;
	selp.b32 	%r63, %r62, 0, %p11;
	cvt.s64.s32 	%rd56, %r63;
	mul.lo.s64 	%rd57, %rd56, %rd9;
	add.s64 	%rd58, %rd5, %rd57;
	add.s32 	%r64, %r63, %r25;
	ld.global.f64 	%fd4, [%rd58];
	ld.global.f64 	%fd5, [%rd58+8];
	ld.global.f64 	%fd6, [%rd58+16];
	cvt.s64.s32 	%rd59, %r64;
	mul.lo.s64 	%rd60, %rd59, %rd10;
	add.s64 	%rd61, %rd4, %rd60;
	st.global.f64 	[%rd61], %fd4;
	st.global.f64 	[%rd61+8], %fd5;
	st.global.f64 	[%rd61+16], %fd6;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p12, %rd74, %rd36;
	@%p12 bra 	$L__BB14_10;
	bra.uni 	$L__BB14_31;

$L__BB14_3:
	cvt.u32.u64 	%r50, %rd8;

$L__BB14_4:
	setp.lt.s32 	%p4, %r6, 2;
	mov.u64 	%rd75, %rd74;
	@%p4 bra 	$L__BB14_8;

	or.b64  	%rd44, %rd74, %rd8;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p5, %rd45, 0;
	@%p5 bra 	$L__BB14_7;

	div.u64 	%rd75, %rd74, %rd8;
	bra.uni 	$L__BB14_8;

$L__BB14_7:
	cvt.u32.u64 	%r51, %rd74;
	div.u32 	%r52, %r51, %r50;
	cvt.u64.u32 	%rd75, %r52;

$L__BB14_8:
	cvt.u32.u64 	%r53, %rd75;
	setp.gt.s32 	%p6, %r6, 0;
	selp.b32 	%r54, %r53, 0, %p6;
	cvt.s64.s32 	%rd46, %r54;
	mul.lo.s64 	%rd47, %rd46, %rd9;
	add.s64 	%rd48, %rd5, %rd47;
	add.s32 	%r55, %r54, %r25;
	ld.global.f64 	%fd1, [%rd48];
	ld.global.f64 	%fd2, [%rd48+8];
	ld.global.f64 	%fd3, [%rd48+16];
	cvt.s64.s32 	%rd49, %r55;
	mul.lo.s64 	%rd50, %rd49, %rd10;
	add.s64 	%rd51, %rd4, %rd50;
	st.global.f64 	[%rd51], %fd1;
	st.global.f64 	[%rd51+8], %fd2;
	st.global.f64 	[%rd51+16], %fd3;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p7, %rd74, %rd36;
	@%p7 bra 	$L__BB14_4;

$L__BB14_31:
	ret;

}
	// .globl	soft_to_sys_grad_cuda_kernel_backward
.visible .entry soft_to_sys_grad_cuda_kernel_backward(
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_backward_param_2[56],
	.param .u32 soft_to_sys_grad_cuda_kernel_backward_param_3,
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_backward_param_5[56],
	.param .u32 soft_to_sys_grad_cuda_kernel_backward_param_6
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<96>;
	.reg .f64 	%fd<34>;
	.reg .b64 	%rd<67>;


	ld.param.v2.u32 	{%r46, %r47}, [soft_to_sys_grad_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r48, %r49}, [soft_to_sys_grad_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r54, %r55}, [soft_to_sys_grad_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r62, %r63}, [soft_to_sys_grad_cuda_kernel_backward_param_2+32];
	ld.param.u32 	%r27, [soft_to_sys_grad_cuda_kernel_backward_param_3];
	ld.param.v2.u32 	{%r70, %r71}, [soft_to_sys_grad_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r78, %r79}, [soft_to_sys_grad_cuda_kernel_backward_param_5+32];
	ld.param.u64 	%rd36, [soft_to_sys_grad_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd34, [soft_to_sys_grad_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd33, [soft_to_sys_grad_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd31, [soft_to_sys_grad_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd29, [soft_to_sys_grad_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r8, [soft_to_sys_grad_cuda_kernel_backward_param_0+16];
	mov.u32 	%r82, %ntid.x;
	cvt.u64.u32 	%rd1, %r82;
	mov.u32 	%r83, %ctaid.x;
	mul.wide.u32 	%rd38, %r82, %r83;
	mov.u32 	%r84, %tid.x;
	cvt.u64.u32 	%rd39, %r84;
	add.s64 	%rd63, %rd38, %rd39;
	setp.ge.u64 	%p1, %rd63, %rd29;
	@%p1 bra 	$L__BB15_23;

	cvta.to.global.u64 	%rd8, %rd36;
	cvta.to.global.u64 	%rd9, %rd33;
	cvt.s64.s32 	%rd10, %r49;
	cvt.s64.s32 	%rd11, %r48;
	cvt.s64.s32 	%rd12, %r47;
	cvt.s64.s32 	%rd13, %r78;
	cvt.s64.s32 	%rd14, %r62;
	mov.u32 	%r85, %nctaid.x;
	cvt.u64.u32 	%rd40, %r85;
	mul.lo.s64 	%rd15, %rd1, %rd40;
	cvt.s64.s32 	%rd16, %r70;
	cvt.s64.s32 	%rd17, %r54;

$L__BB15_2:
	setp.lt.s32 	%p2, %r8, 4;
	mov.u64 	%rd64, %rd63;
	@%p2 bra 	$L__BB15_6;

	or.b64  	%rd41, %rd63, %rd10;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p3, %rd42, 0;
	@%p3 bra 	$L__BB15_5;

	div.u64 	%rd64, %rd63, %rd10;
	bra.uni 	$L__BB15_6;

$L__BB15_5:
	cvt.u32.u64 	%r86, %rd10;
	cvt.u32.u64 	%r87, %rd63;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd64, %r88;

$L__BB15_6:
	setp.lt.s32 	%p4, %r8, 3;
	@%p4 bra 	$L__BB15_10;

	or.b64  	%rd43, %rd64, %rd11;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p5, %rd44, 0;
	@%p5 bra 	$L__BB15_9;

	div.u64 	%rd64, %rd64, %rd11;
	bra.uni 	$L__BB15_10;

$L__BB15_9:
	cvt.u32.u64 	%r89, %rd11;
	cvt.u32.u64 	%r90, %rd64;
	div.u32 	%r91, %r90, %r89;
	cvt.u64.u32 	%rd64, %r91;

$L__BB15_10:
	setp.lt.s32 	%p6, %r8, 2;
	@%p6 bra 	$L__BB15_14;

	or.b64  	%rd45, %rd64, %rd12;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p7, %rd46, 0;
	@%p7 bra 	$L__BB15_13;

	div.u64 	%rd64, %rd64, %rd12;
	bra.uni 	$L__BB15_14;

$L__BB15_13:
	cvt.u32.u64 	%r92, %rd12;
	cvt.u32.u64 	%r93, %rd64;
	div.u32 	%r94, %r93, %r92;
	cvt.u64.u32 	%rd64, %r94;

$L__BB15_14:
	cvt.u32.u64 	%r95, %rd64;
	setp.gt.s32 	%p8, %r8, 0;
	selp.b32 	%r2, %r95, 0, %p8;
	add.s32 	%r3, %r2, %r27;
	setp.eq.s64 	%p9, %rd36, 0;
	@%p9 bra 	$L__BB15_16;

	cvt.s64.s32 	%rd47, %r3;
	mul.lo.s64 	%rd48, %rd47, %rd13;
	add.s64 	%rd49, %rd8, %rd48;
	ld.global.f64 	%fd10, [%rd49];
	add.f64 	%fd33, %fd10, 0d0000000000000000;
	ld.global.f64 	%fd11, [%rd49+8];
	add.f64 	%fd32, %fd11, 0d0000000000000000;
	ld.global.f64 	%fd12, [%rd49+16];
	add.f64 	%fd31, %fd12, 0d0000000000000000;
	bra.uni 	$L__BB15_18;

$L__BB15_16:
	setp.eq.s64 	%p10, %rd33, 0;
	mov.f64 	%fd31, 0d0000000000000000;
	mov.f64 	%fd32, %fd31;
	mov.f64 	%fd33, %fd31;
	@%p10 bra 	$L__BB15_18;

	cvt.s64.s32 	%rd50, %r3;
	mul.lo.s64 	%rd51, %rd50, %rd14;
	add.s64 	%rd52, %rd9, %rd51;
	ld.global.f64 	%fd16, [%rd52];
	add.f64 	%fd33, %fd16, 0d0000000000000000;
	ld.global.f64 	%fd17, [%rd52+8];
	add.f64 	%fd32, %fd17, 0d0000000000000000;
	ld.global.f64 	%fd18, [%rd52+16];
	add.f64 	%fd31, %fd18, 0d0000000000000000;

$L__BB15_18:
	setp.eq.s64 	%p11, %rd34, 0;
	@%p11 bra 	$L__BB15_20;

	cvt.s64.s32 	%rd56, %r2;
	mul.lo.s64 	%rd57, %rd56, %rd16;
	add.s64 	%rd53, %rd34, %rd57;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd53],%fd33; }

	// end inline asm
	add.s64 	%rd54, %rd53, 8;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd54],%fd32; }

	// end inline asm
	add.s64 	%rd55, %rd53, 16;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd55],%fd31; }

	// end inline asm
	bra.uni 	$L__BB15_22;

$L__BB15_20:
	setp.eq.s64 	%p12, %rd31, 0;
	@%p12 bra 	$L__BB15_22;

	cvt.s64.s32 	%rd61, %r2;
	mul.lo.s64 	%rd62, %rd61, %rd17;
	add.s64 	%rd58, %rd31, %rd62;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd58],%fd33; }

	// end inline asm
	add.s64 	%rd59, %rd58, 8;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd59],%fd32; }

	// end inline asm
	add.s64 	%rd60, %rd58, 16;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd60],%fd31; }

	// end inline asm

$L__BB15_22:
	add.s64 	%rd63, %rd63, %rd15;
	setp.lt.u64 	%p13, %rd63, %rd29;
	@%p13 bra 	$L__BB15_2;

$L__BB15_23:
	ret;

}
	// .globl	safeguard_direction_x_kernel_cuda_kernel_forward
.visible .entry safeguard_direction_x_kernel_cuda_kernel_forward(
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_forward_param_3[56]
)
{
	.reg .pred 	%p<23>;
	.reg .b16 	%rs<25>;
	.reg .b32 	%r<92>;
	.reg .b64 	%rd<100>;


	ld.param.v2.u32 	{%r34, %r35}, [safeguard_direction_x_kernel_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r36, %r37}, [safeguard_direction_x_kernel_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r42, %r43}, [safeguard_direction_x_kernel_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r50, %r51}, [safeguard_direction_x_kernel_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r58, %r59}, [safeguard_direction_x_kernel_cuda_kernel_forward_param_3+32];
	ld.param.u64 	%rd46, [safeguard_direction_x_kernel_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd44, [safeguard_direction_x_kernel_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd42, [safeguard_direction_x_kernel_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd41, [safeguard_direction_x_kernel_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [safeguard_direction_x_kernel_cuda_kernel_forward_param_0+16];
	mov.u32 	%r62, %ntid.x;
	cvt.u64.u32 	%rd1, %r62;
	mov.u32 	%r63, %ctaid.x;
	mul.wide.u32 	%rd48, %r62, %r63;
	mov.u32 	%r64, %tid.x;
	cvt.u64.u32 	%rd49, %r64;
	add.s64 	%rd91, %rd48, %rd49;
	setp.ge.u64 	%p1, %rd91, %rd41;
	@%p1 bra 	$L__BB16_37;

	cvta.to.global.u64 	%rd4, %rd46;
	cvta.to.global.u64 	%rd5, %rd44;
	cvta.to.global.u64 	%rd6, %rd42;
	cvt.s64.s32 	%rd7, %r37;
	cvt.s64.s32 	%rd8, %r36;
	cvt.s64.s32 	%rd9, %r35;
	cvt.s64.s32 	%rd10, %r50;
	cvt.s64.s32 	%rd11, %r58;
	cvt.s64.s32 	%rd12, %r42;
	mov.u32 	%r65, %nctaid.x;
	cvt.u64.u32 	%rd50, %r65;
	mul.lo.s64 	%rd13, %rd1, %rd50;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB16_22;
	bra.uni 	$L__BB16_2;

$L__BB16_22:
	cvt.u32.u64 	%r79, %rd7;
	cvt.u32.u64 	%r82, %rd8;
	cvt.u32.u64 	%r85, %rd9;

$L__BB16_23:
	or.b64  	%rd75, %rd91, %rd7;
	and.b64  	%rd76, %rd75, -4294967296;
	setp.eq.s64 	%p15, %rd76, 0;
	@%p15 bra 	$L__BB16_25;

	div.u64 	%rd98, %rd91, %rd7;
	bra.uni 	$L__BB16_26;

$L__BB16_25:
	cvt.u32.u64 	%r80, %rd91;
	div.u32 	%r81, %r80, %r79;
	cvt.u64.u32 	%rd98, %r81;

$L__BB16_26:
	setp.lt.s32 	%p16, %r6, 3;
	@%p16 bra 	$L__BB16_30;

	or.b64  	%rd77, %rd98, %rd8;
	and.b64  	%rd78, %rd77, -4294967296;
	setp.eq.s64 	%p17, %rd78, 0;
	@%p17 bra 	$L__BB16_29;

	div.u64 	%rd98, %rd98, %rd8;
	bra.uni 	$L__BB16_30;

$L__BB16_29:
	cvt.u32.u64 	%r83, %rd98;
	div.u32 	%r84, %r83, %r82;
	cvt.u64.u32 	%rd98, %r84;

$L__BB16_30:
	setp.lt.s32 	%p18, %r6, 2;
	@%p18 bra 	$L__BB16_34;

	or.b64  	%rd79, %rd98, %rd9;
	and.b64  	%rd80, %rd79, -4294967296;
	setp.eq.s64 	%p19, %rd80, 0;
	@%p19 bra 	$L__BB16_33;

	div.u64 	%rd98, %rd98, %rd9;
	bra.uni 	$L__BB16_34;

$L__BB16_33:
	cvt.u32.u64 	%r86, %rd98;
	div.u32 	%r87, %r86, %r85;
	cvt.u64.u32 	%rd98, %r87;

$L__BB16_34:
	cvt.s64.s32 	%rd81, %rd98;
	setp.gt.s32 	%p20, %r6, 0;
	selp.b64 	%rd39, %rd81, 0, %p20;
	mul.lo.s64 	%rd82, %rd39, %rd10;
	add.s64 	%rd83, %rd5, %rd82;
	ld.global.s32 	%rd84, [%rd83];
	mul.lo.s64 	%rd85, %rd84, %rd11;
	add.s64 	%rd86, %rd4, %rd85;
	ld.global.u32 	%r88, [%rd86];
	add.s32 	%r89, %r88, -1;
	setp.gt.u32 	%p21, %r89, 1;
	@%p21 bra 	$L__BB16_36;

	mul.lo.s64 	%rd87, %rd39, %rd12;
	add.s64 	%rd88, %rd6, %rd87;
	mov.u64 	%rd89, 0;
	st.global.u64 	[%rd88], %rd89;
	st.global.u64 	[%rd88+8], %rd89;
	st.global.u64 	[%rd88+16], %rd89;

$L__BB16_36:
	mul.wide.u32 	%rd90, %r62, %r65;
	add.s64 	%rd91, %rd91, %rd90;
	setp.lt.u64 	%p22, %rd91, %rd41;
	@%p22 bra 	$L__BB16_23;
	bra.uni 	$L__BB16_37;

$L__BB16_2:
	setp.gt.s32 	%p3, %r6, 2;
	@%p3 bra 	$L__BB16_11;
	bra.uni 	$L__BB16_3;

$L__BB16_11:
	cvt.u32.u64 	%r71, %rd8;
	cvt.u32.u64 	%r74, %rd9;

$L__BB16_12:
	or.b64  	%rd62, %rd91, %rd8;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p9, %rd63, 0;
	@%p9 bra 	$L__BB16_14;

	div.u64 	%rd95, %rd91, %rd8;
	bra.uni 	$L__BB16_15;

$L__BB16_14:
	cvt.u32.u64 	%r72, %rd91;
	div.u32 	%r73, %r72, %r71;
	cvt.u64.u32 	%rd95, %r73;

$L__BB16_15:
	setp.lt.s32 	%p10, %r6, 2;
	@%p10 bra 	$L__BB16_19;

	or.b64  	%rd64, %rd95, %rd9;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p11, %rd65, 0;
	@%p11 bra 	$L__BB16_18;

	div.u64 	%rd95, %rd95, %rd9;
	bra.uni 	$L__BB16_19;

$L__BB16_18:
	cvt.u32.u64 	%r75, %rd95;
	div.u32 	%r76, %r75, %r74;
	cvt.u64.u32 	%rd95, %r76;

$L__BB16_19:
	cvt.s64.s32 	%rd66, %rd95;
	setp.gt.s32 	%p12, %r6, 0;
	selp.b64 	%rd27, %rd66, 0, %p12;
	mul.lo.s64 	%rd67, %rd27, %rd10;
	add.s64 	%rd68, %rd5, %rd67;
	ld.global.s32 	%rd69, [%rd68];
	mul.lo.s64 	%rd70, %rd69, %rd11;
	add.s64 	%rd71, %rd4, %rd70;
	ld.global.u32 	%r77, [%rd71];
	add.s32 	%r78, %r77, -1;
	setp.gt.u32 	%p13, %r78, 1;
	@%p13 bra 	$L__BB16_21;

	mul.lo.s64 	%rd72, %rd27, %rd12;
	add.s64 	%rd73, %rd6, %rd72;
	mov.u64 	%rd74, 0;
	st.global.u64 	[%rd73], %rd74;
	st.global.u64 	[%rd73+8], %rd74;
	st.global.u64 	[%rd73+16], %rd74;

$L__BB16_21:
	add.s64 	%rd91, %rd91, %rd13;
	setp.lt.u64 	%p14, %rd91, %rd41;
	@%p14 bra 	$L__BB16_12;
	bra.uni 	$L__BB16_37;

$L__BB16_3:
	cvt.u32.u64 	%r66, %rd9;

$L__BB16_4:
	setp.lt.s32 	%p4, %r6, 2;
	mov.u64 	%rd92, %rd91;
	@%p4 bra 	$L__BB16_8;

	or.b64  	%rd51, %rd91, %rd9;
	and.b64  	%rd52, %rd51, -4294967296;
	setp.eq.s64 	%p5, %rd52, 0;
	@%p5 bra 	$L__BB16_7;

	div.u64 	%rd92, %rd91, %rd9;
	bra.uni 	$L__BB16_8;

$L__BB16_7:
	cvt.u32.u64 	%r67, %rd91;
	div.u32 	%r68, %r67, %r66;
	cvt.u64.u32 	%rd92, %r68;

$L__BB16_8:
	cvt.s64.s32 	%rd53, %rd92;
	setp.gt.s32 	%p6, %r6, 0;
	selp.b64 	%rd18, %rd53, 0, %p6;
	mul.lo.s64 	%rd54, %rd18, %rd10;
	add.s64 	%rd55, %rd5, %rd54;
	ld.global.s32 	%rd56, [%rd55];
	mul.lo.s64 	%rd57, %rd56, %rd11;
	add.s64 	%rd58, %rd4, %rd57;
	ld.global.u32 	%r69, [%rd58];
	add.s32 	%r70, %r69, -1;
	setp.gt.u32 	%p7, %r70, 1;
	@%p7 bra 	$L__BB16_10;

	mul.lo.s64 	%rd59, %rd18, %rd12;
	add.s64 	%rd60, %rd6, %rd59;
	mov.u64 	%rd61, 0;
	st.global.u64 	[%rd60], %rd61;
	st.global.u64 	[%rd60+8], %rd61;
	st.global.u64 	[%rd60+16], %rd61;

$L__BB16_10:
	add.s64 	%rd91, %rd91, %rd13;
	setp.lt.u64 	%p8, %rd91, %rd41;
	@%p8 bra 	$L__BB16_4;

$L__BB16_37:
	ret;

}
	// .globl	safeguard_direction_x_kernel_cuda_kernel_backward
.visible .entry safeguard_direction_x_kernel_cuda_kernel_backward(
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_backward_param_6[56]
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<94>;
	.reg .f64 	%fd<37>;
	.reg .b64 	%rd<80>;


	ld.param.v2.u32 	{%r43, %r44}, [safeguard_direction_x_kernel_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r45, %r46}, [safeguard_direction_x_kernel_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r51, %r52}, [safeguard_direction_x_kernel_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r59, %r60}, [safeguard_direction_x_kernel_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r67, %r68}, [safeguard_direction_x_kernel_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r75, %r76}, [safeguard_direction_x_kernel_cuda_kernel_backward_param_4+32];
	ld.param.u64 	%rd35, [safeguard_direction_x_kernel_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd33, [safeguard_direction_x_kernel_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd31, [safeguard_direction_x_kernel_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd30, [safeguard_direction_x_kernel_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd28, [safeguard_direction_x_kernel_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [safeguard_direction_x_kernel_cuda_kernel_backward_param_0+16];
	mov.u32 	%r79, %ntid.x;
	cvt.u64.u32 	%rd1, %r79;
	mov.u32 	%r80, %ctaid.x;
	mul.wide.u32 	%rd37, %r79, %r80;
	mov.u32 	%r81, %tid.x;
	cvt.u64.u32 	%rd38, %r81;
	add.s64 	%rd76, %rd37, %rd38;
	setp.ge.u64 	%p1, %rd76, %rd28;
	@%p1 bra 	$L__BB17_28;

	cvta.to.global.u64 	%rd6, %rd33;
	cvta.to.global.u64 	%rd7, %rd31;
	cvt.s64.s32 	%rd8, %r46;
	cvt.s64.s32 	%rd9, %r45;
	cvt.s64.s32 	%rd10, %r44;
	cvt.s64.s32 	%rd11, %r59;
	cvt.s64.s32 	%rd12, %r67;
	mov.u32 	%r82, %nctaid.x;
	cvt.u64.u32 	%rd39, %r82;
	mul.lo.s64 	%rd13, %rd1, %rd39;
	cvt.s64.s32 	%rd14, %r75;
	cvt.s64.s32 	%rd15, %r51;

$L__BB17_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd77, %rd76;
	@%p2 bra 	$L__BB17_6;

	or.b64  	%rd40, %rd76, %rd8;
	and.b64  	%rd41, %rd40, -4294967296;
	setp.eq.s64 	%p3, %rd41, 0;
	@%p3 bra 	$L__BB17_5;

	div.u64 	%rd77, %rd76, %rd8;
	bra.uni 	$L__BB17_6;

$L__BB17_5:
	cvt.u32.u64 	%r83, %rd8;
	cvt.u32.u64 	%r84, %rd76;
	div.u32 	%r85, %r84, %r83;
	cvt.u64.u32 	%rd77, %r85;

$L__BB17_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB17_10;

	or.b64  	%rd42, %rd77, %rd9;
	and.b64  	%rd43, %rd42, -4294967296;
	setp.eq.s64 	%p5, %rd43, 0;
	@%p5 bra 	$L__BB17_9;

	div.u64 	%rd77, %rd77, %rd9;
	bra.uni 	$L__BB17_10;

$L__BB17_9:
	cvt.u32.u64 	%r86, %rd9;
	cvt.u32.u64 	%r87, %rd77;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd77, %r88;

$L__BB17_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB17_14;

	or.b64  	%rd44, %rd77, %rd10;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p7, %rd45, 0;
	@%p7 bra 	$L__BB17_13;

	div.u64 	%rd77, %rd77, %rd10;
	bra.uni 	$L__BB17_14;

$L__BB17_13:
	cvt.u32.u64 	%r89, %rd10;
	cvt.u32.u64 	%r90, %rd77;
	div.u32 	%r91, %r90, %r89;
	cvt.u64.u32 	%rd77, %r91;

$L__BB17_14:
	cvt.s64.s32 	%rd46, %rd77;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd26, %rd46, 0, %p8;
	mul.lo.s64 	%rd47, %rd26, %rd11;
	add.s64 	%rd48, %rd7, %rd47;
	ld.global.s32 	%rd49, [%rd48];
	mul.lo.s64 	%rd50, %rd49, %rd12;
	add.s64 	%rd51, %rd6, %rd50;
	ld.global.u32 	%r92, [%rd51];
	add.s32 	%r93, %r92, -1;
	setp.gt.u32 	%p9, %r93, 1;
	@%p9 bra 	$L__BB17_27;

	setp.eq.s64 	%p10, %rd35, 0;
	@%p10 bra 	$L__BB17_17;

	mul.lo.s64 	%rd55, %rd26, %rd14;
	add.s64 	%rd52, %rd35, %rd55;
	mov.f64 	%fd6, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1,[%rd52],%fd6; }

	// end inline asm
	add.s64 	%rd53, %rd52, 8;
	// begin inline asm
	{ atom.add.f64 %fd3,[%rd53],%fd6; }

	// end inline asm
	add.s64 	%rd54, %rd52, 16;
	// begin inline asm
	{ atom.add.f64 %fd5,[%rd54],%fd6; }

	// end inline asm
	bra.uni 	$L__BB17_19;

$L__BB17_17:
	setp.eq.s64 	%p11, %rd30, 0;
	@%p11 bra 	$L__BB17_19;

	mul.lo.s64 	%rd59, %rd26, %rd15;
	add.s64 	%rd56, %rd30, %rd59;
	mov.f64 	%fd12, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd7,[%rd56],%fd12; }

	// end inline asm
	add.s64 	%rd57, %rd56, 8;
	// begin inline asm
	{ atom.add.f64 %fd9,[%rd57],%fd12; }

	// end inline asm
	add.s64 	%rd58, %rd56, 16;
	// begin inline asm
	{ atom.add.f64 %fd11,[%rd58],%fd12; }

	// end inline asm

$L__BB17_19:
	@%p10 bra 	$L__BB17_21;

	mul.lo.s64 	%rd63, %rd26, %rd14;
	add.s64 	%rd60, %rd35, %rd63;
	mov.f64 	%fd18, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd13,[%rd60],%fd18; }

	// end inline asm
	add.s64 	%rd61, %rd60, 8;
	// begin inline asm
	{ atom.add.f64 %fd15,[%rd61],%fd18; }

	// end inline asm
	add.s64 	%rd62, %rd60, 16;
	// begin inline asm
	{ atom.add.f64 %fd17,[%rd62],%fd18; }

	// end inline asm
	bra.uni 	$L__BB17_23;

$L__BB17_21:
	setp.eq.s64 	%p13, %rd30, 0;
	@%p13 bra 	$L__BB17_23;

	mul.lo.s64 	%rd67, %rd26, %rd15;
	add.s64 	%rd64, %rd30, %rd67;
	mov.f64 	%fd24, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd64],%fd24; }

	// end inline asm
	add.s64 	%rd65, %rd64, 8;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd65],%fd24; }

	// end inline asm
	add.s64 	%rd66, %rd64, 16;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd66],%fd24; }

	// end inline asm

$L__BB17_23:
	@%p10 bra 	$L__BB17_25;

	mul.lo.s64 	%rd71, %rd26, %rd14;
	add.s64 	%rd68, %rd35, %rd71;
	mov.f64 	%fd30, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd68],%fd30; }

	// end inline asm
	add.s64 	%rd69, %rd68, 8;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd69],%fd30; }

	// end inline asm
	add.s64 	%rd70, %rd68, 16;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd70],%fd30; }

	// end inline asm
	bra.uni 	$L__BB17_27;

$L__BB17_25:
	setp.eq.s64 	%p15, %rd30, 0;
	@%p15 bra 	$L__BB17_27;

	mul.lo.s64 	%rd75, %rd26, %rd15;
	add.s64 	%rd72, %rd30, %rd75;
	mov.f64 	%fd36, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd31,[%rd72],%fd36; }

	// end inline asm
	add.s64 	%rd73, %rd72, 8;
	// begin inline asm
	{ atom.add.f64 %fd33,[%rd73],%fd36; }

	// end inline asm
	add.s64 	%rd74, %rd72, 16;
	// begin inline asm
	{ atom.add.f64 %fd35,[%rd74],%fd36; }

	// end inline asm

$L__BB17_27:
	add.s64 	%rd76, %rd76, %rd13;
	setp.lt.u64 	%p16, %rd76, %rd28;
	@%p16 bra 	$L__BB17_2;

$L__BB17_28:
	ret;

}
	// .globl	step_x_cuda_kernel_forward
.visible .entry step_x_cuda_kernel_forward(
	.param .align 8 .b8 step_x_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 step_x_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 step_x_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 step_x_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 step_x_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 step_x_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 step_x_cuda_kernel_forward_param_6[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<49>;
	.reg .b32 	%r<135>;
	.reg .f64 	%fd<31>;
	.reg .b64 	%rd<129>;


	ld.param.v2.u32 	{%r61, %r62}, [step_x_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r63, %r64}, [step_x_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r69, %r70}, [step_x_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r77, %r78}, [step_x_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r85, %r86}, [step_x_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r93, %r94}, [step_x_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r101, %r102}, [step_x_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r109, %r110}, [step_x_cuda_kernel_forward_param_6+32];
	ld.param.u64 	%rd55, [step_x_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd53, [step_x_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd51, [step_x_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd49, [step_x_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd47, [step_x_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd45, [step_x_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd44, [step_x_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [step_x_cuda_kernel_forward_param_0+16];
	mov.u32 	%r113, %ntid.x;
	cvt.u64.u32 	%rd1, %r113;
	mov.u32 	%r114, %ctaid.x;
	mul.wide.u32 	%rd57, %r113, %r114;
	mov.u32 	%r115, %tid.x;
	cvt.u64.u32 	%rd58, %r115;
	add.s64 	%rd120, %rd57, %rd58;
	setp.ge.u64 	%p1, %rd120, %rd44;
	@%p1 bra 	$L__BB18_31;

	cvta.to.global.u64 	%rd4, %rd55;
	cvta.to.global.u64 	%rd5, %rd53;
	cvta.to.global.u64 	%rd6, %rd51;
	cvta.to.global.u64 	%rd7, %rd49;
	cvta.to.global.u64 	%rd8, %rd47;
	cvta.to.global.u64 	%rd9, %rd45;
	cvt.s64.s32 	%rd10, %r64;
	cvt.s64.s32 	%rd11, %r63;
	cvt.s64.s32 	%rd12, %r62;
	cvt.s64.s32 	%rd13, %r69;
	cvt.s64.s32 	%rd14, %r101;
	cvt.s64.s32 	%rd15, %r109;
	cvt.s64.s32 	%rd16, %r85;
	cvt.s64.s32 	%rd17, %r77;
	cvt.s64.s32 	%rd18, %r93;
	mov.u32 	%r116, %nctaid.x;
	cvt.u64.u32 	%rd59, %r116;
	mul.lo.s64 	%rd19, %rd1, %rd59;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB18_18;
	bra.uni 	$L__BB18_2;

$L__BB18_18:
	cvt.u32.u64 	%r126, %rd10;
	cvt.u32.u64 	%r129, %rd11;
	cvt.u32.u64 	%r132, %rd12;

$L__BB18_19:
	or.b64  	%rd98, %rd120, %rd10;
	and.b64  	%rd99, %rd98, -4294967296;
	setp.eq.s64 	%p13, %rd99, 0;
	@%p13 bra 	$L__BB18_21;

	div.u64 	%rd127, %rd120, %rd10;
	bra.uni 	$L__BB18_22;

$L__BB18_21:
	cvt.u32.u64 	%r127, %rd120;
	div.u32 	%r128, %r127, %r126;
	cvt.u64.u32 	%rd127, %r128;

$L__BB18_22:
	setp.lt.s32 	%p14, %r6, 3;
	@%p14 bra 	$L__BB18_26;

	or.b64  	%rd100, %rd127, %rd11;
	and.b64  	%rd101, %rd100, -4294967296;
	setp.eq.s64 	%p15, %rd101, 0;
	@%p15 bra 	$L__BB18_25;

	div.u64 	%rd127, %rd127, %rd11;
	bra.uni 	$L__BB18_26;

$L__BB18_25:
	cvt.u32.u64 	%r130, %rd127;
	div.u32 	%r131, %r130, %r129;
	cvt.u64.u32 	%rd127, %r131;

$L__BB18_26:
	setp.lt.s32 	%p16, %r6, 2;
	@%p16 bra 	$L__BB18_30;

	or.b64  	%rd102, %rd127, %rd12;
	and.b64  	%rd103, %rd102, -4294967296;
	setp.eq.s64 	%p17, %rd103, 0;
	@%p17 bra 	$L__BB18_29;

	div.u64 	%rd127, %rd127, %rd12;
	bra.uni 	$L__BB18_30;

$L__BB18_29:
	cvt.u32.u64 	%r133, %rd127;
	div.u32 	%r134, %r133, %r132;
	cvt.u64.u32 	%rd127, %r134;

$L__BB18_30:
	cvt.s64.s32 	%rd104, %rd127;
	setp.gt.s32 	%p18, %r6, 0;
	selp.b64 	%rd105, %rd104, 0, %p18;
	mul.lo.s64 	%rd106, %rd105, %rd13;
	add.s64 	%rd107, %rd9, %rd106;
	mul.lo.s64 	%rd108, %rd105, %rd14;
	add.s64 	%rd109, %rd5, %rd108;
	ld.global.s32 	%rd110, [%rd109];
	mul.lo.s64 	%rd111, %rd110, %rd15;
	add.s64 	%rd112, %rd4, %rd111;
	ld.global.s32 	%rd113, [%rd112];
	mul.lo.s64 	%rd114, %rd113, %rd16;
	add.s64 	%rd115, %rd7, %rd114;
	mul.lo.s64 	%rd116, %rd105, %rd17;
	add.s64 	%rd117, %rd8, %rd116;
	ld.global.f64 	%fd21, [%rd117];
	ld.global.f64 	%fd22, [%rd115];
	ld.global.f64 	%fd23, [%rd117+8];
	ld.global.f64 	%fd24, [%rd117+16];
	ld.global.f64 	%fd25, [%rd107];
	fma.rn.f64 	%fd26, %fd22, %fd21, %fd25;
	ld.global.f64 	%fd27, [%rd107+8];
	fma.rn.f64 	%fd28, %fd22, %fd23, %fd27;
	ld.global.f64 	%fd29, [%rd107+16];
	fma.rn.f64 	%fd30, %fd22, %fd24, %fd29;
	mul.lo.s64 	%rd118, %rd105, %rd18;
	add.s64 	%rd119, %rd6, %rd118;
	st.global.f64 	[%rd119], %fd26;
	st.global.f64 	[%rd119+8], %fd28;
	st.global.f64 	[%rd119+16], %fd30;
	add.s64 	%rd120, %rd120, %rd19;
	setp.lt.u64 	%p19, %rd120, %rd44;
	@%p19 bra 	$L__BB18_19;
	bra.uni 	$L__BB18_31;

$L__BB18_2:
	setp.gt.s32 	%p3, %r6, 2;
	@%p3 bra 	$L__BB18_9;
	bra.uni 	$L__BB18_3;

$L__BB18_9:
	cvt.u32.u64 	%r120, %rd11;
	cvt.u32.u64 	%r123, %rd12;

$L__BB18_10:
	or.b64  	%rd78, %rd120, %rd11;
	and.b64  	%rd79, %rd78, -4294967296;
	setp.eq.s64 	%p8, %rd79, 0;
	@%p8 bra 	$L__BB18_12;

	div.u64 	%rd124, %rd120, %rd11;
	bra.uni 	$L__BB18_13;

$L__BB18_12:
	cvt.u32.u64 	%r121, %rd120;
	div.u32 	%r122, %r121, %r120;
	cvt.u64.u32 	%rd124, %r122;

$L__BB18_13:
	setp.lt.s32 	%p9, %r6, 2;
	@%p9 bra 	$L__BB18_17;

	or.b64  	%rd80, %rd124, %rd12;
	and.b64  	%rd81, %rd80, -4294967296;
	setp.eq.s64 	%p10, %rd81, 0;
	@%p10 bra 	$L__BB18_16;

	div.u64 	%rd124, %rd124, %rd12;
	bra.uni 	$L__BB18_17;

$L__BB18_16:
	cvt.u32.u64 	%r124, %rd124;
	div.u32 	%r125, %r124, %r123;
	cvt.u64.u32 	%rd124, %r125;

$L__BB18_17:
	cvt.s64.s32 	%rd82, %rd124;
	setp.gt.s32 	%p11, %r6, 0;
	selp.b64 	%rd83, %rd82, 0, %p11;
	mul.lo.s64 	%rd84, %rd83, %rd13;
	add.s64 	%rd85, %rd9, %rd84;
	mul.lo.s64 	%rd86, %rd83, %rd14;
	add.s64 	%rd87, %rd5, %rd86;
	ld.global.s32 	%rd88, [%rd87];
	mul.lo.s64 	%rd89, %rd88, %rd15;
	add.s64 	%rd90, %rd4, %rd89;
	ld.global.s32 	%rd91, [%rd90];
	mul.lo.s64 	%rd92, %rd91, %rd16;
	add.s64 	%rd93, %rd7, %rd92;
	mul.lo.s64 	%rd94, %rd83, %rd17;
	add.s64 	%rd95, %rd8, %rd94;
	ld.global.f64 	%fd11, [%rd95];
	ld.global.f64 	%fd12, [%rd93];
	ld.global.f64 	%fd13, [%rd95+8];
	ld.global.f64 	%fd14, [%rd95+16];
	ld.global.f64 	%fd15, [%rd85];
	fma.rn.f64 	%fd16, %fd12, %fd11, %fd15;
	ld.global.f64 	%fd17, [%rd85+8];
	fma.rn.f64 	%fd18, %fd12, %fd13, %fd17;
	ld.global.f64 	%fd19, [%rd85+16];
	fma.rn.f64 	%fd20, %fd12, %fd14, %fd19;
	mul.lo.s64 	%rd96, %rd83, %rd18;
	add.s64 	%rd97, %rd6, %rd96;
	st.global.f64 	[%rd97], %fd16;
	st.global.f64 	[%rd97+8], %fd18;
	st.global.f64 	[%rd97+16], %fd20;
	add.s64 	%rd120, %rd120, %rd19;
	setp.lt.u64 	%p12, %rd120, %rd44;
	@%p12 bra 	$L__BB18_10;
	bra.uni 	$L__BB18_31;

$L__BB18_3:
	cvt.u32.u64 	%r117, %rd12;

$L__BB18_4:
	setp.lt.s32 	%p4, %r6, 2;
	mov.u64 	%rd121, %rd120;
	@%p4 bra 	$L__BB18_8;

	or.b64  	%rd60, %rd120, %rd12;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.eq.s64 	%p5, %rd61, 0;
	@%p5 bra 	$L__BB18_7;

	div.u64 	%rd121, %rd120, %rd12;
	bra.uni 	$L__BB18_8;

$L__BB18_7:
	cvt.u32.u64 	%r118, %rd120;
	div.u32 	%r119, %r118, %r117;
	cvt.u64.u32 	%rd121, %r119;

$L__BB18_8:
	cvt.s64.s32 	%rd62, %rd121;
	setp.gt.s32 	%p6, %r6, 0;
	selp.b64 	%rd63, %rd62, 0, %p6;
	mul.lo.s64 	%rd64, %rd63, %rd13;
	add.s64 	%rd65, %rd9, %rd64;
	mul.lo.s64 	%rd66, %rd63, %rd14;
	add.s64 	%rd67, %rd5, %rd66;
	ld.global.s32 	%rd68, [%rd67];
	mul.lo.s64 	%rd69, %rd68, %rd15;
	add.s64 	%rd70, %rd4, %rd69;
	ld.global.s32 	%rd71, [%rd70];
	mul.lo.s64 	%rd72, %rd71, %rd16;
	add.s64 	%rd73, %rd7, %rd72;
	mul.lo.s64 	%rd74, %rd63, %rd17;
	add.s64 	%rd75, %rd8, %rd74;
	ld.global.f64 	%fd1, [%rd75];
	ld.global.f64 	%fd2, [%rd73];
	ld.global.f64 	%fd3, [%rd75+8];
	ld.global.f64 	%fd4, [%rd75+16];
	ld.global.f64 	%fd5, [%rd65];
	fma.rn.f64 	%fd6, %fd2, %fd1, %fd5;
	ld.global.f64 	%fd7, [%rd65+8];
	fma.rn.f64 	%fd8, %fd2, %fd3, %fd7;
	ld.global.f64 	%fd9, [%rd65+16];
	fma.rn.f64 	%fd10, %fd2, %fd4, %fd9;
	mul.lo.s64 	%rd76, %rd63, %rd18;
	add.s64 	%rd77, %rd6, %rd76;
	st.global.f64 	[%rd77], %fd6;
	st.global.f64 	[%rd77+8], %fd8;
	st.global.f64 	[%rd77+16], %fd10;
	add.s64 	%rd120, %rd120, %rd19;
	setp.lt.u64 	%p7, %rd120, %rd44;
	@%p7 bra 	$L__BB18_4;

$L__BB18_31:
	ret;

}
	// .globl	step_x_cuda_kernel_backward
.visible .entry step_x_cuda_kernel_backward(
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_12[56]
)
{
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<81>;
	.reg .b32 	%r<194>;
	.reg .f64 	%fd<64>;
	.reg .b64 	%rd<116>;


	ld.param.v2.u32 	{%r97, %r98}, [step_x_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r99, %r100}, [step_x_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r105, %r106}, [step_x_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r113, %r114}, [step_x_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r121, %r122}, [step_x_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r129, %r130}, [step_x_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r137, %r138}, [step_x_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r145, %r146}, [step_x_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r153, %r154}, [step_x_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r161, %r162}, [step_x_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r169, %r170}, [step_x_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r177, %r178}, [step_x_cuda_kernel_backward_param_10+32];
	ld.param.u64 	%rd66, [step_x_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd64, [step_x_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd62, [step_x_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd60, [step_x_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd58, [step_x_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd56, [step_x_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd55, [step_x_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd53, [step_x_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd52, [step_x_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd51, [step_x_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd50, [step_x_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd49, [step_x_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd47, [step_x_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [step_x_cuda_kernel_backward_param_0+16];
	mov.u32 	%r181, %ntid.x;
	cvt.u64.u32 	%rd1, %r181;
	mov.u32 	%r182, %ctaid.x;
	mul.wide.u32 	%rd68, %r181, %r182;
	mov.u32 	%r183, %tid.x;
	cvt.u64.u32 	%rd69, %r183;
	add.s64 	%rd112, %rd68, %rd69;
	setp.ge.u64 	%p1, %rd112, %rd47;
	@%p1 bra 	$L__BB19_31;

	cvta.to.global.u64 	%rd12, %rd66;
	cvta.to.global.u64 	%rd13, %rd58;
	cvta.to.global.u64 	%rd14, %rd56;
	cvta.to.global.u64 	%rd15, %rd55;
	cvta.to.global.u64 	%rd16, %rd52;
	cvta.to.global.u64 	%rd17, %rd50;
	cvt.s64.s32 	%rd18, %r100;
	cvt.s64.s32 	%rd19, %r99;
	cvt.s64.s32 	%rd20, %r98;
	cvt.s64.s32 	%rd21, %r137;
	cvt.s64.s32 	%rd22, %r145;
	cvt.s64.s32 	%rd23, %r121;
	cvt.s64.s32 	%rd24, %r113;
	cvt.s64.s32 	%rd25, %r177;
	cvt.s64.s32 	%rd26, %r129;
	cvt.s64.s32 	%rd27, %r161;
	cvt.s64.s32 	%rd28, %r169;
	mov.u32 	%r184, %nctaid.x;
	cvt.u64.u32 	%rd70, %r184;
	mul.lo.s64 	%rd29, %rd1, %rd70;
	cvt.s64.s32 	%rd30, %r153;
	cvt.s64.s32 	%rd31, %r105;

$L__BB19_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd113, %rd112;
	@%p2 bra 	$L__BB19_6;

	or.b64  	%rd71, %rd112, %rd18;
	and.b64  	%rd72, %rd71, -4294967296;
	setp.eq.s64 	%p3, %rd72, 0;
	@%p3 bra 	$L__BB19_5;

	div.u64 	%rd113, %rd112, %rd18;
	bra.uni 	$L__BB19_6;

$L__BB19_5:
	cvt.u32.u64 	%r185, %rd18;
	cvt.u32.u64 	%r186, %rd112;
	div.u32 	%r187, %r186, %r185;
	cvt.u64.u32 	%rd113, %r187;

$L__BB19_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB19_10;

	or.b64  	%rd73, %rd113, %rd19;
	and.b64  	%rd74, %rd73, -4294967296;
	setp.eq.s64 	%p5, %rd74, 0;
	@%p5 bra 	$L__BB19_9;

	div.u64 	%rd113, %rd113, %rd19;
	bra.uni 	$L__BB19_10;

$L__BB19_9:
	cvt.u32.u64 	%r188, %rd19;
	cvt.u32.u64 	%r189, %rd113;
	div.u32 	%r190, %r189, %r188;
	cvt.u64.u32 	%rd113, %r190;

$L__BB19_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB19_14;

	or.b64  	%rd75, %rd113, %rd20;
	and.b64  	%rd76, %rd75, -4294967296;
	setp.eq.s64 	%p7, %rd76, 0;
	@%p7 bra 	$L__BB19_13;

	div.u64 	%rd113, %rd113, %rd20;
	bra.uni 	$L__BB19_14;

$L__BB19_13:
	cvt.u32.u64 	%r191, %rd20;
	cvt.u32.u64 	%r192, %rd113;
	div.u32 	%r193, %r192, %r191;
	cvt.u64.u32 	%rd113, %r193;

$L__BB19_14:
	cvt.s64.s32 	%rd109, %r121;
	ld.param.u64 	%rd108, [step_x_cuda_kernel_backward_param_10];
	cvt.s64.s32 	%rd77, %rd113;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd42, %rd77, 0, %p8;
	mul.lo.s64 	%rd78, %rd42, %rd21;
	add.s64 	%rd79, %rd14, %rd78;
	ld.global.s32 	%rd80, [%rd79];
	mul.lo.s64 	%rd81, %rd80, %rd22;
	add.s64 	%rd82, %rd13, %rd81;
	ld.global.s32 	%rd43, [%rd82];
	mul.lo.s64 	%rd44, %rd43, %rd109;
	add.s64 	%rd83, %rd16, %rd44;
	mul.lo.s64 	%rd45, %rd42, %rd24;
	add.s64 	%rd84, %rd17, %rd45;
	ld.global.f64 	%fd1, [%rd83];
	ld.global.f64 	%fd2, [%rd84];
	ld.global.f64 	%fd3, [%rd84+8];
	ld.global.f64 	%fd4, [%rd84+16];
	setp.eq.s64 	%p9, %rd108, 0;
	@%p9 bra 	$L__BB19_16;

	mul.lo.s64 	%rd85, %rd42, %rd25;
	add.s64 	%rd86, %rd12, %rd85;
	ld.global.f64 	%fd21, [%rd86];
	add.f64 	%fd63, %fd21, 0d0000000000000000;
	ld.global.f64 	%fd22, [%rd86+8];
	add.f64 	%fd62, %fd22, 0d0000000000000000;
	ld.global.f64 	%fd23, [%rd86+16];
	add.f64 	%fd61, %fd23, 0d0000000000000000;
	bra.uni 	$L__BB19_18;

$L__BB19_16:
	ld.param.u64 	%rd110, [step_x_cuda_kernel_backward_param_4+8];
	setp.eq.s64 	%p10, %rd110, 0;
	mov.f64 	%fd61, 0d0000000000000000;
	mov.f64 	%fd62, %fd61;
	mov.f64 	%fd63, %fd61;
	@%p10 bra 	$L__BB19_18;

	mul.lo.s64 	%rd87, %rd42, %rd26;
	add.s64 	%rd88, %rd15, %rd87;
	ld.global.f64 	%fd27, [%rd88];
	add.f64 	%fd63, %fd27, 0d0000000000000000;
	ld.global.f64 	%fd28, [%rd88+8];
	add.f64 	%fd62, %fd28, 0d0000000000000000;
	ld.global.f64 	%fd29, [%rd88+16];
	add.f64 	%fd61, %fd29, 0d0000000000000000;

$L__BB19_18:
	add.f64 	%fd14, %fd63, 0d0000000000000000;
	fma.rn.f64 	%fd15, %fd1, %fd14, 0d0000000000000000;
	add.f64 	%fd16, %fd62, 0d0000000000000000;
	fma.rn.f64 	%fd17, %fd1, %fd16, 0d0000000000000000;
	add.f64 	%fd18, %fd61, 0d0000000000000000;
	fma.rn.f64 	%fd19, %fd1, %fd18, 0d0000000000000000;
	mul.f64 	%fd30, %fd3, %fd16;
	fma.rn.f64 	%fd31, %fd2, %fd14, %fd30;
	fma.rn.f64 	%fd32, %fd4, %fd18, %fd31;
	add.f64 	%fd20, %fd32, 0d0000000000000000;
	setp.eq.s64 	%p11, %rd62, 0;
	@%p11 bra 	$L__BB19_20;

	mul.lo.s64 	%rd92, %rd42, %rd27;
	add.s64 	%rd89, %rd62, %rd92;
	// begin inline asm
	{ atom.add.f64 %fd33,[%rd89],%fd15; }

	// end inline asm
	add.s64 	%rd90, %rd89, 8;
	// begin inline asm
	{ atom.add.f64 %fd35,[%rd90],%fd17; }

	// end inline asm
	add.s64 	%rd91, %rd89, 16;
	// begin inline asm
	{ atom.add.f64 %fd37,[%rd91],%fd19; }

	// end inline asm
	bra.uni 	$L__BB19_22;

$L__BB19_20:
	setp.eq.s64 	%p12, %rd51, 0;
	@%p12 bra 	$L__BB19_22;

	add.s64 	%rd93, %rd51, %rd45;
	// begin inline asm
	{ atom.add.f64 %fd39,[%rd93],%fd15; }

	// end inline asm
	add.s64 	%rd94, %rd93, 8;
	// begin inline asm
	{ atom.add.f64 %fd41,[%rd94],%fd17; }

	// end inline asm
	add.s64 	%rd95, %rd93, 16;
	// begin inline asm
	{ atom.add.f64 %fd43,[%rd95],%fd19; }

	// end inline asm

$L__BB19_22:
	setp.eq.s64 	%p13, %rd64, 0;
	@%p13 bra 	$L__BB19_24;

	mul.lo.s64 	%rd97, %rd43, %rd28;
	add.s64 	%rd96, %rd64, %rd97;
	// begin inline asm
	{ atom.add.f64 %fd45,[%rd96],%fd20; }

	// end inline asm
	bra.uni 	$L__BB19_26;

$L__BB19_24:
	setp.eq.s64 	%p14, %rd53, 0;
	@%p14 bra 	$L__BB19_26;

	mul.lo.s64 	%rd111, %rd43, %rd109;
	add.s64 	%rd98, %rd53, %rd111;
	// begin inline asm
	{ atom.add.f64 %fd47,[%rd98],%fd20; }

	// end inline asm

$L__BB19_26:
	setp.eq.s64 	%p15, %rd60, 0;
	@%p15 bra 	$L__BB19_28;

	mul.lo.s64 	%rd102, %rd42, %rd30;
	add.s64 	%rd99, %rd60, %rd102;
	// begin inline asm
	{ atom.add.f64 %fd49,[%rd99],%fd14; }

	// end inline asm
	add.s64 	%rd100, %rd99, 8;
	// begin inline asm
	{ atom.add.f64 %fd51,[%rd100],%fd16; }

	// end inline asm
	add.s64 	%rd101, %rd99, 16;
	// begin inline asm
	{ atom.add.f64 %fd53,[%rd101],%fd18; }

	// end inline asm
	bra.uni 	$L__BB19_30;

$L__BB19_28:
	setp.eq.s64 	%p16, %rd49, 0;
	@%p16 bra 	$L__BB19_30;

	mul.lo.s64 	%rd106, %rd42, %rd31;
	add.s64 	%rd103, %rd49, %rd106;
	// begin inline asm
	{ atom.add.f64 %fd55,[%rd103],%fd14; }

	// end inline asm
	add.s64 	%rd104, %rd103, 8;
	// begin inline asm
	{ atom.add.f64 %fd57,[%rd104],%fd16; }

	// end inline asm
	add.s64 	%rd105, %rd103, 16;
	// begin inline asm
	{ atom.add.f64 %fd59,[%rd105],%fd18; }

	// end inline asm

$L__BB19_30:
	ld.param.u64 	%rd107, [step_x_cuda_kernel_backward_param_0+24];
	add.s64 	%rd112, %rd112, %rd29;
	setp.lt.u64 	%p17, %rd112, %rd107;
	@%p17 bra 	$L__BB19_2;

$L__BB19_31:
	ret;

}
	// .globl	clamp_search_direction_cuda_kernel_forward
.visible .entry clamp_search_direction_cuda_kernel_forward(
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_forward_param_6[56]
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<49>;
	.reg .b32 	%r<126>;
	.reg .f64 	%fd<114>;
	.reg .b64 	%rd<77>;


	ld.param.v2.u32 	{%r61, %r62}, [clamp_search_direction_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r63, %r64}, [clamp_search_direction_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r69, %r70}, [clamp_search_direction_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r77, %r78}, [clamp_search_direction_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r85, %r86}, [clamp_search_direction_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r93, %r94}, [clamp_search_direction_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r101, %r102}, [clamp_search_direction_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r109, %r110}, [clamp_search_direction_cuda_kernel_forward_param_6+32];
	ld.param.u64 	%rd43, [clamp_search_direction_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd41, [clamp_search_direction_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd39, [clamp_search_direction_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd37, [clamp_search_direction_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd35, [clamp_search_direction_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd33, [clamp_search_direction_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd32, [clamp_search_direction_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [clamp_search_direction_cuda_kernel_forward_param_0+16];
	mov.u32 	%r113, %ntid.x;
	cvt.u64.u32 	%rd1, %r113;
	mov.u32 	%r114, %ctaid.x;
	mul.wide.u32 	%rd45, %r113, %r114;
	mov.u32 	%r115, %tid.x;
	cvt.u64.u32 	%rd46, %r115;
	add.s64 	%rd73, %rd45, %rd46;
	setp.ge.u64 	%p1, %rd73, %rd32;
	@%p1 bra 	$L__BB20_17;

	cvta.to.global.u64 	%rd4, %rd43;
	cvta.to.global.u64 	%rd5, %rd41;
	cvta.to.global.u64 	%rd6, %rd39;
	cvta.to.global.u64 	%rd7, %rd37;
	cvta.to.global.u64 	%rd8, %rd35;
	cvta.to.global.u64 	%rd9, %rd33;
	cvt.s64.s32 	%rd10, %r64;
	cvt.s64.s32 	%rd11, %r63;
	cvt.s64.s32 	%rd12, %r62;
	cvt.s64.s32 	%rd13, %r77;
	cvt.s64.s32 	%rd14, %r109;
	cvt.s64.s32 	%rd15, %r85;
	cvt.s64.s32 	%rd16, %r93;
	cvt.s64.s32 	%rd17, %r101;
	cvt.s64.s32 	%rd18, %r69;
	mov.u32 	%r116, %nctaid.x;
	cvt.u64.u32 	%rd47, %r116;
	mul.lo.s64 	%rd19, %rd1, %rd47;

$L__BB20_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd74, %rd73;
	@%p2 bra 	$L__BB20_6;

	or.b64  	%rd48, %rd73, %rd10;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p3, %rd49, 0;
	@%p3 bra 	$L__BB20_5;

	div.u64 	%rd74, %rd73, %rd10;
	bra.uni 	$L__BB20_6;

$L__BB20_5:
	cvt.u32.u64 	%r117, %rd10;
	cvt.u32.u64 	%r118, %rd73;
	div.u32 	%r119, %r118, %r117;
	cvt.u64.u32 	%rd74, %r119;

$L__BB20_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB20_10;

	or.b64  	%rd50, %rd74, %rd11;
	and.b64  	%rd51, %rd50, -4294967296;
	setp.eq.s64 	%p5, %rd51, 0;
	@%p5 bra 	$L__BB20_9;

	div.u64 	%rd74, %rd74, %rd11;
	bra.uni 	$L__BB20_10;

$L__BB20_9:
	cvt.u32.u64 	%r120, %rd11;
	cvt.u32.u64 	%r121, %rd74;
	div.u32 	%r122, %r121, %r120;
	cvt.u64.u32 	%rd74, %r122;

$L__BB20_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB20_14;

	or.b64  	%rd52, %rd74, %rd12;
	and.b64  	%rd53, %rd52, -4294967296;
	setp.eq.s64 	%p7, %rd53, 0;
	@%p7 bra 	$L__BB20_13;

	div.u64 	%rd74, %rd74, %rd12;
	bra.uni 	$L__BB20_14;

$L__BB20_13:
	cvt.u32.u64 	%r123, %rd12;
	cvt.u32.u64 	%r124, %rd74;
	div.u32 	%r125, %r124, %r123;
	cvt.u64.u32 	%rd74, %r125;

$L__BB20_14:
	cvt.s64.s32 	%rd54, %rd74;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd30, %rd54, 0, %p8;
	mul.lo.s64 	%rd55, %rd30, %rd13;
	add.s64 	%rd56, %rd8, %rd55;
	ld.global.s32 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd14;
	add.s64 	%rd59, %rd4, %rd58;
	ld.global.s32 	%rd60, [%rd56+4];
	mul.lo.s64 	%rd61, %rd60, %rd14;
	add.s64 	%rd62, %rd4, %rd61;
	mul.lo.s64 	%rd63, %rd30, %rd15;
	add.s64 	%rd64, %rd7, %rd63;
	ld.global.s32 	%rd65, [%rd64];
	mul.lo.s64 	%rd66, %rd65, %rd16;
	add.s64 	%rd67, %rd6, %rd66;
	mul.lo.s64 	%rd68, %rd65, %rd17;
	add.s64 	%rd69, %rd5, %rd68;
	ld.global.f64 	%fd3, [%rd69];
	ld.global.f64 	%fd4, [%rd67];
	add.f64 	%fd5, %fd4, %fd3;
	ld.global.f64 	%fd6, [%rd69+8];
	add.f64 	%fd7, %fd4, %fd6;
	ld.global.f64 	%fd8, [%rd69+16];
	add.f64 	%fd9, %fd4, %fd8;
	ld.global.f64 	%fd10, [%rd69+24];
	ld.global.f64 	%fd11, [%rd67+24];
	add.f64 	%fd12, %fd11, %fd10;
	ld.global.f64 	%fd13, [%rd69+32];
	add.f64 	%fd14, %fd11, %fd13;
	ld.global.f64 	%fd15, [%rd69+40];
	add.f64 	%fd16, %fd11, %fd15;
	ld.global.f64 	%fd17, [%rd69+48];
	ld.global.f64 	%fd18, [%rd67+48];
	add.f64 	%fd19, %fd18, %fd17;
	ld.global.f64 	%fd20, [%rd69+56];
	add.f64 	%fd21, %fd18, %fd20;
	ld.global.f64 	%fd22, [%rd69+64];
	add.f64 	%fd23, %fd18, %fd22;
	ld.global.f64 	%fd24, [%rd69+72];
	ld.global.f64 	%fd25, [%rd67+72];
	add.f64 	%fd26, %fd25, %fd24;
	ld.global.f64 	%fd27, [%rd69+80];
	add.f64 	%fd28, %fd25, %fd27;
	ld.global.f64 	%fd29, [%rd69+88];
	add.f64 	%fd30, %fd25, %fd29;
	ld.global.f64 	%fd31, [%rd59];
	mov.f64 	%fd32, 0d3FF0000000000000;
	sub.f64 	%fd33, %fd32, %fd31;
	ld.global.f64 	%fd34, [%rd59+8];
	sub.f64 	%fd35, %fd33, %fd34;
	ld.global.f64 	%fd36, [%rd59+16];
	sub.f64 	%fd37, %fd35, %fd36;
	ld.global.f64 	%fd38, [%rd67+8];
	ld.global.f64 	%fd39, [%rd67+16];
	mul.f64 	%fd40, %fd31, %fd11;
	ld.global.f64 	%fd41, [%rd67+32];
	mul.f64 	%fd42, %fd31, %fd41;
	ld.global.f64 	%fd43, [%rd67+40];
	mul.f64 	%fd44, %fd31, %fd43;
	fma.rn.f64 	%fd45, %fd37, %fd4, %fd40;
	fma.rn.f64 	%fd46, %fd37, %fd38, %fd42;
	fma.rn.f64 	%fd47, %fd37, %fd39, %fd44;
	ld.global.f64 	%fd48, [%rd67+56];
	ld.global.f64 	%fd49, [%rd67+64];
	fma.rn.f64 	%fd50, %fd34, %fd18, %fd45;
	fma.rn.f64 	%fd51, %fd34, %fd48, %fd46;
	fma.rn.f64 	%fd52, %fd34, %fd49, %fd47;
	ld.global.f64 	%fd53, [%rd67+80];
	ld.global.f64 	%fd54, [%rd67+88];
	fma.rn.f64 	%fd55, %fd36, %fd25, %fd50;
	fma.rn.f64 	%fd56, %fd36, %fd53, %fd51;
	fma.rn.f64 	%fd57, %fd36, %fd54, %fd52;
	ld.global.f64 	%fd58, [%rd62];
	sub.f64 	%fd59, %fd32, %fd58;
	ld.global.f64 	%fd60, [%rd62+8];
	sub.f64 	%fd61, %fd59, %fd60;
	ld.global.f64 	%fd62, [%rd62+16];
	sub.f64 	%fd63, %fd61, %fd62;
	mul.f64 	%fd64, %fd58, %fd11;
	mul.f64 	%fd65, %fd58, %fd41;
	mul.f64 	%fd66, %fd58, %fd43;
	fma.rn.f64 	%fd67, %fd63, %fd4, %fd64;
	fma.rn.f64 	%fd68, %fd63, %fd38, %fd65;
	fma.rn.f64 	%fd69, %fd63, %fd39, %fd66;
	fma.rn.f64 	%fd70, %fd60, %fd18, %fd67;
	fma.rn.f64 	%fd71, %fd60, %fd48, %fd68;
	fma.rn.f64 	%fd72, %fd60, %fd49, %fd69;
	fma.rn.f64 	%fd73, %fd62, %fd25, %fd70;
	fma.rn.f64 	%fd74, %fd62, %fd53, %fd71;
	fma.rn.f64 	%fd75, %fd62, %fd54, %fd72;
	sub.f64 	%fd76, %fd55, %fd73;
	sub.f64 	%fd77, %fd56, %fd74;
	sub.f64 	%fd78, %fd57, %fd75;
	mul.f64 	%fd79, %fd31, %fd12;
	mul.f64 	%fd80, %fd31, %fd14;
	mul.f64 	%fd81, %fd31, %fd16;
	fma.rn.f64 	%fd82, %fd37, %fd5, %fd79;
	fma.rn.f64 	%fd83, %fd37, %fd7, %fd80;
	fma.rn.f64 	%fd84, %fd37, %fd9, %fd81;
	fma.rn.f64 	%fd85, %fd34, %fd19, %fd82;
	fma.rn.f64 	%fd86, %fd34, %fd21, %fd83;
	fma.rn.f64 	%fd87, %fd34, %fd23, %fd84;
	fma.rn.f64 	%fd88, %fd36, %fd26, %fd85;
	fma.rn.f64 	%fd89, %fd36, %fd28, %fd86;
	fma.rn.f64 	%fd90, %fd36, %fd30, %fd87;
	mul.f64 	%fd91, %fd58, %fd12;
	mul.f64 	%fd92, %fd58, %fd14;
	mul.f64 	%fd93, %fd58, %fd16;
	fma.rn.f64 	%fd94, %fd63, %fd5, %fd91;
	fma.rn.f64 	%fd95, %fd63, %fd7, %fd92;
	fma.rn.f64 	%fd96, %fd63, %fd9, %fd93;
	fma.rn.f64 	%fd97, %fd60, %fd19, %fd94;
	fma.rn.f64 	%fd98, %fd60, %fd21, %fd95;
	fma.rn.f64 	%fd99, %fd60, %fd23, %fd96;
	fma.rn.f64 	%fd100, %fd62, %fd26, %fd97;
	fma.rn.f64 	%fd101, %fd62, %fd28, %fd98;
	fma.rn.f64 	%fd102, %fd62, %fd30, %fd99;
	sub.f64 	%fd103, %fd88, %fd100;
	sub.f64 	%fd104, %fd89, %fd101;
	sub.f64 	%fd105, %fd90, %fd102;
	mul.f64 	%fd106, %fd77, %fd77;
	fma.rn.f64 	%fd107, %fd76, %fd76, %fd106;
	fma.rn.f64 	%fd108, %fd78, %fd78, %fd107;
	sqrt.rn.f64 	%fd109, %fd108;
	mul.f64 	%fd110, %fd104, %fd104;
	fma.rn.f64 	%fd111, %fd103, %fd103, %fd110;
	fma.rn.f64 	%fd112, %fd105, %fd105, %fd111;
	sqrt.rn.f64 	%fd1, %fd112;
	add.f64 	%fd2, %fd109, %fd109;
	setp.leu.f64 	%p9, %fd1, %fd2;
	@%p9 bra 	$L__BB20_16;

	mul.lo.s64 	%rd70, %rd30, %rd18;
	add.s64 	%rd71, %rd9, %rd70;
	div.rn.f64 	%fd113, %fd2, %fd1;
	st.global.f64 	[%rd71], %fd113;

$L__BB20_16:
	ld.param.u64 	%rd72, [clamp_search_direction_cuda_kernel_forward_param_0+24];
	add.s64 	%rd73, %rd73, %rd19;
	setp.lt.u64 	%p10, %rd73, %rd72;
	@%p10 bra 	$L__BB20_2;

$L__BB20_17:
	ret;

}
	// .globl	clamp_search_direction_cuda_kernel_backward
.visible .entry clamp_search_direction_cuda_kernel_backward(
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_12[56]
)
{
	.reg .pred 	%p<68>;
	.reg .b16 	%rs<81>;
	.reg .b32 	%r<194>;
	.reg .f64 	%fd<1548>;
	.reg .b64 	%rd<719>;


	ld.param.v2.u32 	{%r97, %r98}, [clamp_search_direction_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r99, %r100}, [clamp_search_direction_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r105, %r106}, [clamp_search_direction_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r113, %r114}, [clamp_search_direction_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r121, %r122}, [clamp_search_direction_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r129, %r130}, [clamp_search_direction_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r137, %r138}, [clamp_search_direction_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r145, %r146}, [clamp_search_direction_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r153, %r154}, [clamp_search_direction_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r161, %r162}, [clamp_search_direction_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r169, %r170}, [clamp_search_direction_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r177, %r178}, [clamp_search_direction_cuda_kernel_backward_param_12+32];
	ld.param.u64 	%rd71, [clamp_search_direction_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd69, [clamp_search_direction_cuda_kernel_backward_param_11];
	ld.param.u64 	%rd67, [clamp_search_direction_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd65, [clamp_search_direction_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd64, [clamp_search_direction_cuda_kernel_backward_param_6+8];
	ld.param.u64 	%rd63, [clamp_search_direction_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd62, [clamp_search_direction_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd61, [clamp_search_direction_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd60, [clamp_search_direction_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd59, [clamp_search_direction_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd57, [clamp_search_direction_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd55, [clamp_search_direction_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd54, [clamp_search_direction_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd52, [clamp_search_direction_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [clamp_search_direction_cuda_kernel_backward_param_0+16];
	mov.u32 	%r181, %ntid.x;
	cvt.u64.u32 	%rd1, %r181;
	mov.u32 	%r182, %ctaid.x;
	mul.wide.u32 	%rd73, %r181, %r182;
	mov.u32 	%r183, %tid.x;
	cvt.u64.u32 	%rd74, %r183;
	add.s64 	%rd715, %rd73, %rd74;
	setp.ge.u64 	%p1, %rd715, %rd52;
	@%p1 bra 	$L__BB21_131;

	cvta.to.global.u64 	%rd12, %rd65;
	cvta.to.global.u64 	%rd13, %rd63;
	cvta.to.global.u64 	%rd14, %rd61;
	cvta.to.global.u64 	%rd15, %rd59;
	cvta.to.global.u64 	%rd16, %rd57;
	cvta.to.global.u64 	%rd17, %rd55;
	cvta.to.global.u64 	%rd18, %rd54;
	cvt.s64.s32 	%rd19, %r100;
	cvt.s64.s32 	%rd20, %r99;
	cvt.s64.s32 	%rd21, %r98;
	cvt.s64.s32 	%rd22, %r113;
	cvt.s64.s32 	%rd23, %r145;
	cvt.s64.s32 	%rd24, %r121;
	cvt.s64.s32 	%rd25, %r129;
	cvt.s64.s32 	%rd26, %r137;
	cvt.s64.s32 	%rd27, %r153;
	cvt.s64.s32 	%rd28, %r105;
	cvt.s64.s32 	%rd29, %r169;
	cvt.s64.s32 	%rd30, %r161;
	cvt.s64.s32 	%rd31, %r177;
	mov.u32 	%r184, %nctaid.x;
	cvt.u64.u32 	%rd75, %r184;
	mul.lo.s64 	%rd32, %rd1, %rd75;

$L__BB21_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd716, %rd715;
	@%p2 bra 	$L__BB21_6;

	or.b64  	%rd76, %rd715, %rd19;
	and.b64  	%rd77, %rd76, -4294967296;
	setp.eq.s64 	%p3, %rd77, 0;
	@%p3 bra 	$L__BB21_5;

	div.u64 	%rd716, %rd715, %rd19;
	bra.uni 	$L__BB21_6;

$L__BB21_5:
	cvt.u32.u64 	%r185, %rd19;
	cvt.u32.u64 	%r186, %rd715;
	div.u32 	%r187, %r186, %r185;
	cvt.u64.u32 	%rd716, %r187;

$L__BB21_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB21_10;

	or.b64  	%rd78, %rd716, %rd20;
	and.b64  	%rd79, %rd78, -4294967296;
	setp.eq.s64 	%p5, %rd79, 0;
	@%p5 bra 	$L__BB21_9;

	div.u64 	%rd716, %rd716, %rd20;
	bra.uni 	$L__BB21_10;

$L__BB21_9:
	cvt.u32.u64 	%r188, %rd20;
	cvt.u32.u64 	%r189, %rd716;
	div.u32 	%r190, %r189, %r188;
	cvt.u64.u32 	%rd716, %r190;

$L__BB21_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB21_14;

	or.b64  	%rd80, %rd716, %rd21;
	and.b64  	%rd81, %rd80, -4294967296;
	setp.eq.s64 	%p7, %rd81, 0;
	@%p7 bra 	$L__BB21_13;

	div.u64 	%rd716, %rd716, %rd21;
	bra.uni 	$L__BB21_14;

$L__BB21_13:
	cvt.u32.u64 	%r191, %rd21;
	cvt.u32.u64 	%r192, %rd716;
	div.u32 	%r193, %r192, %r191;
	cvt.u64.u32 	%rd716, %r193;

$L__BB21_14:
	cvt.s64.s32 	%rd710, %r145;
	cvt.s64.s32 	%rd82, %rd716;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd43, %rd82, 0, %p8;
	mul.lo.s64 	%rd83, %rd43, %rd22;
	add.s64 	%rd84, %rd17, %rd83;
	ld.global.s32 	%rd44, [%rd84];
	mul.lo.s64 	%rd45, %rd44, %rd710;
	add.s64 	%rd85, %rd13, %rd45;
	ld.global.s32 	%rd46, [%rd84+4];
	mul.lo.s64 	%rd47, %rd46, %rd710;
	add.s64 	%rd86, %rd13, %rd47;
	mul.lo.s64 	%rd87, %rd43, %rd24;
	add.s64 	%rd88, %rd16, %rd87;
	ld.global.s32 	%rd48, [%rd88];
	mul.lo.s64 	%rd49, %rd48, %rd25;
	add.s64 	%rd89, %rd15, %rd49;
	mul.lo.s64 	%rd50, %rd48, %rd26;
	add.s64 	%rd90, %rd14, %rd50;
	ld.global.f64 	%fd134, [%rd90];
	ld.global.f64 	%fd2, [%rd89];
	add.f64 	%fd3, %fd2, %fd134;
	ld.global.f64 	%fd135, [%rd90+8];
	add.f64 	%fd4, %fd2, %fd135;
	ld.global.f64 	%fd136, [%rd90+16];
	add.f64 	%fd5, %fd2, %fd136;
	ld.global.f64 	%fd137, [%rd90+24];
	ld.global.f64 	%fd6, [%rd89+24];
	add.f64 	%fd7, %fd6, %fd137;
	ld.global.f64 	%fd138, [%rd90+32];
	add.f64 	%fd8, %fd6, %fd138;
	ld.global.f64 	%fd139, [%rd90+40];
	add.f64 	%fd9, %fd6, %fd139;
	ld.global.f64 	%fd140, [%rd90+48];
	ld.global.f64 	%fd10, [%rd89+48];
	add.f64 	%fd11, %fd10, %fd140;
	ld.global.f64 	%fd141, [%rd90+56];
	add.f64 	%fd12, %fd10, %fd141;
	ld.global.f64 	%fd142, [%rd90+64];
	add.f64 	%fd13, %fd10, %fd142;
	ld.global.f64 	%fd143, [%rd90+72];
	ld.global.f64 	%fd14, [%rd89+72];
	add.f64 	%fd15, %fd14, %fd143;
	ld.global.f64 	%fd144, [%rd90+80];
	add.f64 	%fd16, %fd14, %fd144;
	ld.global.f64 	%fd145, [%rd90+88];
	add.f64 	%fd17, %fd14, %fd145;
	ld.global.f64 	%fd18, [%rd85];
	mov.f64 	%fd146, 0d3FF0000000000000;
	sub.f64 	%fd147, %fd146, %fd18;
	ld.global.f64 	%fd19, [%rd85+8];
	sub.f64 	%fd148, %fd147, %fd19;
	ld.global.f64 	%fd20, [%rd85+16];
	sub.f64 	%fd21, %fd148, %fd20;
	ld.global.f64 	%fd22, [%rd89+8];
	ld.global.f64 	%fd23, [%rd89+16];
	mul.f64 	%fd149, %fd18, %fd6;
	ld.global.f64 	%fd24, [%rd89+32];
	mul.f64 	%fd150, %fd18, %fd24;
	ld.global.f64 	%fd25, [%rd89+40];
	mul.f64 	%fd151, %fd18, %fd25;
	fma.rn.f64 	%fd152, %fd21, %fd2, %fd149;
	fma.rn.f64 	%fd153, %fd21, %fd22, %fd150;
	fma.rn.f64 	%fd154, %fd21, %fd23, %fd151;
	ld.global.f64 	%fd26, [%rd89+56];
	ld.global.f64 	%fd27, [%rd89+64];
	fma.rn.f64 	%fd155, %fd19, %fd10, %fd152;
	fma.rn.f64 	%fd156, %fd19, %fd26, %fd153;
	fma.rn.f64 	%fd157, %fd19, %fd27, %fd154;
	ld.global.f64 	%fd28, [%rd89+80];
	ld.global.f64 	%fd29, [%rd89+88];
	fma.rn.f64 	%fd158, %fd20, %fd14, %fd155;
	fma.rn.f64 	%fd159, %fd20, %fd28, %fd156;
	fma.rn.f64 	%fd160, %fd20, %fd29, %fd157;
	ld.global.f64 	%fd30, [%rd86];
	sub.f64 	%fd161, %fd146, %fd30;
	ld.global.f64 	%fd31, [%rd86+8];
	sub.f64 	%fd162, %fd161, %fd31;
	ld.global.f64 	%fd32, [%rd86+16];
	sub.f64 	%fd33, %fd162, %fd32;
	mul.f64 	%fd163, %fd30, %fd6;
	mul.f64 	%fd164, %fd30, %fd24;
	mul.f64 	%fd165, %fd30, %fd25;
	fma.rn.f64 	%fd166, %fd33, %fd2, %fd163;
	fma.rn.f64 	%fd167, %fd33, %fd22, %fd164;
	fma.rn.f64 	%fd168, %fd33, %fd23, %fd165;
	fma.rn.f64 	%fd169, %fd31, %fd10, %fd166;
	fma.rn.f64 	%fd170, %fd31, %fd26, %fd167;
	fma.rn.f64 	%fd171, %fd31, %fd27, %fd168;
	fma.rn.f64 	%fd172, %fd32, %fd14, %fd169;
	fma.rn.f64 	%fd173, %fd32, %fd28, %fd170;
	fma.rn.f64 	%fd174, %fd32, %fd29, %fd171;
	sub.f64 	%fd34, %fd158, %fd172;
	sub.f64 	%fd35, %fd159, %fd173;
	sub.f64 	%fd36, %fd160, %fd174;
	mul.f64 	%fd175, %fd18, %fd7;
	mul.f64 	%fd176, %fd18, %fd8;
	mul.f64 	%fd177, %fd18, %fd9;
	fma.rn.f64 	%fd178, %fd21, %fd3, %fd175;
	fma.rn.f64 	%fd179, %fd21, %fd4, %fd176;
	fma.rn.f64 	%fd180, %fd21, %fd5, %fd177;
	fma.rn.f64 	%fd181, %fd19, %fd11, %fd178;
	fma.rn.f64 	%fd182, %fd19, %fd12, %fd179;
	fma.rn.f64 	%fd183, %fd19, %fd13, %fd180;
	fma.rn.f64 	%fd184, %fd20, %fd15, %fd181;
	fma.rn.f64 	%fd185, %fd20, %fd16, %fd182;
	fma.rn.f64 	%fd186, %fd20, %fd17, %fd183;
	mul.f64 	%fd187, %fd30, %fd7;
	mul.f64 	%fd188, %fd30, %fd8;
	mul.f64 	%fd189, %fd30, %fd9;
	fma.rn.f64 	%fd190, %fd33, %fd3, %fd187;
	fma.rn.f64 	%fd191, %fd33, %fd4, %fd188;
	fma.rn.f64 	%fd192, %fd33, %fd5, %fd189;
	fma.rn.f64 	%fd193, %fd31, %fd11, %fd190;
	fma.rn.f64 	%fd194, %fd31, %fd12, %fd191;
	fma.rn.f64 	%fd195, %fd31, %fd13, %fd192;
	fma.rn.f64 	%fd196, %fd32, %fd15, %fd193;
	fma.rn.f64 	%fd197, %fd32, %fd16, %fd194;
	fma.rn.f64 	%fd198, %fd32, %fd17, %fd195;
	sub.f64 	%fd37, %fd184, %fd196;
	sub.f64 	%fd38, %fd185, %fd197;
	sub.f64 	%fd39, %fd186, %fd198;
	mul.f64 	%fd199, %fd35, %fd35;
	fma.rn.f64 	%fd200, %fd34, %fd34, %fd199;
	fma.rn.f64 	%fd201, %fd36, %fd36, %fd200;
	sqrt.rn.f64 	%fd40, %fd201;
	mul.f64 	%fd202, %fd38, %fd38;
	fma.rn.f64 	%fd203, %fd37, %fd37, %fd202;
	fma.rn.f64 	%fd204, %fd39, %fd39, %fd203;
	sqrt.rn.f64 	%fd41, %fd204;
	add.f64 	%fd42, %fd40, %fd40;
	setp.leu.f64 	%p9, %fd41, %fd42;
	@%p9 bra 	$L__BB21_16;

	div.rn.f64 	%fd1538, %fd42, %fd41;

$L__BB21_16:
	mov.f64 	%fd1542, 0d0000000000000000;
	mov.f64 	%fd1540, %fd1542;
	mov.f64 	%fd1541, %fd1542;
	@%p9 bra 	$L__BB21_22;

	ld.param.u64 	%rd713, [clamp_search_direction_cuda_kernel_backward_param_7];
	setp.eq.s64 	%p11, %rd713, 0;
	@%p11 bra 	$L__BB21_19;

	mul.lo.s64 	%rd91, %rd43, %rd27;
	add.s64 	%rd92, %rd12, %rd91;
	ld.global.f64 	%fd207, [%rd92];
	add.f64 	%fd1539, %fd207, 0d0000000000000000;
	bra.uni 	$L__BB21_21;

$L__BB21_19:
	ld.param.u64 	%rd714, [clamp_search_direction_cuda_kernel_backward_param_1+8];
	setp.eq.s64 	%p12, %rd714, 0;
	mov.f64 	%fd1539, 0d0000000000000000;
	@%p12 bra 	$L__BB21_21;

	mul.lo.s64 	%rd93, %rd43, %rd28;
	add.s64 	%rd94, %rd18, %rd93;
	ld.global.f64 	%fd209, [%rd94];
	add.f64 	%fd1539, %fd209, 0d0000000000000000;

$L__BB21_21:
	div.rn.f64 	%fd210, %fd1539, %fd41;
	add.f64 	%fd211, %fd210, 0d0000000000000000;
	mov.f64 	%fd212, 0d0000000000000000;
	mul.f64 	%fd213, %fd1538, %fd1539;
	div.rn.f64 	%fd214, %fd213, %fd41;
	sub.f64 	%fd1541, %fd212, %fd214;
	fma.rn.f64 	%fd1540, %fd211, 0d4000000000000000, 0d0000000000000000;

$L__BB21_22:
	setp.leu.f64 	%p13, %fd41, 0d0000000000000000;
	mov.f64 	%fd1543, %fd1542;
	mov.f64 	%fd1544, %fd1542;
	@%p13 bra 	$L__BB21_24;

	div.rn.f64 	%fd218, %fd37, %fd41;
	div.rn.f64 	%fd219, %fd38, %fd41;
	div.rn.f64 	%fd220, %fd39, %fd41;
	fma.rn.f64 	%fd1544, %fd218, %fd1541, 0d0000000000000000;
	fma.rn.f64 	%fd1543, %fd219, %fd1541, 0d0000000000000000;
	fma.rn.f64 	%fd1542, %fd220, %fd1541, 0d0000000000000000;

$L__BB21_24:
	setp.leu.f64 	%p14, %fd40, 0d0000000000000000;
	mov.f64 	%fd223, 0d0000000000000000;
	mov.f64 	%fd1545, %fd223;
	mov.f64 	%fd1546, %fd223;
	mov.f64 	%fd1547, %fd223;
	@%p14 bra 	$L__BB21_26;

	add.f64 	%fd224, %fd1540, 0d0000000000000000;
	div.rn.f64 	%fd225, %fd34, %fd40;
	div.rn.f64 	%fd226, %fd35, %fd40;
	div.rn.f64 	%fd227, %fd36, %fd40;
	fma.rn.f64 	%fd1547, %fd225, %fd224, 0d0000000000000000;
	fma.rn.f64 	%fd1546, %fd226, %fd224, 0d0000000000000000;
	fma.rn.f64 	%fd1545, %fd227, %fd224, 0d0000000000000000;

$L__BB21_26:
	mov.f64 	%fd1536, 0d3FF0000000000000;
	sub.f64 	%fd1535, %fd1536, %fd18;
	sub.f64 	%fd1534, %fd1535, %fd19;
	sub.f64 	%fd1533, %fd1534, %fd20;
	sub.f64 	%fd1532, %fd1536, %fd30;
	sub.f64 	%fd1531, %fd1532, %fd31;
	sub.f64 	%fd1530, %fd1531, %fd32;
	sub.f64 	%fd229, %fd223, %fd1543;
	add.f64 	%fd230, %fd229, 0d0000000000000000;
	sub.f64 	%fd231, %fd223, %fd1544;
	add.f64 	%fd232, %fd231, 0d0000000000000000;
	sub.f64 	%fd233, %fd223, %fd1542;
	add.f64 	%fd234, %fd233, 0d0000000000000000;
	fma.rn.f64 	%fd235, %fd32, %fd232, 0d0000000000000000;
	fma.rn.f64 	%fd236, %fd32, %fd230, 0d0000000000000000;
	fma.rn.f64 	%fd237, %fd32, %fd234, 0d0000000000000000;
	mul.f64 	%fd238, %fd16, %fd230;
	fma.rn.f64 	%fd239, %fd15, %fd232, %fd238;
	fma.rn.f64 	%fd240, %fd17, %fd234, %fd239;
	fma.rn.f64 	%fd241, %fd31, %fd232, 0d0000000000000000;
	fma.rn.f64 	%fd242, %fd31, %fd230, 0d0000000000000000;
	fma.rn.f64 	%fd243, %fd31, %fd234, 0d0000000000000000;
	mul.f64 	%fd244, %fd12, %fd230;
	fma.rn.f64 	%fd245, %fd11, %fd232, %fd244;
	fma.rn.f64 	%fd246, %fd13, %fd234, %fd245;
	fma.rn.f64 	%fd247, %fd30, %fd232, 0d0000000000000000;
	fma.rn.f64 	%fd248, %fd30, %fd230, 0d0000000000000000;
	fma.rn.f64 	%fd249, %fd30, %fd234, 0d0000000000000000;
	add.f64 	%fd250, %fd240, 0d0000000000000000;
	mul.f64 	%fd251, %fd8, %fd230;
	fma.rn.f64 	%fd252, %fd7, %fd232, %fd251;
	fma.rn.f64 	%fd253, %fd9, %fd234, %fd252;
	fma.rn.f64 	%fd254, %fd1530, %fd232, 0d0000000000000000;
	fma.rn.f64 	%fd255, %fd1530, %fd230, 0d0000000000000000;
	fma.rn.f64 	%fd256, %fd1530, %fd234, 0d0000000000000000;
	add.f64 	%fd257, %fd246, 0d0000000000000000;
	add.f64 	%fd258, %fd253, 0d0000000000000000;
	mul.f64 	%fd259, %fd4, %fd230;
	fma.rn.f64 	%fd260, %fd3, %fd232, %fd259;
	fma.rn.f64 	%fd261, %fd5, %fd234, %fd260;
	add.f64 	%fd262, %fd261, 0d0000000000000000;
	sub.f64 	%fd263, %fd223, %fd262;
	add.f64 	%fd264, %fd1544, 0d0000000000000000;
	fma.rn.f64 	%fd265, %fd20, %fd264, %fd235;
	add.f64 	%fd266, %fd1543, 0d0000000000000000;
	fma.rn.f64 	%fd267, %fd20, %fd266, %fd236;
	add.f64 	%fd268, %fd1542, 0d0000000000000000;
	fma.rn.f64 	%fd269, %fd20, %fd268, %fd237;
	mul.f64 	%fd270, %fd16, %fd266;
	fma.rn.f64 	%fd271, %fd15, %fd264, %fd270;
	fma.rn.f64 	%fd272, %fd17, %fd268, %fd271;
	fma.rn.f64 	%fd64, %fd19, %fd264, %fd241;
	fma.rn.f64 	%fd65, %fd19, %fd266, %fd242;
	fma.rn.f64 	%fd66, %fd19, %fd268, %fd243;
	add.f64 	%fd273, %fd258, %fd263;
	mul.f64 	%fd274, %fd12, %fd266;
	fma.rn.f64 	%fd275, %fd11, %fd264, %fd274;
	fma.rn.f64 	%fd276, %fd13, %fd268, %fd275;
	fma.rn.f64 	%fd67, %fd18, %fd264, %fd247;
	fma.rn.f64 	%fd68, %fd18, %fd266, %fd248;
	fma.rn.f64 	%fd69, %fd18, %fd268, %fd249;
	add.f64 	%fd277, %fd272, 0d0000000000000000;
	mul.f64 	%fd278, %fd8, %fd266;
	fma.rn.f64 	%fd279, %fd7, %fd264, %fd278;
	fma.rn.f64 	%fd280, %fd9, %fd268, %fd279;
	fma.rn.f64 	%fd70, %fd1533, %fd264, %fd254;
	fma.rn.f64 	%fd71, %fd1533, %fd266, %fd255;
	fma.rn.f64 	%fd72, %fd1533, %fd268, %fd256;
	add.f64 	%fd281, %fd276, 0d0000000000000000;
	add.f64 	%fd282, %fd280, 0d0000000000000000;
	mul.f64 	%fd283, %fd4, %fd266;
	fma.rn.f64 	%fd284, %fd3, %fd264, %fd283;
	fma.rn.f64 	%fd285, %fd5, %fd268, %fd284;
	add.f64 	%fd286, %fd285, 0d0000000000000000;
	sub.f64 	%fd287, %fd223, %fd286;
	add.f64 	%fd288, %fd282, %fd287;
	sub.f64 	%fd289, %fd223, %fd1546;
	add.f64 	%fd290, %fd289, 0d0000000000000000;
	sub.f64 	%fd291, %fd223, %fd1547;
	add.f64 	%fd292, %fd291, 0d0000000000000000;
	sub.f64 	%fd293, %fd223, %fd1545;
	add.f64 	%fd294, %fd293, 0d0000000000000000;
	fma.rn.f64 	%fd295, %fd32, %fd292, 0d0000000000000000;
	fma.rn.f64 	%fd296, %fd32, %fd290, 0d0000000000000000;
	fma.rn.f64 	%fd297, %fd32, %fd294, 0d0000000000000000;
	add.f64 	%fd298, %fd250, %fd263;
	mul.f64 	%fd299, %fd28, %fd290;
	fma.rn.f64 	%fd300, %fd14, %fd292, %fd299;
	fma.rn.f64 	%fd301, %fd29, %fd294, %fd300;
	add.f64 	%fd302, %fd301, 0d0000000000000000;
	fma.rn.f64 	%fd303, %fd31, %fd292, 0d0000000000000000;
	fma.rn.f64 	%fd304, %fd31, %fd290, 0d0000000000000000;
	fma.rn.f64 	%fd305, %fd31, %fd294, 0d0000000000000000;
	add.f64 	%fd306, %fd257, %fd263;
	add.f64 	%fd307, %fd298, %fd302;
	mul.f64 	%fd308, %fd26, %fd290;
	fma.rn.f64 	%fd309, %fd10, %fd292, %fd308;
	fma.rn.f64 	%fd310, %fd27, %fd294, %fd309;
	add.f64 	%fd311, %fd310, 0d0000000000000000;
	fma.rn.f64 	%fd312, %fd30, %fd292, 0d0000000000000000;
	fma.rn.f64 	%fd313, %fd30, %fd290, 0d0000000000000000;
	fma.rn.f64 	%fd314, %fd30, %fd294, 0d0000000000000000;
	add.f64 	%fd315, %fd306, %fd311;
	mul.f64 	%fd316, %fd24, %fd290;
	fma.rn.f64 	%fd317, %fd6, %fd292, %fd316;
	fma.rn.f64 	%fd318, %fd25, %fd294, %fd317;
	add.f64 	%fd319, %fd318, 0d0000000000000000;
	fma.rn.f64 	%fd320, %fd1530, %fd292, 0d0000000000000000;
	fma.rn.f64 	%fd321, %fd1530, %fd290, 0d0000000000000000;
	fma.rn.f64 	%fd322, %fd1530, %fd294, 0d0000000000000000;
	add.f64 	%fd323, %fd273, %fd319;
	mul.f64 	%fd324, %fd22, %fd290;
	fma.rn.f64 	%fd325, %fd2, %fd292, %fd324;
	fma.rn.f64 	%fd326, %fd23, %fd294, %fd325;
	add.f64 	%fd327, %fd326, 0d0000000000000000;
	sub.f64 	%fd328, %fd223, %fd327;
	add.f64 	%fd329, %fd1547, 0d0000000000000000;
	fma.rn.f64 	%fd73, %fd20, %fd329, %fd295;
	add.f64 	%fd330, %fd1546, 0d0000000000000000;
	fma.rn.f64 	%fd74, %fd20, %fd330, %fd296;
	add.f64 	%fd331, %fd1545, 0d0000000000000000;
	fma.rn.f64 	%fd75, %fd20, %fd331, %fd297;
	add.f64 	%fd332, %fd277, %fd287;
	mul.f64 	%fd333, %fd28, %fd330;
	fma.rn.f64 	%fd334, %fd14, %fd329, %fd333;
	fma.rn.f64 	%fd335, %fd29, %fd331, %fd334;
	add.f64 	%fd336, %fd335, 0d0000000000000000;
	fma.rn.f64 	%fd76, %fd19, %fd329, %fd303;
	fma.rn.f64 	%fd77, %fd19, %fd330, %fd304;
	fma.rn.f64 	%fd78, %fd19, %fd331, %fd305;
	add.f64 	%fd337, %fd281, %fd287;
	mul.f64 	%fd338, %fd26, %fd330;
	fma.rn.f64 	%fd339, %fd10, %fd329, %fd338;
	fma.rn.f64 	%fd340, %fd27, %fd331, %fd339;
	add.f64 	%fd341, %fd340, 0d0000000000000000;
	fma.rn.f64 	%fd79, %fd18, %fd329, %fd312;
	fma.rn.f64 	%fd80, %fd18, %fd330, %fd313;
	fma.rn.f64 	%fd81, %fd18, %fd331, %fd314;
	mul.f64 	%fd342, %fd24, %fd330;
	fma.rn.f64 	%fd343, %fd6, %fd329, %fd342;
	fma.rn.f64 	%fd344, %fd25, %fd331, %fd343;
	add.f64 	%fd345, %fd344, 0d0000000000000000;
	fma.rn.f64 	%fd82, %fd1533, %fd329, %fd320;
	fma.rn.f64 	%fd83, %fd1533, %fd330, %fd321;
	fma.rn.f64 	%fd84, %fd1533, %fd331, %fd322;
	add.f64 	%fd85, %fd307, %fd328;
	add.f64 	%fd86, %fd315, %fd328;
	add.f64 	%fd87, %fd323, %fd328;
	add.f64 	%fd346, %fd332, %fd336;
	add.f64 	%fd347, %fd337, %fd341;
	add.f64 	%fd348, %fd288, %fd345;
	mul.f64 	%fd349, %fd22, %fd330;
	fma.rn.f64 	%fd350, %fd2, %fd329, %fd349;
	fma.rn.f64 	%fd351, %fd23, %fd331, %fd350;
	add.f64 	%fd352, %fd351, 0d0000000000000000;
	sub.f64 	%fd353, %fd223, %fd352;
	add.f64 	%fd88, %fd346, %fd353;
	add.f64 	%fd89, %fd347, %fd353;
	add.f64 	%fd90, %fd348, %fd353;
	add.f64 	%fd91, %fd265, 0d0000000000000000;
	add.f64 	%fd92, %fd267, 0d0000000000000000;
	add.f64 	%fd93, %fd269, 0d0000000000000000;
	setp.eq.s64 	%p15, %rd69, 0;
	@%p15 bra 	$L__BB21_28;

	mul.lo.s64 	%rd107, %rd48, %rd29;
	add.s64 	%rd95, %rd69, %rd107;
	mov.f64 	%fd375, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd354,[%rd95],%fd375; }

	// end inline asm
	add.s64 	%rd96, %rd95, 8;
	// begin inline asm
	{ atom.add.f64 %fd356,[%rd96],%fd375; }

	// end inline asm
	add.s64 	%rd97, %rd95, 16;
	// begin inline asm
	{ atom.add.f64 %fd358,[%rd97],%fd375; }

	// end inline asm
	add.s64 	%rd98, %rd95, 24;
	// begin inline asm
	{ atom.add.f64 %fd360,[%rd98],%fd375; }

	// end inline asm
	add.s64 	%rd99, %rd95, 32;
	// begin inline asm
	{ atom.add.f64 %fd362,[%rd99],%fd375; }

	// end inline asm
	add.s64 	%rd100, %rd95, 40;
	// begin inline asm
	{ atom.add.f64 %fd364,[%rd100],%fd375; }

	// end inline asm
	add.s64 	%rd101, %rd95, 48;
	// begin inline asm
	{ atom.add.f64 %fd366,[%rd101],%fd375; }

	// end inline asm
	add.s64 	%rd102, %rd95, 56;
	// begin inline asm
	{ atom.add.f64 %fd368,[%rd102],%fd375; }

	// end inline asm
	add.s64 	%rd103, %rd95, 64;
	// begin inline asm
	{ atom.add.f64 %fd370,[%rd103],%fd375; }

	// end inline asm
	add.s64 	%rd104, %rd95, 72;
	// begin inline asm
	{ atom.add.f64 %fd372,[%rd104],%fd375; }

	// end inline asm
	add.s64 	%rd105, %rd95, 80;
	// begin inline asm
	{ atom.add.f64 %fd374,[%rd105],%fd375; }

	// end inline asm
	add.s64 	%rd106, %rd95, 88;
	// begin inline asm
	{ atom.add.f64 %fd376,[%rd106],%fd93; }

	// end inline asm
	bra.uni 	$L__BB21_30;

$L__BB21_28:
	setp.eq.s64 	%p16, %rd62, 0;
	@%p16 bra 	$L__BB21_30;

	add.s64 	%rd108, %rd62, %rd50;
	mov.f64 	%fd399, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd378,[%rd108],%fd399; }

	// end inline asm
	add.s64 	%rd109, %rd108, 8;
	// begin inline asm
	{ atom.add.f64 %fd380,[%rd109],%fd399; }

	// end inline asm
	add.s64 	%rd110, %rd108, 16;
	// begin inline asm
	{ atom.add.f64 %fd382,[%rd110],%fd399; }

	// end inline asm
	add.s64 	%rd111, %rd108, 24;
	// begin inline asm
	{ atom.add.f64 %fd384,[%rd111],%fd399; }

	// end inline asm
	add.s64 	%rd112, %rd108, 32;
	// begin inline asm
	{ atom.add.f64 %fd386,[%rd112],%fd399; }

	// end inline asm
	add.s64 	%rd113, %rd108, 40;
	// begin inline asm
	{ atom.add.f64 %fd388,[%rd113],%fd399; }

	// end inline asm
	add.s64 	%rd114, %rd108, 48;
	// begin inline asm
	{ atom.add.f64 %fd390,[%rd114],%fd399; }

	// end inline asm
	add.s64 	%rd115, %rd108, 56;
	// begin inline asm
	{ atom.add.f64 %fd392,[%rd115],%fd399; }

	// end inline asm
	add.s64 	%rd116, %rd108, 64;
	// begin inline asm
	{ atom.add.f64 %fd394,[%rd116],%fd399; }

	// end inline asm
	add.s64 	%rd117, %rd108, 72;
	// begin inline asm
	{ atom.add.f64 %fd396,[%rd117],%fd399; }

	// end inline asm
	add.s64 	%rd118, %rd108, 80;
	// begin inline asm
	{ atom.add.f64 %fd398,[%rd118],%fd399; }

	// end inline asm
	add.s64 	%rd119, %rd108, 88;
	// begin inline asm
	{ atom.add.f64 %fd400,[%rd119],%fd93; }

	// end inline asm

$L__BB21_30:
	add.f64 	%fd94, %fd93, %fd73;
	@%p15 bra 	$L__BB21_32;

	mul.lo.s64 	%rd132, %rd48, %rd29;
	add.s64 	%rd120, %rd69, %rd132;
	mov.f64 	%fd425, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd402,[%rd120],%fd425; }

	// end inline asm
	add.s64 	%rd121, %rd120, 8;
	// begin inline asm
	{ atom.add.f64 %fd404,[%rd121],%fd425; }

	// end inline asm
	add.s64 	%rd122, %rd120, 16;
	// begin inline asm
	{ atom.add.f64 %fd406,[%rd122],%fd425; }

	// end inline asm
	add.s64 	%rd123, %rd120, 24;
	// begin inline asm
	{ atom.add.f64 %fd408,[%rd123],%fd425; }

	// end inline asm
	add.s64 	%rd124, %rd120, 32;
	// begin inline asm
	{ atom.add.f64 %fd410,[%rd124],%fd425; }

	// end inline asm
	add.s64 	%rd125, %rd120, 40;
	// begin inline asm
	{ atom.add.f64 %fd412,[%rd125],%fd425; }

	// end inline asm
	add.s64 	%rd126, %rd120, 48;
	// begin inline asm
	{ atom.add.f64 %fd414,[%rd126],%fd425; }

	// end inline asm
	add.s64 	%rd127, %rd120, 56;
	// begin inline asm
	{ atom.add.f64 %fd416,[%rd127],%fd425; }

	// end inline asm
	add.s64 	%rd128, %rd120, 64;
	// begin inline asm
	{ atom.add.f64 %fd418,[%rd128],%fd425; }

	// end inline asm
	add.s64 	%rd129, %rd120, 72;
	// begin inline asm
	{ atom.add.f64 %fd420,[%rd129],%fd425; }

	// end inline asm
	add.s64 	%rd130, %rd120, 80;
	// begin inline asm
	{ atom.add.f64 %fd422,[%rd130],%fd92; }

	// end inline asm
	add.s64 	%rd131, %rd120, 88;
	// begin inline asm
	{ atom.add.f64 %fd424,[%rd131],%fd425; }

	// end inline asm
	bra.uni 	$L__BB21_34;

$L__BB21_32:
	setp.eq.s64 	%p18, %rd62, 0;
	@%p18 bra 	$L__BB21_34;

	add.s64 	%rd133, %rd62, %rd50;
	mov.f64 	%fd449, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd426,[%rd133],%fd449; }

	// end inline asm
	add.s64 	%rd134, %rd133, 8;
	// begin inline asm
	{ atom.add.f64 %fd428,[%rd134],%fd449; }

	// end inline asm
	add.s64 	%rd135, %rd133, 16;
	// begin inline asm
	{ atom.add.f64 %fd430,[%rd135],%fd449; }

	// end inline asm
	add.s64 	%rd136, %rd133, 24;
	// begin inline asm
	{ atom.add.f64 %fd432,[%rd136],%fd449; }

	// end inline asm
	add.s64 	%rd137, %rd133, 32;
	// begin inline asm
	{ atom.add.f64 %fd434,[%rd137],%fd449; }

	// end inline asm
	add.s64 	%rd138, %rd133, 40;
	// begin inline asm
	{ atom.add.f64 %fd436,[%rd138],%fd449; }

	// end inline asm
	add.s64 	%rd139, %rd133, 48;
	// begin inline asm
	{ atom.add.f64 %fd438,[%rd139],%fd449; }

	// end inline asm
	add.s64 	%rd140, %rd133, 56;
	// begin inline asm
	{ atom.add.f64 %fd440,[%rd140],%fd449; }

	// end inline asm
	add.s64 	%rd141, %rd133, 64;
	// begin inline asm
	{ atom.add.f64 %fd442,[%rd141],%fd449; }

	// end inline asm
	add.s64 	%rd142, %rd133, 72;
	// begin inline asm
	{ atom.add.f64 %fd444,[%rd142],%fd449; }

	// end inline asm
	add.s64 	%rd143, %rd133, 80;
	// begin inline asm
	{ atom.add.f64 %fd446,[%rd143],%fd92; }

	// end inline asm
	add.s64 	%rd144, %rd133, 88;
	// begin inline asm
	{ atom.add.f64 %fd448,[%rd144],%fd449; }

	// end inline asm

$L__BB21_34:
	add.f64 	%fd95, %fd92, %fd94;
	@%p15 bra 	$L__BB21_36;

	mul.lo.s64 	%rd157, %rd48, %rd29;
	add.s64 	%rd145, %rd69, %rd157;
	mov.f64 	%fd473, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd450,[%rd145],%fd473; }

	// end inline asm
	add.s64 	%rd146, %rd145, 8;
	// begin inline asm
	{ atom.add.f64 %fd452,[%rd146],%fd473; }

	// end inline asm
	add.s64 	%rd147, %rd145, 16;
	// begin inline asm
	{ atom.add.f64 %fd454,[%rd147],%fd473; }

	// end inline asm
	add.s64 	%rd148, %rd145, 24;
	// begin inline asm
	{ atom.add.f64 %fd456,[%rd148],%fd473; }

	// end inline asm
	add.s64 	%rd149, %rd145, 32;
	// begin inline asm
	{ atom.add.f64 %fd458,[%rd149],%fd473; }

	// end inline asm
	add.s64 	%rd150, %rd145, 40;
	// begin inline asm
	{ atom.add.f64 %fd460,[%rd150],%fd473; }

	// end inline asm
	add.s64 	%rd151, %rd145, 48;
	// begin inline asm
	{ atom.add.f64 %fd462,[%rd151],%fd473; }

	// end inline asm
	add.s64 	%rd152, %rd145, 56;
	// begin inline asm
	{ atom.add.f64 %fd464,[%rd152],%fd473; }

	// end inline asm
	add.s64 	%rd153, %rd145, 64;
	// begin inline asm
	{ atom.add.f64 %fd466,[%rd153],%fd473; }

	// end inline asm
	add.s64 	%rd154, %rd145, 72;
	// begin inline asm
	{ atom.add.f64 %fd468,[%rd154],%fd91; }

	// end inline asm
	add.s64 	%rd155, %rd145, 80;
	// begin inline asm
	{ atom.add.f64 %fd470,[%rd155],%fd473; }

	// end inline asm
	add.s64 	%rd156, %rd145, 88;
	// begin inline asm
	{ atom.add.f64 %fd472,[%rd156],%fd473; }

	// end inline asm
	bra.uni 	$L__BB21_38;

$L__BB21_36:
	setp.eq.s64 	%p20, %rd62, 0;
	@%p20 bra 	$L__BB21_38;

	add.s64 	%rd158, %rd62, %rd50;
	mov.f64 	%fd497, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd474,[%rd158],%fd497; }

	// end inline asm
	add.s64 	%rd159, %rd158, 8;
	// begin inline asm
	{ atom.add.f64 %fd476,[%rd159],%fd497; }

	// end inline asm
	add.s64 	%rd160, %rd158, 16;
	// begin inline asm
	{ atom.add.f64 %fd478,[%rd160],%fd497; }

	// end inline asm
	add.s64 	%rd161, %rd158, 24;
	// begin inline asm
	{ atom.add.f64 %fd480,[%rd161],%fd497; }

	// end inline asm
	add.s64 	%rd162, %rd158, 32;
	// begin inline asm
	{ atom.add.f64 %fd482,[%rd162],%fd497; }

	// end inline asm
	add.s64 	%rd163, %rd158, 40;
	// begin inline asm
	{ atom.add.f64 %fd484,[%rd163],%fd497; }

	// end inline asm
	add.s64 	%rd164, %rd158, 48;
	// begin inline asm
	{ atom.add.f64 %fd486,[%rd164],%fd497; }

	// end inline asm
	add.s64 	%rd165, %rd158, 56;
	// begin inline asm
	{ atom.add.f64 %fd488,[%rd165],%fd497; }

	// end inline asm
	add.s64 	%rd166, %rd158, 64;
	// begin inline asm
	{ atom.add.f64 %fd490,[%rd166],%fd497; }

	// end inline asm
	add.s64 	%rd167, %rd158, 72;
	// begin inline asm
	{ atom.add.f64 %fd492,[%rd167],%fd91; }

	// end inline asm
	add.s64 	%rd168, %rd158, 80;
	// begin inline asm
	{ atom.add.f64 %fd494,[%rd168],%fd497; }

	// end inline asm
	add.s64 	%rd169, %rd158, 88;
	// begin inline asm
	{ atom.add.f64 %fd496,[%rd169],%fd497; }

	// end inline asm

$L__BB21_38:
	add.f64 	%fd96, %fd91, %fd95;
	add.f64 	%fd97, %fd64, 0d0000000000000000;
	add.f64 	%fd98, %fd65, 0d0000000000000000;
	add.f64 	%fd99, %fd66, 0d0000000000000000;
	@%p15 bra 	$L__BB21_40;

	mul.lo.s64 	%rd182, %rd48, %rd29;
	add.s64 	%rd170, %rd69, %rd182;
	mov.f64 	%fd521, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd498,[%rd170],%fd521; }

	// end inline asm
	add.s64 	%rd171, %rd170, 8;
	// begin inline asm
	{ atom.add.f64 %fd500,[%rd171],%fd521; }

	// end inline asm
	add.s64 	%rd172, %rd170, 16;
	// begin inline asm
	{ atom.add.f64 %fd502,[%rd172],%fd521; }

	// end inline asm
	add.s64 	%rd173, %rd170, 24;
	// begin inline asm
	{ atom.add.f64 %fd504,[%rd173],%fd521; }

	// end inline asm
	add.s64 	%rd174, %rd170, 32;
	// begin inline asm
	{ atom.add.f64 %fd506,[%rd174],%fd521; }

	// end inline asm
	add.s64 	%rd175, %rd170, 40;
	// begin inline asm
	{ atom.add.f64 %fd508,[%rd175],%fd521; }

	// end inline asm
	add.s64 	%rd176, %rd170, 48;
	// begin inline asm
	{ atom.add.f64 %fd510,[%rd176],%fd521; }

	// end inline asm
	add.s64 	%rd177, %rd170, 56;
	// begin inline asm
	{ atom.add.f64 %fd512,[%rd177],%fd521; }

	// end inline asm
	add.s64 	%rd178, %rd170, 64;
	// begin inline asm
	{ atom.add.f64 %fd514,[%rd178],%fd99; }

	// end inline asm
	add.s64 	%rd179, %rd170, 72;
	// begin inline asm
	{ atom.add.f64 %fd516,[%rd179],%fd521; }

	// end inline asm
	add.s64 	%rd180, %rd170, 80;
	// begin inline asm
	{ atom.add.f64 %fd518,[%rd180],%fd521; }

	// end inline asm
	add.s64 	%rd181, %rd170, 88;
	// begin inline asm
	{ atom.add.f64 %fd520,[%rd181],%fd521; }

	// end inline asm
	bra.uni 	$L__BB21_42;

$L__BB21_40:
	setp.eq.s64 	%p22, %rd62, 0;
	@%p22 bra 	$L__BB21_42;

	add.s64 	%rd183, %rd62, %rd50;
	mov.f64 	%fd545, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd522,[%rd183],%fd545; }

	// end inline asm
	add.s64 	%rd184, %rd183, 8;
	// begin inline asm
	{ atom.add.f64 %fd524,[%rd184],%fd545; }

	// end inline asm
	add.s64 	%rd185, %rd183, 16;
	// begin inline asm
	{ atom.add.f64 %fd526,[%rd185],%fd545; }

	// end inline asm
	add.s64 	%rd186, %rd183, 24;
	// begin inline asm
	{ atom.add.f64 %fd528,[%rd186],%fd545; }

	// end inline asm
	add.s64 	%rd187, %rd183, 32;
	// begin inline asm
	{ atom.add.f64 %fd530,[%rd187],%fd545; }

	// end inline asm
	add.s64 	%rd188, %rd183, 40;
	// begin inline asm
	{ atom.add.f64 %fd532,[%rd188],%fd545; }

	// end inline asm
	add.s64 	%rd189, %rd183, 48;
	// begin inline asm
	{ atom.add.f64 %fd534,[%rd189],%fd545; }

	// end inline asm
	add.s64 	%rd190, %rd183, 56;
	// begin inline asm
	{ atom.add.f64 %fd536,[%rd190],%fd545; }

	// end inline asm
	add.s64 	%rd191, %rd183, 64;
	// begin inline asm
	{ atom.add.f64 %fd538,[%rd191],%fd99; }

	// end inline asm
	add.s64 	%rd192, %rd183, 72;
	// begin inline asm
	{ atom.add.f64 %fd540,[%rd192],%fd545; }

	// end inline asm
	add.s64 	%rd193, %rd183, 80;
	// begin inline asm
	{ atom.add.f64 %fd542,[%rd193],%fd545; }

	// end inline asm
	add.s64 	%rd194, %rd183, 88;
	// begin inline asm
	{ atom.add.f64 %fd544,[%rd194],%fd545; }

	// end inline asm

$L__BB21_42:
	add.f64 	%fd100, %fd99, %fd76;
	@%p15 bra 	$L__BB21_44;

	mul.lo.s64 	%rd207, %rd48, %rd29;
	add.s64 	%rd195, %rd69, %rd207;
	mov.f64 	%fd569, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd546,[%rd195],%fd569; }

	// end inline asm
	add.s64 	%rd196, %rd195, 8;
	// begin inline asm
	{ atom.add.f64 %fd548,[%rd196],%fd569; }

	// end inline asm
	add.s64 	%rd197, %rd195, 16;
	// begin inline asm
	{ atom.add.f64 %fd550,[%rd197],%fd569; }

	// end inline asm
	add.s64 	%rd198, %rd195, 24;
	// begin inline asm
	{ atom.add.f64 %fd552,[%rd198],%fd569; }

	// end inline asm
	add.s64 	%rd199, %rd195, 32;
	// begin inline asm
	{ atom.add.f64 %fd554,[%rd199],%fd569; }

	// end inline asm
	add.s64 	%rd200, %rd195, 40;
	// begin inline asm
	{ atom.add.f64 %fd556,[%rd200],%fd569; }

	// end inline asm
	add.s64 	%rd201, %rd195, 48;
	// begin inline asm
	{ atom.add.f64 %fd558,[%rd201],%fd569; }

	// end inline asm
	add.s64 	%rd202, %rd195, 56;
	// begin inline asm
	{ atom.add.f64 %fd560,[%rd202],%fd98; }

	// end inline asm
	add.s64 	%rd203, %rd195, 64;
	// begin inline asm
	{ atom.add.f64 %fd562,[%rd203],%fd569; }

	// end inline asm
	add.s64 	%rd204, %rd195, 72;
	// begin inline asm
	{ atom.add.f64 %fd564,[%rd204],%fd569; }

	// end inline asm
	add.s64 	%rd205, %rd195, 80;
	// begin inline asm
	{ atom.add.f64 %fd566,[%rd205],%fd569; }

	// end inline asm
	add.s64 	%rd206, %rd195, 88;
	// begin inline asm
	{ atom.add.f64 %fd568,[%rd206],%fd569; }

	// end inline asm
	bra.uni 	$L__BB21_46;

$L__BB21_44:
	setp.eq.s64 	%p24, %rd62, 0;
	@%p24 bra 	$L__BB21_46;

	add.s64 	%rd208, %rd62, %rd50;
	mov.f64 	%fd593, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd570,[%rd208],%fd593; }

	// end inline asm
	add.s64 	%rd209, %rd208, 8;
	// begin inline asm
	{ atom.add.f64 %fd572,[%rd209],%fd593; }

	// end inline asm
	add.s64 	%rd210, %rd208, 16;
	// begin inline asm
	{ atom.add.f64 %fd574,[%rd210],%fd593; }

	// end inline asm
	add.s64 	%rd211, %rd208, 24;
	// begin inline asm
	{ atom.add.f64 %fd576,[%rd211],%fd593; }

	// end inline asm
	add.s64 	%rd212, %rd208, 32;
	// begin inline asm
	{ atom.add.f64 %fd578,[%rd212],%fd593; }

	// end inline asm
	add.s64 	%rd213, %rd208, 40;
	// begin inline asm
	{ atom.add.f64 %fd580,[%rd213],%fd593; }

	// end inline asm
	add.s64 	%rd214, %rd208, 48;
	// begin inline asm
	{ atom.add.f64 %fd582,[%rd214],%fd593; }

	// end inline asm
	add.s64 	%rd215, %rd208, 56;
	// begin inline asm
	{ atom.add.f64 %fd584,[%rd215],%fd98; }

	// end inline asm
	add.s64 	%rd216, %rd208, 64;
	// begin inline asm
	{ atom.add.f64 %fd586,[%rd216],%fd593; }

	// end inline asm
	add.s64 	%rd217, %rd208, 72;
	// begin inline asm
	{ atom.add.f64 %fd588,[%rd217],%fd593; }

	// end inline asm
	add.s64 	%rd218, %rd208, 80;
	// begin inline asm
	{ atom.add.f64 %fd590,[%rd218],%fd593; }

	// end inline asm
	add.s64 	%rd219, %rd208, 88;
	// begin inline asm
	{ atom.add.f64 %fd592,[%rd219],%fd593; }

	// end inline asm

$L__BB21_46:
	add.f64 	%fd101, %fd98, %fd100;
	@%p15 bra 	$L__BB21_48;

	mul.lo.s64 	%rd232, %rd48, %rd29;
	add.s64 	%rd220, %rd69, %rd232;
	mov.f64 	%fd617, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd594,[%rd220],%fd617; }

	// end inline asm
	add.s64 	%rd221, %rd220, 8;
	// begin inline asm
	{ atom.add.f64 %fd596,[%rd221],%fd617; }

	// end inline asm
	add.s64 	%rd222, %rd220, 16;
	// begin inline asm
	{ atom.add.f64 %fd598,[%rd222],%fd617; }

	// end inline asm
	add.s64 	%rd223, %rd220, 24;
	// begin inline asm
	{ atom.add.f64 %fd600,[%rd223],%fd617; }

	// end inline asm
	add.s64 	%rd224, %rd220, 32;
	// begin inline asm
	{ atom.add.f64 %fd602,[%rd224],%fd617; }

	// end inline asm
	add.s64 	%rd225, %rd220, 40;
	// begin inline asm
	{ atom.add.f64 %fd604,[%rd225],%fd617; }

	// end inline asm
	add.s64 	%rd226, %rd220, 48;
	// begin inline asm
	{ atom.add.f64 %fd606,[%rd226],%fd97; }

	// end inline asm
	add.s64 	%rd227, %rd220, 56;
	// begin inline asm
	{ atom.add.f64 %fd608,[%rd227],%fd617; }

	// end inline asm
	add.s64 	%rd228, %rd220, 64;
	// begin inline asm
	{ atom.add.f64 %fd610,[%rd228],%fd617; }

	// end inline asm
	add.s64 	%rd229, %rd220, 72;
	// begin inline asm
	{ atom.add.f64 %fd612,[%rd229],%fd617; }

	// end inline asm
	add.s64 	%rd230, %rd220, 80;
	// begin inline asm
	{ atom.add.f64 %fd614,[%rd230],%fd617; }

	// end inline asm
	add.s64 	%rd231, %rd220, 88;
	// begin inline asm
	{ atom.add.f64 %fd616,[%rd231],%fd617; }

	// end inline asm
	bra.uni 	$L__BB21_50;

$L__BB21_48:
	setp.eq.s64 	%p26, %rd62, 0;
	@%p26 bra 	$L__BB21_50;

	add.s64 	%rd233, %rd62, %rd50;
	mov.f64 	%fd641, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd618,[%rd233],%fd641; }

	// end inline asm
	add.s64 	%rd234, %rd233, 8;
	// begin inline asm
	{ atom.add.f64 %fd620,[%rd234],%fd641; }

	// end inline asm
	add.s64 	%rd235, %rd233, 16;
	// begin inline asm
	{ atom.add.f64 %fd622,[%rd235],%fd641; }

	// end inline asm
	add.s64 	%rd236, %rd233, 24;
	// begin inline asm
	{ atom.add.f64 %fd624,[%rd236],%fd641; }

	// end inline asm
	add.s64 	%rd237, %rd233, 32;
	// begin inline asm
	{ atom.add.f64 %fd626,[%rd237],%fd641; }

	// end inline asm
	add.s64 	%rd238, %rd233, 40;
	// begin inline asm
	{ atom.add.f64 %fd628,[%rd238],%fd641; }

	// end inline asm
	add.s64 	%rd239, %rd233, 48;
	// begin inline asm
	{ atom.add.f64 %fd630,[%rd239],%fd97; }

	// end inline asm
	add.s64 	%rd240, %rd233, 56;
	// begin inline asm
	{ atom.add.f64 %fd632,[%rd240],%fd641; }

	// end inline asm
	add.s64 	%rd241, %rd233, 64;
	// begin inline asm
	{ atom.add.f64 %fd634,[%rd241],%fd641; }

	// end inline asm
	add.s64 	%rd242, %rd233, 72;
	// begin inline asm
	{ atom.add.f64 %fd636,[%rd242],%fd641; }

	// end inline asm
	add.s64 	%rd243, %rd233, 80;
	// begin inline asm
	{ atom.add.f64 %fd638,[%rd243],%fd641; }

	// end inline asm
	add.s64 	%rd244, %rd233, 88;
	// begin inline asm
	{ atom.add.f64 %fd640,[%rd244],%fd641; }

	// end inline asm

$L__BB21_50:
	add.f64 	%fd102, %fd97, %fd101;
	add.f64 	%fd103, %fd67, 0d0000000000000000;
	add.f64 	%fd104, %fd68, 0d0000000000000000;
	add.f64 	%fd105, %fd69, 0d0000000000000000;
	@%p15 bra 	$L__BB21_52;

	mul.lo.s64 	%rd257, %rd48, %rd29;
	add.s64 	%rd245, %rd69, %rd257;
	mov.f64 	%fd665, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd642,[%rd245],%fd665; }

	// end inline asm
	add.s64 	%rd246, %rd245, 8;
	// begin inline asm
	{ atom.add.f64 %fd644,[%rd246],%fd665; }

	// end inline asm
	add.s64 	%rd247, %rd245, 16;
	// begin inline asm
	{ atom.add.f64 %fd646,[%rd247],%fd665; }

	// end inline asm
	add.s64 	%rd248, %rd245, 24;
	// begin inline asm
	{ atom.add.f64 %fd648,[%rd248],%fd665; }

	// end inline asm
	add.s64 	%rd249, %rd245, 32;
	// begin inline asm
	{ atom.add.f64 %fd650,[%rd249],%fd665; }

	// end inline asm
	add.s64 	%rd250, %rd245, 40;
	// begin inline asm
	{ atom.add.f64 %fd652,[%rd250],%fd105; }

	// end inline asm
	add.s64 	%rd251, %rd245, 48;
	// begin inline asm
	{ atom.add.f64 %fd654,[%rd251],%fd665; }

	// end inline asm
	add.s64 	%rd252, %rd245, 56;
	// begin inline asm
	{ atom.add.f64 %fd656,[%rd252],%fd665; }

	// end inline asm
	add.s64 	%rd253, %rd245, 64;
	// begin inline asm
	{ atom.add.f64 %fd658,[%rd253],%fd665; }

	// end inline asm
	add.s64 	%rd254, %rd245, 72;
	// begin inline asm
	{ atom.add.f64 %fd660,[%rd254],%fd665; }

	// end inline asm
	add.s64 	%rd255, %rd245, 80;
	// begin inline asm
	{ atom.add.f64 %fd662,[%rd255],%fd665; }

	// end inline asm
	add.s64 	%rd256, %rd245, 88;
	// begin inline asm
	{ atom.add.f64 %fd664,[%rd256],%fd665; }

	// end inline asm
	bra.uni 	$L__BB21_54;

$L__BB21_52:
	setp.eq.s64 	%p28, %rd62, 0;
	@%p28 bra 	$L__BB21_54;

	add.s64 	%rd258, %rd62, %rd50;
	mov.f64 	%fd689, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd666,[%rd258],%fd689; }

	// end inline asm
	add.s64 	%rd259, %rd258, 8;
	// begin inline asm
	{ atom.add.f64 %fd668,[%rd259],%fd689; }

	// end inline asm
	add.s64 	%rd260, %rd258, 16;
	// begin inline asm
	{ atom.add.f64 %fd670,[%rd260],%fd689; }

	// end inline asm
	add.s64 	%rd261, %rd258, 24;
	// begin inline asm
	{ atom.add.f64 %fd672,[%rd261],%fd689; }

	// end inline asm
	add.s64 	%rd262, %rd258, 32;
	// begin inline asm
	{ atom.add.f64 %fd674,[%rd262],%fd689; }

	// end inline asm
	add.s64 	%rd263, %rd258, 40;
	// begin inline asm
	{ atom.add.f64 %fd676,[%rd263],%fd105; }

	// end inline asm
	add.s64 	%rd264, %rd258, 48;
	// begin inline asm
	{ atom.add.f64 %fd678,[%rd264],%fd689; }

	// end inline asm
	add.s64 	%rd265, %rd258, 56;
	// begin inline asm
	{ atom.add.f64 %fd680,[%rd265],%fd689; }

	// end inline asm
	add.s64 	%rd266, %rd258, 64;
	// begin inline asm
	{ atom.add.f64 %fd682,[%rd266],%fd689; }

	// end inline asm
	add.s64 	%rd267, %rd258, 72;
	// begin inline asm
	{ atom.add.f64 %fd684,[%rd267],%fd689; }

	// end inline asm
	add.s64 	%rd268, %rd258, 80;
	// begin inline asm
	{ atom.add.f64 %fd686,[%rd268],%fd689; }

	// end inline asm
	add.s64 	%rd269, %rd258, 88;
	// begin inline asm
	{ atom.add.f64 %fd688,[%rd269],%fd689; }

	// end inline asm

$L__BB21_54:
	add.f64 	%fd106, %fd105, %fd79;
	@%p15 bra 	$L__BB21_56;

	mul.lo.s64 	%rd282, %rd48, %rd29;
	add.s64 	%rd270, %rd69, %rd282;
	mov.f64 	%fd713, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd690,[%rd270],%fd713; }

	// end inline asm
	add.s64 	%rd271, %rd270, 8;
	// begin inline asm
	{ atom.add.f64 %fd692,[%rd271],%fd713; }

	// end inline asm
	add.s64 	%rd272, %rd270, 16;
	// begin inline asm
	{ atom.add.f64 %fd694,[%rd272],%fd713; }

	// end inline asm
	add.s64 	%rd273, %rd270, 24;
	// begin inline asm
	{ atom.add.f64 %fd696,[%rd273],%fd713; }

	// end inline asm
	add.s64 	%rd274, %rd270, 32;
	// begin inline asm
	{ atom.add.f64 %fd698,[%rd274],%fd104; }

	// end inline asm
	add.s64 	%rd275, %rd270, 40;
	// begin inline asm
	{ atom.add.f64 %fd700,[%rd275],%fd713; }

	// end inline asm
	add.s64 	%rd276, %rd270, 48;
	// begin inline asm
	{ atom.add.f64 %fd702,[%rd276],%fd713; }

	// end inline asm
	add.s64 	%rd277, %rd270, 56;
	// begin inline asm
	{ atom.add.f64 %fd704,[%rd277],%fd713; }

	// end inline asm
	add.s64 	%rd278, %rd270, 64;
	// begin inline asm
	{ atom.add.f64 %fd706,[%rd278],%fd713; }

	// end inline asm
	add.s64 	%rd279, %rd270, 72;
	// begin inline asm
	{ atom.add.f64 %fd708,[%rd279],%fd713; }

	// end inline asm
	add.s64 	%rd280, %rd270, 80;
	// begin inline asm
	{ atom.add.f64 %fd710,[%rd280],%fd713; }

	// end inline asm
	add.s64 	%rd281, %rd270, 88;
	// begin inline asm
	{ atom.add.f64 %fd712,[%rd281],%fd713; }

	// end inline asm
	bra.uni 	$L__BB21_58;

$L__BB21_56:
	setp.eq.s64 	%p30, %rd62, 0;
	@%p30 bra 	$L__BB21_58;

	add.s64 	%rd283, %rd62, %rd50;
	mov.f64 	%fd737, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd714,[%rd283],%fd737; }

	// end inline asm
	add.s64 	%rd284, %rd283, 8;
	// begin inline asm
	{ atom.add.f64 %fd716,[%rd284],%fd737; }

	// end inline asm
	add.s64 	%rd285, %rd283, 16;
	// begin inline asm
	{ atom.add.f64 %fd718,[%rd285],%fd737; }

	// end inline asm
	add.s64 	%rd286, %rd283, 24;
	// begin inline asm
	{ atom.add.f64 %fd720,[%rd286],%fd737; }

	// end inline asm
	add.s64 	%rd287, %rd283, 32;
	// begin inline asm
	{ atom.add.f64 %fd722,[%rd287],%fd104; }

	// end inline asm
	add.s64 	%rd288, %rd283, 40;
	// begin inline asm
	{ atom.add.f64 %fd724,[%rd288],%fd737; }

	// end inline asm
	add.s64 	%rd289, %rd283, 48;
	// begin inline asm
	{ atom.add.f64 %fd726,[%rd289],%fd737; }

	// end inline asm
	add.s64 	%rd290, %rd283, 56;
	// begin inline asm
	{ atom.add.f64 %fd728,[%rd290],%fd737; }

	// end inline asm
	add.s64 	%rd291, %rd283, 64;
	// begin inline asm
	{ atom.add.f64 %fd730,[%rd291],%fd737; }

	// end inline asm
	add.s64 	%rd292, %rd283, 72;
	// begin inline asm
	{ atom.add.f64 %fd732,[%rd292],%fd737; }

	// end inline asm
	add.s64 	%rd293, %rd283, 80;
	// begin inline asm
	{ atom.add.f64 %fd734,[%rd293],%fd737; }

	// end inline asm
	add.s64 	%rd294, %rd283, 88;
	// begin inline asm
	{ atom.add.f64 %fd736,[%rd294],%fd737; }

	// end inline asm

$L__BB21_58:
	add.f64 	%fd107, %fd104, %fd106;
	@%p15 bra 	$L__BB21_60;

	mul.lo.s64 	%rd307, %rd48, %rd29;
	add.s64 	%rd295, %rd69, %rd307;
	mov.f64 	%fd761, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd738,[%rd295],%fd761; }

	// end inline asm
	add.s64 	%rd296, %rd295, 8;
	// begin inline asm
	{ atom.add.f64 %fd740,[%rd296],%fd761; }

	// end inline asm
	add.s64 	%rd297, %rd295, 16;
	// begin inline asm
	{ atom.add.f64 %fd742,[%rd297],%fd761; }

	// end inline asm
	add.s64 	%rd298, %rd295, 24;
	// begin inline asm
	{ atom.add.f64 %fd744,[%rd298],%fd103; }

	// end inline asm
	add.s64 	%rd299, %rd295, 32;
	// begin inline asm
	{ atom.add.f64 %fd746,[%rd299],%fd761; }

	// end inline asm
	add.s64 	%rd300, %rd295, 40;
	// begin inline asm
	{ atom.add.f64 %fd748,[%rd300],%fd761; }

	// end inline asm
	add.s64 	%rd301, %rd295, 48;
	// begin inline asm
	{ atom.add.f64 %fd750,[%rd301],%fd761; }

	// end inline asm
	add.s64 	%rd302, %rd295, 56;
	// begin inline asm
	{ atom.add.f64 %fd752,[%rd302],%fd761; }

	// end inline asm
	add.s64 	%rd303, %rd295, 64;
	// begin inline asm
	{ atom.add.f64 %fd754,[%rd303],%fd761; }

	// end inline asm
	add.s64 	%rd304, %rd295, 72;
	// begin inline asm
	{ atom.add.f64 %fd756,[%rd304],%fd761; }

	// end inline asm
	add.s64 	%rd305, %rd295, 80;
	// begin inline asm
	{ atom.add.f64 %fd758,[%rd305],%fd761; }

	// end inline asm
	add.s64 	%rd306, %rd295, 88;
	// begin inline asm
	{ atom.add.f64 %fd760,[%rd306],%fd761; }

	// end inline asm
	bra.uni 	$L__BB21_62;

$L__BB21_60:
	setp.eq.s64 	%p32, %rd62, 0;
	@%p32 bra 	$L__BB21_62;

	add.s64 	%rd308, %rd62, %rd50;
	mov.f64 	%fd785, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd762,[%rd308],%fd785; }

	// end inline asm
	add.s64 	%rd309, %rd308, 8;
	// begin inline asm
	{ atom.add.f64 %fd764,[%rd309],%fd785; }

	// end inline asm
	add.s64 	%rd310, %rd308, 16;
	// begin inline asm
	{ atom.add.f64 %fd766,[%rd310],%fd785; }

	// end inline asm
	add.s64 	%rd311, %rd308, 24;
	// begin inline asm
	{ atom.add.f64 %fd768,[%rd311],%fd103; }

	// end inline asm
	add.s64 	%rd312, %rd308, 32;
	// begin inline asm
	{ atom.add.f64 %fd770,[%rd312],%fd785; }

	// end inline asm
	add.s64 	%rd313, %rd308, 40;
	// begin inline asm
	{ atom.add.f64 %fd772,[%rd313],%fd785; }

	// end inline asm
	add.s64 	%rd314, %rd308, 48;
	// begin inline asm
	{ atom.add.f64 %fd774,[%rd314],%fd785; }

	// end inline asm
	add.s64 	%rd315, %rd308, 56;
	// begin inline asm
	{ atom.add.f64 %fd776,[%rd315],%fd785; }

	// end inline asm
	add.s64 	%rd316, %rd308, 64;
	// begin inline asm
	{ atom.add.f64 %fd778,[%rd316],%fd785; }

	// end inline asm
	add.s64 	%rd317, %rd308, 72;
	// begin inline asm
	{ atom.add.f64 %fd780,[%rd317],%fd785; }

	// end inline asm
	add.s64 	%rd318, %rd308, 80;
	// begin inline asm
	{ atom.add.f64 %fd782,[%rd318],%fd785; }

	// end inline asm
	add.s64 	%rd319, %rd308, 88;
	// begin inline asm
	{ atom.add.f64 %fd784,[%rd319],%fd785; }

	// end inline asm

$L__BB21_62:
	add.f64 	%fd108, %fd103, %fd107;
	add.f64 	%fd109, %fd70, 0d0000000000000000;
	add.f64 	%fd110, %fd71, 0d0000000000000000;
	add.f64 	%fd111, %fd72, 0d0000000000000000;
	@%p15 bra 	$L__BB21_64;

	mul.lo.s64 	%rd332, %rd48, %rd29;
	add.s64 	%rd320, %rd69, %rd332;
	mov.f64 	%fd809, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd786,[%rd320],%fd809; }

	// end inline asm
	add.s64 	%rd321, %rd320, 8;
	// begin inline asm
	{ atom.add.f64 %fd788,[%rd321],%fd809; }

	// end inline asm
	add.s64 	%rd322, %rd320, 16;
	// begin inline asm
	{ atom.add.f64 %fd790,[%rd322],%fd111; }

	// end inline asm
	add.s64 	%rd323, %rd320, 24;
	// begin inline asm
	{ atom.add.f64 %fd792,[%rd323],%fd809; }

	// end inline asm
	add.s64 	%rd324, %rd320, 32;
	// begin inline asm
	{ atom.add.f64 %fd794,[%rd324],%fd809; }

	// end inline asm
	add.s64 	%rd325, %rd320, 40;
	// begin inline asm
	{ atom.add.f64 %fd796,[%rd325],%fd809; }

	// end inline asm
	add.s64 	%rd326, %rd320, 48;
	// begin inline asm
	{ atom.add.f64 %fd798,[%rd326],%fd809; }

	// end inline asm
	add.s64 	%rd327, %rd320, 56;
	// begin inline asm
	{ atom.add.f64 %fd800,[%rd327],%fd809; }

	// end inline asm
	add.s64 	%rd328, %rd320, 64;
	// begin inline asm
	{ atom.add.f64 %fd802,[%rd328],%fd809; }

	// end inline asm
	add.s64 	%rd329, %rd320, 72;
	// begin inline asm
	{ atom.add.f64 %fd804,[%rd329],%fd809; }

	// end inline asm
	add.s64 	%rd330, %rd320, 80;
	// begin inline asm
	{ atom.add.f64 %fd806,[%rd330],%fd809; }

	// end inline asm
	add.s64 	%rd331, %rd320, 88;
	// begin inline asm
	{ atom.add.f64 %fd808,[%rd331],%fd809; }

	// end inline asm
	bra.uni 	$L__BB21_66;

$L__BB21_64:
	setp.eq.s64 	%p34, %rd62, 0;
	@%p34 bra 	$L__BB21_66;

	add.s64 	%rd333, %rd62, %rd50;
	mov.f64 	%fd833, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd810,[%rd333],%fd833; }

	// end inline asm
	add.s64 	%rd334, %rd333, 8;
	// begin inline asm
	{ atom.add.f64 %fd812,[%rd334],%fd833; }

	// end inline asm
	add.s64 	%rd335, %rd333, 16;
	// begin inline asm
	{ atom.add.f64 %fd814,[%rd335],%fd111; }

	// end inline asm
	add.s64 	%rd336, %rd333, 24;
	// begin inline asm
	{ atom.add.f64 %fd816,[%rd336],%fd833; }

	// end inline asm
	add.s64 	%rd337, %rd333, 32;
	// begin inline asm
	{ atom.add.f64 %fd818,[%rd337],%fd833; }

	// end inline asm
	add.s64 	%rd338, %rd333, 40;
	// begin inline asm
	{ atom.add.f64 %fd820,[%rd338],%fd833; }

	// end inline asm
	add.s64 	%rd339, %rd333, 48;
	// begin inline asm
	{ atom.add.f64 %fd822,[%rd339],%fd833; }

	// end inline asm
	add.s64 	%rd340, %rd333, 56;
	// begin inline asm
	{ atom.add.f64 %fd824,[%rd340],%fd833; }

	// end inline asm
	add.s64 	%rd341, %rd333, 64;
	// begin inline asm
	{ atom.add.f64 %fd826,[%rd341],%fd833; }

	// end inline asm
	add.s64 	%rd342, %rd333, 72;
	// begin inline asm
	{ atom.add.f64 %fd828,[%rd342],%fd833; }

	// end inline asm
	add.s64 	%rd343, %rd333, 80;
	// begin inline asm
	{ atom.add.f64 %fd830,[%rd343],%fd833; }

	// end inline asm
	add.s64 	%rd344, %rd333, 88;
	// begin inline asm
	{ atom.add.f64 %fd832,[%rd344],%fd833; }

	// end inline asm

$L__BB21_66:
	add.f64 	%fd112, %fd111, %fd82;
	@%p15 bra 	$L__BB21_68;

	mul.lo.s64 	%rd357, %rd48, %rd29;
	add.s64 	%rd345, %rd69, %rd357;
	mov.f64 	%fd857, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd834,[%rd345],%fd857; }

	// end inline asm
	add.s64 	%rd346, %rd345, 8;
	// begin inline asm
	{ atom.add.f64 %fd836,[%rd346],%fd110; }

	// end inline asm
	add.s64 	%rd347, %rd345, 16;
	// begin inline asm
	{ atom.add.f64 %fd838,[%rd347],%fd857; }

	// end inline asm
	add.s64 	%rd348, %rd345, 24;
	// begin inline asm
	{ atom.add.f64 %fd840,[%rd348],%fd857; }

	// end inline asm
	add.s64 	%rd349, %rd345, 32;
	// begin inline asm
	{ atom.add.f64 %fd842,[%rd349],%fd857; }

	// end inline asm
	add.s64 	%rd350, %rd345, 40;
	// begin inline asm
	{ atom.add.f64 %fd844,[%rd350],%fd857; }

	// end inline asm
	add.s64 	%rd351, %rd345, 48;
	// begin inline asm
	{ atom.add.f64 %fd846,[%rd351],%fd857; }

	// end inline asm
	add.s64 	%rd352, %rd345, 56;
	// begin inline asm
	{ atom.add.f64 %fd848,[%rd352],%fd857; }

	// end inline asm
	add.s64 	%rd353, %rd345, 64;
	// begin inline asm
	{ atom.add.f64 %fd850,[%rd353],%fd857; }

	// end inline asm
	add.s64 	%rd354, %rd345, 72;
	// begin inline asm
	{ atom.add.f64 %fd852,[%rd354],%fd857; }

	// end inline asm
	add.s64 	%rd355, %rd345, 80;
	// begin inline asm
	{ atom.add.f64 %fd854,[%rd355],%fd857; }

	// end inline asm
	add.s64 	%rd356, %rd345, 88;
	// begin inline asm
	{ atom.add.f64 %fd856,[%rd356],%fd857; }

	// end inline asm
	bra.uni 	$L__BB21_70;

$L__BB21_68:
	setp.eq.s64 	%p36, %rd62, 0;
	@%p36 bra 	$L__BB21_70;

	add.s64 	%rd358, %rd62, %rd50;
	mov.f64 	%fd881, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd858,[%rd358],%fd881; }

	// end inline asm
	add.s64 	%rd359, %rd358, 8;
	// begin inline asm
	{ atom.add.f64 %fd860,[%rd359],%fd110; }

	// end inline asm
	add.s64 	%rd360, %rd358, 16;
	// begin inline asm
	{ atom.add.f64 %fd862,[%rd360],%fd881; }

	// end inline asm
	add.s64 	%rd361, %rd358, 24;
	// begin inline asm
	{ atom.add.f64 %fd864,[%rd361],%fd881; }

	// end inline asm
	add.s64 	%rd362, %rd358, 32;
	// begin inline asm
	{ atom.add.f64 %fd866,[%rd362],%fd881; }

	// end inline asm
	add.s64 	%rd363, %rd358, 40;
	// begin inline asm
	{ atom.add.f64 %fd868,[%rd363],%fd881; }

	// end inline asm
	add.s64 	%rd364, %rd358, 48;
	// begin inline asm
	{ atom.add.f64 %fd870,[%rd364],%fd881; }

	// end inline asm
	add.s64 	%rd365, %rd358, 56;
	// begin inline asm
	{ atom.add.f64 %fd872,[%rd365],%fd881; }

	// end inline asm
	add.s64 	%rd366, %rd358, 64;
	// begin inline asm
	{ atom.add.f64 %fd874,[%rd366],%fd881; }

	// end inline asm
	add.s64 	%rd367, %rd358, 72;
	// begin inline asm
	{ atom.add.f64 %fd876,[%rd367],%fd881; }

	// end inline asm
	add.s64 	%rd368, %rd358, 80;
	// begin inline asm
	{ atom.add.f64 %fd878,[%rd368],%fd881; }

	// end inline asm
	add.s64 	%rd369, %rd358, 88;
	// begin inline asm
	{ atom.add.f64 %fd880,[%rd369],%fd881; }

	// end inline asm

$L__BB21_70:
	add.f64 	%fd113, %fd110, %fd112;
	@%p15 bra 	$L__BB21_72;

	mul.lo.s64 	%rd382, %rd48, %rd29;
	add.s64 	%rd370, %rd69, %rd382;
	// begin inline asm
	{ atom.add.f64 %fd882,[%rd370],%fd109; }

	// end inline asm
	add.s64 	%rd371, %rd370, 8;
	mov.f64 	%fd905, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd884,[%rd371],%fd905; }

	// end inline asm
	add.s64 	%rd372, %rd370, 16;
	// begin inline asm
	{ atom.add.f64 %fd886,[%rd372],%fd905; }

	// end inline asm
	add.s64 	%rd373, %rd370, 24;
	// begin inline asm
	{ atom.add.f64 %fd888,[%rd373],%fd905; }

	// end inline asm
	add.s64 	%rd374, %rd370, 32;
	// begin inline asm
	{ atom.add.f64 %fd890,[%rd374],%fd905; }

	// end inline asm
	add.s64 	%rd375, %rd370, 40;
	// begin inline asm
	{ atom.add.f64 %fd892,[%rd375],%fd905; }

	// end inline asm
	add.s64 	%rd376, %rd370, 48;
	// begin inline asm
	{ atom.add.f64 %fd894,[%rd376],%fd905; }

	// end inline asm
	add.s64 	%rd377, %rd370, 56;
	// begin inline asm
	{ atom.add.f64 %fd896,[%rd377],%fd905; }

	// end inline asm
	add.s64 	%rd378, %rd370, 64;
	// begin inline asm
	{ atom.add.f64 %fd898,[%rd378],%fd905; }

	// end inline asm
	add.s64 	%rd379, %rd370, 72;
	// begin inline asm
	{ atom.add.f64 %fd900,[%rd379],%fd905; }

	// end inline asm
	add.s64 	%rd380, %rd370, 80;
	// begin inline asm
	{ atom.add.f64 %fd902,[%rd380],%fd905; }

	// end inline asm
	add.s64 	%rd381, %rd370, 88;
	// begin inline asm
	{ atom.add.f64 %fd904,[%rd381],%fd905; }

	// end inline asm
	bra.uni 	$L__BB21_74;

$L__BB21_72:
	setp.eq.s64 	%p38, %rd62, 0;
	@%p38 bra 	$L__BB21_74;

	add.s64 	%rd383, %rd62, %rd50;
	// begin inline asm
	{ atom.add.f64 %fd906,[%rd383],%fd109; }

	// end inline asm
	add.s64 	%rd384, %rd383, 8;
	mov.f64 	%fd929, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd908,[%rd384],%fd929; }

	// end inline asm
	add.s64 	%rd385, %rd383, 16;
	// begin inline asm
	{ atom.add.f64 %fd910,[%rd385],%fd929; }

	// end inline asm
	add.s64 	%rd386, %rd383, 24;
	// begin inline asm
	{ atom.add.f64 %fd912,[%rd386],%fd929; }

	// end inline asm
	add.s64 	%rd387, %rd383, 32;
	// begin inline asm
	{ atom.add.f64 %fd914,[%rd387],%fd929; }

	// end inline asm
	add.s64 	%rd388, %rd383, 40;
	// begin inline asm
	{ atom.add.f64 %fd916,[%rd388],%fd929; }

	// end inline asm
	add.s64 	%rd389, %rd383, 48;
	// begin inline asm
	{ atom.add.f64 %fd918,[%rd389],%fd929; }

	// end inline asm
	add.s64 	%rd390, %rd383, 56;
	// begin inline asm
	{ atom.add.f64 %fd920,[%rd390],%fd929; }

	// end inline asm
	add.s64 	%rd391, %rd383, 64;
	// begin inline asm
	{ atom.add.f64 %fd922,[%rd391],%fd929; }

	// end inline asm
	add.s64 	%rd392, %rd383, 72;
	// begin inline asm
	{ atom.add.f64 %fd924,[%rd392],%fd929; }

	// end inline asm
	add.s64 	%rd393, %rd383, 80;
	// begin inline asm
	{ atom.add.f64 %fd926,[%rd393],%fd929; }

	// end inline asm
	add.s64 	%rd394, %rd383, 88;
	// begin inline asm
	{ atom.add.f64 %fd928,[%rd394],%fd929; }

	// end inline asm

$L__BB21_74:
	setp.eq.s64 	%p39, %rd67, 0;
	add.f64 	%fd114, %fd109, %fd113;
	add.f64 	%fd115, %fd96, 0d0000000000000000;
	add.f64 	%fd116, %fd74, 0d0000000000000000;
	add.f64 	%fd117, %fd75, 0d0000000000000000;
	@%p39 bra 	$L__BB21_76;

	mul.lo.s64 	%rd407, %rd48, %rd30;
	add.s64 	%rd395, %rd67, %rd407;
	mov.f64 	%fd951, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd930,[%rd395],%fd951; }

	// end inline asm
	add.s64 	%rd396, %rd395, 8;
	// begin inline asm
	{ atom.add.f64 %fd932,[%rd396],%fd951; }

	// end inline asm
	add.s64 	%rd397, %rd395, 16;
	// begin inline asm
	{ atom.add.f64 %fd934,[%rd397],%fd951; }

	// end inline asm
	add.s64 	%rd398, %rd395, 24;
	// begin inline asm
	{ atom.add.f64 %fd936,[%rd398],%fd951; }

	// end inline asm
	add.s64 	%rd399, %rd395, 32;
	// begin inline asm
	{ atom.add.f64 %fd938,[%rd399],%fd951; }

	// end inline asm
	add.s64 	%rd400, %rd395, 40;
	// begin inline asm
	{ atom.add.f64 %fd940,[%rd400],%fd951; }

	// end inline asm
	add.s64 	%rd401, %rd395, 48;
	// begin inline asm
	{ atom.add.f64 %fd942,[%rd401],%fd951; }

	// end inline asm
	add.s64 	%rd402, %rd395, 56;
	// begin inline asm
	{ atom.add.f64 %fd944,[%rd402],%fd951; }

	// end inline asm
	add.s64 	%rd403, %rd395, 64;
	// begin inline asm
	{ atom.add.f64 %fd946,[%rd403],%fd951; }

	// end inline asm
	add.s64 	%rd404, %rd395, 72;
	// begin inline asm
	{ atom.add.f64 %fd948,[%rd404],%fd951; }

	// end inline asm
	add.s64 	%rd405, %rd395, 80;
	// begin inline asm
	{ atom.add.f64 %fd950,[%rd405],%fd951; }

	// end inline asm
	add.s64 	%rd406, %rd395, 88;
	// begin inline asm
	{ atom.add.f64 %fd952,[%rd406],%fd117; }

	// end inline asm
	bra.uni 	$L__BB21_78;

$L__BB21_76:
	setp.eq.s64 	%p40, %rd60, 0;
	@%p40 bra 	$L__BB21_78;

	add.s64 	%rd408, %rd60, %rd49;
	mov.f64 	%fd975, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd954,[%rd408],%fd975; }

	// end inline asm
	add.s64 	%rd409, %rd408, 8;
	// begin inline asm
	{ atom.add.f64 %fd956,[%rd409],%fd975; }

	// end inline asm
	add.s64 	%rd410, %rd408, 16;
	// begin inline asm
	{ atom.add.f64 %fd958,[%rd410],%fd975; }

	// end inline asm
	add.s64 	%rd411, %rd408, 24;
	// begin inline asm
	{ atom.add.f64 %fd960,[%rd411],%fd975; }

	// end inline asm
	add.s64 	%rd412, %rd408, 32;
	// begin inline asm
	{ atom.add.f64 %fd962,[%rd412],%fd975; }

	// end inline asm
	add.s64 	%rd413, %rd408, 40;
	// begin inline asm
	{ atom.add.f64 %fd964,[%rd413],%fd975; }

	// end inline asm
	add.s64 	%rd414, %rd408, 48;
	// begin inline asm
	{ atom.add.f64 %fd966,[%rd414],%fd975; }

	// end inline asm
	add.s64 	%rd415, %rd408, 56;
	// begin inline asm
	{ atom.add.f64 %fd968,[%rd415],%fd975; }

	// end inline asm
	add.s64 	%rd416, %rd408, 64;
	// begin inline asm
	{ atom.add.f64 %fd970,[%rd416],%fd975; }

	// end inline asm
	add.s64 	%rd417, %rd408, 72;
	// begin inline asm
	{ atom.add.f64 %fd972,[%rd417],%fd975; }

	// end inline asm
	add.s64 	%rd418, %rd408, 80;
	// begin inline asm
	{ atom.add.f64 %fd974,[%rd418],%fd975; }

	// end inline asm
	add.s64 	%rd419, %rd408, 88;
	// begin inline asm
	{ atom.add.f64 %fd976,[%rd419],%fd117; }

	// end inline asm

$L__BB21_78:
	@%p39 bra 	$L__BB21_80;

	mul.lo.s64 	%rd432, %rd48, %rd30;
	add.s64 	%rd420, %rd67, %rd432;
	mov.f64 	%fd1001, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd978,[%rd420],%fd1001; }

	// end inline asm
	add.s64 	%rd421, %rd420, 8;
	// begin inline asm
	{ atom.add.f64 %fd980,[%rd421],%fd1001; }

	// end inline asm
	add.s64 	%rd422, %rd420, 16;
	// begin inline asm
	{ atom.add.f64 %fd982,[%rd422],%fd1001; }

	// end inline asm
	add.s64 	%rd423, %rd420, 24;
	// begin inline asm
	{ atom.add.f64 %fd984,[%rd423],%fd1001; }

	// end inline asm
	add.s64 	%rd424, %rd420, 32;
	// begin inline asm
	{ atom.add.f64 %fd986,[%rd424],%fd1001; }

	// end inline asm
	add.s64 	%rd425, %rd420, 40;
	// begin inline asm
	{ atom.add.f64 %fd988,[%rd425],%fd1001; }

	// end inline asm
	add.s64 	%rd426, %rd420, 48;
	// begin inline asm
	{ atom.add.f64 %fd990,[%rd426],%fd1001; }

	// end inline asm
	add.s64 	%rd427, %rd420, 56;
	// begin inline asm
	{ atom.add.f64 %fd992,[%rd427],%fd1001; }

	// end inline asm
	add.s64 	%rd428, %rd420, 64;
	// begin inline asm
	{ atom.add.f64 %fd994,[%rd428],%fd1001; }

	// end inline asm
	add.s64 	%rd429, %rd420, 72;
	// begin inline asm
	{ atom.add.f64 %fd996,[%rd429],%fd1001; }

	// end inline asm
	add.s64 	%rd430, %rd420, 80;
	// begin inline asm
	{ atom.add.f64 %fd998,[%rd430],%fd116; }

	// end inline asm
	add.s64 	%rd431, %rd420, 88;
	// begin inline asm
	{ atom.add.f64 %fd1000,[%rd431],%fd1001; }

	// end inline asm
	bra.uni 	$L__BB21_82;

$L__BB21_80:
	setp.eq.s64 	%p42, %rd60, 0;
	@%p42 bra 	$L__BB21_82;

	add.s64 	%rd433, %rd60, %rd49;
	mov.f64 	%fd1025, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1002,[%rd433],%fd1025; }

	// end inline asm
	add.s64 	%rd434, %rd433, 8;
	// begin inline asm
	{ atom.add.f64 %fd1004,[%rd434],%fd1025; }

	// end inline asm
	add.s64 	%rd435, %rd433, 16;
	// begin inline asm
	{ atom.add.f64 %fd1006,[%rd435],%fd1025; }

	// end inline asm
	add.s64 	%rd436, %rd433, 24;
	// begin inline asm
	{ atom.add.f64 %fd1008,[%rd436],%fd1025; }

	// end inline asm
	add.s64 	%rd437, %rd433, 32;
	// begin inline asm
	{ atom.add.f64 %fd1010,[%rd437],%fd1025; }

	// end inline asm
	add.s64 	%rd438, %rd433, 40;
	// begin inline asm
	{ atom.add.f64 %fd1012,[%rd438],%fd1025; }

	// end inline asm
	add.s64 	%rd439, %rd433, 48;
	// begin inline asm
	{ atom.add.f64 %fd1014,[%rd439],%fd1025; }

	// end inline asm
	add.s64 	%rd440, %rd433, 56;
	// begin inline asm
	{ atom.add.f64 %fd1016,[%rd440],%fd1025; }

	// end inline asm
	add.s64 	%rd441, %rd433, 64;
	// begin inline asm
	{ atom.add.f64 %fd1018,[%rd441],%fd1025; }

	// end inline asm
	add.s64 	%rd442, %rd433, 72;
	// begin inline asm
	{ atom.add.f64 %fd1020,[%rd442],%fd1025; }

	// end inline asm
	add.s64 	%rd443, %rd433, 80;
	// begin inline asm
	{ atom.add.f64 %fd1022,[%rd443],%fd116; }

	// end inline asm
	add.s64 	%rd444, %rd433, 88;
	// begin inline asm
	{ atom.add.f64 %fd1024,[%rd444],%fd1025; }

	// end inline asm

$L__BB21_82:
	@%p39 bra 	$L__BB21_84;

	mul.lo.s64 	%rd457, %rd48, %rd30;
	add.s64 	%rd445, %rd67, %rd457;
	mov.f64 	%fd1049, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1026,[%rd445],%fd1049; }

	// end inline asm
	add.s64 	%rd446, %rd445, 8;
	// begin inline asm
	{ atom.add.f64 %fd1028,[%rd446],%fd1049; }

	// end inline asm
	add.s64 	%rd447, %rd445, 16;
	// begin inline asm
	{ atom.add.f64 %fd1030,[%rd447],%fd1049; }

	// end inline asm
	add.s64 	%rd448, %rd445, 24;
	// begin inline asm
	{ atom.add.f64 %fd1032,[%rd448],%fd1049; }

	// end inline asm
	add.s64 	%rd449, %rd445, 32;
	// begin inline asm
	{ atom.add.f64 %fd1034,[%rd449],%fd1049; }

	// end inline asm
	add.s64 	%rd450, %rd445, 40;
	// begin inline asm
	{ atom.add.f64 %fd1036,[%rd450],%fd1049; }

	// end inline asm
	add.s64 	%rd451, %rd445, 48;
	// begin inline asm
	{ atom.add.f64 %fd1038,[%rd451],%fd1049; }

	// end inline asm
	add.s64 	%rd452, %rd445, 56;
	// begin inline asm
	{ atom.add.f64 %fd1040,[%rd452],%fd1049; }

	// end inline asm
	add.s64 	%rd453, %rd445, 64;
	// begin inline asm
	{ atom.add.f64 %fd1042,[%rd453],%fd1049; }

	// end inline asm
	add.s64 	%rd454, %rd445, 72;
	// begin inline asm
	{ atom.add.f64 %fd1044,[%rd454],%fd115; }

	// end inline asm
	add.s64 	%rd455, %rd445, 80;
	// begin inline asm
	{ atom.add.f64 %fd1046,[%rd455],%fd1049; }

	// end inline asm
	add.s64 	%rd456, %rd445, 88;
	// begin inline asm
	{ atom.add.f64 %fd1048,[%rd456],%fd1049; }

	// end inline asm
	bra.uni 	$L__BB21_86;

$L__BB21_84:
	setp.eq.s64 	%p44, %rd60, 0;
	@%p44 bra 	$L__BB21_86;

	add.s64 	%rd458, %rd60, %rd49;
	mov.f64 	%fd1073, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1050,[%rd458],%fd1073; }

	// end inline asm
	add.s64 	%rd459, %rd458, 8;
	// begin inline asm
	{ atom.add.f64 %fd1052,[%rd459],%fd1073; }

	// end inline asm
	add.s64 	%rd460, %rd458, 16;
	// begin inline asm
	{ atom.add.f64 %fd1054,[%rd460],%fd1073; }

	// end inline asm
	add.s64 	%rd461, %rd458, 24;
	// begin inline asm
	{ atom.add.f64 %fd1056,[%rd461],%fd1073; }

	// end inline asm
	add.s64 	%rd462, %rd458, 32;
	// begin inline asm
	{ atom.add.f64 %fd1058,[%rd462],%fd1073; }

	// end inline asm
	add.s64 	%rd463, %rd458, 40;
	// begin inline asm
	{ atom.add.f64 %fd1060,[%rd463],%fd1073; }

	// end inline asm
	add.s64 	%rd464, %rd458, 48;
	// begin inline asm
	{ atom.add.f64 %fd1062,[%rd464],%fd1073; }

	// end inline asm
	add.s64 	%rd465, %rd458, 56;
	// begin inline asm
	{ atom.add.f64 %fd1064,[%rd465],%fd1073; }

	// end inline asm
	add.s64 	%rd466, %rd458, 64;
	// begin inline asm
	{ atom.add.f64 %fd1066,[%rd466],%fd1073; }

	// end inline asm
	add.s64 	%rd467, %rd458, 72;
	// begin inline asm
	{ atom.add.f64 %fd1068,[%rd467],%fd115; }

	// end inline asm
	add.s64 	%rd468, %rd458, 80;
	// begin inline asm
	{ atom.add.f64 %fd1070,[%rd468],%fd1073; }

	// end inline asm
	add.s64 	%rd469, %rd458, 88;
	// begin inline asm
	{ atom.add.f64 %fd1072,[%rd469],%fd1073; }

	// end inline asm

$L__BB21_86:
	add.f64 	%fd118, %fd102, 0d0000000000000000;
	add.f64 	%fd119, %fd77, 0d0000000000000000;
	add.f64 	%fd120, %fd78, 0d0000000000000000;
	@%p39 bra 	$L__BB21_88;

	mul.lo.s64 	%rd482, %rd48, %rd30;
	add.s64 	%rd470, %rd67, %rd482;
	mov.f64 	%fd1097, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1074,[%rd470],%fd1097; }

	// end inline asm
	add.s64 	%rd471, %rd470, 8;
	// begin inline asm
	{ atom.add.f64 %fd1076,[%rd471],%fd1097; }

	// end inline asm
	add.s64 	%rd472, %rd470, 16;
	// begin inline asm
	{ atom.add.f64 %fd1078,[%rd472],%fd1097; }

	// end inline asm
	add.s64 	%rd473, %rd470, 24;
	// begin inline asm
	{ atom.add.f64 %fd1080,[%rd473],%fd1097; }

	// end inline asm
	add.s64 	%rd474, %rd470, 32;
	// begin inline asm
	{ atom.add.f64 %fd1082,[%rd474],%fd1097; }

	// end inline asm
	add.s64 	%rd475, %rd470, 40;
	// begin inline asm
	{ atom.add.f64 %fd1084,[%rd475],%fd1097; }

	// end inline asm
	add.s64 	%rd476, %rd470, 48;
	// begin inline asm
	{ atom.add.f64 %fd1086,[%rd476],%fd1097; }

	// end inline asm
	add.s64 	%rd477, %rd470, 56;
	// begin inline asm
	{ atom.add.f64 %fd1088,[%rd477],%fd1097; }

	// end inline asm
	add.s64 	%rd478, %rd470, 64;
	// begin inline asm
	{ atom.add.f64 %fd1090,[%rd478],%fd120; }

	// end inline asm
	add.s64 	%rd479, %rd470, 72;
	// begin inline asm
	{ atom.add.f64 %fd1092,[%rd479],%fd1097; }

	// end inline asm
	add.s64 	%rd480, %rd470, 80;
	// begin inline asm
	{ atom.add.f64 %fd1094,[%rd480],%fd1097; }

	// end inline asm
	add.s64 	%rd481, %rd470, 88;
	// begin inline asm
	{ atom.add.f64 %fd1096,[%rd481],%fd1097; }

	// end inline asm
	bra.uni 	$L__BB21_90;

$L__BB21_88:
	setp.eq.s64 	%p46, %rd60, 0;
	@%p46 bra 	$L__BB21_90;

	add.s64 	%rd483, %rd60, %rd49;
	mov.f64 	%fd1121, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1098,[%rd483],%fd1121; }

	// end inline asm
	add.s64 	%rd484, %rd483, 8;
	// begin inline asm
	{ atom.add.f64 %fd1100,[%rd484],%fd1121; }

	// end inline asm
	add.s64 	%rd485, %rd483, 16;
	// begin inline asm
	{ atom.add.f64 %fd1102,[%rd485],%fd1121; }

	// end inline asm
	add.s64 	%rd486, %rd483, 24;
	// begin inline asm
	{ atom.add.f64 %fd1104,[%rd486],%fd1121; }

	// end inline asm
	add.s64 	%rd487, %rd483, 32;
	// begin inline asm
	{ atom.add.f64 %fd1106,[%rd487],%fd1121; }

	// end inline asm
	add.s64 	%rd488, %rd483, 40;
	// begin inline asm
	{ atom.add.f64 %fd1108,[%rd488],%fd1121; }

	// end inline asm
	add.s64 	%rd489, %rd483, 48;
	// begin inline asm
	{ atom.add.f64 %fd1110,[%rd489],%fd1121; }

	// end inline asm
	add.s64 	%rd490, %rd483, 56;
	// begin inline asm
	{ atom.add.f64 %fd1112,[%rd490],%fd1121; }

	// end inline asm
	add.s64 	%rd491, %rd483, 64;
	// begin inline asm
	{ atom.add.f64 %fd1114,[%rd491],%fd120; }

	// end inline asm
	add.s64 	%rd492, %rd483, 72;
	// begin inline asm
	{ atom.add.f64 %fd1116,[%rd492],%fd1121; }

	// end inline asm
	add.s64 	%rd493, %rd483, 80;
	// begin inline asm
	{ atom.add.f64 %fd1118,[%rd493],%fd1121; }

	// end inline asm
	add.s64 	%rd494, %rd483, 88;
	// begin inline asm
	{ atom.add.f64 %fd1120,[%rd494],%fd1121; }

	// end inline asm

$L__BB21_90:
	@%p39 bra 	$L__BB21_92;

	mul.lo.s64 	%rd507, %rd48, %rd30;
	add.s64 	%rd495, %rd67, %rd507;
	mov.f64 	%fd1145, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1122,[%rd495],%fd1145; }

	// end inline asm
	add.s64 	%rd496, %rd495, 8;
	// begin inline asm
	{ atom.add.f64 %fd1124,[%rd496],%fd1145; }

	// end inline asm
	add.s64 	%rd497, %rd495, 16;
	// begin inline asm
	{ atom.add.f64 %fd1126,[%rd497],%fd1145; }

	// end inline asm
	add.s64 	%rd498, %rd495, 24;
	// begin inline asm
	{ atom.add.f64 %fd1128,[%rd498],%fd1145; }

	// end inline asm
	add.s64 	%rd499, %rd495, 32;
	// begin inline asm
	{ atom.add.f64 %fd1130,[%rd499],%fd1145; }

	// end inline asm
	add.s64 	%rd500, %rd495, 40;
	// begin inline asm
	{ atom.add.f64 %fd1132,[%rd500],%fd1145; }

	// end inline asm
	add.s64 	%rd501, %rd495, 48;
	// begin inline asm
	{ atom.add.f64 %fd1134,[%rd501],%fd1145; }

	// end inline asm
	add.s64 	%rd502, %rd495, 56;
	// begin inline asm
	{ atom.add.f64 %fd1136,[%rd502],%fd119; }

	// end inline asm
	add.s64 	%rd503, %rd495, 64;
	// begin inline asm
	{ atom.add.f64 %fd1138,[%rd503],%fd1145; }

	// end inline asm
	add.s64 	%rd504, %rd495, 72;
	// begin inline asm
	{ atom.add.f64 %fd1140,[%rd504],%fd1145; }

	// end inline asm
	add.s64 	%rd505, %rd495, 80;
	// begin inline asm
	{ atom.add.f64 %fd1142,[%rd505],%fd1145; }

	// end inline asm
	add.s64 	%rd506, %rd495, 88;
	// begin inline asm
	{ atom.add.f64 %fd1144,[%rd506],%fd1145; }

	// end inline asm
	bra.uni 	$L__BB21_94;

$L__BB21_92:
	setp.eq.s64 	%p48, %rd60, 0;
	@%p48 bra 	$L__BB21_94;

	add.s64 	%rd508, %rd60, %rd49;
	mov.f64 	%fd1169, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1146,[%rd508],%fd1169; }

	// end inline asm
	add.s64 	%rd509, %rd508, 8;
	// begin inline asm
	{ atom.add.f64 %fd1148,[%rd509],%fd1169; }

	// end inline asm
	add.s64 	%rd510, %rd508, 16;
	// begin inline asm
	{ atom.add.f64 %fd1150,[%rd510],%fd1169; }

	// end inline asm
	add.s64 	%rd511, %rd508, 24;
	// begin inline asm
	{ atom.add.f64 %fd1152,[%rd511],%fd1169; }

	// end inline asm
	add.s64 	%rd512, %rd508, 32;
	// begin inline asm
	{ atom.add.f64 %fd1154,[%rd512],%fd1169; }

	// end inline asm
	add.s64 	%rd513, %rd508, 40;
	// begin inline asm
	{ atom.add.f64 %fd1156,[%rd513],%fd1169; }

	// end inline asm
	add.s64 	%rd514, %rd508, 48;
	// begin inline asm
	{ atom.add.f64 %fd1158,[%rd514],%fd1169; }

	// end inline asm
	add.s64 	%rd515, %rd508, 56;
	// begin inline asm
	{ atom.add.f64 %fd1160,[%rd515],%fd119; }

	// end inline asm
	add.s64 	%rd516, %rd508, 64;
	// begin inline asm
	{ atom.add.f64 %fd1162,[%rd516],%fd1169; }

	// end inline asm
	add.s64 	%rd517, %rd508, 72;
	// begin inline asm
	{ atom.add.f64 %fd1164,[%rd517],%fd1169; }

	// end inline asm
	add.s64 	%rd518, %rd508, 80;
	// begin inline asm
	{ atom.add.f64 %fd1166,[%rd518],%fd1169; }

	// end inline asm
	add.s64 	%rd519, %rd508, 88;
	// begin inline asm
	{ atom.add.f64 %fd1168,[%rd519],%fd1169; }

	// end inline asm

$L__BB21_94:
	@%p39 bra 	$L__BB21_96;

	mul.lo.s64 	%rd532, %rd48, %rd30;
	add.s64 	%rd520, %rd67, %rd532;
	mov.f64 	%fd1193, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1170,[%rd520],%fd1193; }

	// end inline asm
	add.s64 	%rd521, %rd520, 8;
	// begin inline asm
	{ atom.add.f64 %fd1172,[%rd521],%fd1193; }

	// end inline asm
	add.s64 	%rd522, %rd520, 16;
	// begin inline asm
	{ atom.add.f64 %fd1174,[%rd522],%fd1193; }

	// end inline asm
	add.s64 	%rd523, %rd520, 24;
	// begin inline asm
	{ atom.add.f64 %fd1176,[%rd523],%fd1193; }

	// end inline asm
	add.s64 	%rd524, %rd520, 32;
	// begin inline asm
	{ atom.add.f64 %fd1178,[%rd524],%fd1193; }

	// end inline asm
	add.s64 	%rd525, %rd520, 40;
	// begin inline asm
	{ atom.add.f64 %fd1180,[%rd525],%fd1193; }

	// end inline asm
	add.s64 	%rd526, %rd520, 48;
	// begin inline asm
	{ atom.add.f64 %fd1182,[%rd526],%fd118; }

	// end inline asm
	add.s64 	%rd527, %rd520, 56;
	// begin inline asm
	{ atom.add.f64 %fd1184,[%rd527],%fd1193; }

	// end inline asm
	add.s64 	%rd528, %rd520, 64;
	// begin inline asm
	{ atom.add.f64 %fd1186,[%rd528],%fd1193; }

	// end inline asm
	add.s64 	%rd529, %rd520, 72;
	// begin inline asm
	{ atom.add.f64 %fd1188,[%rd529],%fd1193; }

	// end inline asm
	add.s64 	%rd530, %rd520, 80;
	// begin inline asm
	{ atom.add.f64 %fd1190,[%rd530],%fd1193; }

	// end inline asm
	add.s64 	%rd531, %rd520, 88;
	// begin inline asm
	{ atom.add.f64 %fd1192,[%rd531],%fd1193; }

	// end inline asm
	bra.uni 	$L__BB21_98;

$L__BB21_96:
	setp.eq.s64 	%p50, %rd60, 0;
	@%p50 bra 	$L__BB21_98;

	add.s64 	%rd533, %rd60, %rd49;
	mov.f64 	%fd1217, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1194,[%rd533],%fd1217; }

	// end inline asm
	add.s64 	%rd534, %rd533, 8;
	// begin inline asm
	{ atom.add.f64 %fd1196,[%rd534],%fd1217; }

	// end inline asm
	add.s64 	%rd535, %rd533, 16;
	// begin inline asm
	{ atom.add.f64 %fd1198,[%rd535],%fd1217; }

	// end inline asm
	add.s64 	%rd536, %rd533, 24;
	// begin inline asm
	{ atom.add.f64 %fd1200,[%rd536],%fd1217; }

	// end inline asm
	add.s64 	%rd537, %rd533, 32;
	// begin inline asm
	{ atom.add.f64 %fd1202,[%rd537],%fd1217; }

	// end inline asm
	add.s64 	%rd538, %rd533, 40;
	// begin inline asm
	{ atom.add.f64 %fd1204,[%rd538],%fd1217; }

	// end inline asm
	add.s64 	%rd539, %rd533, 48;
	// begin inline asm
	{ atom.add.f64 %fd1206,[%rd539],%fd118; }

	// end inline asm
	add.s64 	%rd540, %rd533, 56;
	// begin inline asm
	{ atom.add.f64 %fd1208,[%rd540],%fd1217; }

	// end inline asm
	add.s64 	%rd541, %rd533, 64;
	// begin inline asm
	{ atom.add.f64 %fd1210,[%rd541],%fd1217; }

	// end inline asm
	add.s64 	%rd542, %rd533, 72;
	// begin inline asm
	{ atom.add.f64 %fd1212,[%rd542],%fd1217; }

	// end inline asm
	add.s64 	%rd543, %rd533, 80;
	// begin inline asm
	{ atom.add.f64 %fd1214,[%rd543],%fd1217; }

	// end inline asm
	add.s64 	%rd544, %rd533, 88;
	// begin inline asm
	{ atom.add.f64 %fd1216,[%rd544],%fd1217; }

	// end inline asm

$L__BB21_98:
	add.f64 	%fd121, %fd108, 0d0000000000000000;
	add.f64 	%fd122, %fd80, 0d0000000000000000;
	add.f64 	%fd123, %fd81, 0d0000000000000000;
	@%p39 bra 	$L__BB21_100;

	mul.lo.s64 	%rd557, %rd48, %rd30;
	add.s64 	%rd545, %rd67, %rd557;
	mov.f64 	%fd1241, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1218,[%rd545],%fd1241; }

	// end inline asm
	add.s64 	%rd546, %rd545, 8;
	// begin inline asm
	{ atom.add.f64 %fd1220,[%rd546],%fd1241; }

	// end inline asm
	add.s64 	%rd547, %rd545, 16;
	// begin inline asm
	{ atom.add.f64 %fd1222,[%rd547],%fd1241; }

	// end inline asm
	add.s64 	%rd548, %rd545, 24;
	// begin inline asm
	{ atom.add.f64 %fd1224,[%rd548],%fd1241; }

	// end inline asm
	add.s64 	%rd549, %rd545, 32;
	// begin inline asm
	{ atom.add.f64 %fd1226,[%rd549],%fd1241; }

	// end inline asm
	add.s64 	%rd550, %rd545, 40;
	// begin inline asm
	{ atom.add.f64 %fd1228,[%rd550],%fd123; }

	// end inline asm
	add.s64 	%rd551, %rd545, 48;
	// begin inline asm
	{ atom.add.f64 %fd1230,[%rd551],%fd1241; }

	// end inline asm
	add.s64 	%rd552, %rd545, 56;
	// begin inline asm
	{ atom.add.f64 %fd1232,[%rd552],%fd1241; }

	// end inline asm
	add.s64 	%rd553, %rd545, 64;
	// begin inline asm
	{ atom.add.f64 %fd1234,[%rd553],%fd1241; }

	// end inline asm
	add.s64 	%rd554, %rd545, 72;
	// begin inline asm
	{ atom.add.f64 %fd1236,[%rd554],%fd1241; }

	// end inline asm
	add.s64 	%rd555, %rd545, 80;
	// begin inline asm
	{ atom.add.f64 %fd1238,[%rd555],%fd1241; }

	// end inline asm
	add.s64 	%rd556, %rd545, 88;
	// begin inline asm
	{ atom.add.f64 %fd1240,[%rd556],%fd1241; }

	// end inline asm
	bra.uni 	$L__BB21_102;

$L__BB21_100:
	setp.eq.s64 	%p52, %rd60, 0;
	@%p52 bra 	$L__BB21_102;

	add.s64 	%rd558, %rd60, %rd49;
	mov.f64 	%fd1265, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1242,[%rd558],%fd1265; }

	// end inline asm
	add.s64 	%rd559, %rd558, 8;
	// begin inline asm
	{ atom.add.f64 %fd1244,[%rd559],%fd1265; }

	// end inline asm
	add.s64 	%rd560, %rd558, 16;
	// begin inline asm
	{ atom.add.f64 %fd1246,[%rd560],%fd1265; }

	// end inline asm
	add.s64 	%rd561, %rd558, 24;
	// begin inline asm
	{ atom.add.f64 %fd1248,[%rd561],%fd1265; }

	// end inline asm
	add.s64 	%rd562, %rd558, 32;
	// begin inline asm
	{ atom.add.f64 %fd1250,[%rd562],%fd1265; }

	// end inline asm
	add.s64 	%rd563, %rd558, 40;
	// begin inline asm
	{ atom.add.f64 %fd1252,[%rd563],%fd123; }

	// end inline asm
	add.s64 	%rd564, %rd558, 48;
	// begin inline asm
	{ atom.add.f64 %fd1254,[%rd564],%fd1265; }

	// end inline asm
	add.s64 	%rd565, %rd558, 56;
	// begin inline asm
	{ atom.add.f64 %fd1256,[%rd565],%fd1265; }

	// end inline asm
	add.s64 	%rd566, %rd558, 64;
	// begin inline asm
	{ atom.add.f64 %fd1258,[%rd566],%fd1265; }

	// end inline asm
	add.s64 	%rd567, %rd558, 72;
	// begin inline asm
	{ atom.add.f64 %fd1260,[%rd567],%fd1265; }

	// end inline asm
	add.s64 	%rd568, %rd558, 80;
	// begin inline asm
	{ atom.add.f64 %fd1262,[%rd568],%fd1265; }

	// end inline asm
	add.s64 	%rd569, %rd558, 88;
	// begin inline asm
	{ atom.add.f64 %fd1264,[%rd569],%fd1265; }

	// end inline asm

$L__BB21_102:
	@%p39 bra 	$L__BB21_104;

	mul.lo.s64 	%rd582, %rd48, %rd30;
	add.s64 	%rd570, %rd67, %rd582;
	mov.f64 	%fd1289, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1266,[%rd570],%fd1289; }

	// end inline asm
	add.s64 	%rd571, %rd570, 8;
	// begin inline asm
	{ atom.add.f64 %fd1268,[%rd571],%fd1289; }

	// end inline asm
	add.s64 	%rd572, %rd570, 16;
	// begin inline asm
	{ atom.add.f64 %fd1270,[%rd572],%fd1289; }

	// end inline asm
	add.s64 	%rd573, %rd570, 24;
	// begin inline asm
	{ atom.add.f64 %fd1272,[%rd573],%fd1289; }

	// end inline asm
	add.s64 	%rd574, %rd570, 32;
	// begin inline asm
	{ atom.add.f64 %fd1274,[%rd574],%fd122; }

	// end inline asm
	add.s64 	%rd575, %rd570, 40;
	// begin inline asm
	{ atom.add.f64 %fd1276,[%rd575],%fd1289; }

	// end inline asm
	add.s64 	%rd576, %rd570, 48;
	// begin inline asm
	{ atom.add.f64 %fd1278,[%rd576],%fd1289; }

	// end inline asm
	add.s64 	%rd577, %rd570, 56;
	// begin inline asm
	{ atom.add.f64 %fd1280,[%rd577],%fd1289; }

	// end inline asm
	add.s64 	%rd578, %rd570, 64;
	// begin inline asm
	{ atom.add.f64 %fd1282,[%rd578],%fd1289; }

	// end inline asm
	add.s64 	%rd579, %rd570, 72;
	// begin inline asm
	{ atom.add.f64 %fd1284,[%rd579],%fd1289; }

	// end inline asm
	add.s64 	%rd580, %rd570, 80;
	// begin inline asm
	{ atom.add.f64 %fd1286,[%rd580],%fd1289; }

	// end inline asm
	add.s64 	%rd581, %rd570, 88;
	// begin inline asm
	{ atom.add.f64 %fd1288,[%rd581],%fd1289; }

	// end inline asm
	bra.uni 	$L__BB21_106;

$L__BB21_104:
	setp.eq.s64 	%p54, %rd60, 0;
	@%p54 bra 	$L__BB21_106;

	add.s64 	%rd583, %rd60, %rd49;
	mov.f64 	%fd1313, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1290,[%rd583],%fd1313; }

	// end inline asm
	add.s64 	%rd584, %rd583, 8;
	// begin inline asm
	{ atom.add.f64 %fd1292,[%rd584],%fd1313; }

	// end inline asm
	add.s64 	%rd585, %rd583, 16;
	// begin inline asm
	{ atom.add.f64 %fd1294,[%rd585],%fd1313; }

	// end inline asm
	add.s64 	%rd586, %rd583, 24;
	// begin inline asm
	{ atom.add.f64 %fd1296,[%rd586],%fd1313; }

	// end inline asm
	add.s64 	%rd587, %rd583, 32;
	// begin inline asm
	{ atom.add.f64 %fd1298,[%rd587],%fd122; }

	// end inline asm
	add.s64 	%rd588, %rd583, 40;
	// begin inline asm
	{ atom.add.f64 %fd1300,[%rd588],%fd1313; }

	// end inline asm
	add.s64 	%rd589, %rd583, 48;
	// begin inline asm
	{ atom.add.f64 %fd1302,[%rd589],%fd1313; }

	// end inline asm
	add.s64 	%rd590, %rd583, 56;
	// begin inline asm
	{ atom.add.f64 %fd1304,[%rd590],%fd1313; }

	// end inline asm
	add.s64 	%rd591, %rd583, 64;
	// begin inline asm
	{ atom.add.f64 %fd1306,[%rd591],%fd1313; }

	// end inline asm
	add.s64 	%rd592, %rd583, 72;
	// begin inline asm
	{ atom.add.f64 %fd1308,[%rd592],%fd1313; }

	// end inline asm
	add.s64 	%rd593, %rd583, 80;
	// begin inline asm
	{ atom.add.f64 %fd1310,[%rd593],%fd1313; }

	// end inline asm
	add.s64 	%rd594, %rd583, 88;
	// begin inline asm
	{ atom.add.f64 %fd1312,[%rd594],%fd1313; }

	// end inline asm

$L__BB21_106:
	@%p39 bra 	$L__BB21_108;

	mul.lo.s64 	%rd607, %rd48, %rd30;
	add.s64 	%rd595, %rd67, %rd607;
	mov.f64 	%fd1337, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1314,[%rd595],%fd1337; }

	// end inline asm
	add.s64 	%rd596, %rd595, 8;
	// begin inline asm
	{ atom.add.f64 %fd1316,[%rd596],%fd1337; }

	// end inline asm
	add.s64 	%rd597, %rd595, 16;
	// begin inline asm
	{ atom.add.f64 %fd1318,[%rd597],%fd1337; }

	// end inline asm
	add.s64 	%rd598, %rd595, 24;
	// begin inline asm
	{ atom.add.f64 %fd1320,[%rd598],%fd121; }

	// end inline asm
	add.s64 	%rd599, %rd595, 32;
	// begin inline asm
	{ atom.add.f64 %fd1322,[%rd599],%fd1337; }

	// end inline asm
	add.s64 	%rd600, %rd595, 40;
	// begin inline asm
	{ atom.add.f64 %fd1324,[%rd600],%fd1337; }

	// end inline asm
	add.s64 	%rd601, %rd595, 48;
	// begin inline asm
	{ atom.add.f64 %fd1326,[%rd601],%fd1337; }

	// end inline asm
	add.s64 	%rd602, %rd595, 56;
	// begin inline asm
	{ atom.add.f64 %fd1328,[%rd602],%fd1337; }

	// end inline asm
	add.s64 	%rd603, %rd595, 64;
	// begin inline asm
	{ atom.add.f64 %fd1330,[%rd603],%fd1337; }

	// end inline asm
	add.s64 	%rd604, %rd595, 72;
	// begin inline asm
	{ atom.add.f64 %fd1332,[%rd604],%fd1337; }

	// end inline asm
	add.s64 	%rd605, %rd595, 80;
	// begin inline asm
	{ atom.add.f64 %fd1334,[%rd605],%fd1337; }

	// end inline asm
	add.s64 	%rd606, %rd595, 88;
	// begin inline asm
	{ atom.add.f64 %fd1336,[%rd606],%fd1337; }

	// end inline asm
	bra.uni 	$L__BB21_110;

$L__BB21_108:
	setp.eq.s64 	%p56, %rd60, 0;
	@%p56 bra 	$L__BB21_110;

	add.s64 	%rd608, %rd60, %rd49;
	mov.f64 	%fd1361, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1338,[%rd608],%fd1361; }

	// end inline asm
	add.s64 	%rd609, %rd608, 8;
	// begin inline asm
	{ atom.add.f64 %fd1340,[%rd609],%fd1361; }

	// end inline asm
	add.s64 	%rd610, %rd608, 16;
	// begin inline asm
	{ atom.add.f64 %fd1342,[%rd610],%fd1361; }

	// end inline asm
	add.s64 	%rd611, %rd608, 24;
	// begin inline asm
	{ atom.add.f64 %fd1344,[%rd611],%fd121; }

	// end inline asm
	add.s64 	%rd612, %rd608, 32;
	// begin inline asm
	{ atom.add.f64 %fd1346,[%rd612],%fd1361; }

	// end inline asm
	add.s64 	%rd613, %rd608, 40;
	// begin inline asm
	{ atom.add.f64 %fd1348,[%rd613],%fd1361; }

	// end inline asm
	add.s64 	%rd614, %rd608, 48;
	// begin inline asm
	{ atom.add.f64 %fd1350,[%rd614],%fd1361; }

	// end inline asm
	add.s64 	%rd615, %rd608, 56;
	// begin inline asm
	{ atom.add.f64 %fd1352,[%rd615],%fd1361; }

	// end inline asm
	add.s64 	%rd616, %rd608, 64;
	// begin inline asm
	{ atom.add.f64 %fd1354,[%rd616],%fd1361; }

	// end inline asm
	add.s64 	%rd617, %rd608, 72;
	// begin inline asm
	{ atom.add.f64 %fd1356,[%rd617],%fd1361; }

	// end inline asm
	add.s64 	%rd618, %rd608, 80;
	// begin inline asm
	{ atom.add.f64 %fd1358,[%rd618],%fd1361; }

	// end inline asm
	add.s64 	%rd619, %rd608, 88;
	// begin inline asm
	{ atom.add.f64 %fd1360,[%rd619],%fd1361; }

	// end inline asm

$L__BB21_110:
	add.f64 	%fd124, %fd114, 0d0000000000000000;
	add.f64 	%fd125, %fd83, 0d0000000000000000;
	add.f64 	%fd126, %fd84, 0d0000000000000000;
	@%p39 bra 	$L__BB21_112;

	mul.lo.s64 	%rd632, %rd48, %rd30;
	add.s64 	%rd620, %rd67, %rd632;
	mov.f64 	%fd1385, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1362,[%rd620],%fd1385; }

	// end inline asm
	add.s64 	%rd621, %rd620, 8;
	// begin inline asm
	{ atom.add.f64 %fd1364,[%rd621],%fd1385; }

	// end inline asm
	add.s64 	%rd622, %rd620, 16;
	// begin inline asm
	{ atom.add.f64 %fd1366,[%rd622],%fd126; }

	// end inline asm
	add.s64 	%rd623, %rd620, 24;
	// begin inline asm
	{ atom.add.f64 %fd1368,[%rd623],%fd1385; }

	// end inline asm
	add.s64 	%rd624, %rd620, 32;
	// begin inline asm
	{ atom.add.f64 %fd1370,[%rd624],%fd1385; }

	// end inline asm
	add.s64 	%rd625, %rd620, 40;
	// begin inline asm
	{ atom.add.f64 %fd1372,[%rd625],%fd1385; }

	// end inline asm
	add.s64 	%rd626, %rd620, 48;
	// begin inline asm
	{ atom.add.f64 %fd1374,[%rd626],%fd1385; }

	// end inline asm
	add.s64 	%rd627, %rd620, 56;
	// begin inline asm
	{ atom.add.f64 %fd1376,[%rd627],%fd1385; }

	// end inline asm
	add.s64 	%rd628, %rd620, 64;
	// begin inline asm
	{ atom.add.f64 %fd1378,[%rd628],%fd1385; }

	// end inline asm
	add.s64 	%rd629, %rd620, 72;
	// begin inline asm
	{ atom.add.f64 %fd1380,[%rd629],%fd1385; }

	// end inline asm
	add.s64 	%rd630, %rd620, 80;
	// begin inline asm
	{ atom.add.f64 %fd1382,[%rd630],%fd1385; }

	// end inline asm
	add.s64 	%rd631, %rd620, 88;
	// begin inline asm
	{ atom.add.f64 %fd1384,[%rd631],%fd1385; }

	// end inline asm
	bra.uni 	$L__BB21_114;

$L__BB21_112:
	setp.eq.s64 	%p58, %rd60, 0;
	@%p58 bra 	$L__BB21_114;

	add.s64 	%rd633, %rd60, %rd49;
	mov.f64 	%fd1409, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1386,[%rd633],%fd1409; }

	// end inline asm
	add.s64 	%rd634, %rd633, 8;
	// begin inline asm
	{ atom.add.f64 %fd1388,[%rd634],%fd1409; }

	// end inline asm
	add.s64 	%rd635, %rd633, 16;
	// begin inline asm
	{ atom.add.f64 %fd1390,[%rd635],%fd126; }

	// end inline asm
	add.s64 	%rd636, %rd633, 24;
	// begin inline asm
	{ atom.add.f64 %fd1392,[%rd636],%fd1409; }

	// end inline asm
	add.s64 	%rd637, %rd633, 32;
	// begin inline asm
	{ atom.add.f64 %fd1394,[%rd637],%fd1409; }

	// end inline asm
	add.s64 	%rd638, %rd633, 40;
	// begin inline asm
	{ atom.add.f64 %fd1396,[%rd638],%fd1409; }

	// end inline asm
	add.s64 	%rd639, %rd633, 48;
	// begin inline asm
	{ atom.add.f64 %fd1398,[%rd639],%fd1409; }

	// end inline asm
	add.s64 	%rd640, %rd633, 56;
	// begin inline asm
	{ atom.add.f64 %fd1400,[%rd640],%fd1409; }

	// end inline asm
	add.s64 	%rd641, %rd633, 64;
	// begin inline asm
	{ atom.add.f64 %fd1402,[%rd641],%fd1409; }

	// end inline asm
	add.s64 	%rd642, %rd633, 72;
	// begin inline asm
	{ atom.add.f64 %fd1404,[%rd642],%fd1409; }

	// end inline asm
	add.s64 	%rd643, %rd633, 80;
	// begin inline asm
	{ atom.add.f64 %fd1406,[%rd643],%fd1409; }

	// end inline asm
	add.s64 	%rd644, %rd633, 88;
	// begin inline asm
	{ atom.add.f64 %fd1408,[%rd644],%fd1409; }

	// end inline asm

$L__BB21_114:
	@%p39 bra 	$L__BB21_116;

	mul.lo.s64 	%rd657, %rd48, %rd30;
	add.s64 	%rd645, %rd67, %rd657;
	mov.f64 	%fd1433, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1410,[%rd645],%fd1433; }

	// end inline asm
	add.s64 	%rd646, %rd645, 8;
	// begin inline asm
	{ atom.add.f64 %fd1412,[%rd646],%fd125; }

	// end inline asm
	add.s64 	%rd647, %rd645, 16;
	// begin inline asm
	{ atom.add.f64 %fd1414,[%rd647],%fd1433; }

	// end inline asm
	add.s64 	%rd648, %rd645, 24;
	// begin inline asm
	{ atom.add.f64 %fd1416,[%rd648],%fd1433; }

	// end inline asm
	add.s64 	%rd649, %rd645, 32;
	// begin inline asm
	{ atom.add.f64 %fd1418,[%rd649],%fd1433; }

	// end inline asm
	add.s64 	%rd650, %rd645, 40;
	// begin inline asm
	{ atom.add.f64 %fd1420,[%rd650],%fd1433; }

	// end inline asm
	add.s64 	%rd651, %rd645, 48;
	// begin inline asm
	{ atom.add.f64 %fd1422,[%rd651],%fd1433; }

	// end inline asm
	add.s64 	%rd652, %rd645, 56;
	// begin inline asm
	{ atom.add.f64 %fd1424,[%rd652],%fd1433; }

	// end inline asm
	add.s64 	%rd653, %rd645, 64;
	// begin inline asm
	{ atom.add.f64 %fd1426,[%rd653],%fd1433; }

	// end inline asm
	add.s64 	%rd654, %rd645, 72;
	// begin inline asm
	{ atom.add.f64 %fd1428,[%rd654],%fd1433; }

	// end inline asm
	add.s64 	%rd655, %rd645, 80;
	// begin inline asm
	{ atom.add.f64 %fd1430,[%rd655],%fd1433; }

	// end inline asm
	add.s64 	%rd656, %rd645, 88;
	// begin inline asm
	{ atom.add.f64 %fd1432,[%rd656],%fd1433; }

	// end inline asm
	bra.uni 	$L__BB21_118;

$L__BB21_116:
	setp.eq.s64 	%p60, %rd60, 0;
	@%p60 bra 	$L__BB21_118;

	add.s64 	%rd658, %rd60, %rd49;
	mov.f64 	%fd1457, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1434,[%rd658],%fd1457; }

	// end inline asm
	add.s64 	%rd659, %rd658, 8;
	// begin inline asm
	{ atom.add.f64 %fd1436,[%rd659],%fd125; }

	// end inline asm
	add.s64 	%rd660, %rd658, 16;
	// begin inline asm
	{ atom.add.f64 %fd1438,[%rd660],%fd1457; }

	// end inline asm
	add.s64 	%rd661, %rd658, 24;
	// begin inline asm
	{ atom.add.f64 %fd1440,[%rd661],%fd1457; }

	// end inline asm
	add.s64 	%rd662, %rd658, 32;
	// begin inline asm
	{ atom.add.f64 %fd1442,[%rd662],%fd1457; }

	// end inline asm
	add.s64 	%rd663, %rd658, 40;
	// begin inline asm
	{ atom.add.f64 %fd1444,[%rd663],%fd1457; }

	// end inline asm
	add.s64 	%rd664, %rd658, 48;
	// begin inline asm
	{ atom.add.f64 %fd1446,[%rd664],%fd1457; }

	// end inline asm
	add.s64 	%rd665, %rd658, 56;
	// begin inline asm
	{ atom.add.f64 %fd1448,[%rd665],%fd1457; }

	// end inline asm
	add.s64 	%rd666, %rd658, 64;
	// begin inline asm
	{ atom.add.f64 %fd1450,[%rd666],%fd1457; }

	// end inline asm
	add.s64 	%rd667, %rd658, 72;
	// begin inline asm
	{ atom.add.f64 %fd1452,[%rd667],%fd1457; }

	// end inline asm
	add.s64 	%rd668, %rd658, 80;
	// begin inline asm
	{ atom.add.f64 %fd1454,[%rd668],%fd1457; }

	// end inline asm
	add.s64 	%rd669, %rd658, 88;
	// begin inline asm
	{ atom.add.f64 %fd1456,[%rd669],%fd1457; }

	// end inline asm

$L__BB21_118:
	@%p39 bra 	$L__BB21_120;

	mul.lo.s64 	%rd682, %rd48, %rd30;
	add.s64 	%rd670, %rd67, %rd682;
	// begin inline asm
	{ atom.add.f64 %fd1458,[%rd670],%fd124; }

	// end inline asm
	add.s64 	%rd671, %rd670, 8;
	mov.f64 	%fd1481, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1460,[%rd671],%fd1481; }

	// end inline asm
	add.s64 	%rd672, %rd670, 16;
	// begin inline asm
	{ atom.add.f64 %fd1462,[%rd672],%fd1481; }

	// end inline asm
	add.s64 	%rd673, %rd670, 24;
	// begin inline asm
	{ atom.add.f64 %fd1464,[%rd673],%fd1481; }

	// end inline asm
	add.s64 	%rd674, %rd670, 32;
	// begin inline asm
	{ atom.add.f64 %fd1466,[%rd674],%fd1481; }

	// end inline asm
	add.s64 	%rd675, %rd670, 40;
	// begin inline asm
	{ atom.add.f64 %fd1468,[%rd675],%fd1481; }

	// end inline asm
	add.s64 	%rd676, %rd670, 48;
	// begin inline asm
	{ atom.add.f64 %fd1470,[%rd676],%fd1481; }

	// end inline asm
	add.s64 	%rd677, %rd670, 56;
	// begin inline asm
	{ atom.add.f64 %fd1472,[%rd677],%fd1481; }

	// end inline asm
	add.s64 	%rd678, %rd670, 64;
	// begin inline asm
	{ atom.add.f64 %fd1474,[%rd678],%fd1481; }

	// end inline asm
	add.s64 	%rd679, %rd670, 72;
	// begin inline asm
	{ atom.add.f64 %fd1476,[%rd679],%fd1481; }

	// end inline asm
	add.s64 	%rd680, %rd670, 80;
	// begin inline asm
	{ atom.add.f64 %fd1478,[%rd680],%fd1481; }

	// end inline asm
	add.s64 	%rd681, %rd670, 88;
	// begin inline asm
	{ atom.add.f64 %fd1480,[%rd681],%fd1481; }

	// end inline asm
	bra.uni 	$L__BB21_122;

$L__BB21_120:
	setp.eq.s64 	%p62, %rd60, 0;
	@%p62 bra 	$L__BB21_122;

	add.s64 	%rd683, %rd60, %rd49;
	// begin inline asm
	{ atom.add.f64 %fd1482,[%rd683],%fd124; }

	// end inline asm
	add.s64 	%rd684, %rd683, 8;
	mov.f64 	%fd1505, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1484,[%rd684],%fd1505; }

	// end inline asm
	add.s64 	%rd685, %rd683, 16;
	// begin inline asm
	{ atom.add.f64 %fd1486,[%rd685],%fd1505; }

	// end inline asm
	add.s64 	%rd686, %rd683, 24;
	// begin inline asm
	{ atom.add.f64 %fd1488,[%rd686],%fd1505; }

	// end inline asm
	add.s64 	%rd687, %rd683, 32;
	// begin inline asm
	{ atom.add.f64 %fd1490,[%rd687],%fd1505; }

	// end inline asm
	add.s64 	%rd688, %rd683, 40;
	// begin inline asm
	{ atom.add.f64 %fd1492,[%rd688],%fd1505; }

	// end inline asm
	add.s64 	%rd689, %rd683, 48;
	// begin inline asm
	{ atom.add.f64 %fd1494,[%rd689],%fd1505; }

	// end inline asm
	add.s64 	%rd690, %rd683, 56;
	// begin inline asm
	{ atom.add.f64 %fd1496,[%rd690],%fd1505; }

	// end inline asm
	add.s64 	%rd691, %rd683, 64;
	// begin inline asm
	{ atom.add.f64 %fd1498,[%rd691],%fd1505; }

	// end inline asm
	add.s64 	%rd692, %rd683, 72;
	// begin inline asm
	{ atom.add.f64 %fd1500,[%rd692],%fd1505; }

	// end inline asm
	add.s64 	%rd693, %rd683, 80;
	// begin inline asm
	{ atom.add.f64 %fd1502,[%rd693],%fd1505; }

	// end inline asm
	add.s64 	%rd694, %rd683, 88;
	// begin inline asm
	{ atom.add.f64 %fd1504,[%rd694],%fd1505; }

	// end inline asm

$L__BB21_122:
	setp.eq.s64 	%p63, %rd71, 0;
	add.f64 	%fd127, %fd87, 0d0000000000000000;
	add.f64 	%fd128, %fd86, 0d0000000000000000;
	add.f64 	%fd129, %fd85, 0d0000000000000000;
	@%p63 bra 	$L__BB21_124;

	mul.lo.s64 	%rd698, %rd46, %rd31;
	add.s64 	%rd695, %rd71, %rd698;
	// begin inline asm
	{ atom.add.f64 %fd1506,[%rd695],%fd127; }

	// end inline asm
	add.s64 	%rd696, %rd695, 8;
	// begin inline asm
	{ atom.add.f64 %fd1508,[%rd696],%fd128; }

	// end inline asm
	add.s64 	%rd697, %rd695, 16;
	// begin inline asm
	{ atom.add.f64 %fd1510,[%rd697],%fd129; }

	// end inline asm
	bra.uni 	$L__BB21_126;

$L__BB21_124:
	setp.eq.s64 	%p64, %rd64, 0;
	@%p64 bra 	$L__BB21_126;

	mul.lo.s64 	%rd712, %rd46, %rd710;
	add.s64 	%rd699, %rd64, %rd712;
	// begin inline asm
	{ atom.add.f64 %fd1512,[%rd699],%fd127; }

	// end inline asm
	add.s64 	%rd700, %rd699, 8;
	// begin inline asm
	{ atom.add.f64 %fd1514,[%rd700],%fd128; }

	// end inline asm
	add.s64 	%rd701, %rd699, 16;
	// begin inline asm
	{ atom.add.f64 %fd1516,[%rd701],%fd129; }

	// end inline asm

$L__BB21_126:
	add.f64 	%fd130, %fd90, 0d0000000000000000;
	add.f64 	%fd131, %fd89, 0d0000000000000000;
	add.f64 	%fd132, %fd88, 0d0000000000000000;
	@%p63 bra 	$L__BB21_128;

	mul.lo.s64 	%rd705, %rd44, %rd31;
	add.s64 	%rd702, %rd71, %rd705;
	// begin inline asm
	{ atom.add.f64 %fd1518,[%rd702],%fd130; }

	// end inline asm
	add.s64 	%rd703, %rd702, 8;
	// begin inline asm
	{ atom.add.f64 %fd1520,[%rd703],%fd131; }

	// end inline asm
	add.s64 	%rd704, %rd702, 16;
	// begin inline asm
	{ atom.add.f64 %fd1522,[%rd704],%fd132; }

	// end inline asm
	bra.uni 	$L__BB21_130;

$L__BB21_128:
	setp.eq.s64 	%p66, %rd64, 0;
	@%p66 bra 	$L__BB21_130;

	mul.lo.s64 	%rd711, %rd44, %rd710;
	add.s64 	%rd706, %rd64, %rd711;
	// begin inline asm
	{ atom.add.f64 %fd1524,[%rd706],%fd130; }

	// end inline asm
	add.s64 	%rd707, %rd706, 8;
	// begin inline asm
	{ atom.add.f64 %fd1526,[%rd707],%fd131; }

	// end inline asm
	add.s64 	%rd708, %rd706, 16;
	// begin inline asm
	{ atom.add.f64 %fd1528,[%rd708],%fd132; }

	// end inline asm

$L__BB21_130:
	ld.param.u64 	%rd709, [clamp_search_direction_cuda_kernel_backward_param_0+24];
	add.s64 	%rd715, %rd715, %rd32;
	setp.lt.u64 	%p67, %rd715, %rd709;
	@%p67 bra 	$L__BB21_2;

$L__BB21_131:
	ret;

}
	// .globl	affine_to_sys_grad_cuda_kernel_forward
.visible .entry affine_to_sys_grad_cuda_kernel_forward(
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_forward_param_2[56]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<76>;
	.reg .f64 	%fd<25>;
	.reg .b64 	%rd<86>;


	ld.param.v2.u32 	{%r25, %r26}, [affine_to_sys_grad_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r27, %r28}, [affine_to_sys_grad_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r33, %r34}, [affine_to_sys_grad_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r41, %r42}, [affine_to_sys_grad_cuda_kernel_forward_param_2+32];
	ld.param.u64 	%rd34, [affine_to_sys_grad_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd32, [affine_to_sys_grad_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd31, [affine_to_sys_grad_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [affine_to_sys_grad_cuda_kernel_forward_param_0+16];
	mov.u32 	%r45, %ntid.x;
	cvt.u64.u32 	%rd1, %r45;
	mov.u32 	%r46, %ctaid.x;
	mul.wide.u32 	%rd36, %r45, %r46;
	mov.u32 	%r47, %tid.x;
	cvt.u64.u32 	%rd37, %r47;
	add.s64 	%rd79, %rd36, %rd37;
	setp.ge.u64 	%p1, %rd79, %rd31;
	@%p1 bra 	$L__BB22_25;

	cvta.to.global.u64 	%rd4, %rd34;
	cvta.to.global.u64 	%rd5, %rd32;
	cvt.s64.s32 	%rd6, %r28;
	cvt.s64.s32 	%rd7, %r27;
	cvt.s64.s32 	%rd8, %r26;
	cvt.s64.s32 	%rd9, %r33;
	cvt.s64.s32 	%rd10, %r41;
	mov.u32 	%r48, %nctaid.x;
	cvt.u64.u32 	%rd38, %r48;
	mul.lo.s64 	%rd11, %rd1, %rd38;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB22_12;
	bra.uni 	$L__BB22_2;

$L__BB22_12:
	cvt.u32.u64 	%r61, %rd6;
	cvt.u32.u64 	%r64, %rd7;
	cvt.u32.u64 	%r67, %rd8;

$L__BB22_13:
	or.b64  	%rd58, %rd79, %rd6;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p9, %rd59, 0;
	@%p9 bra 	$L__BB22_15;

	div.u64 	%rd84, %rd79, %rd6;
	bra.uni 	$L__BB22_16;

$L__BB22_15:
	cvt.u32.u64 	%r62, %rd79;
	div.u32 	%r63, %r62, %r61;
	cvt.u64.u32 	%rd84, %r63;

$L__BB22_16:
	setp.lt.s32 	%p10, %r6, 3;
	@%p10 bra 	$L__BB22_20;

	or.b64  	%rd60, %rd84, %rd7;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.eq.s64 	%p11, %rd61, 0;
	@%p11 bra 	$L__BB22_19;

	div.u64 	%rd84, %rd84, %rd7;
	bra.uni 	$L__BB22_20;

$L__BB22_19:
	cvt.u32.u64 	%r65, %rd84;
	div.u32 	%r66, %r65, %r64;
	cvt.u64.u32 	%rd84, %r66;

$L__BB22_20:
	setp.lt.s32 	%p12, %r6, 2;
	@%p12 bra 	$L__BB22_24;

	or.b64  	%rd62, %rd84, %rd8;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p13, %rd63, 0;
	@%p13 bra 	$L__BB22_23;

	div.u64 	%rd84, %rd84, %rd8;
	bra.uni 	$L__BB22_24;

$L__BB22_23:
	cvt.u32.u64 	%r68, %rd84;
	div.u32 	%r69, %r68, %r67;
	cvt.u64.u32 	%rd84, %r69;

$L__BB22_24:
	cvt.u32.u64 	%r70, %rd84;
	setp.gt.s32 	%p14, %r6, 0;
	selp.b32 	%r71, %r70, 0, %p14;
	cvt.s64.s32 	%rd64, %r71;
	mul.lo.s64 	%rd65, %rd64, %rd9;
	add.s64 	%rd66, %rd5, %rd65;
	ld.global.f64 	%fd13, [%rd66];
	ld.global.f64 	%fd14, [%rd66+8];
	ld.global.f64 	%fd15, [%rd66+16];
	ld.global.f64 	%fd16, [%rd66+24];
	ld.global.f64 	%fd17, [%rd66+32];
	ld.global.f64 	%fd18, [%rd66+40];
	ld.global.f64 	%fd19, [%rd66+48];
	ld.global.f64 	%fd20, [%rd66+56];
	ld.global.f64 	%fd21, [%rd66+64];
	ld.global.f64 	%fd22, [%rd66+72];
	ld.global.f64 	%fd23, [%rd66+80];
	ld.global.f64 	%fd24, [%rd66+88];
	shl.b32 	%r72, %r71, 2;
	cvt.s64.s32 	%rd67, %r72;
	mul.lo.s64 	%rd68, %rd67, %rd10;
	add.s64 	%rd69, %rd4, %rd68;
	st.global.f64 	[%rd69], %fd13;
	st.global.f64 	[%rd69+8], %fd14;
	st.global.f64 	[%rd69+16], %fd15;
	or.b32  	%r73, %r72, 1;
	cvt.s64.s32 	%rd70, %r73;
	mul.lo.s64 	%rd71, %rd70, %rd10;
	add.s64 	%rd72, %rd4, %rd71;
	st.global.f64 	[%rd72], %fd16;
	st.global.f64 	[%rd72+8], %fd17;
	st.global.f64 	[%rd72+16], %fd18;
	or.b32  	%r74, %r72, 2;
	cvt.s64.s32 	%rd73, %r74;
	mul.lo.s64 	%rd74, %rd73, %rd10;
	add.s64 	%rd75, %rd4, %rd74;
	st.global.f64 	[%rd75], %fd19;
	st.global.f64 	[%rd75+8], %fd20;
	st.global.f64 	[%rd75+16], %fd21;
	or.b32  	%r75, %r72, 3;
	cvt.s64.s32 	%rd76, %r75;
	mul.lo.s64 	%rd77, %rd76, %rd10;
	add.s64 	%rd78, %rd4, %rd77;
	st.global.f64 	[%rd78], %fd22;
	st.global.f64 	[%rd78+8], %fd23;
	st.global.f64 	[%rd78+16], %fd24;
	add.s64 	%rd79, %rd79, %rd11;
	setp.lt.u64 	%p15, %rd79, %rd31;
	@%p15 bra 	$L__BB22_13;
	bra.uni 	$L__BB22_25;

$L__BB22_2:
	cvt.u32.u64 	%r49, %rd7;
	cvt.u32.u64 	%r52, %rd8;

$L__BB22_3:
	setp.lt.s32 	%p3, %r6, 3;
	mov.u64 	%rd80, %rd79;
	@%p3 bra 	$L__BB22_7;

	or.b64  	%rd39, %rd79, %rd7;
	and.b64  	%rd40, %rd39, -4294967296;
	setp.eq.s64 	%p4, %rd40, 0;
	@%p4 bra 	$L__BB22_6;

	div.u64 	%rd80, %rd79, %rd7;
	bra.uni 	$L__BB22_7;

$L__BB22_6:
	cvt.u32.u64 	%r50, %rd79;
	div.u32 	%r51, %r50, %r49;
	cvt.u64.u32 	%rd80, %r51;

$L__BB22_7:
	setp.lt.s32 	%p5, %r6, 2;
	@%p5 bra 	$L__BB22_11;

	or.b64  	%rd41, %rd80, %rd8;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p6, %rd42, 0;
	@%p6 bra 	$L__BB22_10;

	div.u64 	%rd80, %rd80, %rd8;
	bra.uni 	$L__BB22_11;

$L__BB22_10:
	cvt.u32.u64 	%r53, %rd80;
	div.u32 	%r54, %r53, %r52;
	cvt.u64.u32 	%rd80, %r54;

$L__BB22_11:
	cvt.u32.u64 	%r55, %rd80;
	setp.gt.s32 	%p7, %r6, 0;
	selp.b32 	%r56, %r55, 0, %p7;
	cvt.s64.s32 	%rd43, %r56;
	mul.lo.s64 	%rd44, %rd43, %rd9;
	add.s64 	%rd45, %rd5, %rd44;
	ld.global.f64 	%fd1, [%rd45];
	ld.global.f64 	%fd2, [%rd45+8];
	ld.global.f64 	%fd3, [%rd45+16];
	ld.global.f64 	%fd4, [%rd45+24];
	ld.global.f64 	%fd5, [%rd45+32];
	ld.global.f64 	%fd6, [%rd45+40];
	ld.global.f64 	%fd7, [%rd45+48];
	ld.global.f64 	%fd8, [%rd45+56];
	ld.global.f64 	%fd9, [%rd45+64];
	ld.global.f64 	%fd10, [%rd45+72];
	ld.global.f64 	%fd11, [%rd45+80];
	ld.global.f64 	%fd12, [%rd45+88];
	shl.b32 	%r57, %r56, 2;
	cvt.s64.s32 	%rd46, %r57;
	mul.lo.s64 	%rd47, %rd46, %rd10;
	add.s64 	%rd48, %rd4, %rd47;
	st.global.f64 	[%rd48], %fd1;
	st.global.f64 	[%rd48+8], %fd2;
	st.global.f64 	[%rd48+16], %fd3;
	or.b32  	%r58, %r57, 1;
	cvt.s64.s32 	%rd49, %r58;
	mul.lo.s64 	%rd50, %rd49, %rd10;
	add.s64 	%rd51, %rd4, %rd50;
	st.global.f64 	[%rd51], %fd4;
	st.global.f64 	[%rd51+8], %fd5;
	st.global.f64 	[%rd51+16], %fd6;
	or.b32  	%r59, %r57, 2;
	cvt.s64.s32 	%rd52, %r59;
	mul.lo.s64 	%rd53, %rd52, %rd10;
	add.s64 	%rd54, %rd4, %rd53;
	st.global.f64 	[%rd54], %fd7;
	st.global.f64 	[%rd54+8], %fd8;
	st.global.f64 	[%rd54+16], %fd9;
	or.b32  	%r60, %r57, 3;
	cvt.s64.s32 	%rd55, %r60;
	mul.lo.s64 	%rd56, %rd55, %rd10;
	add.s64 	%rd57, %rd4, %rd56;
	st.global.f64 	[%rd57], %fd10;
	st.global.f64 	[%rd57+8], %fd11;
	st.global.f64 	[%rd57+16], %fd12;
	add.s64 	%rd79, %rd79, %rd11;
	setp.lt.u64 	%p8, %rd79, %rd31;
	@%p8 bra 	$L__BB22_3;

$L__BB22_25:
	ret;

}
	// .globl	affine_to_sys_grad_cuda_kernel_backward
.visible .entry affine_to_sys_grad_cuda_kernel_backward(
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_backward_param_4[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<101>;
	.reg .f64 	%fd<145>;
	.reg .b64 	%rd<103>;


	ld.param.v2.u32 	{%r45, %r46}, [affine_to_sys_grad_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r47, %r48}, [affine_to_sys_grad_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r53, %r54}, [affine_to_sys_grad_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r61, %r62}, [affine_to_sys_grad_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r69, %r70}, [affine_to_sys_grad_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r77, %r78}, [affine_to_sys_grad_cuda_kernel_backward_param_4+32];
	ld.param.u64 	%rd36, [affine_to_sys_grad_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd34, [affine_to_sys_grad_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd33, [affine_to_sys_grad_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd31, [affine_to_sys_grad_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd29, [affine_to_sys_grad_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r8, [affine_to_sys_grad_cuda_kernel_backward_param_0+16];
	mov.u32 	%r81, %ntid.x;
	cvt.u64.u32 	%rd1, %r81;
	mov.u32 	%r82, %ctaid.x;
	mul.wide.u32 	%rd38, %r81, %r82;
	mov.u32 	%r83, %tid.x;
	cvt.u64.u32 	%rd39, %r83;
	add.s64 	%rd99, %rd38, %rd39;
	setp.ge.u64 	%p1, %rd99, %rd29;
	@%p1 bra 	$L__BB23_35;

	cvta.to.global.u64 	%rd8, %rd36;
	cvta.to.global.u64 	%rd9, %rd33;
	cvt.s64.s32 	%rd10, %r48;
	cvt.s64.s32 	%rd11, %r47;
	cvt.s64.s32 	%rd12, %r46;
	cvt.s64.s32 	%rd13, %r77;
	cvt.s64.s32 	%rd14, %r61;
	mov.u32 	%r84, %nctaid.x;
	cvt.u64.u32 	%rd40, %r84;
	mul.lo.s64 	%rd15, %rd1, %rd40;
	cvt.s64.s32 	%rd16, %r69;
	cvt.s64.s32 	%rd17, %r53;

$L__BB23_2:
	setp.lt.s32 	%p2, %r8, 4;
	mov.u64 	%rd100, %rd99;
	@%p2 bra 	$L__BB23_6;

	or.b64  	%rd41, %rd99, %rd10;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p3, %rd42, 0;
	@%p3 bra 	$L__BB23_5;

	div.u64 	%rd100, %rd99, %rd10;
	bra.uni 	$L__BB23_6;

$L__BB23_5:
	cvt.u32.u64 	%r85, %rd10;
	cvt.u32.u64 	%r86, %rd99;
	div.u32 	%r87, %r86, %r85;
	cvt.u64.u32 	%rd100, %r87;

$L__BB23_6:
	setp.lt.s32 	%p4, %r8, 3;
	@%p4 bra 	$L__BB23_10;

	or.b64  	%rd43, %rd100, %rd11;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p5, %rd44, 0;
	@%p5 bra 	$L__BB23_9;

	div.u64 	%rd100, %rd100, %rd11;
	bra.uni 	$L__BB23_10;

$L__BB23_9:
	cvt.u32.u64 	%r88, %rd11;
	cvt.u32.u64 	%r89, %rd100;
	div.u32 	%r90, %r89, %r88;
	cvt.u64.u32 	%rd100, %r90;

$L__BB23_10:
	setp.lt.s32 	%p6, %r8, 2;
	@%p6 bra 	$L__BB23_14;

	or.b64  	%rd45, %rd100, %rd12;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p7, %rd46, 0;
	@%p7 bra 	$L__BB23_13;

	div.u64 	%rd100, %rd100, %rd12;
	bra.uni 	$L__BB23_14;

$L__BB23_13:
	cvt.u32.u64 	%r91, %rd12;
	cvt.u32.u64 	%r92, %rd100;
	div.u32 	%r93, %r92, %r91;
	cvt.u64.u32 	%rd100, %r93;

$L__BB23_14:
	cvt.u32.u64 	%r94, %rd100;
	setp.gt.s32 	%p8, %r8, 0;
	selp.b32 	%r2, %r94, 0, %p8;
	shl.b32 	%r3, %r2, 2;
	setp.eq.s64 	%p9, %rd36, 0;
	@%p9 bra 	$L__BB23_16;

	add.s32 	%r95, %r3, 3;
	cvt.s64.s32 	%rd47, %r95;
	mul.lo.s64 	%rd48, %rd47, %rd13;
	add.s64 	%rd49, %rd8, %rd48;
	ld.global.f64 	%fd49, [%rd49];
	add.f64 	%fd135, %fd49, 0d0000000000000000;
	ld.global.f64 	%fd50, [%rd49+8];
	add.f64 	%fd134, %fd50, 0d0000000000000000;
	ld.global.f64 	%fd51, [%rd49+16];
	add.f64 	%fd133, %fd51, 0d0000000000000000;
	bra.uni 	$L__BB23_18;

$L__BB23_16:
	setp.eq.s64 	%p10, %rd33, 0;
	mov.f64 	%fd133, 0d0000000000000000;
	mov.f64 	%fd134, %fd133;
	mov.f64 	%fd135, %fd133;
	@%p10 bra 	$L__BB23_18;

	add.s32 	%r96, %r3, 3;
	cvt.s64.s32 	%rd50, %r96;
	mul.lo.s64 	%rd51, %rd50, %rd14;
	add.s64 	%rd52, %rd9, %rd51;
	ld.global.f64 	%fd55, [%rd52];
	add.f64 	%fd135, %fd55, 0d0000000000000000;
	ld.global.f64 	%fd56, [%rd52+8];
	add.f64 	%fd134, %fd56, 0d0000000000000000;
	ld.global.f64 	%fd57, [%rd52+16];
	add.f64 	%fd133, %fd57, 0d0000000000000000;

$L__BB23_18:
	add.f64 	%fd10, %fd135, 0d0000000000000000;
	add.f64 	%fd11, %fd134, 0d0000000000000000;
	add.f64 	%fd12, %fd133, 0d0000000000000000;
	@%p9 bra 	$L__BB23_20;

	add.s32 	%r97, %r3, 2;
	cvt.s64.s32 	%rd53, %r97;
	mul.lo.s64 	%rd54, %rd53, %rd13;
	add.s64 	%rd55, %rd8, %rd54;
	ld.global.f64 	%fd58, [%rd55];
	add.f64 	%fd138, %fd58, 0d0000000000000000;
	ld.global.f64 	%fd59, [%rd55+8];
	add.f64 	%fd137, %fd59, 0d0000000000000000;
	ld.global.f64 	%fd60, [%rd55+16];
	add.f64 	%fd136, %fd60, 0d0000000000000000;
	bra.uni 	$L__BB23_22;

$L__BB23_20:
	setp.eq.s64 	%p12, %rd33, 0;
	mov.f64 	%fd136, 0d0000000000000000;
	mov.f64 	%fd137, %fd136;
	mov.f64 	%fd138, %fd136;
	@%p12 bra 	$L__BB23_22;

	add.s32 	%r98, %r3, 2;
	cvt.s64.s32 	%rd56, %r98;
	mul.lo.s64 	%rd57, %rd56, %rd14;
	add.s64 	%rd58, %rd9, %rd57;
	ld.global.f64 	%fd64, [%rd58];
	add.f64 	%fd138, %fd64, 0d0000000000000000;
	ld.global.f64 	%fd65, [%rd58+8];
	add.f64 	%fd137, %fd65, 0d0000000000000000;
	ld.global.f64 	%fd66, [%rd58+16];
	add.f64 	%fd136, %fd66, 0d0000000000000000;

$L__BB23_22:
	add.f64 	%fd22, %fd138, 0d0000000000000000;
	add.f64 	%fd23, %fd137, 0d0000000000000000;
	add.f64 	%fd24, %fd136, 0d0000000000000000;
	@%p9 bra 	$L__BB23_24;

	add.s32 	%r99, %r3, 1;
	cvt.s64.s32 	%rd59, %r99;
	mul.lo.s64 	%rd60, %rd59, %rd13;
	add.s64 	%rd61, %rd8, %rd60;
	ld.global.f64 	%fd67, [%rd61];
	add.f64 	%fd141, %fd67, 0d0000000000000000;
	ld.global.f64 	%fd68, [%rd61+8];
	add.f64 	%fd140, %fd68, 0d0000000000000000;
	ld.global.f64 	%fd69, [%rd61+16];
	add.f64 	%fd139, %fd69, 0d0000000000000000;
	bra.uni 	$L__BB23_26;

$L__BB23_24:
	setp.eq.s64 	%p14, %rd33, 0;
	mov.f64 	%fd139, 0d0000000000000000;
	mov.f64 	%fd140, %fd139;
	mov.f64 	%fd141, %fd139;
	@%p14 bra 	$L__BB23_26;

	add.s32 	%r100, %r3, 1;
	cvt.s64.s32 	%rd62, %r100;
	mul.lo.s64 	%rd63, %rd62, %rd14;
	add.s64 	%rd64, %rd9, %rd63;
	ld.global.f64 	%fd73, [%rd64];
	add.f64 	%fd141, %fd73, 0d0000000000000000;
	ld.global.f64 	%fd74, [%rd64+8];
	add.f64 	%fd140, %fd74, 0d0000000000000000;
	ld.global.f64 	%fd75, [%rd64+16];
	add.f64 	%fd139, %fd75, 0d0000000000000000;

$L__BB23_26:
	add.f64 	%fd34, %fd141, 0d0000000000000000;
	add.f64 	%fd35, %fd140, 0d0000000000000000;
	add.f64 	%fd36, %fd139, 0d0000000000000000;
	@%p9 bra 	$L__BB23_28;

	cvt.s64.s32 	%rd65, %r3;
	mul.lo.s64 	%rd66, %rd65, %rd13;
	add.s64 	%rd67, %rd8, %rd66;
	ld.global.f64 	%fd76, [%rd67];
	add.f64 	%fd144, %fd76, 0d0000000000000000;
	ld.global.f64 	%fd77, [%rd67+8];
	add.f64 	%fd143, %fd77, 0d0000000000000000;
	ld.global.f64 	%fd78, [%rd67+16];
	add.f64 	%fd142, %fd78, 0d0000000000000000;
	bra.uni 	$L__BB23_30;

$L__BB23_28:
	setp.eq.s64 	%p16, %rd33, 0;
	mov.f64 	%fd142, 0d0000000000000000;
	mov.f64 	%fd143, %fd142;
	mov.f64 	%fd144, %fd142;
	@%p16 bra 	$L__BB23_30;

	cvt.s64.s32 	%rd68, %r3;
	mul.lo.s64 	%rd69, %rd68, %rd14;
	add.s64 	%rd70, %rd9, %rd69;
	ld.global.f64 	%fd82, [%rd70];
	add.f64 	%fd144, %fd82, 0d0000000000000000;
	ld.global.f64 	%fd83, [%rd70+8];
	add.f64 	%fd143, %fd83, 0d0000000000000000;
	ld.global.f64 	%fd84, [%rd70+16];
	add.f64 	%fd142, %fd84, 0d0000000000000000;

$L__BB23_30:
	add.f64 	%fd46, %fd144, 0d0000000000000000;
	add.f64 	%fd47, %fd143, 0d0000000000000000;
	add.f64 	%fd48, %fd142, 0d0000000000000000;
	setp.eq.s64 	%p17, %rd34, 0;
	@%p17 bra 	$L__BB23_32;

	cvt.s64.s32 	%rd83, %r2;
	mul.lo.s64 	%rd84, %rd83, %rd16;
	add.s64 	%rd71, %rd34, %rd84;
	// begin inline asm
	{ atom.add.f64 %fd85,[%rd71],%fd46; }

	// end inline asm
	add.s64 	%rd72, %rd71, 8;
	// begin inline asm
	{ atom.add.f64 %fd87,[%rd72],%fd47; }

	// end inline asm
	add.s64 	%rd73, %rd71, 16;
	// begin inline asm
	{ atom.add.f64 %fd89,[%rd73],%fd48; }

	// end inline asm
	add.s64 	%rd74, %rd71, 24;
	// begin inline asm
	{ atom.add.f64 %fd91,[%rd74],%fd34; }

	// end inline asm
	add.s64 	%rd75, %rd71, 32;
	// begin inline asm
	{ atom.add.f64 %fd93,[%rd75],%fd35; }

	// end inline asm
	add.s64 	%rd76, %rd71, 40;
	// begin inline asm
	{ atom.add.f64 %fd95,[%rd76],%fd36; }

	// end inline asm
	add.s64 	%rd77, %rd71, 48;
	// begin inline asm
	{ atom.add.f64 %fd97,[%rd77],%fd22; }

	// end inline asm
	add.s64 	%rd78, %rd71, 56;
	// begin inline asm
	{ atom.add.f64 %fd99,[%rd78],%fd23; }

	// end inline asm
	add.s64 	%rd79, %rd71, 64;
	// begin inline asm
	{ atom.add.f64 %fd101,[%rd79],%fd24; }

	// end inline asm
	add.s64 	%rd80, %rd71, 72;
	// begin inline asm
	{ atom.add.f64 %fd103,[%rd80],%fd10; }

	// end inline asm
	add.s64 	%rd81, %rd71, 80;
	// begin inline asm
	{ atom.add.f64 %fd105,[%rd81],%fd11; }

	// end inline asm
	add.s64 	%rd82, %rd71, 88;
	// begin inline asm
	{ atom.add.f64 %fd107,[%rd82],%fd12; }

	// end inline asm
	bra.uni 	$L__BB23_34;

$L__BB23_32:
	setp.eq.s64 	%p18, %rd31, 0;
	@%p18 bra 	$L__BB23_34;

	cvt.s64.s32 	%rd97, %r2;
	mul.lo.s64 	%rd98, %rd97, %rd17;
	add.s64 	%rd85, %rd31, %rd98;
	// begin inline asm
	{ atom.add.f64 %fd109,[%rd85],%fd46; }

	// end inline asm
	add.s64 	%rd86, %rd85, 8;
	// begin inline asm
	{ atom.add.f64 %fd111,[%rd86],%fd47; }

	// end inline asm
	add.s64 	%rd87, %rd85, 16;
	// begin inline asm
	{ atom.add.f64 %fd113,[%rd87],%fd48; }

	// end inline asm
	add.s64 	%rd88, %rd85, 24;
	// begin inline asm
	{ atom.add.f64 %fd115,[%rd88],%fd34; }

	// end inline asm
	add.s64 	%rd89, %rd85, 32;
	// begin inline asm
	{ atom.add.f64 %fd117,[%rd89],%fd35; }

	// end inline asm
	add.s64 	%rd90, %rd85, 40;
	// begin inline asm
	{ atom.add.f64 %fd119,[%rd90],%fd36; }

	// end inline asm
	add.s64 	%rd91, %rd85, 48;
	// begin inline asm
	{ atom.add.f64 %fd121,[%rd91],%fd22; }

	// end inline asm
	add.s64 	%rd92, %rd85, 56;
	// begin inline asm
	{ atom.add.f64 %fd123,[%rd92],%fd23; }

	// end inline asm
	add.s64 	%rd93, %rd85, 64;
	// begin inline asm
	{ atom.add.f64 %fd125,[%rd93],%fd24; }

	// end inline asm
	add.s64 	%rd94, %rd85, 72;
	// begin inline asm
	{ atom.add.f64 %fd127,[%rd94],%fd10; }

	// end inline asm
	add.s64 	%rd95, %rd85, 80;
	// begin inline asm
	{ atom.add.f64 %fd129,[%rd95],%fd11; }

	// end inline asm
	add.s64 	%rd96, %rd85, 88;
	// begin inline asm
	{ atom.add.f64 %fd131,[%rd96],%fd12; }

	// end inline asm

$L__BB23_34:
	add.s64 	%rd99, %rd99, %rd15;
	setp.lt.u64 	%p19, %rd99, %rd29;
	@%p19 bra 	$L__BB23_2;

$L__BB23_35:
	ret;

}
	// .globl	negate_arr_vec3d_cuda_kernel_forward
.visible .entry negate_arr_vec3d_cuda_kernel_forward(
	.param .align 8 .b8 negate_arr_vec3d_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 negate_arr_vec3d_cuda_kernel_forward_param_1[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<50>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<73>;


	ld.param.v2.u32 	{%r16, %r17}, [negate_arr_vec3d_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r18, %r19}, [negate_arr_vec3d_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r24, %r25}, [negate_arr_vec3d_cuda_kernel_forward_param_1+32];
	ld.param.u64 	%rd35, [negate_arr_vec3d_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd34, [negate_arr_vec3d_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [negate_arr_vec3d_cuda_kernel_forward_param_0+16];
	mov.u32 	%r28, %ntid.x;
	cvt.u64.u32 	%rd1, %r28;
	mov.u32 	%r29, %ctaid.x;
	mul.wide.u32 	%rd37, %r28, %r29;
	mov.u32 	%r30, %tid.x;
	cvt.u64.u32 	%rd38, %r30;
	add.s64 	%rd64, %rd37, %rd38;
	setp.ge.u64 	%p1, %rd64, %rd34;
	@%p1 bra 	$L__BB24_31;

	cvta.to.global.u64 	%rd4, %rd35;
	cvt.s64.s32 	%rd5, %r19;
	cvt.s64.s32 	%rd6, %r18;
	cvt.s64.s32 	%rd7, %r17;
	cvt.s64.s32 	%rd8, %r24;
	mov.u32 	%r31, %nctaid.x;
	cvt.u64.u32 	%rd39, %r31;
	mul.lo.s64 	%rd9, %rd1, %rd39;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB24_18;
	bra.uni 	$L__BB24_2;

$L__BB24_18:
	cvt.u32.u64 	%r41, %rd5;
	cvt.u32.u64 	%r44, %rd6;
	cvt.u32.u64 	%r47, %rd7;

$L__BB24_19:
	or.b64  	%rd54, %rd64, %rd5;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p13, %rd55, 0;
	@%p13 bra 	$L__BB24_21;

	div.u64 	%rd71, %rd64, %rd5;
	bra.uni 	$L__BB24_22;

$L__BB24_21:
	cvt.u32.u64 	%r42, %rd64;
	div.u32 	%r43, %r42, %r41;
	cvt.u64.u32 	%rd71, %r43;

$L__BB24_22:
	setp.lt.s32 	%p14, %r6, 3;
	@%p14 bra 	$L__BB24_26;

	or.b64  	%rd56, %rd71, %rd6;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p15, %rd57, 0;
	@%p15 bra 	$L__BB24_25;

	div.u64 	%rd71, %rd71, %rd6;
	bra.uni 	$L__BB24_26;

$L__BB24_25:
	cvt.u32.u64 	%r45, %rd71;
	div.u32 	%r46, %r45, %r44;
	cvt.u64.u32 	%rd71, %r46;

$L__BB24_26:
	setp.lt.s32 	%p16, %r6, 2;
	@%p16 bra 	$L__BB24_30;

	or.b64  	%rd58, %rd71, %rd7;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p17, %rd59, 0;
	@%p17 bra 	$L__BB24_29;

	div.u64 	%rd71, %rd71, %rd7;
	bra.uni 	$L__BB24_30;

$L__BB24_29:
	cvt.u32.u64 	%r48, %rd71;
	div.u32 	%r49, %r48, %r47;
	cvt.u64.u32 	%rd71, %r49;

$L__BB24_30:
	cvt.s64.s32 	%rd60, %rd71;
	setp.gt.s32 	%p18, %r6, 0;
	selp.b64 	%rd61, %rd60, 0, %p18;
	mul.lo.s64 	%rd62, %rd61, %rd8;
	add.s64 	%rd63, %rd4, %rd62;
	ld.global.f64 	%fd13, [%rd63];
	neg.f64 	%fd14, %fd13;
	ld.global.f64 	%fd15, [%rd63+8];
	neg.f64 	%fd16, %fd15;
	ld.global.f64 	%fd17, [%rd63+16];
	neg.f64 	%fd18, %fd17;
	st.global.f64 	[%rd63], %fd14;
	st.global.f64 	[%rd63+8], %fd16;
	st.global.f64 	[%rd63+16], %fd18;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p19, %rd64, %rd34;
	@%p19 bra 	$L__BB24_19;
	bra.uni 	$L__BB24_31;

$L__BB24_2:
	setp.gt.s32 	%p3, %r6, 2;
	@%p3 bra 	$L__BB24_9;
	bra.uni 	$L__BB24_3;

$L__BB24_9:
	cvt.u32.u64 	%r35, %rd6;
	cvt.u32.u64 	%r38, %rd7;

$L__BB24_10:
	or.b64  	%rd46, %rd64, %rd6;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p8, %rd47, 0;
	@%p8 bra 	$L__BB24_12;

	div.u64 	%rd68, %rd64, %rd6;
	bra.uni 	$L__BB24_13;

$L__BB24_12:
	cvt.u32.u64 	%r36, %rd64;
	div.u32 	%r37, %r36, %r35;
	cvt.u64.u32 	%rd68, %r37;

$L__BB24_13:
	setp.lt.s32 	%p9, %r6, 2;
	@%p9 bra 	$L__BB24_17;

	or.b64  	%rd48, %rd68, %rd7;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p10, %rd49, 0;
	@%p10 bra 	$L__BB24_16;

	div.u64 	%rd68, %rd68, %rd7;
	bra.uni 	$L__BB24_17;

$L__BB24_16:
	cvt.u32.u64 	%r39, %rd68;
	div.u32 	%r40, %r39, %r38;
	cvt.u64.u32 	%rd68, %r40;

$L__BB24_17:
	cvt.s64.s32 	%rd50, %rd68;
	setp.gt.s32 	%p11, %r6, 0;
	selp.b64 	%rd51, %rd50, 0, %p11;
	mul.lo.s64 	%rd52, %rd51, %rd8;
	add.s64 	%rd53, %rd4, %rd52;
	ld.global.f64 	%fd7, [%rd53];
	neg.f64 	%fd8, %fd7;
	ld.global.f64 	%fd9, [%rd53+8];
	neg.f64 	%fd10, %fd9;
	ld.global.f64 	%fd11, [%rd53+16];
	neg.f64 	%fd12, %fd11;
	st.global.f64 	[%rd53], %fd8;
	st.global.f64 	[%rd53+8], %fd10;
	st.global.f64 	[%rd53+16], %fd12;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p12, %rd64, %rd34;
	@%p12 bra 	$L__BB24_10;
	bra.uni 	$L__BB24_31;

$L__BB24_3:
	cvt.u32.u64 	%r32, %rd7;

$L__BB24_4:
	setp.lt.s32 	%p4, %r6, 2;
	mov.u64 	%rd65, %rd64;
	@%p4 bra 	$L__BB24_8;

	or.b64  	%rd40, %rd64, %rd7;
	and.b64  	%rd41, %rd40, -4294967296;
	setp.eq.s64 	%p5, %rd41, 0;
	@%p5 bra 	$L__BB24_7;

	div.u64 	%rd65, %rd64, %rd7;
	bra.uni 	$L__BB24_8;

$L__BB24_7:
	cvt.u32.u64 	%r33, %rd64;
	div.u32 	%r34, %r33, %r32;
	cvt.u64.u32 	%rd65, %r34;

$L__BB24_8:
	cvt.s64.s32 	%rd42, %rd65;
	setp.gt.s32 	%p6, %r6, 0;
	selp.b64 	%rd43, %rd42, 0, %p6;
	mul.lo.s64 	%rd44, %rd43, %rd8;
	add.s64 	%rd45, %rd4, %rd44;
	ld.global.f64 	%fd1, [%rd45];
	neg.f64 	%fd2, %fd1;
	ld.global.f64 	%fd3, [%rd45+8];
	neg.f64 	%fd4, %fd3;
	ld.global.f64 	%fd5, [%rd45+16];
	neg.f64 	%fd6, %fd5;
	st.global.f64 	[%rd45], %fd2;
	st.global.f64 	[%rd45+8], %fd4;
	st.global.f64 	[%rd45+16], %fd6;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p7, %rd64, %rd34;
	@%p7 bra 	$L__BB24_4;

$L__BB24_31:
	ret;

}
	// .globl	negate_arr_vec3d_cuda_kernel_backward
.visible .entry negate_arr_vec3d_cuda_kernel_backward(
	.param .align 8 .b8 negate_arr_vec3d_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 negate_arr_vec3d_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 negate_arr_vec3d_cuda_kernel_backward_param_2[56]
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<60>;
	.reg .f64 	%fd<38>;
	.reg .b64 	%rd<59>;


	ld.param.v2.u32 	{%r26, %r27}, [negate_arr_vec3d_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r28, %r29}, [negate_arr_vec3d_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r34, %r35}, [negate_arr_vec3d_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r42, %r43}, [negate_arr_vec3d_cuda_kernel_backward_param_2+32];
	ld.param.u64 	%rd26, [negate_arr_vec3d_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd25, [negate_arr_vec3d_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd23, [negate_arr_vec3d_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [negate_arr_vec3d_cuda_kernel_backward_param_0+16];
	mov.u32 	%r46, %ntid.x;
	cvt.u64.u32 	%rd1, %r46;
	mov.u32 	%r47, %ctaid.x;
	mul.wide.u32 	%rd28, %r46, %r47;
	mov.u32 	%r48, %tid.x;
	cvt.u64.u32 	%rd29, %r48;
	add.s64 	%rd55, %rd28, %rd29;
	setp.ge.u64 	%p1, %rd55, %rd23;
	@%p1 bra 	$L__BB25_23;

	cvt.s64.s32 	%rd6, %r29;
	cvt.s64.s32 	%rd7, %r28;
	cvt.s64.s32 	%rd8, %r27;
	cvt.s64.s32 	%rd9, %r42;
	cvt.s64.s32 	%rd10, %r34;
	mov.u32 	%r49, %nctaid.x;
	cvt.u64.u32 	%rd30, %r49;
	mul.lo.s64 	%rd11, %rd1, %rd30;

$L__BB25_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd56, %rd55;
	@%p2 bra 	$L__BB25_6;

	or.b64  	%rd31, %rd55, %rd6;
	and.b64  	%rd32, %rd31, -4294967296;
	setp.eq.s64 	%p3, %rd32, 0;
	@%p3 bra 	$L__BB25_5;

	div.u64 	%rd56, %rd55, %rd6;
	bra.uni 	$L__BB25_6;

$L__BB25_5:
	cvt.u32.u64 	%r50, %rd6;
	cvt.u32.u64 	%r51, %rd55;
	div.u32 	%r52, %r51, %r50;
	cvt.u64.u32 	%rd56, %r52;

$L__BB25_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB25_10;

	or.b64  	%rd33, %rd56, %rd7;
	and.b64  	%rd34, %rd33, -4294967296;
	setp.eq.s64 	%p5, %rd34, 0;
	@%p5 bra 	$L__BB25_9;

	div.u64 	%rd56, %rd56, %rd7;
	bra.uni 	$L__BB25_10;

$L__BB25_9:
	cvt.u32.u64 	%r53, %rd7;
	cvt.u32.u64 	%r54, %rd56;
	div.u32 	%r55, %r54, %r53;
	cvt.u64.u32 	%rd56, %r55;

$L__BB25_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB25_14;

	or.b64  	%rd35, %rd56, %rd8;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p7, %rd36, 0;
	@%p7 bra 	$L__BB25_13;

	div.u64 	%rd56, %rd56, %rd8;
	bra.uni 	$L__BB25_14;

$L__BB25_13:
	cvt.u32.u64 	%r56, %rd8;
	cvt.u32.u64 	%r57, %rd56;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd56, %r58;

$L__BB25_14:
	cvt.u32.u64 	%r59, %rd56;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r59, 0, %p8;
	setp.eq.s64 	%p9, %rd26, 0;
	@%p9 bra 	$L__BB25_16;

	cvta.to.global.u64 	%rd37, %rd26;
	cvt.s64.s32 	%rd38, %r2;
	mul.lo.s64 	%rd39, %rd38, %rd9;
	add.s64 	%rd40, %rd37, %rd39;
	ld.global.f64 	%fd13, [%rd40];
	add.f64 	%fd37, %fd13, 0d0000000000000000;
	ld.global.f64 	%fd14, [%rd40+8];
	add.f64 	%fd36, %fd14, 0d0000000000000000;
	ld.global.f64 	%fd15, [%rd40+16];
	add.f64 	%fd35, %fd15, 0d0000000000000000;
	bra.uni 	$L__BB25_18;

$L__BB25_16:
	setp.eq.s64 	%p10, %rd25, 0;
	mov.f64 	%fd35, 0d0000000000000000;
	mov.f64 	%fd36, %fd35;
	mov.f64 	%fd37, %fd35;
	@%p10 bra 	$L__BB25_18;

	cvta.to.global.u64 	%rd41, %rd25;
	cvt.s64.s32 	%rd42, %r2;
	mul.lo.s64 	%rd43, %rd42, %rd10;
	add.s64 	%rd44, %rd41, %rd43;
	ld.global.f64 	%fd19, [%rd44];
	add.f64 	%fd37, %fd19, 0d0000000000000000;
	ld.global.f64 	%fd20, [%rd44+8];
	add.f64 	%fd36, %fd20, 0d0000000000000000;
	ld.global.f64 	%fd21, [%rd44+16];
	add.f64 	%fd35, %fd21, 0d0000000000000000;

$L__BB25_18:
	mov.f64 	%fd22, 0d0000000000000000;
	sub.f64 	%fd10, %fd22, %fd37;
	sub.f64 	%fd11, %fd22, %fd36;
	sub.f64 	%fd12, %fd22, %fd35;
	@%p9 bra 	$L__BB25_20;

	cvt.s64.s32 	%rd48, %r2;
	mul.lo.s64 	%rd49, %rd48, %rd9;
	add.s64 	%rd45, %rd26, %rd49;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd45],%fd10; }

	// end inline asm
	add.s64 	%rd46, %rd45, 8;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd46],%fd11; }

	// end inline asm
	add.s64 	%rd47, %rd45, 16;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd47],%fd12; }

	// end inline asm
	bra.uni 	$L__BB25_22;

$L__BB25_20:
	setp.eq.s64 	%p12, %rd25, 0;
	@%p12 bra 	$L__BB25_22;

	cvt.s64.s32 	%rd53, %r2;
	mul.lo.s64 	%rd54, %rd53, %rd10;
	add.s64 	%rd50, %rd25, %rd54;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd50],%fd10; }

	// end inline asm
	add.s64 	%rd51, %rd50, 8;
	// begin inline asm
	{ atom.add.f64 %fd31,[%rd51],%fd11; }

	// end inline asm
	add.s64 	%rd52, %rd50, 16;
	// begin inline asm
	{ atom.add.f64 %fd33,[%rd52],%fd12; }

	// end inline asm

$L__BB25_22:
	add.s64 	%rd55, %rd55, %rd11;
	setp.lt.u64 	%p13, %rd55, %rd23;
	@%p13 bra 	$L__BB25_2;

$L__BB25_23:
	ret;

}
	// .globl	advection_y_cuda_kernel_forward
.visible .entry advection_y_cuda_kernel_forward(
	.param .align 8 .b8 advection_y_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 advection_y_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 advection_y_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 advection_y_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 advection_y_cuda_kernel_forward_param_4[56],
	.param .f64 advection_y_cuda_kernel_forward_param_5,
	.param .u32 advection_y_cuda_kernel_forward_param_6
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<95>;
	.reg .f64 	%fd<159>;
	.reg .b64 	%rd<59>;


	ld.param.v2.u32 	{%r45, %r46}, [advection_y_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r47, %r48}, [advection_y_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r53, %r54}, [advection_y_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r61, %r62}, [advection_y_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r69, %r70}, [advection_y_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r77, %r78}, [advection_y_cuda_kernel_forward_param_4+32];
	ld.param.f64 	%fd2, [advection_y_cuda_kernel_forward_param_5];
	ld.param.u32 	%r44, [advection_y_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd37, [advection_y_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd35, [advection_y_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd33, [advection_y_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd31, [advection_y_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd30, [advection_y_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r7, [advection_y_cuda_kernel_forward_param_0+16];
	mov.u32 	%r81, %ntid.x;
	cvt.u64.u32 	%rd1, %r81;
	mov.u32 	%r82, %ctaid.x;
	mul.wide.u32 	%rd39, %r81, %r82;
	mov.u32 	%r83, %tid.x;
	cvt.u64.u32 	%rd40, %r83;
	add.s64 	%rd55, %rd39, %rd40;
	setp.ge.u64 	%p1, %rd55, %rd30;
	@%p1 bra 	$L__BB26_19;

	cvta.to.global.u64 	%rd4, %rd37;
	cvta.to.global.u64 	%rd5, %rd35;
	cvta.to.global.u64 	%rd6, %rd31;
	cvta.to.global.u64 	%rd7, %rd33;
	cvt.s64.s32 	%rd8, %r48;
	cvt.s64.s32 	%rd9, %r47;
	cvt.s64.s32 	%rd10, %r46;
	mov.u32 	%r84, %nctaid.x;
	cvt.u64.u32 	%rd41, %r84;
	mul.lo.s64 	%rd11, %rd1, %rd41;
	cvt.s64.s32 	%rd12, %r69;
	cvt.s64.s32 	%rd13, %r53;
	mul.f64 	%fd1, %fd2, %fd2;
	cvt.s64.s32 	%rd14, %r77;
	cvt.s64.s32 	%rd15, %r61;

$L__BB26_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd56, %rd55;
	@%p2 bra 	$L__BB26_6;

	or.b64  	%rd42, %rd55, %rd8;
	and.b64  	%rd43, %rd42, -4294967296;
	setp.eq.s64 	%p3, %rd43, 0;
	@%p3 bra 	$L__BB26_5;

	div.u64 	%rd56, %rd55, %rd8;
	bra.uni 	$L__BB26_6;

$L__BB26_5:
	cvt.u32.u64 	%r85, %rd8;
	cvt.u32.u64 	%r86, %rd55;
	div.u32 	%r87, %r86, %r85;
	cvt.u64.u32 	%rd56, %r87;

$L__BB26_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB26_10;

	or.b64  	%rd44, %rd56, %rd9;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p5, %rd45, 0;
	@%p5 bra 	$L__BB26_9;

	div.u64 	%rd56, %rd56, %rd9;
	bra.uni 	$L__BB26_10;

$L__BB26_9:
	cvt.u32.u64 	%r88, %rd9;
	cvt.u32.u64 	%r89, %rd56;
	div.u32 	%r90, %r89, %r88;
	cvt.u64.u32 	%rd56, %r90;

$L__BB26_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB26_14;

	or.b64  	%rd46, %rd56, %rd10;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p7, %rd47, 0;
	@%p7 bra 	$L__BB26_13;

	div.u64 	%rd56, %rd56, %rd10;
	bra.uni 	$L__BB26_14;

$L__BB26_13:
	cvt.u32.u64 	%r91, %rd10;
	cvt.u32.u64 	%r92, %rd56;
	div.u32 	%r93, %r92, %r91;
	cvt.u64.u32 	%rd56, %r93;

$L__BB26_14:
	cvt.u32.u64 	%r94, %rd56;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r94, 0, %p8;
	cvt.s64.s32 	%rd48, %r2;
	mul.lo.s64 	%rd49, %rd48, %rd12;
	add.s64 	%rd26, %rd5, %rd49;
	mul.lo.s64 	%rd50, %rd48, %rd13;
	add.s64 	%rd27, %rd6, %rd50;
	mul.lo.s64 	%rd51, %rd48, %rd14;
	add.s64 	%rd28, %rd4, %rd51;
	setp.eq.s32 	%p9, %r44, 0;
	@%p9 bra 	$L__BB26_17;

	setp.ne.s32 	%p10, %r44, 1;
	@%p10 bra 	$L__BB26_18;

	ld.global.f64 	%fd3, [%rd27];
	ld.global.f64 	%fd4, [%rd26];
	sub.f64 	%fd5, %fd4, %fd3;
	ld.global.f64 	%fd6, [%rd27+8];
	ld.global.f64 	%fd7, [%rd26+8];
	sub.f64 	%fd8, %fd7, %fd6;
	ld.global.f64 	%fd9, [%rd27+16];
	ld.global.f64 	%fd10, [%rd26+16];
	sub.f64 	%fd11, %fd10, %fd9;
	ld.global.f64 	%fd12, [%rd27+24];
	ld.global.f64 	%fd13, [%rd26+24];
	sub.f64 	%fd14, %fd13, %fd12;
	ld.global.f64 	%fd15, [%rd27+32];
	ld.global.f64 	%fd16, [%rd26+32];
	sub.f64 	%fd17, %fd16, %fd15;
	ld.global.f64 	%fd18, [%rd27+40];
	ld.global.f64 	%fd19, [%rd26+40];
	sub.f64 	%fd20, %fd19, %fd18;
	ld.global.f64 	%fd21, [%rd27+48];
	ld.global.f64 	%fd22, [%rd26+48];
	sub.f64 	%fd23, %fd22, %fd21;
	ld.global.f64 	%fd24, [%rd27+56];
	ld.global.f64 	%fd25, [%rd26+56];
	sub.f64 	%fd26, %fd25, %fd24;
	ld.global.f64 	%fd27, [%rd27+64];
	ld.global.f64 	%fd28, [%rd26+64];
	sub.f64 	%fd29, %fd28, %fd27;
	ld.global.f64 	%fd30, [%rd27+72];
	ld.global.f64 	%fd31, [%rd26+72];
	sub.f64 	%fd32, %fd31, %fd30;
	ld.global.f64 	%fd33, [%rd27+80];
	ld.global.f64 	%fd34, [%rd26+80];
	sub.f64 	%fd35, %fd34, %fd33;
	ld.global.f64 	%fd36, [%rd27+88];
	ld.global.f64 	%fd37, [%rd26+88];
	sub.f64 	%fd38, %fd37, %fd36;
	div.rn.f64 	%fd39, %fd5, %fd2;
	div.rn.f64 	%fd40, %fd8, %fd2;
	div.rn.f64 	%fd41, %fd11, %fd2;
	div.rn.f64 	%fd42, %fd14, %fd2;
	div.rn.f64 	%fd43, %fd17, %fd2;
	div.rn.f64 	%fd44, %fd20, %fd2;
	div.rn.f64 	%fd45, %fd23, %fd2;
	div.rn.f64 	%fd46, %fd26, %fd2;
	div.rn.f64 	%fd47, %fd29, %fd2;
	div.rn.f64 	%fd48, %fd32, %fd2;
	div.rn.f64 	%fd49, %fd35, %fd2;
	div.rn.f64 	%fd50, %fd38, %fd2;
	mul.lo.s64 	%rd53, %rd48, %rd15;
	add.s64 	%rd54, %rd7, %rd53;
	ld.global.f64 	%fd51, [%rd54];
	sub.f64 	%fd52, %fd39, %fd51;
	ld.global.f64 	%fd53, [%rd54+8];
	sub.f64 	%fd54, %fd40, %fd53;
	ld.global.f64 	%fd55, [%rd54+16];
	sub.f64 	%fd56, %fd41, %fd55;
	ld.global.f64 	%fd57, [%rd54+24];
	sub.f64 	%fd58, %fd42, %fd57;
	ld.global.f64 	%fd59, [%rd54+32];
	sub.f64 	%fd60, %fd43, %fd59;
	ld.global.f64 	%fd61, [%rd54+40];
	sub.f64 	%fd62, %fd44, %fd61;
	ld.global.f64 	%fd63, [%rd54+48];
	sub.f64 	%fd64, %fd45, %fd63;
	ld.global.f64 	%fd65, [%rd54+56];
	sub.f64 	%fd66, %fd46, %fd65;
	ld.global.f64 	%fd67, [%rd54+64];
	sub.f64 	%fd68, %fd47, %fd67;
	ld.global.f64 	%fd69, [%rd54+72];
	sub.f64 	%fd70, %fd48, %fd69;
	ld.global.f64 	%fd71, [%rd54+80];
	sub.f64 	%fd72, %fd49, %fd71;
	ld.global.f64 	%fd73, [%rd54+88];
	sub.f64 	%fd74, %fd50, %fd73;
	div.rn.f64 	%fd75, %fd52, %fd2;
	div.rn.f64 	%fd76, %fd54, %fd2;
	div.rn.f64 	%fd77, %fd56, %fd2;
	div.rn.f64 	%fd78, %fd58, %fd2;
	div.rn.f64 	%fd79, %fd60, %fd2;
	div.rn.f64 	%fd80, %fd62, %fd2;
	div.rn.f64 	%fd81, %fd64, %fd2;
	div.rn.f64 	%fd82, %fd66, %fd2;
	div.rn.f64 	%fd83, %fd68, %fd2;
	div.rn.f64 	%fd84, %fd70, %fd2;
	div.rn.f64 	%fd85, %fd72, %fd2;
	div.rn.f64 	%fd86, %fd74, %fd2;
	st.global.f64 	[%rd28], %fd75;
	st.global.f64 	[%rd28+8], %fd76;
	st.global.f64 	[%rd28+16], %fd77;
	st.global.f64 	[%rd28+24], %fd78;
	st.global.f64 	[%rd28+32], %fd79;
	st.global.f64 	[%rd28+40], %fd80;
	st.global.f64 	[%rd28+48], %fd81;
	st.global.f64 	[%rd28+56], %fd82;
	st.global.f64 	[%rd28+64], %fd83;
	st.global.f64 	[%rd28+72], %fd84;
	st.global.f64 	[%rd28+80], %fd85;
	st.global.f64 	[%rd28+88], %fd86;
	st.global.f64 	[%rd54], %fd39;
	st.global.f64 	[%rd54+8], %fd40;
	st.global.f64 	[%rd54+16], %fd41;
	st.global.f64 	[%rd54+24], %fd42;
	st.global.f64 	[%rd54+32], %fd43;
	st.global.f64 	[%rd54+40], %fd44;
	st.global.f64 	[%rd54+48], %fd45;
	st.global.f64 	[%rd54+56], %fd46;
	st.global.f64 	[%rd54+64], %fd47;
	st.global.f64 	[%rd54+72], %fd48;
	st.global.f64 	[%rd54+80], %fd49;
	st.global.f64 	[%rd54+88], %fd50;
	ld.global.f64 	%fd87, [%rd26];
	ld.global.f64 	%fd88, [%rd26+8];
	ld.global.f64 	%fd89, [%rd26+16];
	ld.global.f64 	%fd90, [%rd26+24];
	ld.global.f64 	%fd91, [%rd26+32];
	ld.global.f64 	%fd92, [%rd26+40];
	ld.global.f64 	%fd93, [%rd26+48];
	ld.global.f64 	%fd94, [%rd26+56];
	ld.global.f64 	%fd95, [%rd26+64];
	ld.global.f64 	%fd96, [%rd26+72];
	ld.global.f64 	%fd97, [%rd26+80];
	ld.global.f64 	%fd98, [%rd26+88];
	st.global.f64 	[%rd27], %fd87;
	st.global.f64 	[%rd27+8], %fd88;
	st.global.f64 	[%rd27+16], %fd89;
	st.global.f64 	[%rd27+24], %fd90;
	st.global.f64 	[%rd27+32], %fd91;
	st.global.f64 	[%rd27+40], %fd92;
	st.global.f64 	[%rd27+48], %fd93;
	st.global.f64 	[%rd27+56], %fd94;
	st.global.f64 	[%rd27+64], %fd95;
	st.global.f64 	[%rd27+72], %fd96;
	st.global.f64 	[%rd27+80], %fd97;
	st.global.f64 	[%rd27+88], %fd98;
	bra.uni 	$L__BB26_18;

$L__BB26_17:
	ld.global.f64 	%fd99, [%rd26];
	ld.global.f64 	%fd100, [%rd27];
	sub.f64 	%fd101, %fd99, %fd100;
	ld.global.f64 	%fd102, [%rd27+8];
	ld.global.f64 	%fd103, [%rd26+8];
	sub.f64 	%fd104, %fd103, %fd102;
	ld.global.f64 	%fd105, [%rd27+16];
	ld.global.f64 	%fd106, [%rd26+16];
	sub.f64 	%fd107, %fd106, %fd105;
	ld.global.f64 	%fd108, [%rd27+24];
	ld.global.f64 	%fd109, [%rd26+24];
	sub.f64 	%fd110, %fd109, %fd108;
	ld.global.f64 	%fd111, [%rd27+32];
	ld.global.f64 	%fd112, [%rd26+32];
	sub.f64 	%fd113, %fd112, %fd111;
	ld.global.f64 	%fd114, [%rd27+40];
	ld.global.f64 	%fd115, [%rd26+40];
	sub.f64 	%fd116, %fd115, %fd114;
	ld.global.f64 	%fd117, [%rd27+48];
	ld.global.f64 	%fd118, [%rd26+48];
	sub.f64 	%fd119, %fd118, %fd117;
	ld.global.f64 	%fd120, [%rd27+56];
	ld.global.f64 	%fd121, [%rd26+56];
	sub.f64 	%fd122, %fd121, %fd120;
	ld.global.f64 	%fd123, [%rd27+64];
	ld.global.f64 	%fd124, [%rd26+64];
	sub.f64 	%fd125, %fd124, %fd123;
	ld.global.f64 	%fd126, [%rd27+72];
	ld.global.f64 	%fd127, [%rd26+72];
	sub.f64 	%fd128, %fd127, %fd126;
	ld.global.f64 	%fd129, [%rd27+80];
	ld.global.f64 	%fd130, [%rd26+80];
	sub.f64 	%fd131, %fd130, %fd129;
	ld.global.f64 	%fd132, [%rd27+88];
	ld.global.f64 	%fd133, [%rd26+88];
	sub.f64 	%fd134, %fd133, %fd132;
	div.rn.f64 	%fd135, %fd101, %fd1;
	div.rn.f64 	%fd136, %fd104, %fd1;
	div.rn.f64 	%fd137, %fd107, %fd1;
	div.rn.f64 	%fd138, %fd110, %fd1;
	div.rn.f64 	%fd139, %fd113, %fd1;
	div.rn.f64 	%fd140, %fd116, %fd1;
	div.rn.f64 	%fd141, %fd119, %fd1;
	div.rn.f64 	%fd142, %fd122, %fd1;
	div.rn.f64 	%fd143, %fd125, %fd1;
	div.rn.f64 	%fd144, %fd128, %fd1;
	div.rn.f64 	%fd145, %fd131, %fd1;
	div.rn.f64 	%fd146, %fd134, %fd1;
	st.global.f64 	[%rd28], %fd135;
	st.global.f64 	[%rd28+8], %fd136;
	st.global.f64 	[%rd28+16], %fd137;
	st.global.f64 	[%rd28+24], %fd138;
	st.global.f64 	[%rd28+32], %fd139;
	st.global.f64 	[%rd28+40], %fd140;
	st.global.f64 	[%rd28+48], %fd141;
	st.global.f64 	[%rd28+56], %fd142;
	st.global.f64 	[%rd28+64], %fd143;
	st.global.f64 	[%rd28+72], %fd144;
	st.global.f64 	[%rd28+80], %fd145;
	st.global.f64 	[%rd28+88], %fd146;
	ld.global.f64 	%fd147, [%rd26];
	ld.global.f64 	%fd148, [%rd26+8];
	ld.global.f64 	%fd149, [%rd26+16];
	ld.global.f64 	%fd150, [%rd26+24];
	ld.global.f64 	%fd151, [%rd26+32];
	ld.global.f64 	%fd152, [%rd26+40];
	ld.global.f64 	%fd153, [%rd26+48];
	ld.global.f64 	%fd154, [%rd26+56];
	ld.global.f64 	%fd155, [%rd26+64];
	ld.global.f64 	%fd156, [%rd26+72];
	ld.global.f64 	%fd157, [%rd26+80];
	ld.global.f64 	%fd158, [%rd26+88];
	st.global.f64 	[%rd27], %fd147;
	st.global.f64 	[%rd27+8], %fd148;
	st.global.f64 	[%rd27+16], %fd149;
	st.global.f64 	[%rd27+24], %fd150;
	st.global.f64 	[%rd27+32], %fd151;
	st.global.f64 	[%rd27+40], %fd152;
	st.global.f64 	[%rd27+48], %fd153;
	st.global.f64 	[%rd27+56], %fd154;
	st.global.f64 	[%rd27+64], %fd155;
	st.global.f64 	[%rd27+72], %fd156;
	st.global.f64 	[%rd27+80], %fd157;
	st.global.f64 	[%rd27+88], %fd158;

$L__BB26_18:
	add.s64 	%rd55, %rd55, %rd11;
	setp.lt.u64 	%p11, %rd55, %rd30;
	@%p11 bra 	$L__BB26_2;

$L__BB26_19:
	ret;

}
	// .globl	advection_y_cuda_kernel_backward
.visible .entry advection_y_cuda_kernel_backward(
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_4[56],
	.param .f64 advection_y_cuda_kernel_backward_param_5,
	.param .u32 advection_y_cuda_kernel_backward_param_6,
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_10[56],
	.param .f64 advection_y_cuda_kernel_backward_param_11,
	.param .u32 advection_y_cuda_kernel_backward_param_12
)
{
	.reg .pred 	%p<44>;
	.reg .b16 	%rs<71>;
	.reg .b32 	%r<164>;
	.reg .f64 	%fd<954>;
	.reg .b64 	%rd<289>;


	ld.param.v2.u32 	{%r81, %r82}, [advection_y_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r83, %r84}, [advection_y_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r89, %r90}, [advection_y_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r97, %r98}, [advection_y_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r105, %r106}, [advection_y_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r113, %r114}, [advection_y_cuda_kernel_backward_param_4+32];
	ld.param.f64 	%fd256, [advection_y_cuda_kernel_backward_param_5];
	ld.param.u32 	%r44, [advection_y_cuda_kernel_backward_param_6];
	ld.param.v2.u32 	{%r121, %r122}, [advection_y_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r129, %r130}, [advection_y_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r137, %r138}, [advection_y_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r145, %r146}, [advection_y_cuda_kernel_backward_param_10+32];
	ld.param.u64 	%rd60, [advection_y_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd58, [advection_y_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd56, [advection_y_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd54, [advection_y_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd53, [advection_y_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd51, [advection_y_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd49, [advection_y_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd47, [advection_y_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd45, [advection_y_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [advection_y_cuda_kernel_backward_param_0+16];
	mov.u32 	%r149, %ntid.x;
	cvt.u64.u32 	%rd1, %r149;
	mov.u32 	%r150, %ctaid.x;
	mul.wide.u32 	%rd62, %r149, %r150;
	mov.u32 	%r151, %tid.x;
	cvt.u64.u32 	%rd63, %r151;
	add.s64 	%rd285, %rd62, %rd63;
	setp.ge.u64 	%p2, %rd285, %rd45;
	@%p2 bra 	$L__BB27_65;

	cvt.s64.s32 	%rd18, %r84;
	cvt.s64.s32 	%rd19, %r83;
	cvt.s64.s32 	%rd20, %r82;
	cvt.s64.s32 	%rd21, %r105;
	cvt.s64.s32 	%rd22, %r89;
	setp.ne.s32 	%p1, %r44, 0;
	mul.f64 	%fd1, %fd256, %fd256;
	cvt.s64.s32 	%rd23, %r121;
	mov.u32 	%r152, %nctaid.x;
	cvt.u64.u32 	%rd64, %r152;
	mul.lo.s64 	%rd24, %rd1, %rd64;
	cvt.s64.s32 	%rd25, %r137;
	cvt.s64.s32 	%rd26, %r129;
	cvt.s64.s32 	%rd27, %r97;
	cvt.s64.s32 	%rd28, %r145;
	cvt.s64.s32 	%rd29, %r113;
	not.pred 	%p13, %p1;

$L__BB27_2:
	setp.lt.s32 	%p3, %r7, 4;
	mov.u64 	%rd286, %rd285;
	@%p3 bra 	$L__BB27_6;

	or.b64  	%rd65, %rd285, %rd18;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p4, %rd66, 0;
	@%p4 bra 	$L__BB27_5;

	div.u64 	%rd286, %rd285, %rd18;
	bra.uni 	$L__BB27_6;

$L__BB27_5:
	cvt.u32.u64 	%r153, %rd18;
	cvt.u32.u64 	%r154, %rd285;
	div.u32 	%r155, %r154, %r153;
	cvt.u64.u32 	%rd286, %r155;

$L__BB27_6:
	setp.lt.s32 	%p5, %r7, 3;
	@%p5 bra 	$L__BB27_10;

	or.b64  	%rd67, %rd286, %rd19;
	and.b64  	%rd68, %rd67, -4294967296;
	setp.eq.s64 	%p6, %rd68, 0;
	@%p6 bra 	$L__BB27_9;

	div.u64 	%rd286, %rd286, %rd19;
	bra.uni 	$L__BB27_10;

$L__BB27_9:
	cvt.u32.u64 	%r156, %rd19;
	cvt.u32.u64 	%r157, %rd286;
	div.u32 	%r158, %r157, %r156;
	cvt.u64.u32 	%rd286, %r158;

$L__BB27_10:
	setp.lt.s32 	%p7, %r7, 2;
	@%p7 bra 	$L__BB27_14;

	or.b64  	%rd69, %rd286, %rd20;
	and.b64  	%rd70, %rd69, -4294967296;
	setp.eq.s64 	%p8, %rd70, 0;
	@%p8 bra 	$L__BB27_13;

	div.u64 	%rd286, %rd286, %rd20;
	bra.uni 	$L__BB27_14;

$L__BB27_13:
	cvt.u32.u64 	%r159, %rd20;
	cvt.u32.u64 	%r160, %rd286;
	div.u32 	%r161, %r160, %r159;
	cvt.u64.u32 	%rd286, %r161;

$L__BB27_14:
	cvta.to.global.u64 	%rd282, %rd53;
	cvta.to.global.u64 	%rd281, %rd60;
	cvta.to.global.u64 	%rd280, %rd47;
	cvta.to.global.u64 	%rd279, %rd54;
	ld.param.u32 	%r163, [advection_y_cuda_kernel_backward_param_6];
	cvt.u32.u64 	%r162, %rd286;
	setp.gt.s32 	%p9, %r7, 0;
	selp.b32 	%r2, %r162, 0, %p9;
	setp.eq.s32 	%p10, %r163, 0;
	selp.f64 	%fd893, %fd1, %fd893, %p10;
	setp.eq.s32 	%p11, %r163, 1;
	selp.u16 	%rs68, 1, 0, %p11;
	selp.b16 	%rs70, %rs70, %rs68, %p10;
	and.b16  	%rs69, %rs70, 255;
	setp.eq.s16 	%p12, %rs69, 0;
	cvt.s64.s32 	%rd71, %r2;
	mul.lo.s64 	%rd72, %rd71, %rd23;
	add.s64 	%rd40, %rd279, %rd72;
	mul.lo.s64 	%rd73, %rd71, %rd22;
	add.s64 	%rd41, %rd280, %rd73;
	mul.lo.s64 	%rd74, %rd71, %rd28;
	add.s64 	%rd42, %rd281, %rd74;
	mul.lo.s64 	%rd75, %rd71, %rd29;
	add.s64 	%rd43, %rd282, %rd75;
	or.pred  	%p14, %p12, %p13;
	@%p14 bra 	$L__BB27_43;

	setp.eq.s64 	%p15, %rd54, 0;
	@%p15 bra 	$L__BB27_17;

	ld.global.f64 	%fd258, [%rd40];
	add.f64 	%fd905, %fd258, 0d0000000000000000;
	ld.global.f64 	%fd259, [%rd40+8];
	add.f64 	%fd904, %fd259, 0d0000000000000000;
	ld.global.f64 	%fd260, [%rd40+16];
	add.f64 	%fd903, %fd260, 0d0000000000000000;
	ld.global.f64 	%fd261, [%rd40+24];
	add.f64 	%fd902, %fd261, 0d0000000000000000;
	ld.global.f64 	%fd262, [%rd40+32];
	add.f64 	%fd901, %fd262, 0d0000000000000000;
	ld.global.f64 	%fd263, [%rd40+40];
	add.f64 	%fd900, %fd263, 0d0000000000000000;
	ld.global.f64 	%fd264, [%rd40+48];
	add.f64 	%fd899, %fd264, 0d0000000000000000;
	ld.global.f64 	%fd265, [%rd40+56];
	add.f64 	%fd898, %fd265, 0d0000000000000000;
	ld.global.f64 	%fd266, [%rd40+64];
	add.f64 	%fd897, %fd266, 0d0000000000000000;
	ld.global.f64 	%fd267, [%rd40+72];
	add.f64 	%fd896, %fd267, 0d0000000000000000;
	ld.global.f64 	%fd268, [%rd40+80];
	add.f64 	%fd895, %fd268, 0d0000000000000000;
	ld.global.f64 	%fd269, [%rd40+88];
	add.f64 	%fd894, %fd269, 0d0000000000000000;
	bra.uni 	$L__BB27_19;

$L__BB27_17:
	setp.eq.s64 	%p16, %rd47, 0;
	mov.f64 	%fd894, 0d0000000000000000;
	mov.f64 	%fd895, %fd894;
	mov.f64 	%fd896, %fd894;
	mov.f64 	%fd897, %fd894;
	mov.f64 	%fd898, %fd894;
	mov.f64 	%fd899, %fd894;
	mov.f64 	%fd900, %fd894;
	mov.f64 	%fd901, %fd894;
	mov.f64 	%fd902, %fd894;
	mov.f64 	%fd903, %fd894;
	mov.f64 	%fd904, %fd894;
	mov.f64 	%fd905, %fd894;
	@%p16 bra 	$L__BB27_19;

	ld.global.f64 	%fd282, [%rd41];
	add.f64 	%fd905, %fd282, 0d0000000000000000;
	ld.global.f64 	%fd283, [%rd41+8];
	add.f64 	%fd904, %fd283, 0d0000000000000000;
	ld.global.f64 	%fd284, [%rd41+16];
	add.f64 	%fd903, %fd284, 0d0000000000000000;
	ld.global.f64 	%fd285, [%rd41+24];
	add.f64 	%fd902, %fd285, 0d0000000000000000;
	ld.global.f64 	%fd286, [%rd41+32];
	add.f64 	%fd901, %fd286, 0d0000000000000000;
	ld.global.f64 	%fd287, [%rd41+40];
	add.f64 	%fd900, %fd287, 0d0000000000000000;
	ld.global.f64 	%fd288, [%rd41+48];
	add.f64 	%fd899, %fd288, 0d0000000000000000;
	ld.global.f64 	%fd289, [%rd41+56];
	add.f64 	%fd898, %fd289, 0d0000000000000000;
	ld.global.f64 	%fd290, [%rd41+64];
	add.f64 	%fd897, %fd290, 0d0000000000000000;
	ld.global.f64 	%fd291, [%rd41+72];
	add.f64 	%fd896, %fd291, 0d0000000000000000;
	ld.global.f64 	%fd292, [%rd41+80];
	add.f64 	%fd895, %fd292, 0d0000000000000000;
	ld.global.f64 	%fd293, [%rd41+88];
	add.f64 	%fd894, %fd293, 0d0000000000000000;

$L__BB27_19:
	setp.eq.s64 	%p17, %rd58, 0;
	@%p17 bra 	$L__BB27_21;

	mul.lo.s64 	%rd89, %rd71, %rd25;
	add.s64 	%rd76, %rd58, %rd89;
	// begin inline asm
	{ atom.add.f64 %fd294,[%rd76],%fd905; }

	// end inline asm
	add.s64 	%rd77, %rd76, 8;
	// begin inline asm
	{ atom.add.f64 %fd296,[%rd77],%fd904; }

	// end inline asm
	add.s64 	%rd78, %rd76, 16;
	// begin inline asm
	{ atom.add.f64 %fd298,[%rd78],%fd903; }

	// end inline asm
	add.s64 	%rd79, %rd76, 24;
	// begin inline asm
	{ atom.add.f64 %fd300,[%rd79],%fd902; }

	// end inline asm
	add.s64 	%rd80, %rd76, 32;
	// begin inline asm
	{ atom.add.f64 %fd302,[%rd80],%fd901; }

	// end inline asm
	add.s64 	%rd81, %rd76, 40;
	// begin inline asm
	{ atom.add.f64 %fd304,[%rd81],%fd900; }

	// end inline asm
	add.s64 	%rd82, %rd76, 48;
	// begin inline asm
	{ atom.add.f64 %fd306,[%rd82],%fd899; }

	// end inline asm
	add.s64 	%rd83, %rd76, 56;
	// begin inline asm
	{ atom.add.f64 %fd308,[%rd83],%fd898; }

	// end inline asm
	add.s64 	%rd84, %rd76, 64;
	// begin inline asm
	{ atom.add.f64 %fd310,[%rd84],%fd897; }

	// end inline asm
	add.s64 	%rd85, %rd76, 72;
	// begin inline asm
	{ atom.add.f64 %fd312,[%rd85],%fd896; }

	// end inline asm
	add.s64 	%rd86, %rd76, 80;
	// begin inline asm
	{ atom.add.f64 %fd314,[%rd86],%fd895; }

	// end inline asm
	add.s64 	%rd87, %rd76, 88;
	// begin inline asm
	{ atom.add.f64 %fd316,[%rd87],%fd894; }

	// end inline asm
	bra.uni 	$L__BB27_23;

$L__BB27_21:
	setp.eq.s64 	%p18, %rd51, 0;
	@%p18 bra 	$L__BB27_23;

	mul.lo.s64 	%rd103, %rd71, %rd21;
	add.s64 	%rd90, %rd51, %rd103;
	// begin inline asm
	{ atom.add.f64 %fd318,[%rd90],%fd905; }

	// end inline asm
	add.s64 	%rd91, %rd90, 8;
	// begin inline asm
	{ atom.add.f64 %fd320,[%rd91],%fd904; }

	// end inline asm
	add.s64 	%rd92, %rd90, 16;
	// begin inline asm
	{ atom.add.f64 %fd322,[%rd92],%fd903; }

	// end inline asm
	add.s64 	%rd93, %rd90, 24;
	// begin inline asm
	{ atom.add.f64 %fd324,[%rd93],%fd902; }

	// end inline asm
	add.s64 	%rd94, %rd90, 32;
	// begin inline asm
	{ atom.add.f64 %fd326,[%rd94],%fd901; }

	// end inline asm
	add.s64 	%rd95, %rd90, 40;
	// begin inline asm
	{ atom.add.f64 %fd328,[%rd95],%fd900; }

	// end inline asm
	add.s64 	%rd96, %rd90, 48;
	// begin inline asm
	{ atom.add.f64 %fd330,[%rd96],%fd899; }

	// end inline asm
	add.s64 	%rd97, %rd90, 56;
	// begin inline asm
	{ atom.add.f64 %fd332,[%rd97],%fd898; }

	// end inline asm
	add.s64 	%rd98, %rd90, 64;
	// begin inline asm
	{ atom.add.f64 %fd334,[%rd98],%fd897; }

	// end inline asm
	add.s64 	%rd99, %rd90, 72;
	// begin inline asm
	{ atom.add.f64 %fd336,[%rd99],%fd896; }

	// end inline asm
	add.s64 	%rd100, %rd90, 80;
	// begin inline asm
	{ atom.add.f64 %fd338,[%rd100],%fd895; }

	// end inline asm
	add.s64 	%rd101, %rd90, 88;
	// begin inline asm
	{ atom.add.f64 %fd340,[%rd101],%fd894; }

	// end inline asm

$L__BB27_23:
	setp.eq.s64 	%p19, %rd56, 0;
	@%p19 bra 	$L__BB27_25;

	cvta.to.global.u64 	%rd283, %rd56;
	mul.lo.s64 	%rd105, %rd71, %rd26;
	add.s64 	%rd106, %rd283, %rd105;
	ld.global.f64 	%fd342, [%rd106];
	add.f64 	%fd917, %fd342, 0d0000000000000000;
	ld.global.f64 	%fd343, [%rd106+8];
	add.f64 	%fd916, %fd343, 0d0000000000000000;
	ld.global.f64 	%fd344, [%rd106+16];
	add.f64 	%fd915, %fd344, 0d0000000000000000;
	ld.global.f64 	%fd345, [%rd106+24];
	add.f64 	%fd914, %fd345, 0d0000000000000000;
	ld.global.f64 	%fd346, [%rd106+32];
	add.f64 	%fd913, %fd346, 0d0000000000000000;
	ld.global.f64 	%fd347, [%rd106+40];
	add.f64 	%fd912, %fd347, 0d0000000000000000;
	ld.global.f64 	%fd348, [%rd106+48];
	add.f64 	%fd911, %fd348, 0d0000000000000000;
	ld.global.f64 	%fd349, [%rd106+56];
	add.f64 	%fd910, %fd349, 0d0000000000000000;
	ld.global.f64 	%fd350, [%rd106+64];
	add.f64 	%fd909, %fd350, 0d0000000000000000;
	ld.global.f64 	%fd351, [%rd106+72];
	add.f64 	%fd908, %fd351, 0d0000000000000000;
	ld.global.f64 	%fd352, [%rd106+80];
	add.f64 	%fd907, %fd352, 0d0000000000000000;
	ld.global.f64 	%fd353, [%rd106+88];
	add.f64 	%fd906, %fd353, 0d0000000000000000;
	bra.uni 	$L__BB27_27;

$L__BB27_25:
	setp.eq.s64 	%p20, %rd49, 0;
	mov.f64 	%fd906, 0d0000000000000000;
	mov.f64 	%fd907, %fd906;
	mov.f64 	%fd908, %fd906;
	mov.f64 	%fd909, %fd906;
	mov.f64 	%fd910, %fd906;
	mov.f64 	%fd911, %fd906;
	mov.f64 	%fd912, %fd906;
	mov.f64 	%fd913, %fd906;
	mov.f64 	%fd914, %fd906;
	mov.f64 	%fd915, %fd906;
	mov.f64 	%fd916, %fd906;
	mov.f64 	%fd917, %fd906;
	@%p20 bra 	$L__BB27_27;

	cvta.to.global.u64 	%rd284, %rd49;
	mul.lo.s64 	%rd108, %rd71, %rd27;
	add.s64 	%rd109, %rd284, %rd108;
	ld.global.f64 	%fd366, [%rd109];
	add.f64 	%fd917, %fd366, 0d0000000000000000;
	ld.global.f64 	%fd367, [%rd109+8];
	add.f64 	%fd916, %fd367, 0d0000000000000000;
	ld.global.f64 	%fd368, [%rd109+16];
	add.f64 	%fd915, %fd368, 0d0000000000000000;
	ld.global.f64 	%fd369, [%rd109+24];
	add.f64 	%fd914, %fd369, 0d0000000000000000;
	ld.global.f64 	%fd370, [%rd109+32];
	add.f64 	%fd913, %fd370, 0d0000000000000000;
	ld.global.f64 	%fd371, [%rd109+40];
	add.f64 	%fd912, %fd371, 0d0000000000000000;
	ld.global.f64 	%fd372, [%rd109+48];
	add.f64 	%fd911, %fd372, 0d0000000000000000;
	ld.global.f64 	%fd373, [%rd109+56];
	add.f64 	%fd910, %fd373, 0d0000000000000000;
	ld.global.f64 	%fd374, [%rd109+64];
	add.f64 	%fd909, %fd374, 0d0000000000000000;
	ld.global.f64 	%fd375, [%rd109+72];
	add.f64 	%fd908, %fd375, 0d0000000000000000;
	ld.global.f64 	%fd376, [%rd109+80];
	add.f64 	%fd907, %fd376, 0d0000000000000000;
	ld.global.f64 	%fd377, [%rd109+88];
	add.f64 	%fd906, %fd377, 0d0000000000000000;

$L__BB27_27:
	setp.eq.s64 	%p21, %rd60, 0;
	@%p21 bra 	$L__BB27_29;

	ld.global.f64 	%fd378, [%rd42];
	add.f64 	%fd929, %fd378, 0d0000000000000000;
	ld.global.f64 	%fd379, [%rd42+8];
	add.f64 	%fd928, %fd379, 0d0000000000000000;
	ld.global.f64 	%fd380, [%rd42+16];
	add.f64 	%fd927, %fd380, 0d0000000000000000;
	ld.global.f64 	%fd381, [%rd42+24];
	add.f64 	%fd926, %fd381, 0d0000000000000000;
	ld.global.f64 	%fd382, [%rd42+32];
	add.f64 	%fd925, %fd382, 0d0000000000000000;
	ld.global.f64 	%fd383, [%rd42+40];
	add.f64 	%fd924, %fd383, 0d0000000000000000;
	ld.global.f64 	%fd384, [%rd42+48];
	add.f64 	%fd923, %fd384, 0d0000000000000000;
	ld.global.f64 	%fd385, [%rd42+56];
	add.f64 	%fd922, %fd385, 0d0000000000000000;
	ld.global.f64 	%fd386, [%rd42+64];
	add.f64 	%fd921, %fd386, 0d0000000000000000;
	ld.global.f64 	%fd387, [%rd42+72];
	add.f64 	%fd920, %fd387, 0d0000000000000000;
	ld.global.f64 	%fd388, [%rd42+80];
	add.f64 	%fd919, %fd388, 0d0000000000000000;
	ld.global.f64 	%fd389, [%rd42+88];
	add.f64 	%fd918, %fd389, 0d0000000000000000;
	bra.uni 	$L__BB27_31;

$L__BB27_29:
	setp.eq.s64 	%p22, %rd53, 0;
	mov.f64 	%fd918, 0d0000000000000000;
	mov.f64 	%fd919, %fd918;
	mov.f64 	%fd920, %fd918;
	mov.f64 	%fd921, %fd918;
	mov.f64 	%fd922, %fd918;
	mov.f64 	%fd923, %fd918;
	mov.f64 	%fd924, %fd918;
	mov.f64 	%fd925, %fd918;
	mov.f64 	%fd926, %fd918;
	mov.f64 	%fd927, %fd918;
	mov.f64 	%fd928, %fd918;
	mov.f64 	%fd929, %fd918;
	@%p22 bra 	$L__BB27_31;

	ld.global.f64 	%fd402, [%rd43];
	add.f64 	%fd929, %fd402, 0d0000000000000000;
	ld.global.f64 	%fd403, [%rd43+8];
	add.f64 	%fd928, %fd403, 0d0000000000000000;
	ld.global.f64 	%fd404, [%rd43+16];
	add.f64 	%fd927, %fd404, 0d0000000000000000;
	ld.global.f64 	%fd405, [%rd43+24];
	add.f64 	%fd926, %fd405, 0d0000000000000000;
	ld.global.f64 	%fd406, [%rd43+32];
	add.f64 	%fd925, %fd406, 0d0000000000000000;
	ld.global.f64 	%fd407, [%rd43+40];
	add.f64 	%fd924, %fd407, 0d0000000000000000;
	ld.global.f64 	%fd408, [%rd43+48];
	add.f64 	%fd923, %fd408, 0d0000000000000000;
	ld.global.f64 	%fd409, [%rd43+56];
	add.f64 	%fd922, %fd409, 0d0000000000000000;
	ld.global.f64 	%fd410, [%rd43+64];
	add.f64 	%fd921, %fd410, 0d0000000000000000;
	ld.global.f64 	%fd411, [%rd43+72];
	add.f64 	%fd920, %fd411, 0d0000000000000000;
	ld.global.f64 	%fd412, [%rd43+80];
	add.f64 	%fd919, %fd412, 0d0000000000000000;
	ld.global.f64 	%fd413, [%rd43+88];
	add.f64 	%fd918, %fd413, 0d0000000000000000;

$L__BB27_31:
	div.rn.f64 	%fd414, %fd929, %fd256;
	add.f64 	%fd415, %fd414, 0d0000000000000000;
	mov.f64 	%fd416, 0d0000000000000000;
	div.rn.f64 	%fd417, %fd928, %fd256;
	add.f64 	%fd418, %fd417, 0d0000000000000000;
	div.rn.f64 	%fd419, %fd927, %fd256;
	add.f64 	%fd420, %fd419, 0d0000000000000000;
	div.rn.f64 	%fd421, %fd926, %fd256;
	add.f64 	%fd422, %fd421, 0d0000000000000000;
	div.rn.f64 	%fd423, %fd925, %fd256;
	add.f64 	%fd424, %fd423, 0d0000000000000000;
	div.rn.f64 	%fd425, %fd924, %fd256;
	add.f64 	%fd426, %fd425, 0d0000000000000000;
	div.rn.f64 	%fd427, %fd923, %fd256;
	add.f64 	%fd428, %fd427, 0d0000000000000000;
	div.rn.f64 	%fd429, %fd922, %fd256;
	add.f64 	%fd430, %fd429, 0d0000000000000000;
	div.rn.f64 	%fd431, %fd921, %fd256;
	add.f64 	%fd432, %fd431, 0d0000000000000000;
	div.rn.f64 	%fd433, %fd920, %fd256;
	add.f64 	%fd434, %fd433, 0d0000000000000000;
	div.rn.f64 	%fd435, %fd919, %fd256;
	add.f64 	%fd436, %fd435, 0d0000000000000000;
	div.rn.f64 	%fd437, %fd918, %fd256;
	add.f64 	%fd438, %fd437, 0d0000000000000000;
	add.f64 	%fd112, %fd917, %fd415;
	add.f64 	%fd113, %fd916, %fd418;
	add.f64 	%fd114, %fd915, %fd420;
	add.f64 	%fd115, %fd914, %fd422;
	add.f64 	%fd116, %fd913, %fd424;
	add.f64 	%fd117, %fd912, %fd426;
	add.f64 	%fd118, %fd911, %fd428;
	add.f64 	%fd119, %fd910, %fd430;
	add.f64 	%fd120, %fd909, %fd432;
	add.f64 	%fd121, %fd908, %fd434;
	add.f64 	%fd122, %fd907, %fd436;
	add.f64 	%fd123, %fd906, %fd438;
	sub.f64 	%fd124, %fd416, %fd415;
	sub.f64 	%fd125, %fd416, %fd418;
	sub.f64 	%fd126, %fd416, %fd420;
	sub.f64 	%fd127, %fd416, %fd422;
	sub.f64 	%fd128, %fd416, %fd424;
	sub.f64 	%fd129, %fd416, %fd426;
	sub.f64 	%fd130, %fd416, %fd428;
	sub.f64 	%fd131, %fd416, %fd430;
	sub.f64 	%fd132, %fd416, %fd432;
	sub.f64 	%fd133, %fd416, %fd434;
	sub.f64 	%fd134, %fd416, %fd436;
	sub.f64 	%fd135, %fd416, %fd438;
	@%p19 bra 	$L__BB27_33;

	mul.lo.s64 	%rd123, %rd71, %rd26;
	add.s64 	%rd110, %rd56, %rd123;
	// begin inline asm
	{ atom.add.f64 %fd439,[%rd110],%fd124; }

	// end inline asm
	add.s64 	%rd111, %rd110, 8;
	// begin inline asm
	{ atom.add.f64 %fd441,[%rd111],%fd125; }

	// end inline asm
	add.s64 	%rd112, %rd110, 16;
	// begin inline asm
	{ atom.add.f64 %fd443,[%rd112],%fd126; }

	// end inline asm
	add.s64 	%rd113, %rd110, 24;
	// begin inline asm
	{ atom.add.f64 %fd445,[%rd113],%fd127; }

	// end inline asm
	add.s64 	%rd114, %rd110, 32;
	// begin inline asm
	{ atom.add.f64 %fd447,[%rd114],%fd128; }

	// end inline asm
	add.s64 	%rd115, %rd110, 40;
	// begin inline asm
	{ atom.add.f64 %fd449,[%rd115],%fd129; }

	// end inline asm
	add.s64 	%rd116, %rd110, 48;
	// begin inline asm
	{ atom.add.f64 %fd451,[%rd116],%fd130; }

	// end inline asm
	add.s64 	%rd117, %rd110, 56;
	// begin inline asm
	{ atom.add.f64 %fd453,[%rd117],%fd131; }

	// end inline asm
	add.s64 	%rd118, %rd110, 64;
	// begin inline asm
	{ atom.add.f64 %fd455,[%rd118],%fd132; }

	// end inline asm
	add.s64 	%rd119, %rd110, 72;
	// begin inline asm
	{ atom.add.f64 %fd457,[%rd119],%fd133; }

	// end inline asm
	add.s64 	%rd120, %rd110, 80;
	// begin inline asm
	{ atom.add.f64 %fd459,[%rd120],%fd134; }

	// end inline asm
	add.s64 	%rd121, %rd110, 88;
	// begin inline asm
	{ atom.add.f64 %fd461,[%rd121],%fd135; }

	// end inline asm
	bra.uni 	$L__BB27_35;

$L__BB27_33:
	setp.eq.s64 	%p24, %rd49, 0;
	@%p24 bra 	$L__BB27_35;

	mul.lo.s64 	%rd137, %rd71, %rd27;
	add.s64 	%rd124, %rd49, %rd137;
	// begin inline asm
	{ atom.add.f64 %fd463,[%rd124],%fd124; }

	// end inline asm
	add.s64 	%rd125, %rd124, 8;
	// begin inline asm
	{ atom.add.f64 %fd465,[%rd125],%fd125; }

	// end inline asm
	add.s64 	%rd126, %rd124, 16;
	// begin inline asm
	{ atom.add.f64 %fd467,[%rd126],%fd126; }

	// end inline asm
	add.s64 	%rd127, %rd124, 24;
	// begin inline asm
	{ atom.add.f64 %fd469,[%rd127],%fd127; }

	// end inline asm
	add.s64 	%rd128, %rd124, 32;
	// begin inline asm
	{ atom.add.f64 %fd471,[%rd128],%fd128; }

	// end inline asm
	add.s64 	%rd129, %rd124, 40;
	// begin inline asm
	{ atom.add.f64 %fd473,[%rd129],%fd129; }

	// end inline asm
	add.s64 	%rd130, %rd124, 48;
	// begin inline asm
	{ atom.add.f64 %fd475,[%rd130],%fd130; }

	// end inline asm
	add.s64 	%rd131, %rd124, 56;
	// begin inline asm
	{ atom.add.f64 %fd477,[%rd131],%fd131; }

	// end inline asm
	add.s64 	%rd132, %rd124, 64;
	// begin inline asm
	{ atom.add.f64 %fd479,[%rd132],%fd132; }

	// end inline asm
	add.s64 	%rd133, %rd124, 72;
	// begin inline asm
	{ atom.add.f64 %fd481,[%rd133],%fd133; }

	// end inline asm
	add.s64 	%rd134, %rd124, 80;
	// begin inline asm
	{ atom.add.f64 %fd483,[%rd134],%fd134; }

	// end inline asm
	add.s64 	%rd135, %rd124, 88;
	// begin inline asm
	{ atom.add.f64 %fd485,[%rd135],%fd135; }

	// end inline asm

$L__BB27_35:
	setp.eq.s64 	%p41, %rd54, 0;
	div.rn.f64 	%fd487, %fd112, %fd256;
	mov.f64 	%fd488, 0d0000000000000000;
	div.rn.f64 	%fd489, %fd113, %fd256;
	div.rn.f64 	%fd490, %fd114, %fd256;
	add.f64 	%fd138, %fd490, 0d0000000000000000;
	div.rn.f64 	%fd491, %fd115, %fd256;
	add.f64 	%fd139, %fd491, 0d0000000000000000;
	div.rn.f64 	%fd492, %fd116, %fd256;
	add.f64 	%fd140, %fd492, 0d0000000000000000;
	div.rn.f64 	%fd493, %fd117, %fd256;
	add.f64 	%fd141, %fd493, 0d0000000000000000;
	div.rn.f64 	%fd494, %fd118, %fd256;
	add.f64 	%fd142, %fd494, 0d0000000000000000;
	div.rn.f64 	%fd495, %fd119, %fd256;
	add.f64 	%fd143, %fd495, 0d0000000000000000;
	div.rn.f64 	%fd496, %fd120, %fd256;
	add.f64 	%fd144, %fd496, 0d0000000000000000;
	div.rn.f64 	%fd497, %fd121, %fd256;
	add.f64 	%fd145, %fd497, 0d0000000000000000;
	div.rn.f64 	%fd498, %fd122, %fd256;
	add.f64 	%fd146, %fd498, 0d0000000000000000;
	div.rn.f64 	%fd499, %fd123, %fd256;
	add.f64 	%fd147, %fd499, 0d0000000000000000;
	sub.f64 	%fd150, %fd488, %fd138;
	sub.f64 	%fd151, %fd488, %fd139;
	sub.f64 	%fd152, %fd488, %fd140;
	sub.f64 	%fd153, %fd488, %fd141;
	sub.f64 	%fd154, %fd488, %fd142;
	sub.f64 	%fd155, %fd488, %fd143;
	sub.f64 	%fd156, %fd488, %fd144;
	sub.f64 	%fd157, %fd488, %fd145;
	sub.f64 	%fd158, %fd488, %fd146;
	sub.f64 	%fd159, %fd488, %fd147;
	@%p41 bra 	$L__BB27_37;

	mov.f64 	%fd829, 0d0000000000000000;
	add.f64 	%fd828, %fd489, 0d0000000000000000;
	sub.f64 	%fd827, %fd829, %fd828;
	add.f64 	%fd826, %fd487, 0d0000000000000000;
	sub.f64 	%fd825, %fd829, %fd826;
	add.s64 	%rd138, %rd54, %rd72;
	// begin inline asm
	{ atom.add.f64 %fd500,[%rd138],%fd825; }

	// end inline asm
	add.s64 	%rd139, %rd138, 8;
	// begin inline asm
	{ atom.add.f64 %fd502,[%rd139],%fd827; }

	// end inline asm
	add.s64 	%rd140, %rd138, 16;
	// begin inline asm
	{ atom.add.f64 %fd504,[%rd140],%fd150; }

	// end inline asm
	add.s64 	%rd141, %rd138, 24;
	// begin inline asm
	{ atom.add.f64 %fd506,[%rd141],%fd151; }

	// end inline asm
	add.s64 	%rd142, %rd138, 32;
	// begin inline asm
	{ atom.add.f64 %fd508,[%rd142],%fd152; }

	// end inline asm
	add.s64 	%rd143, %rd138, 40;
	// begin inline asm
	{ atom.add.f64 %fd510,[%rd143],%fd153; }

	// end inline asm
	add.s64 	%rd144, %rd138, 48;
	// begin inline asm
	{ atom.add.f64 %fd512,[%rd144],%fd154; }

	// end inline asm
	add.s64 	%rd145, %rd138, 56;
	// begin inline asm
	{ atom.add.f64 %fd514,[%rd145],%fd155; }

	// end inline asm
	add.s64 	%rd146, %rd138, 64;
	// begin inline asm
	{ atom.add.f64 %fd516,[%rd146],%fd156; }

	// end inline asm
	add.s64 	%rd147, %rd138, 72;
	// begin inline asm
	{ atom.add.f64 %fd518,[%rd147],%fd157; }

	// end inline asm
	add.s64 	%rd148, %rd138, 80;
	// begin inline asm
	{ atom.add.f64 %fd520,[%rd148],%fd158; }

	// end inline asm
	add.s64 	%rd149, %rd138, 88;
	// begin inline asm
	{ atom.add.f64 %fd522,[%rd149],%fd159; }

	// end inline asm
	bra.uni 	$L__BB27_39;

$L__BB27_37:
	setp.eq.s64 	%p26, %rd47, 0;
	@%p26 bra 	$L__BB27_39;

	mov.f64 	%fd858, 0d0000000000000000;
	add.f64 	%fd857, %fd489, 0d0000000000000000;
	sub.f64 	%fd856, %fd858, %fd857;
	add.f64 	%fd855, %fd487, 0d0000000000000000;
	sub.f64 	%fd854, %fd858, %fd855;
	add.s64 	%rd152, %rd47, %rd73;
	// begin inline asm
	{ atom.add.f64 %fd524,[%rd152],%fd854; }

	// end inline asm
	add.s64 	%rd153, %rd152, 8;
	// begin inline asm
	{ atom.add.f64 %fd526,[%rd153],%fd856; }

	// end inline asm
	add.s64 	%rd154, %rd152, 16;
	// begin inline asm
	{ atom.add.f64 %fd528,[%rd154],%fd150; }

	// end inline asm
	add.s64 	%rd155, %rd152, 24;
	// begin inline asm
	{ atom.add.f64 %fd530,[%rd155],%fd151; }

	// end inline asm
	add.s64 	%rd156, %rd152, 32;
	// begin inline asm
	{ atom.add.f64 %fd532,[%rd156],%fd152; }

	// end inline asm
	add.s64 	%rd157, %rd152, 40;
	// begin inline asm
	{ atom.add.f64 %fd534,[%rd157],%fd153; }

	// end inline asm
	add.s64 	%rd158, %rd152, 48;
	// begin inline asm
	{ atom.add.f64 %fd536,[%rd158],%fd154; }

	// end inline asm
	add.s64 	%rd159, %rd152, 56;
	// begin inline asm
	{ atom.add.f64 %fd538,[%rd159],%fd155; }

	// end inline asm
	add.s64 	%rd160, %rd152, 64;
	// begin inline asm
	{ atom.add.f64 %fd540,[%rd160],%fd156; }

	// end inline asm
	add.s64 	%rd161, %rd152, 72;
	// begin inline asm
	{ atom.add.f64 %fd542,[%rd161],%fd157; }

	// end inline asm
	add.s64 	%rd162, %rd152, 80;
	// begin inline asm
	{ atom.add.f64 %fd544,[%rd162],%fd158; }

	// end inline asm
	add.s64 	%rd163, %rd152, 88;
	// begin inline asm
	{ atom.add.f64 %fd546,[%rd163],%fd159; }

	// end inline asm

$L__BB27_39:
	setp.eq.s64 	%p42, %rd58, 0;
	@%p42 bra 	$L__BB27_41;

	add.f64 	%fd841, %fd499, 0d0000000000000000;
	add.f64 	%fd840, %fd498, 0d0000000000000000;
	add.f64 	%fd839, %fd497, 0d0000000000000000;
	add.f64 	%fd838, %fd496, 0d0000000000000000;
	add.f64 	%fd837, %fd495, 0d0000000000000000;
	add.f64 	%fd836, %fd494, 0d0000000000000000;
	add.f64 	%fd835, %fd493, 0d0000000000000000;
	add.f64 	%fd834, %fd492, 0d0000000000000000;
	add.f64 	%fd833, %fd491, 0d0000000000000000;
	add.f64 	%fd832, %fd490, 0d0000000000000000;
	add.f64 	%fd831, %fd489, 0d0000000000000000;
	add.f64 	%fd830, %fd487, 0d0000000000000000;
	mul.lo.s64 	%rd179, %rd71, %rd25;
	add.s64 	%rd166, %rd58, %rd179;
	// begin inline asm
	{ atom.add.f64 %fd548,[%rd166],%fd830; }

	// end inline asm
	add.s64 	%rd167, %rd166, 8;
	// begin inline asm
	{ atom.add.f64 %fd550,[%rd167],%fd831; }

	// end inline asm
	add.s64 	%rd168, %rd166, 16;
	// begin inline asm
	{ atom.add.f64 %fd552,[%rd168],%fd832; }

	// end inline asm
	add.s64 	%rd169, %rd166, 24;
	// begin inline asm
	{ atom.add.f64 %fd554,[%rd169],%fd833; }

	// end inline asm
	add.s64 	%rd170, %rd166, 32;
	// begin inline asm
	{ atom.add.f64 %fd556,[%rd170],%fd834; }

	// end inline asm
	add.s64 	%rd171, %rd166, 40;
	// begin inline asm
	{ atom.add.f64 %fd558,[%rd171],%fd835; }

	// end inline asm
	add.s64 	%rd172, %rd166, 48;
	// begin inline asm
	{ atom.add.f64 %fd560,[%rd172],%fd836; }

	// end inline asm
	add.s64 	%rd173, %rd166, 56;
	// begin inline asm
	{ atom.add.f64 %fd562,[%rd173],%fd837; }

	// end inline asm
	add.s64 	%rd174, %rd166, 64;
	// begin inline asm
	{ atom.add.f64 %fd564,[%rd174],%fd838; }

	// end inline asm
	add.s64 	%rd175, %rd166, 72;
	// begin inline asm
	{ atom.add.f64 %fd566,[%rd175],%fd839; }

	// end inline asm
	add.s64 	%rd176, %rd166, 80;
	// begin inline asm
	{ atom.add.f64 %fd568,[%rd176],%fd840; }

	// end inline asm
	add.s64 	%rd177, %rd166, 88;
	// begin inline asm
	{ atom.add.f64 %fd570,[%rd177],%fd841; }

	// end inline asm
	bra.uni 	$L__BB27_43;

$L__BB27_41:
	setp.eq.s64 	%p28, %rd51, 0;
	@%p28 bra 	$L__BB27_43;

	add.f64 	%fd853, %fd499, 0d0000000000000000;
	add.f64 	%fd852, %fd498, 0d0000000000000000;
	add.f64 	%fd851, %fd497, 0d0000000000000000;
	add.f64 	%fd850, %fd496, 0d0000000000000000;
	add.f64 	%fd849, %fd495, 0d0000000000000000;
	add.f64 	%fd848, %fd494, 0d0000000000000000;
	add.f64 	%fd847, %fd493, 0d0000000000000000;
	add.f64 	%fd846, %fd492, 0d0000000000000000;
	add.f64 	%fd845, %fd491, 0d0000000000000000;
	add.f64 	%fd844, %fd490, 0d0000000000000000;
	add.f64 	%fd843, %fd489, 0d0000000000000000;
	add.f64 	%fd842, %fd487, 0d0000000000000000;
	mul.lo.s64 	%rd193, %rd71, %rd21;
	add.s64 	%rd180, %rd51, %rd193;
	// begin inline asm
	{ atom.add.f64 %fd572,[%rd180],%fd842; }

	// end inline asm
	add.s64 	%rd181, %rd180, 8;
	// begin inline asm
	{ atom.add.f64 %fd574,[%rd181],%fd843; }

	// end inline asm
	add.s64 	%rd182, %rd180, 16;
	// begin inline asm
	{ atom.add.f64 %fd576,[%rd182],%fd844; }

	// end inline asm
	add.s64 	%rd183, %rd180, 24;
	// begin inline asm
	{ atom.add.f64 %fd578,[%rd183],%fd845; }

	// end inline asm
	add.s64 	%rd184, %rd180, 32;
	// begin inline asm
	{ atom.add.f64 %fd580,[%rd184],%fd846; }

	// end inline asm
	add.s64 	%rd185, %rd180, 40;
	// begin inline asm
	{ atom.add.f64 %fd582,[%rd185],%fd847; }

	// end inline asm
	add.s64 	%rd186, %rd180, 48;
	// begin inline asm
	{ atom.add.f64 %fd584,[%rd186],%fd848; }

	// end inline asm
	add.s64 	%rd187, %rd180, 56;
	// begin inline asm
	{ atom.add.f64 %fd586,[%rd187],%fd849; }

	// end inline asm
	add.s64 	%rd188, %rd180, 64;
	// begin inline asm
	{ atom.add.f64 %fd588,[%rd188],%fd850; }

	// end inline asm
	add.s64 	%rd189, %rd180, 72;
	// begin inline asm
	{ atom.add.f64 %fd590,[%rd189],%fd851; }

	// end inline asm
	add.s64 	%rd190, %rd180, 80;
	// begin inline asm
	{ atom.add.f64 %fd592,[%rd190],%fd852; }

	// end inline asm
	add.s64 	%rd191, %rd180, 88;
	// begin inline asm
	{ atom.add.f64 %fd594,[%rd191],%fd853; }

	// end inline asm

$L__BB27_43:
	@%p1 bra 	$L__BB27_64;

	setp.eq.s64 	%p30, %rd54, 0;
	@%p30 bra 	$L__BB27_46;

	ld.global.f64 	%fd596, [%rd40];
	add.f64 	%fd941, %fd596, 0d0000000000000000;
	ld.global.f64 	%fd597, [%rd40+8];
	add.f64 	%fd940, %fd597, 0d0000000000000000;
	ld.global.f64 	%fd598, [%rd40+16];
	add.f64 	%fd939, %fd598, 0d0000000000000000;
	ld.global.f64 	%fd599, [%rd40+24];
	add.f64 	%fd938, %fd599, 0d0000000000000000;
	ld.global.f64 	%fd600, [%rd40+32];
	add.f64 	%fd937, %fd600, 0d0000000000000000;
	ld.global.f64 	%fd601, [%rd40+40];
	add.f64 	%fd936, %fd601, 0d0000000000000000;
	ld.global.f64 	%fd602, [%rd40+48];
	add.f64 	%fd935, %fd602, 0d0000000000000000;
	ld.global.f64 	%fd603, [%rd40+56];
	add.f64 	%fd934, %fd603, 0d0000000000000000;
	ld.global.f64 	%fd604, [%rd40+64];
	add.f64 	%fd933, %fd604, 0d0000000000000000;
	ld.global.f64 	%fd605, [%rd40+72];
	add.f64 	%fd932, %fd605, 0d0000000000000000;
	ld.global.f64 	%fd606, [%rd40+80];
	add.f64 	%fd931, %fd606, 0d0000000000000000;
	ld.global.f64 	%fd607, [%rd40+88];
	add.f64 	%fd930, %fd607, 0d0000000000000000;
	bra.uni 	$L__BB27_48;

$L__BB27_46:
	setp.eq.s64 	%p31, %rd47, 0;
	mov.f64 	%fd930, 0d0000000000000000;
	mov.f64 	%fd931, %fd930;
	mov.f64 	%fd932, %fd930;
	mov.f64 	%fd933, %fd930;
	mov.f64 	%fd934, %fd930;
	mov.f64 	%fd935, %fd930;
	mov.f64 	%fd936, %fd930;
	mov.f64 	%fd937, %fd930;
	mov.f64 	%fd938, %fd930;
	mov.f64 	%fd939, %fd930;
	mov.f64 	%fd940, %fd930;
	mov.f64 	%fd941, %fd930;
	@%p31 bra 	$L__BB27_48;

	ld.global.f64 	%fd620, [%rd41];
	add.f64 	%fd941, %fd620, 0d0000000000000000;
	ld.global.f64 	%fd621, [%rd41+8];
	add.f64 	%fd940, %fd621, 0d0000000000000000;
	ld.global.f64 	%fd622, [%rd41+16];
	add.f64 	%fd939, %fd622, 0d0000000000000000;
	ld.global.f64 	%fd623, [%rd41+24];
	add.f64 	%fd938, %fd623, 0d0000000000000000;
	ld.global.f64 	%fd624, [%rd41+32];
	add.f64 	%fd937, %fd624, 0d0000000000000000;
	ld.global.f64 	%fd625, [%rd41+40];
	add.f64 	%fd936, %fd625, 0d0000000000000000;
	ld.global.f64 	%fd626, [%rd41+48];
	add.f64 	%fd935, %fd626, 0d0000000000000000;
	ld.global.f64 	%fd627, [%rd41+56];
	add.f64 	%fd934, %fd627, 0d0000000000000000;
	ld.global.f64 	%fd628, [%rd41+64];
	add.f64 	%fd933, %fd628, 0d0000000000000000;
	ld.global.f64 	%fd629, [%rd41+72];
	add.f64 	%fd932, %fd629, 0d0000000000000000;
	ld.global.f64 	%fd630, [%rd41+80];
	add.f64 	%fd931, %fd630, 0d0000000000000000;
	ld.global.f64 	%fd631, [%rd41+88];
	add.f64 	%fd930, %fd631, 0d0000000000000000;

$L__BB27_48:
	setp.eq.s64 	%p32, %rd58, 0;
	@%p32 bra 	$L__BB27_50;

	mul.lo.s64 	%rd207, %rd71, %rd25;
	add.s64 	%rd194, %rd58, %rd207;
	// begin inline asm
	{ atom.add.f64 %fd632,[%rd194],%fd941; }

	// end inline asm
	add.s64 	%rd195, %rd194, 8;
	// begin inline asm
	{ atom.add.f64 %fd634,[%rd195],%fd940; }

	// end inline asm
	add.s64 	%rd196, %rd194, 16;
	// begin inline asm
	{ atom.add.f64 %fd636,[%rd196],%fd939; }

	// end inline asm
	add.s64 	%rd197, %rd194, 24;
	// begin inline asm
	{ atom.add.f64 %fd638,[%rd197],%fd938; }

	// end inline asm
	add.s64 	%rd198, %rd194, 32;
	// begin inline asm
	{ atom.add.f64 %fd640,[%rd198],%fd937; }

	// end inline asm
	add.s64 	%rd199, %rd194, 40;
	// begin inline asm
	{ atom.add.f64 %fd642,[%rd199],%fd936; }

	// end inline asm
	add.s64 	%rd200, %rd194, 48;
	// begin inline asm
	{ atom.add.f64 %fd644,[%rd200],%fd935; }

	// end inline asm
	add.s64 	%rd201, %rd194, 56;
	// begin inline asm
	{ atom.add.f64 %fd646,[%rd201],%fd934; }

	// end inline asm
	add.s64 	%rd202, %rd194, 64;
	// begin inline asm
	{ atom.add.f64 %fd648,[%rd202],%fd933; }

	// end inline asm
	add.s64 	%rd203, %rd194, 72;
	// begin inline asm
	{ atom.add.f64 %fd650,[%rd203],%fd932; }

	// end inline asm
	add.s64 	%rd204, %rd194, 80;
	// begin inline asm
	{ atom.add.f64 %fd652,[%rd204],%fd931; }

	// end inline asm
	add.s64 	%rd205, %rd194, 88;
	// begin inline asm
	{ atom.add.f64 %fd654,[%rd205],%fd930; }

	// end inline asm
	bra.uni 	$L__BB27_52;

$L__BB27_50:
	setp.eq.s64 	%p33, %rd51, 0;
	@%p33 bra 	$L__BB27_52;

	mul.lo.s64 	%rd221, %rd71, %rd21;
	add.s64 	%rd208, %rd51, %rd221;
	// begin inline asm
	{ atom.add.f64 %fd656,[%rd208],%fd941; }

	// end inline asm
	add.s64 	%rd209, %rd208, 8;
	// begin inline asm
	{ atom.add.f64 %fd658,[%rd209],%fd940; }

	// end inline asm
	add.s64 	%rd210, %rd208, 16;
	// begin inline asm
	{ atom.add.f64 %fd660,[%rd210],%fd939; }

	// end inline asm
	add.s64 	%rd211, %rd208, 24;
	// begin inline asm
	{ atom.add.f64 %fd662,[%rd211],%fd938; }

	// end inline asm
	add.s64 	%rd212, %rd208, 32;
	// begin inline asm
	{ atom.add.f64 %fd664,[%rd212],%fd937; }

	// end inline asm
	add.s64 	%rd213, %rd208, 40;
	// begin inline asm
	{ atom.add.f64 %fd666,[%rd213],%fd936; }

	// end inline asm
	add.s64 	%rd214, %rd208, 48;
	// begin inline asm
	{ atom.add.f64 %fd668,[%rd214],%fd935; }

	// end inline asm
	add.s64 	%rd215, %rd208, 56;
	// begin inline asm
	{ atom.add.f64 %fd670,[%rd215],%fd934; }

	// end inline asm
	add.s64 	%rd216, %rd208, 64;
	// begin inline asm
	{ atom.add.f64 %fd672,[%rd216],%fd933; }

	// end inline asm
	add.s64 	%rd217, %rd208, 72;
	// begin inline asm
	{ atom.add.f64 %fd674,[%rd217],%fd932; }

	// end inline asm
	add.s64 	%rd218, %rd208, 80;
	// begin inline asm
	{ atom.add.f64 %fd676,[%rd218],%fd931; }

	// end inline asm
	add.s64 	%rd219, %rd208, 88;
	// begin inline asm
	{ atom.add.f64 %fd678,[%rd219],%fd930; }

	// end inline asm

$L__BB27_52:
	setp.eq.s64 	%p34, %rd60, 0;
	@%p34 bra 	$L__BB27_54;

	ld.global.f64 	%fd680, [%rd42];
	add.f64 	%fd953, %fd680, 0d0000000000000000;
	ld.global.f64 	%fd681, [%rd42+8];
	add.f64 	%fd952, %fd681, 0d0000000000000000;
	ld.global.f64 	%fd682, [%rd42+16];
	add.f64 	%fd951, %fd682, 0d0000000000000000;
	ld.global.f64 	%fd683, [%rd42+24];
	add.f64 	%fd950, %fd683, 0d0000000000000000;
	ld.global.f64 	%fd684, [%rd42+32];
	add.f64 	%fd949, %fd684, 0d0000000000000000;
	ld.global.f64 	%fd685, [%rd42+40];
	add.f64 	%fd948, %fd685, 0d0000000000000000;
	ld.global.f64 	%fd686, [%rd42+48];
	add.f64 	%fd947, %fd686, 0d0000000000000000;
	ld.global.f64 	%fd687, [%rd42+56];
	add.f64 	%fd946, %fd687, 0d0000000000000000;
	ld.global.f64 	%fd688, [%rd42+64];
	add.f64 	%fd945, %fd688, 0d0000000000000000;
	ld.global.f64 	%fd689, [%rd42+72];
	add.f64 	%fd944, %fd689, 0d0000000000000000;
	ld.global.f64 	%fd690, [%rd42+80];
	add.f64 	%fd943, %fd690, 0d0000000000000000;
	ld.global.f64 	%fd691, [%rd42+88];
	add.f64 	%fd942, %fd691, 0d0000000000000000;
	bra.uni 	$L__BB27_56;

$L__BB27_54:
	setp.eq.s64 	%p35, %rd53, 0;
	mov.f64 	%fd942, 0d0000000000000000;
	mov.f64 	%fd943, %fd942;
	mov.f64 	%fd944, %fd942;
	mov.f64 	%fd945, %fd942;
	mov.f64 	%fd946, %fd942;
	mov.f64 	%fd947, %fd942;
	mov.f64 	%fd948, %fd942;
	mov.f64 	%fd949, %fd942;
	mov.f64 	%fd950, %fd942;
	mov.f64 	%fd951, %fd942;
	mov.f64 	%fd952, %fd942;
	mov.f64 	%fd953, %fd942;
	@%p35 bra 	$L__BB27_56;

	ld.global.f64 	%fd704, [%rd43];
	add.f64 	%fd953, %fd704, 0d0000000000000000;
	ld.global.f64 	%fd705, [%rd43+8];
	add.f64 	%fd952, %fd705, 0d0000000000000000;
	ld.global.f64 	%fd706, [%rd43+16];
	add.f64 	%fd951, %fd706, 0d0000000000000000;
	ld.global.f64 	%fd707, [%rd43+24];
	add.f64 	%fd950, %fd707, 0d0000000000000000;
	ld.global.f64 	%fd708, [%rd43+32];
	add.f64 	%fd949, %fd708, 0d0000000000000000;
	ld.global.f64 	%fd709, [%rd43+40];
	add.f64 	%fd948, %fd709, 0d0000000000000000;
	ld.global.f64 	%fd710, [%rd43+48];
	add.f64 	%fd947, %fd710, 0d0000000000000000;
	ld.global.f64 	%fd711, [%rd43+56];
	add.f64 	%fd946, %fd711, 0d0000000000000000;
	ld.global.f64 	%fd712, [%rd43+64];
	add.f64 	%fd945, %fd712, 0d0000000000000000;
	ld.global.f64 	%fd713, [%rd43+72];
	add.f64 	%fd944, %fd713, 0d0000000000000000;
	ld.global.f64 	%fd714, [%rd43+80];
	add.f64 	%fd943, %fd714, 0d0000000000000000;
	ld.global.f64 	%fd715, [%rd43+88];
	add.f64 	%fd942, %fd715, 0d0000000000000000;

$L__BB27_56:
	div.rn.f64 	%fd716, %fd953, %fd893;
	mov.f64 	%fd717, 0d0000000000000000;
	div.rn.f64 	%fd718, %fd952, %fd893;
	div.rn.f64 	%fd719, %fd951, %fd893;
	add.f64 	%fd234, %fd719, 0d0000000000000000;
	div.rn.f64 	%fd720, %fd950, %fd893;
	add.f64 	%fd235, %fd720, 0d0000000000000000;
	div.rn.f64 	%fd721, %fd949, %fd893;
	add.f64 	%fd236, %fd721, 0d0000000000000000;
	div.rn.f64 	%fd722, %fd948, %fd893;
	add.f64 	%fd237, %fd722, 0d0000000000000000;
	div.rn.f64 	%fd723, %fd947, %fd893;
	add.f64 	%fd238, %fd723, 0d0000000000000000;
	div.rn.f64 	%fd724, %fd946, %fd893;
	add.f64 	%fd239, %fd724, 0d0000000000000000;
	div.rn.f64 	%fd725, %fd945, %fd893;
	add.f64 	%fd240, %fd725, 0d0000000000000000;
	div.rn.f64 	%fd726, %fd944, %fd893;
	add.f64 	%fd241, %fd726, 0d0000000000000000;
	div.rn.f64 	%fd727, %fd943, %fd893;
	add.f64 	%fd242, %fd727, 0d0000000000000000;
	div.rn.f64 	%fd728, %fd942, %fd893;
	add.f64 	%fd243, %fd728, 0d0000000000000000;
	sub.f64 	%fd246, %fd717, %fd234;
	sub.f64 	%fd247, %fd717, %fd235;
	sub.f64 	%fd248, %fd717, %fd236;
	sub.f64 	%fd249, %fd717, %fd237;
	sub.f64 	%fd250, %fd717, %fd238;
	sub.f64 	%fd251, %fd717, %fd239;
	sub.f64 	%fd252, %fd717, %fd240;
	sub.f64 	%fd253, %fd717, %fd241;
	sub.f64 	%fd254, %fd717, %fd242;
	sub.f64 	%fd255, %fd717, %fd243;
	@%p30 bra 	$L__BB27_58;

	mov.f64 	%fd863, 0d0000000000000000;
	add.f64 	%fd862, %fd718, 0d0000000000000000;
	sub.f64 	%fd861, %fd863, %fd862;
	add.f64 	%fd860, %fd716, 0d0000000000000000;
	sub.f64 	%fd859, %fd863, %fd860;
	add.s64 	%rd222, %rd54, %rd72;
	// begin inline asm
	{ atom.add.f64 %fd729,[%rd222],%fd859; }

	// end inline asm
	add.s64 	%rd223, %rd222, 8;
	// begin inline asm
	{ atom.add.f64 %fd731,[%rd223],%fd861; }

	// end inline asm
	add.s64 	%rd224, %rd222, 16;
	// begin inline asm
	{ atom.add.f64 %fd733,[%rd224],%fd246; }

	// end inline asm
	add.s64 	%rd225, %rd222, 24;
	// begin inline asm
	{ atom.add.f64 %fd735,[%rd225],%fd247; }

	// end inline asm
	add.s64 	%rd226, %rd222, 32;
	// begin inline asm
	{ atom.add.f64 %fd737,[%rd226],%fd248; }

	// end inline asm
	add.s64 	%rd227, %rd222, 40;
	// begin inline asm
	{ atom.add.f64 %fd739,[%rd227],%fd249; }

	// end inline asm
	add.s64 	%rd228, %rd222, 48;
	// begin inline asm
	{ atom.add.f64 %fd741,[%rd228],%fd250; }

	// end inline asm
	add.s64 	%rd229, %rd222, 56;
	// begin inline asm
	{ atom.add.f64 %fd743,[%rd229],%fd251; }

	// end inline asm
	add.s64 	%rd230, %rd222, 64;
	// begin inline asm
	{ atom.add.f64 %fd745,[%rd230],%fd252; }

	// end inline asm
	add.s64 	%rd231, %rd222, 72;
	// begin inline asm
	{ atom.add.f64 %fd747,[%rd231],%fd253; }

	// end inline asm
	add.s64 	%rd232, %rd222, 80;
	// begin inline asm
	{ atom.add.f64 %fd749,[%rd232],%fd254; }

	// end inline asm
	add.s64 	%rd233, %rd222, 88;
	// begin inline asm
	{ atom.add.f64 %fd751,[%rd233],%fd255; }

	// end inline asm
	bra.uni 	$L__BB27_60;

$L__BB27_58:
	setp.eq.s64 	%p37, %rd47, 0;
	@%p37 bra 	$L__BB27_60;

	mov.f64 	%fd892, 0d0000000000000000;
	add.f64 	%fd891, %fd718, 0d0000000000000000;
	sub.f64 	%fd890, %fd892, %fd891;
	add.f64 	%fd889, %fd716, 0d0000000000000000;
	sub.f64 	%fd888, %fd892, %fd889;
	add.s64 	%rd236, %rd47, %rd73;
	// begin inline asm
	{ atom.add.f64 %fd753,[%rd236],%fd888; }

	// end inline asm
	add.s64 	%rd237, %rd236, 8;
	// begin inline asm
	{ atom.add.f64 %fd755,[%rd237],%fd890; }

	// end inline asm
	add.s64 	%rd238, %rd236, 16;
	// begin inline asm
	{ atom.add.f64 %fd757,[%rd238],%fd246; }

	// end inline asm
	add.s64 	%rd239, %rd236, 24;
	// begin inline asm
	{ atom.add.f64 %fd759,[%rd239],%fd247; }

	// end inline asm
	add.s64 	%rd240, %rd236, 32;
	// begin inline asm
	{ atom.add.f64 %fd761,[%rd240],%fd248; }

	// end inline asm
	add.s64 	%rd241, %rd236, 40;
	// begin inline asm
	{ atom.add.f64 %fd763,[%rd241],%fd249; }

	// end inline asm
	add.s64 	%rd242, %rd236, 48;
	// begin inline asm
	{ atom.add.f64 %fd765,[%rd242],%fd250; }

	// end inline asm
	add.s64 	%rd243, %rd236, 56;
	// begin inline asm
	{ atom.add.f64 %fd767,[%rd243],%fd251; }

	// end inline asm
	add.s64 	%rd244, %rd236, 64;
	// begin inline asm
	{ atom.add.f64 %fd769,[%rd244],%fd252; }

	// end inline asm
	add.s64 	%rd245, %rd236, 72;
	// begin inline asm
	{ atom.add.f64 %fd771,[%rd245],%fd253; }

	// end inline asm
	add.s64 	%rd246, %rd236, 80;
	// begin inline asm
	{ atom.add.f64 %fd773,[%rd246],%fd254; }

	// end inline asm
	add.s64 	%rd247, %rd236, 88;
	// begin inline asm
	{ atom.add.f64 %fd775,[%rd247],%fd255; }

	// end inline asm

$L__BB27_60:
	setp.eq.s64 	%p43, %rd58, 0;
	@%p43 bra 	$L__BB27_62;

	add.f64 	%fd875, %fd728, 0d0000000000000000;
	add.f64 	%fd874, %fd727, 0d0000000000000000;
	add.f64 	%fd873, %fd726, 0d0000000000000000;
	add.f64 	%fd872, %fd725, 0d0000000000000000;
	add.f64 	%fd871, %fd724, 0d0000000000000000;
	add.f64 	%fd870, %fd723, 0d0000000000000000;
	add.f64 	%fd869, %fd722, 0d0000000000000000;
	add.f64 	%fd868, %fd721, 0d0000000000000000;
	add.f64 	%fd867, %fd720, 0d0000000000000000;
	add.f64 	%fd866, %fd719, 0d0000000000000000;
	add.f64 	%fd865, %fd718, 0d0000000000000000;
	add.f64 	%fd864, %fd716, 0d0000000000000000;
	mul.lo.s64 	%rd263, %rd71, %rd25;
	add.s64 	%rd250, %rd58, %rd263;
	// begin inline asm
	{ atom.add.f64 %fd777,[%rd250],%fd864; }

	// end inline asm
	add.s64 	%rd251, %rd250, 8;
	// begin inline asm
	{ atom.add.f64 %fd779,[%rd251],%fd865; }

	// end inline asm
	add.s64 	%rd252, %rd250, 16;
	// begin inline asm
	{ atom.add.f64 %fd781,[%rd252],%fd866; }

	// end inline asm
	add.s64 	%rd253, %rd250, 24;
	// begin inline asm
	{ atom.add.f64 %fd783,[%rd253],%fd867; }

	// end inline asm
	add.s64 	%rd254, %rd250, 32;
	// begin inline asm
	{ atom.add.f64 %fd785,[%rd254],%fd868; }

	// end inline asm
	add.s64 	%rd255, %rd250, 40;
	// begin inline asm
	{ atom.add.f64 %fd787,[%rd255],%fd869; }

	// end inline asm
	add.s64 	%rd256, %rd250, 48;
	// begin inline asm
	{ atom.add.f64 %fd789,[%rd256],%fd870; }

	// end inline asm
	add.s64 	%rd257, %rd250, 56;
	// begin inline asm
	{ atom.add.f64 %fd791,[%rd257],%fd871; }

	// end inline asm
	add.s64 	%rd258, %rd250, 64;
	// begin inline asm
	{ atom.add.f64 %fd793,[%rd258],%fd872; }

	// end inline asm
	add.s64 	%rd259, %rd250, 72;
	// begin inline asm
	{ atom.add.f64 %fd795,[%rd259],%fd873; }

	// end inline asm
	add.s64 	%rd260, %rd250, 80;
	// begin inline asm
	{ atom.add.f64 %fd797,[%rd260],%fd874; }

	// end inline asm
	add.s64 	%rd261, %rd250, 88;
	// begin inline asm
	{ atom.add.f64 %fd799,[%rd261],%fd875; }

	// end inline asm
	bra.uni 	$L__BB27_64;

$L__BB27_62:
	setp.eq.s64 	%p39, %rd51, 0;
	@%p39 bra 	$L__BB27_64;

	add.f64 	%fd887, %fd728, 0d0000000000000000;
	add.f64 	%fd886, %fd727, 0d0000000000000000;
	add.f64 	%fd885, %fd726, 0d0000000000000000;
	add.f64 	%fd884, %fd725, 0d0000000000000000;
	add.f64 	%fd883, %fd724, 0d0000000000000000;
	add.f64 	%fd882, %fd723, 0d0000000000000000;
	add.f64 	%fd881, %fd722, 0d0000000000000000;
	add.f64 	%fd880, %fd721, 0d0000000000000000;
	add.f64 	%fd879, %fd720, 0d0000000000000000;
	add.f64 	%fd878, %fd719, 0d0000000000000000;
	add.f64 	%fd877, %fd718, 0d0000000000000000;
	add.f64 	%fd876, %fd716, 0d0000000000000000;
	mul.lo.s64 	%rd277, %rd71, %rd21;
	add.s64 	%rd264, %rd51, %rd277;
	// begin inline asm
	{ atom.add.f64 %fd801,[%rd264],%fd876; }

	// end inline asm
	add.s64 	%rd265, %rd264, 8;
	// begin inline asm
	{ atom.add.f64 %fd803,[%rd265],%fd877; }

	// end inline asm
	add.s64 	%rd266, %rd264, 16;
	// begin inline asm
	{ atom.add.f64 %fd805,[%rd266],%fd878; }

	// end inline asm
	add.s64 	%rd267, %rd264, 24;
	// begin inline asm
	{ atom.add.f64 %fd807,[%rd267],%fd879; }

	// end inline asm
	add.s64 	%rd268, %rd264, 32;
	// begin inline asm
	{ atom.add.f64 %fd809,[%rd268],%fd880; }

	// end inline asm
	add.s64 	%rd269, %rd264, 40;
	// begin inline asm
	{ atom.add.f64 %fd811,[%rd269],%fd881; }

	// end inline asm
	add.s64 	%rd270, %rd264, 48;
	// begin inline asm
	{ atom.add.f64 %fd813,[%rd270],%fd882; }

	// end inline asm
	add.s64 	%rd271, %rd264, 56;
	// begin inline asm
	{ atom.add.f64 %fd815,[%rd271],%fd883; }

	// end inline asm
	add.s64 	%rd272, %rd264, 64;
	// begin inline asm
	{ atom.add.f64 %fd817,[%rd272],%fd884; }

	// end inline asm
	add.s64 	%rd273, %rd264, 72;
	// begin inline asm
	{ atom.add.f64 %fd819,[%rd273],%fd885; }

	// end inline asm
	add.s64 	%rd274, %rd264, 80;
	// begin inline asm
	{ atom.add.f64 %fd821,[%rd274],%fd886; }

	// end inline asm
	add.s64 	%rd275, %rd264, 88;
	// begin inline asm
	{ atom.add.f64 %fd823,[%rd275],%fd887; }

	// end inline asm

$L__BB27_64:
	ld.param.u64 	%rd278, [advection_y_cuda_kernel_backward_param_0+24];
	add.s64 	%rd285, %rd285, %rd24;
	setp.lt.u64 	%p40, %rd285, %rd278;
	@%p40 bra 	$L__BB27_2;

$L__BB27_65:
	ret;

}
	// .globl	multiply_arr_vec12d_mul_scalar_cuda_kernel_forward
.visible .entry multiply_arr_vec12d_mul_scalar_cuda_kernel_forward(
	.param .align 8 .b8 multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_1[56]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<47>;
	.reg .f64 	%fd<49>;
	.reg .b64 	%rd<60>;


	ld.param.v2.u32 	{%r16, %r17}, [multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r18, %r19}, [multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r24, %r25}, [multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_1+32];
	ld.param.u64 	%rd30, [multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd29, [multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_0+16];
	mov.u32 	%r28, %ntid.x;
	cvt.u64.u32 	%rd1, %r28;
	mov.u32 	%r29, %ctaid.x;
	mul.wide.u32 	%rd32, %r28, %r29;
	mov.u32 	%r30, %tid.x;
	cvt.u64.u32 	%rd33, %r30;
	add.s64 	%rd53, %rd32, %rd33;
	setp.ge.u64 	%p1, %rd53, %rd29;
	@%p1 bra 	$L__BB28_25;

	cvta.to.global.u64 	%rd4, %rd30;
	cvt.s64.s32 	%rd5, %r19;
	cvt.s64.s32 	%rd6, %r18;
	cvt.s64.s32 	%rd7, %r17;
	cvt.s64.s32 	%rd8, %r24;
	mov.u32 	%r31, %nctaid.x;
	cvt.u64.u32 	%rd34, %r31;
	mul.lo.s64 	%rd9, %rd1, %rd34;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB28_12;
	bra.uni 	$L__BB28_2;

$L__BB28_12:
	cvt.u32.u64 	%r38, %rd5;
	cvt.u32.u64 	%r41, %rd6;
	cvt.u32.u64 	%r44, %rd7;

$L__BB28_13:
	or.b64  	%rd43, %rd53, %rd5;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p9, %rd44, 0;
	@%p9 bra 	$L__BB28_15;

	div.u64 	%rd58, %rd53, %rd5;
	bra.uni 	$L__BB28_16;

$L__BB28_15:
	cvt.u32.u64 	%r39, %rd53;
	div.u32 	%r40, %r39, %r38;
	cvt.u64.u32 	%rd58, %r40;

$L__BB28_16:
	setp.lt.s32 	%p10, %r6, 3;
	@%p10 bra 	$L__BB28_20;

	or.b64  	%rd45, %rd58, %rd6;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p11, %rd46, 0;
	@%p11 bra 	$L__BB28_19;

	div.u64 	%rd58, %rd58, %rd6;
	bra.uni 	$L__BB28_20;

$L__BB28_19:
	cvt.u32.u64 	%r42, %rd58;
	div.u32 	%r43, %r42, %r41;
	cvt.u64.u32 	%rd58, %r43;

$L__BB28_20:
	setp.lt.s32 	%p12, %r6, 2;
	@%p12 bra 	$L__BB28_24;

	or.b64  	%rd47, %rd58, %rd7;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p13, %rd48, 0;
	@%p13 bra 	$L__BB28_23;

	div.u64 	%rd58, %rd58, %rd7;
	bra.uni 	$L__BB28_24;

$L__BB28_23:
	cvt.u32.u64 	%r45, %rd58;
	div.u32 	%r46, %r45, %r44;
	cvt.u64.u32 	%rd58, %r46;

$L__BB28_24:
	cvt.s64.s32 	%rd49, %rd58;
	setp.gt.s32 	%p14, %r6, 0;
	selp.b64 	%rd50, %rd49, 0, %p14;
	mul.lo.s64 	%rd51, %rd50, %rd8;
	add.s64 	%rd52, %rd4, %rd51;
	ld.global.f64 	%fd25, [%rd52];
	neg.f64 	%fd26, %fd25;
	ld.global.f64 	%fd27, [%rd52+8];
	neg.f64 	%fd28, %fd27;
	ld.global.f64 	%fd29, [%rd52+16];
	neg.f64 	%fd30, %fd29;
	ld.global.f64 	%fd31, [%rd52+24];
	neg.f64 	%fd32, %fd31;
	ld.global.f64 	%fd33, [%rd52+32];
	neg.f64 	%fd34, %fd33;
	ld.global.f64 	%fd35, [%rd52+40];
	neg.f64 	%fd36, %fd35;
	ld.global.f64 	%fd37, [%rd52+48];
	neg.f64 	%fd38, %fd37;
	ld.global.f64 	%fd39, [%rd52+56];
	neg.f64 	%fd40, %fd39;
	ld.global.f64 	%fd41, [%rd52+64];
	neg.f64 	%fd42, %fd41;
	ld.global.f64 	%fd43, [%rd52+72];
	neg.f64 	%fd44, %fd43;
	ld.global.f64 	%fd45, [%rd52+80];
	neg.f64 	%fd46, %fd45;
	ld.global.f64 	%fd47, [%rd52+88];
	neg.f64 	%fd48, %fd47;
	st.global.f64 	[%rd52], %fd26;
	st.global.f64 	[%rd52+8], %fd28;
	st.global.f64 	[%rd52+16], %fd30;
	st.global.f64 	[%rd52+24], %fd32;
	st.global.f64 	[%rd52+32], %fd34;
	st.global.f64 	[%rd52+40], %fd36;
	st.global.f64 	[%rd52+48], %fd38;
	st.global.f64 	[%rd52+56], %fd40;
	st.global.f64 	[%rd52+64], %fd42;
	st.global.f64 	[%rd52+72], %fd44;
	st.global.f64 	[%rd52+80], %fd46;
	st.global.f64 	[%rd52+88], %fd48;
	add.s64 	%rd53, %rd53, %rd9;
	setp.lt.u64 	%p15, %rd53, %rd29;
	@%p15 bra 	$L__BB28_13;
	bra.uni 	$L__BB28_25;

$L__BB28_2:
	cvt.u32.u64 	%r32, %rd6;
	cvt.u32.u64 	%r35, %rd7;

$L__BB28_3:
	setp.lt.s32 	%p3, %r6, 3;
	mov.u64 	%rd54, %rd53;
	@%p3 bra 	$L__BB28_7;

	or.b64  	%rd35, %rd53, %rd6;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p4, %rd36, 0;
	@%p4 bra 	$L__BB28_6;

	div.u64 	%rd54, %rd53, %rd6;
	bra.uni 	$L__BB28_7;

$L__BB28_6:
	cvt.u32.u64 	%r33, %rd53;
	div.u32 	%r34, %r33, %r32;
	cvt.u64.u32 	%rd54, %r34;

$L__BB28_7:
	setp.lt.s32 	%p5, %r6, 2;
	@%p5 bra 	$L__BB28_11;

	or.b64  	%rd37, %rd54, %rd7;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.eq.s64 	%p6, %rd38, 0;
	@%p6 bra 	$L__BB28_10;

	div.u64 	%rd54, %rd54, %rd7;
	bra.uni 	$L__BB28_11;

$L__BB28_10:
	cvt.u32.u64 	%r36, %rd54;
	div.u32 	%r37, %r36, %r35;
	cvt.u64.u32 	%rd54, %r37;

$L__BB28_11:
	cvt.s64.s32 	%rd39, %rd54;
	setp.gt.s32 	%p7, %r6, 0;
	selp.b64 	%rd40, %rd39, 0, %p7;
	mul.lo.s64 	%rd41, %rd40, %rd8;
	add.s64 	%rd42, %rd4, %rd41;
	ld.global.f64 	%fd1, [%rd42];
	neg.f64 	%fd2, %fd1;
	ld.global.f64 	%fd3, [%rd42+8];
	neg.f64 	%fd4, %fd3;
	ld.global.f64 	%fd5, [%rd42+16];
	neg.f64 	%fd6, %fd5;
	ld.global.f64 	%fd7, [%rd42+24];
	neg.f64 	%fd8, %fd7;
	ld.global.f64 	%fd9, [%rd42+32];
	neg.f64 	%fd10, %fd9;
	ld.global.f64 	%fd11, [%rd42+40];
	neg.f64 	%fd12, %fd11;
	ld.global.f64 	%fd13, [%rd42+48];
	neg.f64 	%fd14, %fd13;
	ld.global.f64 	%fd15, [%rd42+56];
	neg.f64 	%fd16, %fd15;
	ld.global.f64 	%fd17, [%rd42+64];
	neg.f64 	%fd18, %fd17;
	ld.global.f64 	%fd19, [%rd42+72];
	neg.f64 	%fd20, %fd19;
	ld.global.f64 	%fd21, [%rd42+80];
	neg.f64 	%fd22, %fd21;
	ld.global.f64 	%fd23, [%rd42+88];
	neg.f64 	%fd24, %fd23;
	st.global.f64 	[%rd42], %fd2;
	st.global.f64 	[%rd42+8], %fd4;
	st.global.f64 	[%rd42+16], %fd6;
	st.global.f64 	[%rd42+24], %fd8;
	st.global.f64 	[%rd42+32], %fd10;
	st.global.f64 	[%rd42+40], %fd12;
	st.global.f64 	[%rd42+48], %fd14;
	st.global.f64 	[%rd42+56], %fd16;
	st.global.f64 	[%rd42+64], %fd18;
	st.global.f64 	[%rd42+72], %fd20;
	st.global.f64 	[%rd42+80], %fd22;
	st.global.f64 	[%rd42+88], %fd24;
	add.s64 	%rd53, %rd53, %rd9;
	setp.lt.u64 	%p8, %rd53, %rd29;
	@%p8 bra 	$L__BB28_3;

$L__BB28_25:
	ret;

}
	// .globl	multiply_arr_vec12d_mul_scalar_cuda_kernel_backward
.visible .entry multiply_arr_vec12d_mul_scalar_cuda_kernel_backward(
	.param .align 8 .b8 multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_2[56]
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<58>;
	.reg .f64 	%fd<146>;
	.reg .b64 	%rd<74>;


	ld.param.v2.u32 	{%r25, %r26}, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r27, %r28}, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r33, %r34}, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r41, %r42}, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_2+32];
	ld.param.u64 	%rd30, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd29, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd27, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_0+16];
	mov.u32 	%r45, %ntid.x;
	cvt.u64.u32 	%rd1, %r45;
	mov.u32 	%r46, %ctaid.x;
	mul.wide.u32 	%rd32, %r45, %r46;
	mov.u32 	%r47, %tid.x;
	cvt.u64.u32 	%rd33, %r47;
	add.s64 	%rd70, %rd32, %rd33;
	setp.ge.u64 	%p1, %rd70, %rd27;
	@%p1 bra 	$L__BB29_23;

	cvta.to.global.u64 	%rd6, %rd30;
	cvta.to.global.u64 	%rd7, %rd29;
	cvt.s64.s32 	%rd8, %r28;
	cvt.s64.s32 	%rd9, %r27;
	cvt.s64.s32 	%rd10, %r26;
	cvt.s64.s32 	%rd11, %r33;
	cvt.s64.s32 	%rd12, %r41;
	mov.u32 	%r48, %nctaid.x;
	cvt.u64.u32 	%rd34, %r48;
	mul.lo.s64 	%rd13, %rd1, %rd34;

$L__BB29_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd71, %rd70;
	@%p2 bra 	$L__BB29_6;

	or.b64  	%rd35, %rd70, %rd8;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p3, %rd36, 0;
	@%p3 bra 	$L__BB29_5;

	div.u64 	%rd71, %rd70, %rd8;
	bra.uni 	$L__BB29_6;

$L__BB29_5:
	cvt.u32.u64 	%r49, %rd8;
	cvt.u32.u64 	%r50, %rd70;
	div.u32 	%r51, %r50, %r49;
	cvt.u64.u32 	%rd71, %r51;

$L__BB29_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB29_10;

	or.b64  	%rd37, %rd71, %rd9;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.eq.s64 	%p5, %rd38, 0;
	@%p5 bra 	$L__BB29_9;

	div.u64 	%rd71, %rd71, %rd9;
	bra.uni 	$L__BB29_10;

$L__BB29_9:
	cvt.u32.u64 	%r52, %rd9;
	cvt.u32.u64 	%r53, %rd71;
	div.u32 	%r54, %r53, %r52;
	cvt.u64.u32 	%rd71, %r54;

$L__BB29_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB29_14;

	or.b64  	%rd39, %rd71, %rd10;
	and.b64  	%rd40, %rd39, -4294967296;
	setp.eq.s64 	%p7, %rd40, 0;
	@%p7 bra 	$L__BB29_13;

	div.u64 	%rd71, %rd71, %rd10;
	bra.uni 	$L__BB29_14;

$L__BB29_13:
	cvt.u32.u64 	%r55, %rd10;
	cvt.u32.u64 	%r56, %rd71;
	div.u32 	%r57, %r56, %r55;
	cvt.u64.u32 	%rd71, %r57;

$L__BB29_14:
	cvt.s64.s32 	%rd41, %rd71;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd24, %rd41, 0, %p8;
	mul.lo.s64 	%rd25, %rd24, %rd11;
	setp.eq.s64 	%p9, %rd30, 0;
	@%p9 bra 	$L__BB29_16;

	mul.lo.s64 	%rd42, %rd24, %rd12;
	add.s64 	%rd43, %rd6, %rd42;
	ld.global.f64 	%fd49, [%rd43];
	add.f64 	%fd145, %fd49, 0d0000000000000000;
	ld.global.f64 	%fd50, [%rd43+8];
	add.f64 	%fd144, %fd50, 0d0000000000000000;
	ld.global.f64 	%fd51, [%rd43+16];
	add.f64 	%fd143, %fd51, 0d0000000000000000;
	ld.global.f64 	%fd52, [%rd43+24];
	add.f64 	%fd142, %fd52, 0d0000000000000000;
	ld.global.f64 	%fd53, [%rd43+32];
	add.f64 	%fd141, %fd53, 0d0000000000000000;
	ld.global.f64 	%fd54, [%rd43+40];
	add.f64 	%fd140, %fd54, 0d0000000000000000;
	ld.global.f64 	%fd55, [%rd43+48];
	add.f64 	%fd139, %fd55, 0d0000000000000000;
	ld.global.f64 	%fd56, [%rd43+56];
	add.f64 	%fd138, %fd56, 0d0000000000000000;
	ld.global.f64 	%fd57, [%rd43+64];
	add.f64 	%fd137, %fd57, 0d0000000000000000;
	ld.global.f64 	%fd58, [%rd43+72];
	add.f64 	%fd136, %fd58, 0d0000000000000000;
	ld.global.f64 	%fd59, [%rd43+80];
	add.f64 	%fd135, %fd59, 0d0000000000000000;
	ld.global.f64 	%fd60, [%rd43+88];
	add.f64 	%fd134, %fd60, 0d0000000000000000;
	bra.uni 	$L__BB29_18;

$L__BB29_16:
	setp.eq.s64 	%p10, %rd29, 0;
	mov.f64 	%fd134, 0d0000000000000000;
	mov.f64 	%fd135, %fd134;
	mov.f64 	%fd136, %fd134;
	mov.f64 	%fd137, %fd134;
	mov.f64 	%fd138, %fd134;
	mov.f64 	%fd139, %fd134;
	mov.f64 	%fd140, %fd134;
	mov.f64 	%fd141, %fd134;
	mov.f64 	%fd142, %fd134;
	mov.f64 	%fd143, %fd134;
	mov.f64 	%fd144, %fd134;
	mov.f64 	%fd145, %fd134;
	@%p10 bra 	$L__BB29_18;

	add.s64 	%rd44, %rd7, %rd25;
	ld.global.f64 	%fd73, [%rd44];
	add.f64 	%fd145, %fd73, 0d0000000000000000;
	ld.global.f64 	%fd74, [%rd44+8];
	add.f64 	%fd144, %fd74, 0d0000000000000000;
	ld.global.f64 	%fd75, [%rd44+16];
	add.f64 	%fd143, %fd75, 0d0000000000000000;
	ld.global.f64 	%fd76, [%rd44+24];
	add.f64 	%fd142, %fd76, 0d0000000000000000;
	ld.global.f64 	%fd77, [%rd44+32];
	add.f64 	%fd141, %fd77, 0d0000000000000000;
	ld.global.f64 	%fd78, [%rd44+40];
	add.f64 	%fd140, %fd78, 0d0000000000000000;
	ld.global.f64 	%fd79, [%rd44+48];
	add.f64 	%fd139, %fd79, 0d0000000000000000;
	ld.global.f64 	%fd80, [%rd44+56];
	add.f64 	%fd138, %fd80, 0d0000000000000000;
	ld.global.f64 	%fd81, [%rd44+64];
	add.f64 	%fd137, %fd81, 0d0000000000000000;
	ld.global.f64 	%fd82, [%rd44+72];
	add.f64 	%fd136, %fd82, 0d0000000000000000;
	ld.global.f64 	%fd83, [%rd44+80];
	add.f64 	%fd135, %fd83, 0d0000000000000000;
	ld.global.f64 	%fd84, [%rd44+88];
	add.f64 	%fd134, %fd84, 0d0000000000000000;

$L__BB29_18:
	mov.f64 	%fd85, 0d0000000000000000;
	sub.f64 	%fd37, %fd85, %fd145;
	sub.f64 	%fd38, %fd85, %fd144;
	sub.f64 	%fd39, %fd85, %fd143;
	sub.f64 	%fd40, %fd85, %fd142;
	sub.f64 	%fd41, %fd85, %fd141;
	sub.f64 	%fd42, %fd85, %fd140;
	sub.f64 	%fd43, %fd85, %fd139;
	sub.f64 	%fd44, %fd85, %fd138;
	sub.f64 	%fd45, %fd85, %fd137;
	sub.f64 	%fd46, %fd85, %fd136;
	sub.f64 	%fd47, %fd85, %fd135;
	sub.f64 	%fd48, %fd85, %fd134;
	@%p9 bra 	$L__BB29_20;

	mul.lo.s64 	%rd57, %rd24, %rd12;
	add.s64 	%rd45, %rd30, %rd57;
	// begin inline asm
	{ atom.add.f64 %fd86,[%rd45],%fd37; }

	// end inline asm
	add.s64 	%rd46, %rd45, 8;
	// begin inline asm
	{ atom.add.f64 %fd88,[%rd46],%fd38; }

	// end inline asm
	add.s64 	%rd47, %rd45, 16;
	// begin inline asm
	{ atom.add.f64 %fd90,[%rd47],%fd39; }

	// end inline asm
	add.s64 	%rd48, %rd45, 24;
	// begin inline asm
	{ atom.add.f64 %fd92,[%rd48],%fd40; }

	// end inline asm
	add.s64 	%rd49, %rd45, 32;
	// begin inline asm
	{ atom.add.f64 %fd94,[%rd49],%fd41; }

	// end inline asm
	add.s64 	%rd50, %rd45, 40;
	// begin inline asm
	{ atom.add.f64 %fd96,[%rd50],%fd42; }

	// end inline asm
	add.s64 	%rd51, %rd45, 48;
	// begin inline asm
	{ atom.add.f64 %fd98,[%rd51],%fd43; }

	// end inline asm
	add.s64 	%rd52, %rd45, 56;
	// begin inline asm
	{ atom.add.f64 %fd100,[%rd52],%fd44; }

	// end inline asm
	add.s64 	%rd53, %rd45, 64;
	// begin inline asm
	{ atom.add.f64 %fd102,[%rd53],%fd45; }

	// end inline asm
	add.s64 	%rd54, %rd45, 72;
	// begin inline asm
	{ atom.add.f64 %fd104,[%rd54],%fd46; }

	// end inline asm
	add.s64 	%rd55, %rd45, 80;
	// begin inline asm
	{ atom.add.f64 %fd106,[%rd55],%fd47; }

	// end inline asm
	add.s64 	%rd56, %rd45, 88;
	// begin inline asm
	{ atom.add.f64 %fd108,[%rd56],%fd48; }

	// end inline asm
	bra.uni 	$L__BB29_22;

$L__BB29_20:
	setp.eq.s64 	%p12, %rd29, 0;
	@%p12 bra 	$L__BB29_22;

	add.s64 	%rd58, %rd29, %rd25;
	// begin inline asm
	{ atom.add.f64 %fd110,[%rd58],%fd37; }

	// end inline asm
	add.s64 	%rd59, %rd58, 8;
	// begin inline asm
	{ atom.add.f64 %fd112,[%rd59],%fd38; }

	// end inline asm
	add.s64 	%rd60, %rd58, 16;
	// begin inline asm
	{ atom.add.f64 %fd114,[%rd60],%fd39; }

	// end inline asm
	add.s64 	%rd61, %rd58, 24;
	// begin inline asm
	{ atom.add.f64 %fd116,[%rd61],%fd40; }

	// end inline asm
	add.s64 	%rd62, %rd58, 32;
	// begin inline asm
	{ atom.add.f64 %fd118,[%rd62],%fd41; }

	// end inline asm
	add.s64 	%rd63, %rd58, 40;
	// begin inline asm
	{ atom.add.f64 %fd120,[%rd63],%fd42; }

	// end inline asm
	add.s64 	%rd64, %rd58, 48;
	// begin inline asm
	{ atom.add.f64 %fd122,[%rd64],%fd43; }

	// end inline asm
	add.s64 	%rd65, %rd58, 56;
	// begin inline asm
	{ atom.add.f64 %fd124,[%rd65],%fd44; }

	// end inline asm
	add.s64 	%rd66, %rd58, 64;
	// begin inline asm
	{ atom.add.f64 %fd126,[%rd66],%fd45; }

	// end inline asm
	add.s64 	%rd67, %rd58, 72;
	// begin inline asm
	{ atom.add.f64 %fd128,[%rd67],%fd46; }

	// end inline asm
	add.s64 	%rd68, %rd58, 80;
	// begin inline asm
	{ atom.add.f64 %fd130,[%rd68],%fd47; }

	// end inline asm
	add.s64 	%rd69, %rd58, 88;
	// begin inline asm
	{ atom.add.f64 %fd132,[%rd69],%fd48; }

	// end inline asm

$L__BB29_22:
	add.s64 	%rd70, %rd70, %rd13;
	setp.lt.u64 	%p13, %rd70, %rd27;
	@%p13 bra 	$L__BB29_2;

$L__BB29_23:
	ret;

}
	// .globl	init_affine_diag_hess_inds_kernel_cuda_kernel_forward
.visible .entry init_affine_diag_hess_inds_kernel_cuda_kernel_forward(
	.param .align 8 .b8 init_affine_diag_hess_inds_kernel_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 init_affine_diag_hess_inds_kernel_cuda_kernel_forward_param_1[184]
)
{
	.local .align 8 .b8 	__local_depot30[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<122>;
	.reg .b32 	%r<107>;
	.reg .b64 	%rd<172>;


	mov.u64 	%SPL, __local_depot30;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r40, %r41}, [init_affine_diag_hess_inds_kernel_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r42, %r43}, [init_affine_diag_hess_inds_kernel_cuda_kernel_forward_param_0+8];
	mov.b64 	%rd26, init_affine_diag_hess_inds_kernel_cuda_kernel_forward_param_1;
	ld.param.u64 	%rd25, [init_affine_diag_hess_inds_kernel_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r39, [init_affine_diag_hess_inds_kernel_cuda_kernel_forward_param_0+16];
	mov.u32 	%r44, %ntid.x;
	cvt.u64.u32 	%rd1, %r44;
	mov.u32 	%r45, %ctaid.x;
	mul.wide.u32 	%rd27, %r44, %r45;
	mov.u32 	%r46, %tid.x;
	cvt.u64.u32 	%rd28, %r46;
	add.s64 	%rd168, %rd27, %rd28;
	setp.ge.u64 	%p1, %rd168, %rd25;
	@%p1 bra 	$L__BB30_63;

	cvt.s64.s32 	%rd4, %r43;
	cvt.s64.s32 	%rd5, %r42;
	cvt.s64.s32 	%rd6, %r41;
	mov.u64 	%rd29, %rd26;
	ld.param.v2.u32 	{%r47, %r48}, [%rd29+176];
	ld.param.u32 	%r6, [%rd29+172];
	ld.param.u64 	%rd30, [%rd29];
	cvta.to.global.u64 	%rd7, %rd30;
	ld.param.s32 	%rd8, [%rd29+32];
	ld.param.u64 	%rd31, [%rd29+56];
	cvta.to.global.u64 	%rd9, %rd31;
	ld.param.s32 	%rd10, [%rd29+88];
	mov.u32 	%r49, %nctaid.x;
	cvt.u64.u32 	%rd32, %r49;
	mul.lo.s64 	%rd11, %rd1, %rd32;
	add.u64 	%rd33, %SP, 0;
	add.u64 	%rd12, %SPL, 0;

$L__BB30_2:
	setp.lt.s32 	%p2, %r39, 4;
	mov.u64 	%rd169, %rd168;
	@%p2 bra 	$L__BB30_6;

	or.b64  	%rd34, %rd168, %rd4;
	and.b64  	%rd35, %rd34, -4294967296;
	setp.eq.s64 	%p3, %rd35, 0;
	@%p3 bra 	$L__BB30_5;

	div.u64 	%rd169, %rd168, %rd4;
	bra.uni 	$L__BB30_6;

$L__BB30_5:
	cvt.u32.u64 	%r50, %rd4;
	cvt.u32.u64 	%r51, %rd168;
	div.u32 	%r52, %r51, %r50;
	cvt.u64.u32 	%rd169, %r52;

$L__BB30_6:
	setp.lt.s32 	%p4, %r39, 3;
	@%p4 bra 	$L__BB30_10;

	or.b64  	%rd36, %rd169, %rd5;
	and.b64  	%rd37, %rd36, -4294967296;
	setp.eq.s64 	%p5, %rd37, 0;
	@%p5 bra 	$L__BB30_9;

	div.u64 	%rd169, %rd169, %rd5;
	bra.uni 	$L__BB30_10;

$L__BB30_9:
	cvt.u32.u64 	%r53, %rd5;
	cvt.u32.u64 	%r54, %rd169;
	div.u32 	%r55, %r54, %r53;
	cvt.u64.u32 	%rd169, %r55;

$L__BB30_10:
	setp.lt.s32 	%p6, %r39, 2;
	@%p6 bra 	$L__BB30_14;

	or.b64  	%rd38, %rd169, %rd6;
	and.b64  	%rd39, %rd38, -4294967296;
	setp.eq.s64 	%p7, %rd39, 0;
	@%p7 bra 	$L__BB30_13;

	div.u64 	%rd169, %rd169, %rd6;
	bra.uni 	$L__BB30_14;

$L__BB30_13:
	cvt.u32.u64 	%r56, %rd6;
	cvt.u32.u64 	%r57, %rd169;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd169, %r58;

$L__BB30_14:
	cvt.u32.u64 	%r59, %rd169;
	setp.gt.s32 	%p8, %r39, 0;
	selp.b32 	%r7, %r59, 0, %p8;
	shl.b32 	%r8, %r7, 4;
	shl.b32 	%r9, %r7, 2;
	setp.le.s32 	%p9, %r47, %r9;
	setp.le.s32 	%p10, %r48, %r9;
	setp.le.s32 	%p11, %r6, %r8;
	or.pred  	%p12, %p9, %p10;
	or.b32  	%r60, %r9, %r8;
	setp.lt.s32 	%p13, %r60, 0;
	or.pred  	%p14, %p13, %p12;
	or.pred  	%p15, %p11, %p14;
	@%p15 bra 	$L__BB30_16;
	bra.uni 	$L__BB30_15;

$L__BB30_16:
	st.local.v2.u32 	[%rd12], {%r9, %r9};
	st.local.v2.u32 	[%rd12+8], {%r8, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd45, $str;
	cvta.global.u64 	%rd46, %rd45;
	{ // callseq 7, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r61, [retval0+0];
	} // callseq 7
	bra.uni 	$L__BB30_17;

$L__BB30_15:
	cvt.s64.s32 	%rd40, %r8;
	mul.lo.s64 	%rd41, %rd8, %rd40;
	add.s64 	%rd42, %rd7, %rd41;
	st.global.u32 	[%rd42], %r9;
	mul.lo.s64 	%rd43, %rd10, %rd40;
	add.s64 	%rd44, %rd9, %rd43;
	st.global.u32 	[%rd44], %r9;

$L__BB30_17:
	mul.lo.s32 	%r10, %r7, -12;
	add.s32 	%r11, %r8, 1;
	add.s32 	%r12, %r11, %r10;
	setp.le.s32 	%p17, %r48, %r12;
	setp.le.s32 	%p18, %r6, %r11;
	or.pred  	%p19, %p9, %p17;
	or.b32  	%r13, %r12, %r9;
	or.b32  	%r62, %r13, %r11;
	setp.lt.s32 	%p20, %r62, 0;
	or.pred  	%p21, %p20, %p19;
	or.pred  	%p22, %p18, %p21;
	@%p22 bra 	$L__BB30_19;
	bra.uni 	$L__BB30_18;

$L__BB30_19:
	st.local.v2.u32 	[%rd12], {%r9, %r12};
	add.s32 	%r92, %r8, 1;
	st.local.v2.u32 	[%rd12+8], {%r92, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd53, $str;
	cvta.global.u64 	%rd54, %rd53;
	{ // callseq 8, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd54;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r63, [retval0+0];
	} // callseq 8
	bra.uni 	$L__BB30_20;

$L__BB30_18:
	cvt.s64.s32 	%rd48, %r11;
	mul.lo.s64 	%rd49, %rd8, %rd48;
	add.s64 	%rd50, %rd7, %rd49;
	st.global.u32 	[%rd50], %r9;
	mul.lo.s64 	%rd51, %rd10, %rd48;
	add.s64 	%rd52, %rd9, %rd51;
	st.global.u32 	[%rd52], %r12;

$L__BB30_20:
	add.s32 	%r14, %r8, 2;
	add.s32 	%r15, %r14, %r10;
	setp.le.s32 	%p24, %r48, %r15;
	setp.le.s32 	%p25, %r6, %r14;
	or.pred  	%p26, %p9, %p24;
	or.b32  	%r16, %r15, %r9;
	or.b32  	%r64, %r16, %r14;
	setp.lt.s32 	%p27, %r64, 0;
	or.pred  	%p28, %p27, %p26;
	or.pred  	%p29, %p25, %p28;
	@%p29 bra 	$L__BB30_22;
	bra.uni 	$L__BB30_21;

$L__BB30_22:
	st.local.v2.u32 	[%rd12], {%r9, %r15};
	add.s32 	%r93, %r8, 2;
	st.local.v2.u32 	[%rd12+8], {%r93, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd61, $str;
	cvta.global.u64 	%rd62, %rd61;
	{ // callseq 9, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd62;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r65, [retval0+0];
	} // callseq 9
	bra.uni 	$L__BB30_23;

$L__BB30_21:
	cvt.s64.s32 	%rd56, %r14;
	mul.lo.s64 	%rd57, %rd8, %rd56;
	add.s64 	%rd58, %rd7, %rd57;
	st.global.u32 	[%rd58], %r9;
	mul.lo.s64 	%rd59, %rd10, %rd56;
	add.s64 	%rd60, %rd9, %rd59;
	st.global.u32 	[%rd60], %r15;

$L__BB30_23:
	add.s32 	%r17, %r8, 3;
	add.s32 	%r18, %r17, %r10;
	setp.le.s32 	%p31, %r48, %r18;
	setp.le.s32 	%p32, %r6, %r17;
	or.pred  	%p33, %p9, %p31;
	or.b32  	%r19, %r18, %r9;
	or.b32  	%r66, %r19, %r17;
	setp.lt.s32 	%p34, %r66, 0;
	or.pred  	%p35, %p34, %p33;
	or.pred  	%p36, %p32, %p35;
	@%p36 bra 	$L__BB30_25;
	bra.uni 	$L__BB30_24;

$L__BB30_25:
	st.local.v2.u32 	[%rd12], {%r9, %r18};
	add.s32 	%r94, %r8, 3;
	st.local.v2.u32 	[%rd12+8], {%r94, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd69, $str;
	cvta.global.u64 	%rd70, %rd69;
	{ // callseq 10, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd70;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r67, [retval0+0];
	} // callseq 10
	bra.uni 	$L__BB30_26;

$L__BB30_24:
	cvt.s64.s32 	%rd64, %r17;
	mul.lo.s64 	%rd65, %rd8, %rd64;
	add.s64 	%rd66, %rd7, %rd65;
	st.global.u32 	[%rd66], %r9;
	mul.lo.s64 	%rd67, %rd10, %rd64;
	add.s64 	%rd68, %rd9, %rd67;
	st.global.u32 	[%rd68], %r18;

$L__BB30_26:
	add.s32 	%r20, %r8, 4;
	setp.le.s32 	%p38, %r6, %r20;
	setp.le.s32 	%p39, %r47, %r12;
	or.pred  	%p40, %p39, %p10;
	or.b32  	%r68, %r13, %r20;
	setp.lt.s32 	%p41, %r68, 0;
	or.pred  	%p42, %p41, %p40;
	or.pred  	%p43, %p38, %p42;
	@%p43 bra 	$L__BB30_28;
	bra.uni 	$L__BB30_27;

$L__BB30_28:
	st.local.v2.u32 	[%rd12], {%r12, %r9};
	add.s32 	%r95, %r8, 4;
	st.local.v2.u32 	[%rd12+8], {%r95, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd77, $str;
	cvta.global.u64 	%rd78, %rd77;
	{ // callseq 11, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r69, [retval0+0];
	} // callseq 11
	bra.uni 	$L__BB30_29;

$L__BB30_27:
	cvt.s64.s32 	%rd72, %r20;
	mul.lo.s64 	%rd73, %rd8, %rd72;
	add.s64 	%rd74, %rd7, %rd73;
	st.global.u32 	[%rd74], %r12;
	mul.lo.s64 	%rd75, %rd10, %rd72;
	add.s64 	%rd76, %rd9, %rd75;
	st.global.u32 	[%rd76], %r9;

$L__BB30_29:
	add.s32 	%r21, %r8, 5;
	setp.le.s32 	%p45, %r6, %r21;
	or.pred  	%p47, %p39, %p17;
	or.b32  	%r70, %r21, %r12;
	setp.lt.s32 	%p48, %r70, 0;
	or.pred  	%p49, %p48, %p47;
	or.pred  	%p50, %p45, %p49;
	@%p50 bra 	$L__BB30_31;
	bra.uni 	$L__BB30_30;

$L__BB30_31:
	st.local.v2.u32 	[%rd12], {%r12, %r12};
	add.s32 	%r96, %r8, 5;
	st.local.v2.u32 	[%rd12+8], {%r96, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd85, $str;
	cvta.global.u64 	%rd86, %rd85;
	{ // callseq 12, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd86;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r71, [retval0+0];
	} // callseq 12
	bra.uni 	$L__BB30_32;

$L__BB30_30:
	cvt.s64.s32 	%rd80, %r21;
	mul.lo.s64 	%rd81, %rd8, %rd80;
	add.s64 	%rd82, %rd7, %rd81;
	st.global.u32 	[%rd82], %r12;
	mul.lo.s64 	%rd83, %rd10, %rd80;
	add.s64 	%rd84, %rd9, %rd83;
	st.global.u32 	[%rd84], %r12;

$L__BB30_32:
	add.s32 	%r22, %r8, 6;
	setp.le.s32 	%p52, %r6, %r22;
	or.pred  	%p54, %p39, %p24;
	or.b32  	%r23, %r12, %r15;
	or.b32  	%r72, %r23, %r22;
	setp.lt.s32 	%p55, %r72, 0;
	or.pred  	%p56, %p55, %p54;
	or.pred  	%p57, %p52, %p56;
	@%p57 bra 	$L__BB30_34;
	bra.uni 	$L__BB30_33;

$L__BB30_34:
	st.local.v2.u32 	[%rd12], {%r12, %r15};
	add.s32 	%r97, %r8, 6;
	st.local.v2.u32 	[%rd12+8], {%r97, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd93, $str;
	cvta.global.u64 	%rd94, %rd93;
	{ // callseq 13, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd94;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r73, [retval0+0];
	} // callseq 13
	bra.uni 	$L__BB30_35;

$L__BB30_33:
	cvt.s64.s32 	%rd88, %r22;
	mul.lo.s64 	%rd89, %rd8, %rd88;
	add.s64 	%rd90, %rd7, %rd89;
	st.global.u32 	[%rd90], %r12;
	mul.lo.s64 	%rd91, %rd10, %rd88;
	add.s64 	%rd92, %rd9, %rd91;
	st.global.u32 	[%rd92], %r15;

$L__BB30_35:
	add.s32 	%r24, %r8, 7;
	setp.le.s32 	%p59, %r6, %r24;
	or.pred  	%p61, %p39, %p31;
	or.b32  	%r25, %r12, %r18;
	or.b32  	%r74, %r25, %r24;
	setp.lt.s32 	%p62, %r74, 0;
	or.pred  	%p63, %p62, %p61;
	or.pred  	%p64, %p59, %p63;
	@%p64 bra 	$L__BB30_37;
	bra.uni 	$L__BB30_36;

$L__BB30_37:
	st.local.v2.u32 	[%rd12], {%r12, %r18};
	add.s32 	%r98, %r8, 7;
	st.local.v2.u32 	[%rd12+8], {%r98, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd101, $str;
	cvta.global.u64 	%rd102, %rd101;
	{ // callseq 14, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd102;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r75, [retval0+0];
	} // callseq 14
	bra.uni 	$L__BB30_38;

$L__BB30_36:
	cvt.s64.s32 	%rd96, %r24;
	mul.lo.s64 	%rd97, %rd8, %rd96;
	add.s64 	%rd98, %rd7, %rd97;
	st.global.u32 	[%rd98], %r12;
	mul.lo.s64 	%rd99, %rd10, %rd96;
	add.s64 	%rd100, %rd9, %rd99;
	st.global.u32 	[%rd100], %r18;

$L__BB30_38:
	add.s32 	%r26, %r8, 8;
	setp.le.s32 	%p66, %r6, %r26;
	setp.le.s32 	%p67, %r47, %r15;
	or.pred  	%p68, %p67, %p10;
	or.b32  	%r76, %r16, %r26;
	setp.lt.s32 	%p69, %r76, 0;
	or.pred  	%p70, %p69, %p68;
	or.pred  	%p71, %p66, %p70;
	@%p71 bra 	$L__BB30_40;
	bra.uni 	$L__BB30_39;

$L__BB30_40:
	st.local.v2.u32 	[%rd12], {%r15, %r9};
	add.s32 	%r99, %r8, 8;
	st.local.v2.u32 	[%rd12+8], {%r99, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd109, $str;
	cvta.global.u64 	%rd110, %rd109;
	{ // callseq 15, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd110;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r77, [retval0+0];
	} // callseq 15
	bra.uni 	$L__BB30_41;

$L__BB30_39:
	cvt.s64.s32 	%rd104, %r26;
	mul.lo.s64 	%rd105, %rd8, %rd104;
	add.s64 	%rd106, %rd7, %rd105;
	st.global.u32 	[%rd106], %r15;
	mul.lo.s64 	%rd107, %rd10, %rd104;
	add.s64 	%rd108, %rd9, %rd107;
	st.global.u32 	[%rd108], %r9;

$L__BB30_41:
	add.s32 	%r27, %r8, 9;
	setp.le.s32 	%p73, %r6, %r27;
	or.pred  	%p75, %p67, %p17;
	or.b32  	%r78, %r23, %r27;
	setp.lt.s32 	%p76, %r78, 0;
	or.pred  	%p77, %p76, %p75;
	or.pred  	%p78, %p73, %p77;
	@%p78 bra 	$L__BB30_43;
	bra.uni 	$L__BB30_42;

$L__BB30_43:
	st.local.v2.u32 	[%rd12], {%r15, %r12};
	add.s32 	%r100, %r8, 9;
	st.local.v2.u32 	[%rd12+8], {%r100, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd117, $str;
	cvta.global.u64 	%rd118, %rd117;
	{ // callseq 16, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd118;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r79, [retval0+0];
	} // callseq 16
	bra.uni 	$L__BB30_44;

$L__BB30_42:
	cvt.s64.s32 	%rd112, %r27;
	mul.lo.s64 	%rd113, %rd8, %rd112;
	add.s64 	%rd114, %rd7, %rd113;
	st.global.u32 	[%rd114], %r15;
	mul.lo.s64 	%rd115, %rd10, %rd112;
	add.s64 	%rd116, %rd9, %rd115;
	st.global.u32 	[%rd116], %r12;

$L__BB30_44:
	add.s32 	%r28, %r8, 10;
	setp.le.s32 	%p80, %r6, %r28;
	or.pred  	%p82, %p67, %p24;
	or.b32  	%r80, %r28, %r15;
	setp.lt.s32 	%p83, %r80, 0;
	or.pred  	%p84, %p83, %p82;
	or.pred  	%p85, %p80, %p84;
	@%p85 bra 	$L__BB30_46;
	bra.uni 	$L__BB30_45;

$L__BB30_46:
	st.local.v2.u32 	[%rd12], {%r15, %r15};
	add.s32 	%r101, %r8, 10;
	st.local.v2.u32 	[%rd12+8], {%r101, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd125, $str;
	cvta.global.u64 	%rd126, %rd125;
	{ // callseq 17, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd126;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r81, [retval0+0];
	} // callseq 17
	bra.uni 	$L__BB30_47;

$L__BB30_45:
	cvt.s64.s32 	%rd120, %r28;
	mul.lo.s64 	%rd121, %rd8, %rd120;
	add.s64 	%rd122, %rd7, %rd121;
	st.global.u32 	[%rd122], %r15;
	mul.lo.s64 	%rd123, %rd10, %rd120;
	add.s64 	%rd124, %rd9, %rd123;
	st.global.u32 	[%rd124], %r15;

$L__BB30_47:
	add.s32 	%r29, %r8, 11;
	setp.le.s32 	%p87, %r6, %r29;
	or.pred  	%p89, %p67, %p31;
	or.b32  	%r30, %r15, %r18;
	or.b32  	%r82, %r30, %r29;
	setp.lt.s32 	%p90, %r82, 0;
	or.pred  	%p91, %p90, %p89;
	or.pred  	%p92, %p87, %p91;
	@%p92 bra 	$L__BB30_49;
	bra.uni 	$L__BB30_48;

$L__BB30_49:
	st.local.v2.u32 	[%rd12], {%r15, %r18};
	add.s32 	%r102, %r8, 11;
	st.local.v2.u32 	[%rd12+8], {%r102, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd133, $str;
	cvta.global.u64 	%rd134, %rd133;
	{ // callseq 18, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd134;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r83, [retval0+0];
	} // callseq 18
	bra.uni 	$L__BB30_50;

$L__BB30_48:
	cvt.s64.s32 	%rd128, %r29;
	mul.lo.s64 	%rd129, %rd8, %rd128;
	add.s64 	%rd130, %rd7, %rd129;
	st.global.u32 	[%rd130], %r15;
	mul.lo.s64 	%rd131, %rd10, %rd128;
	add.s64 	%rd132, %rd9, %rd131;
	st.global.u32 	[%rd132], %r18;

$L__BB30_50:
	add.s32 	%r31, %r8, 12;
	setp.le.s32 	%p94, %r6, %r31;
	setp.le.s32 	%p95, %r47, %r18;
	or.pred  	%p96, %p95, %p10;
	or.b32  	%r84, %r19, %r31;
	setp.lt.s32 	%p97, %r84, 0;
	or.pred  	%p98, %p97, %p96;
	or.pred  	%p99, %p94, %p98;
	@%p99 bra 	$L__BB30_52;
	bra.uni 	$L__BB30_51;

$L__BB30_52:
	st.local.v2.u32 	[%rd12], {%r18, %r9};
	add.s32 	%r103, %r8, 12;
	st.local.v2.u32 	[%rd12+8], {%r103, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd141, $str;
	cvta.global.u64 	%rd142, %rd141;
	{ // callseq 19, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r85, [retval0+0];
	} // callseq 19
	bra.uni 	$L__BB30_53;

$L__BB30_51:
	cvt.s64.s32 	%rd136, %r31;
	mul.lo.s64 	%rd137, %rd8, %rd136;
	add.s64 	%rd138, %rd7, %rd137;
	st.global.u32 	[%rd138], %r18;
	mul.lo.s64 	%rd139, %rd10, %rd136;
	add.s64 	%rd140, %rd9, %rd139;
	st.global.u32 	[%rd140], %r9;

$L__BB30_53:
	add.s32 	%r32, %r8, 13;
	setp.le.s32 	%p101, %r6, %r32;
	or.pred  	%p103, %p95, %p17;
	or.b32  	%r86, %r25, %r32;
	setp.lt.s32 	%p104, %r86, 0;
	or.pred  	%p105, %p104, %p103;
	or.pred  	%p106, %p101, %p105;
	@%p106 bra 	$L__BB30_55;
	bra.uni 	$L__BB30_54;

$L__BB30_55:
	st.local.v2.u32 	[%rd12], {%r18, %r12};
	add.s32 	%r104, %r8, 13;
	st.local.v2.u32 	[%rd12+8], {%r104, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd149, $str;
	cvta.global.u64 	%rd150, %rd149;
	{ // callseq 20, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd150;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r87, [retval0+0];
	} // callseq 20
	bra.uni 	$L__BB30_56;

$L__BB30_54:
	cvt.s64.s32 	%rd144, %r32;
	mul.lo.s64 	%rd145, %rd8, %rd144;
	add.s64 	%rd146, %rd7, %rd145;
	st.global.u32 	[%rd146], %r18;
	mul.lo.s64 	%rd147, %rd10, %rd144;
	add.s64 	%rd148, %rd9, %rd147;
	st.global.u32 	[%rd148], %r12;

$L__BB30_56:
	add.s32 	%r33, %r8, 14;
	setp.le.s32 	%p108, %r6, %r33;
	or.pred  	%p110, %p95, %p24;
	or.b32  	%r88, %r30, %r33;
	setp.lt.s32 	%p111, %r88, 0;
	or.pred  	%p112, %p111, %p110;
	or.pred  	%p113, %p108, %p112;
	@%p113 bra 	$L__BB30_58;
	bra.uni 	$L__BB30_57;

$L__BB30_58:
	st.local.v2.u32 	[%rd12], {%r18, %r15};
	add.s32 	%r105, %r8, 14;
	st.local.v2.u32 	[%rd12+8], {%r105, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd157, $str;
	cvta.global.u64 	%rd158, %rd157;
	{ // callseq 21, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd158;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r89, [retval0+0];
	} // callseq 21
	bra.uni 	$L__BB30_59;

$L__BB30_57:
	cvt.s64.s32 	%rd152, %r33;
	mul.lo.s64 	%rd153, %rd8, %rd152;
	add.s64 	%rd154, %rd7, %rd153;
	st.global.u32 	[%rd154], %r18;
	mul.lo.s64 	%rd155, %rd10, %rd152;
	add.s64 	%rd156, %rd9, %rd155;
	st.global.u32 	[%rd156], %r15;

$L__BB30_59:
	add.s32 	%r34, %r8, 15;
	setp.le.s32 	%p115, %r6, %r34;
	or.pred  	%p117, %p95, %p31;
	or.b32  	%r90, %r34, %r18;
	setp.lt.s32 	%p118, %r90, 0;
	or.pred  	%p119, %p118, %p117;
	or.pred  	%p120, %p115, %p119;
	@%p120 bra 	$L__BB30_61;
	bra.uni 	$L__BB30_60;

$L__BB30_61:
	st.local.v2.u32 	[%rd12], {%r18, %r18};
	add.s32 	%r106, %r8, 15;
	st.local.v2.u32 	[%rd12+8], {%r106, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd165, $str;
	cvta.global.u64 	%rd166, %rd165;
	{ // callseq 22, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd166;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r91, [retval0+0];
	} // callseq 22
	bra.uni 	$L__BB30_62;

$L__BB30_60:
	cvt.s64.s32 	%rd160, %r34;
	mul.lo.s64 	%rd161, %rd8, %rd160;
	add.s64 	%rd162, %rd7, %rd161;
	st.global.u32 	[%rd162], %r18;
	mul.lo.s64 	%rd163, %rd10, %rd160;
	add.s64 	%rd164, %rd9, %rd163;
	st.global.u32 	[%rd164], %r18;

$L__BB30_62:
	add.s64 	%rd168, %rd168, %rd11;
	setp.lt.u64 	%p121, %rd168, %rd25;
	@%p121 bra 	$L__BB30_2;

$L__BB30_63:
	ret;

}
	// .globl	init_affine_diag_hess_inds_kernel_cuda_kernel_backward
.visible .entry init_affine_diag_hess_inds_kernel_cuda_kernel_backward(
	.param .align 8 .b8 init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_1[184],
	.param .align 8 .b8 init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_2[184]
)
{
	.local .align 8 .b8 	__local_depot31[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<138>;
	.reg .b32 	%r<138>;
	.reg .b64 	%rd<220>;


	mov.u64 	%SPL, __local_depot31;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r40, %r41}, [init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r42, %r43}, [init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_0+8];
	mov.b64 	%rd26, init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_1;
	ld.param.u64 	%rd25, [init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r39, [init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_0+16];
	mov.u32 	%r44, %ntid.x;
	cvt.u64.u32 	%rd1, %r44;
	mov.u32 	%r45, %ctaid.x;
	mul.wide.u32 	%rd27, %r44, %r45;
	mov.u32 	%r46, %tid.x;
	cvt.u64.u32 	%rd28, %r46;
	add.s64 	%rd216, %rd27, %rd28;
	setp.ge.u64 	%p17, %rd216, %rd25;
	@%p17 bra 	$L__BB31_95;

	cvt.s64.s32 	%rd4, %r43;
	cvt.s64.s32 	%rd5, %r42;
	cvt.s64.s32 	%rd6, %r41;
	mov.u64 	%rd29, %rd26;
	ld.param.v2.u32 	{%r47, %r48}, [%rd29+176];
	ld.param.u32 	%r6, [%rd29+172];
	ld.param.u64 	%rd30, [%rd29];
	cvta.to.global.u64 	%rd7, %rd30;
	ld.param.s32 	%rd8, [%rd29+32];
	ld.param.u64 	%rd31, [%rd29+56];
	cvta.to.global.u64 	%rd9, %rd31;
	ld.param.s32 	%rd10, [%rd29+88];
	mov.u32 	%r49, %nctaid.x;
	cvt.u64.u32 	%rd32, %r49;
	mul.lo.s64 	%rd11, %rd1, %rd32;
	add.u64 	%rd33, %SP, 0;
	add.u64 	%rd12, %SPL, 0;

$L__BB31_2:
	setp.lt.s32 	%p18, %r39, 4;
	mov.u64 	%rd217, %rd216;
	@%p18 bra 	$L__BB31_6;

	or.b64  	%rd34, %rd216, %rd4;
	and.b64  	%rd35, %rd34, -4294967296;
	setp.eq.s64 	%p19, %rd35, 0;
	@%p19 bra 	$L__BB31_5;

	div.u64 	%rd217, %rd216, %rd4;
	bra.uni 	$L__BB31_6;

$L__BB31_5:
	cvt.u32.u64 	%r50, %rd4;
	cvt.u32.u64 	%r51, %rd216;
	div.u32 	%r52, %r51, %r50;
	cvt.u64.u32 	%rd217, %r52;

$L__BB31_6:
	setp.lt.s32 	%p20, %r39, 3;
	@%p20 bra 	$L__BB31_10;

	or.b64  	%rd36, %rd217, %rd5;
	and.b64  	%rd37, %rd36, -4294967296;
	setp.eq.s64 	%p21, %rd37, 0;
	@%p21 bra 	$L__BB31_9;

	div.u64 	%rd217, %rd217, %rd5;
	bra.uni 	$L__BB31_10;

$L__BB31_9:
	cvt.u32.u64 	%r53, %rd5;
	cvt.u32.u64 	%r54, %rd217;
	div.u32 	%r55, %r54, %r53;
	cvt.u64.u32 	%rd217, %r55;

$L__BB31_10:
	setp.lt.s32 	%p22, %r39, 2;
	@%p22 bra 	$L__BB31_14;

	or.b64  	%rd38, %rd217, %rd6;
	and.b64  	%rd39, %rd38, -4294967296;
	setp.eq.s64 	%p23, %rd39, 0;
	@%p23 bra 	$L__BB31_13;

	div.u64 	%rd217, %rd217, %rd6;
	bra.uni 	$L__BB31_14;

$L__BB31_13:
	cvt.u32.u64 	%r56, %rd6;
	cvt.u32.u64 	%r57, %rd217;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd217, %r58;

$L__BB31_14:
	cvt.u32.u64 	%r59, %rd217;
	setp.gt.s32 	%p24, %r39, 0;
	selp.b32 	%r7, %r59, 0, %p24;
	shl.b32 	%r8, %r7, 4;
	shl.b32 	%r9, %r7, 2;
	setp.le.s32 	%p25, %r47, %r9;
	setp.le.s32 	%p26, %r48, %r9;
	setp.le.s32 	%p27, %r6, %r8;
	or.pred  	%p28, %p25, %p26;
	or.b32  	%r60, %r9, %r8;
	setp.lt.s32 	%p29, %r60, 0;
	or.pred  	%p30, %p29, %p28;
	or.pred  	%p1, %p27, %p30;
	@%p1 bra 	$L__BB31_16;
	bra.uni 	$L__BB31_15;

$L__BB31_16:
	st.local.v2.u32 	[%rd12], {%r9, %r9};
	st.local.v2.u32 	[%rd12+8], {%r8, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd45, $str;
	cvta.global.u64 	%rd46, %rd45;
	{ // callseq 23, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r61, [retval0+0];
	} // callseq 23
	bra.uni 	$L__BB31_17;

$L__BB31_15:
	cvt.s64.s32 	%rd40, %r8;
	mul.lo.s64 	%rd41, %rd8, %rd40;
	add.s64 	%rd42, %rd7, %rd41;
	st.global.u32 	[%rd42], %r9;
	mul.lo.s64 	%rd43, %rd10, %rd40;
	add.s64 	%rd44, %rd9, %rd43;
	st.global.u32 	[%rd44], %r9;

$L__BB31_17:
	mul.lo.s32 	%r10, %r7, -12;
	add.s32 	%r11, %r8, 1;
	add.s32 	%r12, %r11, %r10;
	setp.le.s32 	%p32, %r48, %r12;
	setp.le.s32 	%p33, %r6, %r11;
	or.pred  	%p34, %p25, %p32;
	or.b32  	%r13, %r12, %r9;
	or.b32  	%r62, %r13, %r11;
	setp.lt.s32 	%p35, %r62, 0;
	or.pred  	%p36, %p35, %p34;
	or.pred  	%p2, %p33, %p36;
	@%p2 bra 	$L__BB31_19;
	bra.uni 	$L__BB31_18;

$L__BB31_19:
	st.local.v2.u32 	[%rd12], {%r9, %r12};
	add.s32 	%r108, %r8, 1;
	st.local.v2.u32 	[%rd12+8], {%r108, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd53, $str;
	cvta.global.u64 	%rd54, %rd53;
	{ // callseq 24, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd54;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r63, [retval0+0];
	} // callseq 24
	bra.uni 	$L__BB31_20;

$L__BB31_18:
	cvt.s64.s32 	%rd48, %r11;
	mul.lo.s64 	%rd49, %rd8, %rd48;
	add.s64 	%rd50, %rd7, %rd49;
	st.global.u32 	[%rd50], %r9;
	mul.lo.s64 	%rd51, %rd10, %rd48;
	add.s64 	%rd52, %rd9, %rd51;
	st.global.u32 	[%rd52], %r12;

$L__BB31_20:
	add.s32 	%r14, %r8, 2;
	add.s32 	%r15, %r14, %r10;
	setp.le.s32 	%p38, %r48, %r15;
	setp.le.s32 	%p39, %r6, %r14;
	or.pred  	%p40, %p25, %p38;
	or.b32  	%r16, %r15, %r9;
	or.b32  	%r64, %r16, %r14;
	setp.lt.s32 	%p41, %r64, 0;
	or.pred  	%p42, %p41, %p40;
	or.pred  	%p3, %p39, %p42;
	@%p3 bra 	$L__BB31_22;
	bra.uni 	$L__BB31_21;

$L__BB31_22:
	st.local.v2.u32 	[%rd12], {%r9, %r15};
	add.s32 	%r109, %r8, 2;
	st.local.v2.u32 	[%rd12+8], {%r109, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd61, $str;
	cvta.global.u64 	%rd62, %rd61;
	{ // callseq 25, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd62;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r65, [retval0+0];
	} // callseq 25
	bra.uni 	$L__BB31_23;

$L__BB31_21:
	cvt.s64.s32 	%rd56, %r14;
	mul.lo.s64 	%rd57, %rd8, %rd56;
	add.s64 	%rd58, %rd7, %rd57;
	st.global.u32 	[%rd58], %r9;
	mul.lo.s64 	%rd59, %rd10, %rd56;
	add.s64 	%rd60, %rd9, %rd59;
	st.global.u32 	[%rd60], %r15;

$L__BB31_23:
	add.s32 	%r17, %r8, 3;
	add.s32 	%r18, %r17, %r10;
	setp.le.s32 	%p44, %r48, %r18;
	setp.le.s32 	%p45, %r6, %r17;
	or.pred  	%p46, %p25, %p44;
	or.b32  	%r19, %r18, %r9;
	or.b32  	%r66, %r19, %r17;
	setp.lt.s32 	%p47, %r66, 0;
	or.pred  	%p48, %p47, %p46;
	or.pred  	%p4, %p45, %p48;
	@%p4 bra 	$L__BB31_25;
	bra.uni 	$L__BB31_24;

$L__BB31_25:
	st.local.v2.u32 	[%rd12], {%r9, %r18};
	add.s32 	%r110, %r8, 3;
	st.local.v2.u32 	[%rd12+8], {%r110, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd69, $str;
	cvta.global.u64 	%rd70, %rd69;
	{ // callseq 26, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd70;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r67, [retval0+0];
	} // callseq 26
	bra.uni 	$L__BB31_26;

$L__BB31_24:
	cvt.s64.s32 	%rd64, %r17;
	mul.lo.s64 	%rd65, %rd8, %rd64;
	add.s64 	%rd66, %rd7, %rd65;
	st.global.u32 	[%rd66], %r9;
	mul.lo.s64 	%rd67, %rd10, %rd64;
	add.s64 	%rd68, %rd9, %rd67;
	st.global.u32 	[%rd68], %r18;

$L__BB31_26:
	add.s32 	%r20, %r8, 4;
	setp.le.s32 	%p50, %r6, %r20;
	setp.le.s32 	%p51, %r47, %r12;
	or.pred  	%p52, %p51, %p26;
	or.b32  	%r68, %r13, %r20;
	setp.lt.s32 	%p53, %r68, 0;
	or.pred  	%p54, %p53, %p52;
	or.pred  	%p5, %p50, %p54;
	@%p5 bra 	$L__BB31_28;
	bra.uni 	$L__BB31_27;

$L__BB31_28:
	st.local.v2.u32 	[%rd12], {%r12, %r9};
	add.s32 	%r111, %r8, 4;
	st.local.v2.u32 	[%rd12+8], {%r111, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd77, $str;
	cvta.global.u64 	%rd78, %rd77;
	{ // callseq 27, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r69, [retval0+0];
	} // callseq 27
	bra.uni 	$L__BB31_29;

$L__BB31_27:
	cvt.s64.s32 	%rd72, %r20;
	mul.lo.s64 	%rd73, %rd8, %rd72;
	add.s64 	%rd74, %rd7, %rd73;
	st.global.u32 	[%rd74], %r12;
	mul.lo.s64 	%rd75, %rd10, %rd72;
	add.s64 	%rd76, %rd9, %rd75;
	st.global.u32 	[%rd76], %r9;

$L__BB31_29:
	add.s32 	%r21, %r8, 5;
	setp.le.s32 	%p56, %r6, %r21;
	or.pred  	%p58, %p51, %p32;
	or.b32  	%r70, %r12, %r21;
	setp.lt.s32 	%p59, %r70, 0;
	or.pred  	%p60, %p59, %p58;
	or.pred  	%p6, %p56, %p60;
	@%p6 bra 	$L__BB31_31;
	bra.uni 	$L__BB31_30;

$L__BB31_31:
	st.local.v2.u32 	[%rd12], {%r12, %r12};
	add.s32 	%r112, %r8, 5;
	st.local.v2.u32 	[%rd12+8], {%r112, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd85, $str;
	cvta.global.u64 	%rd86, %rd85;
	{ // callseq 28, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd86;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r71, [retval0+0];
	} // callseq 28
	bra.uni 	$L__BB31_32;

$L__BB31_30:
	cvt.s64.s32 	%rd80, %r21;
	mul.lo.s64 	%rd81, %rd8, %rd80;
	add.s64 	%rd82, %rd7, %rd81;
	st.global.u32 	[%rd82], %r12;
	mul.lo.s64 	%rd83, %rd10, %rd80;
	add.s64 	%rd84, %rd9, %rd83;
	st.global.u32 	[%rd84], %r12;

$L__BB31_32:
	add.s32 	%r22, %r8, 6;
	setp.le.s32 	%p62, %r6, %r22;
	or.pred  	%p64, %p51, %p38;
	or.b32  	%r23, %r12, %r15;
	or.b32  	%r72, %r23, %r22;
	setp.lt.s32 	%p65, %r72, 0;
	or.pred  	%p66, %p65, %p64;
	or.pred  	%p7, %p62, %p66;
	@%p7 bra 	$L__BB31_34;
	bra.uni 	$L__BB31_33;

$L__BB31_34:
	st.local.v2.u32 	[%rd12], {%r12, %r15};
	add.s32 	%r113, %r8, 6;
	st.local.v2.u32 	[%rd12+8], {%r113, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd93, $str;
	cvta.global.u64 	%rd94, %rd93;
	{ // callseq 29, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd94;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r73, [retval0+0];
	} // callseq 29
	bra.uni 	$L__BB31_35;

$L__BB31_33:
	cvt.s64.s32 	%rd88, %r22;
	mul.lo.s64 	%rd89, %rd8, %rd88;
	add.s64 	%rd90, %rd7, %rd89;
	st.global.u32 	[%rd90], %r12;
	mul.lo.s64 	%rd91, %rd10, %rd88;
	add.s64 	%rd92, %rd9, %rd91;
	st.global.u32 	[%rd92], %r15;

$L__BB31_35:
	add.s32 	%r24, %r8, 7;
	setp.le.s32 	%p68, %r6, %r24;
	or.pred  	%p70, %p51, %p44;
	or.b32  	%r25, %r12, %r18;
	or.b32  	%r74, %r25, %r24;
	setp.lt.s32 	%p71, %r74, 0;
	or.pred  	%p72, %p71, %p70;
	or.pred  	%p8, %p68, %p72;
	@%p8 bra 	$L__BB31_37;
	bra.uni 	$L__BB31_36;

$L__BB31_37:
	st.local.v2.u32 	[%rd12], {%r12, %r18};
	add.s32 	%r114, %r8, 7;
	st.local.v2.u32 	[%rd12+8], {%r114, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd101, $str;
	cvta.global.u64 	%rd102, %rd101;
	{ // callseq 30, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd102;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r75, [retval0+0];
	} // callseq 30
	bra.uni 	$L__BB31_38;

$L__BB31_36:
	cvt.s64.s32 	%rd96, %r24;
	mul.lo.s64 	%rd97, %rd8, %rd96;
	add.s64 	%rd98, %rd7, %rd97;
	st.global.u32 	[%rd98], %r12;
	mul.lo.s64 	%rd99, %rd10, %rd96;
	add.s64 	%rd100, %rd9, %rd99;
	st.global.u32 	[%rd100], %r18;

$L__BB31_38:
	add.s32 	%r26, %r8, 8;
	setp.le.s32 	%p74, %r6, %r26;
	setp.le.s32 	%p75, %r47, %r15;
	or.pred  	%p76, %p75, %p26;
	or.b32  	%r76, %r16, %r26;
	setp.lt.s32 	%p77, %r76, 0;
	or.pred  	%p78, %p77, %p76;
	or.pred  	%p9, %p74, %p78;
	@%p9 bra 	$L__BB31_40;
	bra.uni 	$L__BB31_39;

$L__BB31_40:
	st.local.v2.u32 	[%rd12], {%r15, %r9};
	add.s32 	%r115, %r8, 8;
	st.local.v2.u32 	[%rd12+8], {%r115, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd109, $str;
	cvta.global.u64 	%rd110, %rd109;
	{ // callseq 31, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd110;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r77, [retval0+0];
	} // callseq 31
	bra.uni 	$L__BB31_41;

$L__BB31_39:
	cvt.s64.s32 	%rd104, %r26;
	mul.lo.s64 	%rd105, %rd8, %rd104;
	add.s64 	%rd106, %rd7, %rd105;
	st.global.u32 	[%rd106], %r15;
	mul.lo.s64 	%rd107, %rd10, %rd104;
	add.s64 	%rd108, %rd9, %rd107;
	st.global.u32 	[%rd108], %r9;

$L__BB31_41:
	add.s32 	%r27, %r8, 9;
	setp.le.s32 	%p80, %r6, %r27;
	or.pred  	%p82, %p75, %p32;
	or.b32  	%r78, %r23, %r27;
	setp.lt.s32 	%p83, %r78, 0;
	or.pred  	%p84, %p83, %p82;
	or.pred  	%p10, %p80, %p84;
	@%p10 bra 	$L__BB31_43;
	bra.uni 	$L__BB31_42;

$L__BB31_43:
	st.local.v2.u32 	[%rd12], {%r15, %r12};
	add.s32 	%r116, %r8, 9;
	st.local.v2.u32 	[%rd12+8], {%r116, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd117, $str;
	cvta.global.u64 	%rd118, %rd117;
	{ // callseq 32, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd118;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r79, [retval0+0];
	} // callseq 32
	bra.uni 	$L__BB31_44;

$L__BB31_42:
	cvt.s64.s32 	%rd112, %r27;
	mul.lo.s64 	%rd113, %rd8, %rd112;
	add.s64 	%rd114, %rd7, %rd113;
	st.global.u32 	[%rd114], %r15;
	mul.lo.s64 	%rd115, %rd10, %rd112;
	add.s64 	%rd116, %rd9, %rd115;
	st.global.u32 	[%rd116], %r12;

$L__BB31_44:
	add.s32 	%r28, %r8, 10;
	setp.le.s32 	%p86, %r6, %r28;
	or.pred  	%p88, %p75, %p38;
	or.b32  	%r80, %r15, %r28;
	setp.lt.s32 	%p89, %r80, 0;
	or.pred  	%p90, %p89, %p88;
	or.pred  	%p11, %p86, %p90;
	@%p11 bra 	$L__BB31_46;
	bra.uni 	$L__BB31_45;

$L__BB31_46:
	st.local.v2.u32 	[%rd12], {%r15, %r15};
	add.s32 	%r117, %r8, 10;
	st.local.v2.u32 	[%rd12+8], {%r117, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd125, $str;
	cvta.global.u64 	%rd126, %rd125;
	{ // callseq 33, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd126;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r81, [retval0+0];
	} // callseq 33
	bra.uni 	$L__BB31_47;

$L__BB31_45:
	cvt.s64.s32 	%rd120, %r28;
	mul.lo.s64 	%rd121, %rd8, %rd120;
	add.s64 	%rd122, %rd7, %rd121;
	st.global.u32 	[%rd122], %r15;
	mul.lo.s64 	%rd123, %rd10, %rd120;
	add.s64 	%rd124, %rd9, %rd123;
	st.global.u32 	[%rd124], %r15;

$L__BB31_47:
	add.s32 	%r29, %r8, 11;
	setp.le.s32 	%p92, %r6, %r29;
	or.pred  	%p94, %p75, %p44;
	or.b32  	%r30, %r15, %r18;
	or.b32  	%r82, %r30, %r29;
	setp.lt.s32 	%p95, %r82, 0;
	or.pred  	%p96, %p95, %p94;
	or.pred  	%p12, %p92, %p96;
	@%p12 bra 	$L__BB31_49;
	bra.uni 	$L__BB31_48;

$L__BB31_49:
	st.local.v2.u32 	[%rd12], {%r15, %r18};
	add.s32 	%r118, %r8, 11;
	st.local.v2.u32 	[%rd12+8], {%r118, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd133, $str;
	cvta.global.u64 	%rd134, %rd133;
	{ // callseq 34, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd134;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r83, [retval0+0];
	} // callseq 34
	bra.uni 	$L__BB31_50;

$L__BB31_48:
	cvt.s64.s32 	%rd128, %r29;
	mul.lo.s64 	%rd129, %rd8, %rd128;
	add.s64 	%rd130, %rd7, %rd129;
	st.global.u32 	[%rd130], %r15;
	mul.lo.s64 	%rd131, %rd10, %rd128;
	add.s64 	%rd132, %rd9, %rd131;
	st.global.u32 	[%rd132], %r18;

$L__BB31_50:
	add.s32 	%r31, %r8, 12;
	setp.le.s32 	%p98, %r6, %r31;
	setp.le.s32 	%p99, %r47, %r18;
	or.pred  	%p100, %p99, %p26;
	or.b32  	%r84, %r19, %r31;
	setp.lt.s32 	%p101, %r84, 0;
	or.pred  	%p102, %p101, %p100;
	or.pred  	%p13, %p98, %p102;
	@%p13 bra 	$L__BB31_52;
	bra.uni 	$L__BB31_51;

$L__BB31_52:
	st.local.v2.u32 	[%rd12], {%r18, %r9};
	add.s32 	%r119, %r8, 12;
	st.local.v2.u32 	[%rd12+8], {%r119, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd141, $str;
	cvta.global.u64 	%rd142, %rd141;
	{ // callseq 35, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r85, [retval0+0];
	} // callseq 35
	bra.uni 	$L__BB31_53;

$L__BB31_51:
	cvt.s64.s32 	%rd136, %r31;
	mul.lo.s64 	%rd137, %rd8, %rd136;
	add.s64 	%rd138, %rd7, %rd137;
	st.global.u32 	[%rd138], %r18;
	mul.lo.s64 	%rd139, %rd10, %rd136;
	add.s64 	%rd140, %rd9, %rd139;
	st.global.u32 	[%rd140], %r9;

$L__BB31_53:
	add.s32 	%r32, %r8, 13;
	setp.le.s32 	%p104, %r6, %r32;
	or.pred  	%p106, %p99, %p32;
	or.b32  	%r86, %r25, %r32;
	setp.lt.s32 	%p107, %r86, 0;
	or.pred  	%p108, %p107, %p106;
	or.pred  	%p14, %p104, %p108;
	@%p14 bra 	$L__BB31_55;
	bra.uni 	$L__BB31_54;

$L__BB31_55:
	st.local.v2.u32 	[%rd12], {%r18, %r12};
	add.s32 	%r120, %r8, 13;
	st.local.v2.u32 	[%rd12+8], {%r120, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd149, $str;
	cvta.global.u64 	%rd150, %rd149;
	{ // callseq 36, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd150;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r87, [retval0+0];
	} // callseq 36
	bra.uni 	$L__BB31_56;

$L__BB31_54:
	cvt.s64.s32 	%rd144, %r32;
	mul.lo.s64 	%rd145, %rd8, %rd144;
	add.s64 	%rd146, %rd7, %rd145;
	st.global.u32 	[%rd146], %r18;
	mul.lo.s64 	%rd147, %rd10, %rd144;
	add.s64 	%rd148, %rd9, %rd147;
	st.global.u32 	[%rd148], %r12;

$L__BB31_56:
	add.s32 	%r33, %r8, 14;
	setp.le.s32 	%p110, %r6, %r33;
	or.pred  	%p112, %p99, %p38;
	or.b32  	%r88, %r30, %r33;
	setp.lt.s32 	%p113, %r88, 0;
	or.pred  	%p114, %p113, %p112;
	or.pred  	%p15, %p110, %p114;
	@%p15 bra 	$L__BB31_58;
	bra.uni 	$L__BB31_57;

$L__BB31_58:
	st.local.v2.u32 	[%rd12], {%r18, %r15};
	add.s32 	%r121, %r8, 14;
	st.local.v2.u32 	[%rd12+8], {%r121, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd157, $str;
	cvta.global.u64 	%rd158, %rd157;
	{ // callseq 37, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd158;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r89, [retval0+0];
	} // callseq 37
	bra.uni 	$L__BB31_59;

$L__BB31_57:
	cvt.s64.s32 	%rd152, %r33;
	mul.lo.s64 	%rd153, %rd8, %rd152;
	add.s64 	%rd154, %rd7, %rd153;
	st.global.u32 	[%rd154], %r18;
	mul.lo.s64 	%rd155, %rd10, %rd152;
	add.s64 	%rd156, %rd9, %rd155;
	st.global.u32 	[%rd156], %r15;

$L__BB31_59:
	add.s32 	%r34, %r8, 15;
	setp.le.s32 	%p116, %r6, %r34;
	or.pred  	%p118, %p99, %p44;
	or.b32  	%r90, %r18, %r34;
	setp.lt.s32 	%p119, %r90, 0;
	or.pred  	%p120, %p119, %p118;
	or.pred  	%p16, %p116, %p120;
	@%p16 bra 	$L__BB31_61;
	bra.uni 	$L__BB31_60;

$L__BB31_61:
	st.local.v2.u32 	[%rd12], {%r18, %r18};
	add.s32 	%r122, %r8, 15;
	st.local.v2.u32 	[%rd12+8], {%r122, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd165, $str;
	cvta.global.u64 	%rd166, %rd165;
	{ // callseq 38, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd166;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r91, [retval0+0];
	} // callseq 38
	bra.uni 	$L__BB31_62;

$L__BB31_60:
	cvt.s64.s32 	%rd160, %r34;
	mul.lo.s64 	%rd161, %rd8, %rd160;
	add.s64 	%rd162, %rd7, %rd161;
	st.global.u32 	[%rd162], %r18;
	mul.lo.s64 	%rd163, %rd10, %rd160;
	add.s64 	%rd164, %rd9, %rd163;
	st.global.u32 	[%rd164], %r18;

$L__BB31_62:
	not.pred 	%p121, %p16;
	@%p121 bra 	$L__BB31_64;

	st.local.v2.u32 	[%rd12], {%r18, %r18};
	add.s32 	%r123, %r8, 15;
	st.local.v2.u32 	[%rd12+8], {%r123, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd168, $str;
	cvta.global.u64 	%rd169, %rd168;
	{ // callseq 39, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd169;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r92, [retval0+0];
	} // callseq 39

$L__BB31_64:
	not.pred 	%p122, %p15;
	@%p122 bra 	$L__BB31_66;

	st.local.v2.u32 	[%rd12], {%r18, %r15};
	add.s32 	%r124, %r8, 14;
	st.local.v2.u32 	[%rd12+8], {%r124, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd171, $str;
	cvta.global.u64 	%rd172, %rd171;
	{ // callseq 40, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd172;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r93, [retval0+0];
	} // callseq 40

$L__BB31_66:
	not.pred 	%p123, %p14;
	@%p123 bra 	$L__BB31_68;

	st.local.v2.u32 	[%rd12], {%r18, %r12};
	add.s32 	%r125, %r8, 13;
	st.local.v2.u32 	[%rd12+8], {%r125, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd174, $str;
	cvta.global.u64 	%rd175, %rd174;
	{ // callseq 41, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd175;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r94, [retval0+0];
	} // callseq 41

$L__BB31_68:
	not.pred 	%p124, %p13;
	@%p124 bra 	$L__BB31_70;

	st.local.v2.u32 	[%rd12], {%r18, %r9};
	add.s32 	%r126, %r8, 12;
	st.local.v2.u32 	[%rd12+8], {%r126, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd177, $str;
	cvta.global.u64 	%rd178, %rd177;
	{ // callseq 42, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd178;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r95, [retval0+0];
	} // callseq 42

$L__BB31_70:
	not.pred 	%p125, %p12;
	@%p125 bra 	$L__BB31_72;

	st.local.v2.u32 	[%rd12], {%r15, %r18};
	add.s32 	%r127, %r8, 11;
	st.local.v2.u32 	[%rd12+8], {%r127, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd180, $str;
	cvta.global.u64 	%rd181, %rd180;
	{ // callseq 43, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd181;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r96, [retval0+0];
	} // callseq 43

$L__BB31_72:
	not.pred 	%p126, %p11;
	@%p126 bra 	$L__BB31_74;

	st.local.v2.u32 	[%rd12], {%r15, %r15};
	add.s32 	%r128, %r8, 10;
	st.local.v2.u32 	[%rd12+8], {%r128, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd183, $str;
	cvta.global.u64 	%rd184, %rd183;
	{ // callseq 44, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd184;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r97, [retval0+0];
	} // callseq 44

$L__BB31_74:
	not.pred 	%p127, %p10;
	@%p127 bra 	$L__BB31_76;

	st.local.v2.u32 	[%rd12], {%r15, %r12};
	add.s32 	%r129, %r8, 9;
	st.local.v2.u32 	[%rd12+8], {%r129, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd186, $str;
	cvta.global.u64 	%rd187, %rd186;
	{ // callseq 45, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd187;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r98, [retval0+0];
	} // callseq 45

$L__BB31_76:
	not.pred 	%p128, %p9;
	@%p128 bra 	$L__BB31_78;

	st.local.v2.u32 	[%rd12], {%r15, %r9};
	add.s32 	%r130, %r8, 8;
	st.local.v2.u32 	[%rd12+8], {%r130, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd189, $str;
	cvta.global.u64 	%rd190, %rd189;
	{ // callseq 46, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd190;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r99, [retval0+0];
	} // callseq 46

$L__BB31_78:
	not.pred 	%p129, %p8;
	@%p129 bra 	$L__BB31_80;

	st.local.v2.u32 	[%rd12], {%r12, %r18};
	add.s32 	%r131, %r8, 7;
	st.local.v2.u32 	[%rd12+8], {%r131, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd192, $str;
	cvta.global.u64 	%rd193, %rd192;
	{ // callseq 47, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd193;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r100, [retval0+0];
	} // callseq 47

$L__BB31_80:
	not.pred 	%p130, %p7;
	@%p130 bra 	$L__BB31_82;

	st.local.v2.u32 	[%rd12], {%r12, %r15};
	add.s32 	%r132, %r8, 6;
	st.local.v2.u32 	[%rd12+8], {%r132, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd195, $str;
	cvta.global.u64 	%rd196, %rd195;
	{ // callseq 48, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd196;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r101, [retval0+0];
	} // callseq 48

$L__BB31_82:
	not.pred 	%p131, %p6;
	@%p131 bra 	$L__BB31_84;

	st.local.v2.u32 	[%rd12], {%r12, %r12};
	add.s32 	%r133, %r8, 5;
	st.local.v2.u32 	[%rd12+8], {%r133, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd198, $str;
	cvta.global.u64 	%rd199, %rd198;
	{ // callseq 49, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd199;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r102, [retval0+0];
	} // callseq 49

$L__BB31_84:
	not.pred 	%p132, %p5;
	@%p132 bra 	$L__BB31_86;

	st.local.v2.u32 	[%rd12], {%r12, %r9};
	add.s32 	%r134, %r8, 4;
	st.local.v2.u32 	[%rd12+8], {%r134, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd201, $str;
	cvta.global.u64 	%rd202, %rd201;
	{ // callseq 50, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd202;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r103, [retval0+0];
	} // callseq 50

$L__BB31_86:
	not.pred 	%p133, %p4;
	@%p133 bra 	$L__BB31_88;

	st.local.v2.u32 	[%rd12], {%r9, %r18};
	add.s32 	%r135, %r8, 3;
	st.local.v2.u32 	[%rd12+8], {%r135, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd204, $str;
	cvta.global.u64 	%rd205, %rd204;
	{ // callseq 51, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd205;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r104, [retval0+0];
	} // callseq 51

$L__BB31_88:
	not.pred 	%p134, %p3;
	@%p134 bra 	$L__BB31_90;

	st.local.v2.u32 	[%rd12], {%r9, %r15};
	add.s32 	%r136, %r8, 2;
	st.local.v2.u32 	[%rd12+8], {%r136, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd207, $str;
	cvta.global.u64 	%rd208, %rd207;
	{ // callseq 52, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd208;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r105, [retval0+0];
	} // callseq 52

$L__BB31_90:
	not.pred 	%p135, %p2;
	@%p135 bra 	$L__BB31_92;

	st.local.v2.u32 	[%rd12], {%r9, %r12};
	add.s32 	%r137, %r8, 1;
	st.local.v2.u32 	[%rd12+8], {%r137, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd210, $str;
	cvta.global.u64 	%rd211, %rd210;
	{ // callseq 53, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd211;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r106, [retval0+0];
	} // callseq 53

$L__BB31_92:
	not.pred 	%p136, %p1;
	@%p136 bra 	$L__BB31_94;

	st.local.v2.u32 	[%rd12], {%r9, %r9};
	st.local.v2.u32 	[%rd12+8], {%r8, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd213, $str;
	cvta.global.u64 	%rd214, %rd213;
	{ // callseq 54, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd214;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r107, [retval0+0];
	} // callseq 54

$L__BB31_94:
	add.s64 	%rd216, %rd216, %rd11;
	setp.lt.u64 	%p137, %rd216, %rd25;
	@%p137 bra 	$L__BB31_2;

$L__BB31_95:
	ret;

}
	// .globl	step_affine_y_cuda_kernel_forward
.visible .entry step_affine_y_cuda_kernel_forward(
	.param .align 8 .b8 step_affine_y_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 step_affine_y_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_forward_param_5[56]
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<41>;
	.reg .b32 	%r<115>;
	.reg .f64 	%fd<38>;
	.reg .b64 	%rd<75>;


	ld.param.v2.u32 	{%r52, %r53}, [step_affine_y_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r54, %r55}, [step_affine_y_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r60, %r61}, [step_affine_y_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r68, %r69}, [step_affine_y_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r76, %r77}, [step_affine_y_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r84, %r85}, [step_affine_y_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r92, %r93}, [step_affine_y_cuda_kernel_forward_param_5+32];
	ld.param.u64 	%rd38, [step_affine_y_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd36, [step_affine_y_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd34, [step_affine_y_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd32, [step_affine_y_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd30, [step_affine_y_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd29, [step_affine_y_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [step_affine_y_cuda_kernel_forward_param_0+16];
	mov.u32 	%r96, %ntid.x;
	cvt.u64.u32 	%rd1, %r96;
	mov.u32 	%r97, %ctaid.x;
	mul.wide.u32 	%rd40, %r96, %r97;
	mov.u32 	%r98, %tid.x;
	cvt.u64.u32 	%rd41, %r98;
	add.s64 	%rd71, %rd40, %rd41;
	setp.ge.u64 	%p1, %rd71, %rd29;
	@%p1 bra 	$L__BB32_15;

	cvta.to.global.u64 	%rd4, %rd38;
	cvta.to.global.u64 	%rd5, %rd36;
	cvta.to.global.u64 	%rd6, %rd34;
	cvta.to.global.u64 	%rd7, %rd32;
	cvta.to.global.u64 	%rd8, %rd30;
	cvt.s64.s32 	%rd9, %r55;
	cvt.s64.s32 	%rd10, %r54;
	cvt.s64.s32 	%rd11, %r53;
	cvt.s64.s32 	%rd12, %r68;
	cvt.s64.s32 	%rd13, %r60;
	cvt.s64.s32 	%rd14, %r92;
	cvt.s64.s32 	%rd15, %r76;
	cvt.s64.s32 	%rd16, %r84;
	mov.u32 	%r99, %nctaid.x;
	cvt.u64.u32 	%rd42, %r99;
	mul.lo.s64 	%rd17, %rd1, %rd42;

$L__BB32_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd72, %rd71;
	@%p2 bra 	$L__BB32_6;

	or.b64  	%rd43, %rd71, %rd9;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p3, %rd44, 0;
	@%p3 bra 	$L__BB32_5;

	div.u64 	%rd72, %rd71, %rd9;
	bra.uni 	$L__BB32_6;

$L__BB32_5:
	cvt.u32.u64 	%r100, %rd9;
	cvt.u32.u64 	%r101, %rd71;
	div.u32 	%r102, %r101, %r100;
	cvt.u64.u32 	%rd72, %r102;

$L__BB32_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB32_10;

	or.b64  	%rd45, %rd72, %rd10;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p5, %rd46, 0;
	@%p5 bra 	$L__BB32_9;

	div.u64 	%rd72, %rd72, %rd10;
	bra.uni 	$L__BB32_10;

$L__BB32_9:
	cvt.u32.u64 	%r103, %rd10;
	cvt.u32.u64 	%r104, %rd72;
	div.u32 	%r105, %r104, %r103;
	cvt.u64.u32 	%rd72, %r105;

$L__BB32_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB32_14;

	or.b64  	%rd47, %rd72, %rd11;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p7, %rd48, 0;
	@%p7 bra 	$L__BB32_13;

	div.u64 	%rd72, %rd72, %rd11;
	bra.uni 	$L__BB32_14;

$L__BB32_13:
	cvt.u32.u64 	%r106, %rd11;
	cvt.u32.u64 	%r107, %rd72;
	div.u32 	%r108, %r107, %r106;
	cvt.u64.u32 	%rd72, %r108;

$L__BB32_14:
	cvt.u32.u64 	%r109, %rd72;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b32 	%r110, %r109, 0, %p8;
	shl.b32 	%r111, %r110, 2;
	cvt.s64.s32 	%rd49, %r111;
	mul.lo.s64 	%rd50, %rd49, %rd12;
	add.s64 	%rd51, %rd7, %rd50;
	or.b32  	%r112, %r111, 1;
	cvt.s64.s32 	%rd52, %r112;
	mul.lo.s64 	%rd53, %rd52, %rd12;
	add.s64 	%rd54, %rd7, %rd53;
	or.b32  	%r113, %r111, 2;
	cvt.s64.s32 	%rd55, %r113;
	mul.lo.s64 	%rd56, %rd55, %rd12;
	add.s64 	%rd57, %rd7, %rd56;
	or.b32  	%r114, %r111, 3;
	cvt.s64.s32 	%rd58, %r114;
	mul.lo.s64 	%rd59, %rd58, %rd12;
	add.s64 	%rd60, %rd7, %rd59;
	cvt.s64.s32 	%rd61, %r110;
	mul.lo.s64 	%rd62, %rd61, %rd13;
	mul.lo.s64 	%rd63, %rd61, %rd14;
	add.s64 	%rd64, %rd4, %rd63;
	ld.global.s32 	%rd65, [%rd64];
	mul.lo.s64 	%rd66, %rd65, %rd15;
	add.s64 	%rd67, %rd6, %rd66;
	ld.global.f64 	%fd1, [%rd67];
	ld.global.f64 	%fd2, [%rd51];
	ld.global.f64 	%fd3, [%rd51+8];
	ld.global.f64 	%fd4, [%rd51+16];
	ld.global.f64 	%fd5, [%rd54];
	ld.global.f64 	%fd6, [%rd54+8];
	ld.global.f64 	%fd7, [%rd54+16];
	ld.global.f64 	%fd8, [%rd57];
	ld.global.f64 	%fd9, [%rd57+8];
	ld.global.f64 	%fd10, [%rd57+16];
	ld.global.f64 	%fd11, [%rd60];
	ld.global.f64 	%fd12, [%rd60+8];
	ld.global.f64 	%fd13, [%rd60+16];
	add.s64 	%rd68, %rd8, %rd62;
	ld.global.f64 	%fd14, [%rd68];
	fma.rn.f64 	%fd15, %fd1, %fd2, %fd14;
	ld.global.f64 	%fd16, [%rd68+8];
	fma.rn.f64 	%fd17, %fd1, %fd3, %fd16;
	ld.global.f64 	%fd18, [%rd68+16];
	fma.rn.f64 	%fd19, %fd1, %fd4, %fd18;
	ld.global.f64 	%fd20, [%rd68+24];
	fma.rn.f64 	%fd21, %fd1, %fd5, %fd20;
	ld.global.f64 	%fd22, [%rd68+32];
	fma.rn.f64 	%fd23, %fd1, %fd6, %fd22;
	ld.global.f64 	%fd24, [%rd68+40];
	fma.rn.f64 	%fd25, %fd1, %fd7, %fd24;
	ld.global.f64 	%fd26, [%rd68+48];
	fma.rn.f64 	%fd27, %fd1, %fd8, %fd26;
	ld.global.f64 	%fd28, [%rd68+56];
	fma.rn.f64 	%fd29, %fd1, %fd9, %fd28;
	ld.global.f64 	%fd30, [%rd68+64];
	fma.rn.f64 	%fd31, %fd1, %fd10, %fd30;
	ld.global.f64 	%fd32, [%rd68+72];
	fma.rn.f64 	%fd33, %fd1, %fd11, %fd32;
	ld.global.f64 	%fd34, [%rd68+80];
	fma.rn.f64 	%fd35, %fd1, %fd12, %fd34;
	ld.global.f64 	%fd36, [%rd68+88];
	fma.rn.f64 	%fd37, %fd1, %fd13, %fd36;
	mul.lo.s64 	%rd69, %rd61, %rd16;
	add.s64 	%rd70, %rd5, %rd69;
	st.global.f64 	[%rd70], %fd15;
	st.global.f64 	[%rd70+8], %fd17;
	st.global.f64 	[%rd70+16], %fd19;
	st.global.f64 	[%rd70+24], %fd21;
	st.global.f64 	[%rd70+32], %fd23;
	st.global.f64 	[%rd70+40], %fd25;
	st.global.f64 	[%rd70+48], %fd27;
	st.global.f64 	[%rd70+56], %fd29;
	st.global.f64 	[%rd70+64], %fd31;
	st.global.f64 	[%rd70+72], %fd33;
	st.global.f64 	[%rd70+80], %fd35;
	st.global.f64 	[%rd70+88], %fd37;
	add.s64 	%rd71, %rd71, %rd17;
	setp.lt.u64 	%p9, %rd71, %rd29;
	@%p9 bra 	$L__BB32_2;

$L__BB32_15:
	ret;

}
	// .globl	step_affine_y_cuda_kernel_backward
.visible .entry step_affine_y_cuda_kernel_backward(
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_10[56]
)
{
	.reg .pred 	%p<40>;
	.reg .b16 	%rs<73>;
	.reg .b32 	%r<180>;
	.reg .f64 	%fd<331>;
	.reg .b64 	%rd<222>;


	ld.param.v2.u32 	{%r88, %r89}, [step_affine_y_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r90, %r91}, [step_affine_y_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r96, %r97}, [step_affine_y_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r104, %r105}, [step_affine_y_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r112, %r113}, [step_affine_y_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r120, %r121}, [step_affine_y_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r128, %r129}, [step_affine_y_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r136, %r137}, [step_affine_y_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r144, %r145}, [step_affine_y_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r152, %r153}, [step_affine_y_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r160, %r161}, [step_affine_y_cuda_kernel_backward_param_9+32];
	ld.param.u64 	%rd66, [step_affine_y_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd64, [step_affine_y_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd62, [step_affine_y_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd60, [step_affine_y_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd58, [step_affine_y_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd57, [step_affine_y_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd55, [step_affine_y_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd54, [step_affine_y_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd53, [step_affine_y_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd52, [step_affine_y_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd51, [step_affine_y_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd49, [step_affine_y_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [step_affine_y_cuda_kernel_backward_param_0+16];
	mov.u32 	%r164, %ntid.x;
	cvt.u64.u32 	%rd1, %r164;
	mov.u32 	%r165, %ctaid.x;
	mul.wide.u32 	%rd68, %r164, %r165;
	mov.u32 	%r166, %tid.x;
	cvt.u64.u32 	%rd69, %r166;
	add.s64 	%rd218, %rd68, %rd69;
	setp.ge.u64 	%p1, %rd218, %rd49;
	@%p1 bra 	$L__BB33_75;

	cvta.to.global.u64 	%rd12, %rd66;
	cvta.to.global.u64 	%rd13, %rd58;
	cvta.to.global.u64 	%rd14, %rd57;
	cvta.to.global.u64 	%rd15, %rd54;
	cvta.to.global.u64 	%rd16, %rd52;
	cvt.s64.s32 	%rd17, %r91;
	cvt.s64.s32 	%rd18, %r90;
	cvt.s64.s32 	%rd19, %r89;
	cvt.s64.s32 	%rd20, %r104;
	cvt.s64.s32 	%rd21, %r96;
	cvt.s64.s32 	%rd22, %r128;
	cvt.s64.s32 	%rd23, %r112;
	cvt.s64.s32 	%rd24, %r160;
	cvt.s64.s32 	%rd25, %r120;
	cvt.s64.s32 	%rd26, %r152;
	cvt.s64.s32 	%rd27, %r136;
	cvt.s64.s32 	%rd28, %r144;
	mov.u32 	%r167, %nctaid.x;
	cvt.u64.u32 	%rd70, %r167;
	mul.lo.s64 	%rd29, %rd1, %rd70;

$L__BB33_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd219, %rd218;
	@%p2 bra 	$L__BB33_6;

	or.b64  	%rd71, %rd218, %rd17;
	and.b64  	%rd72, %rd71, -4294967296;
	setp.eq.s64 	%p3, %rd72, 0;
	@%p3 bra 	$L__BB33_5;

	div.u64 	%rd219, %rd218, %rd17;
	bra.uni 	$L__BB33_6;

$L__BB33_5:
	cvt.u32.u64 	%r168, %rd17;
	cvt.u32.u64 	%r169, %rd218;
	div.u32 	%r170, %r169, %r168;
	cvt.u64.u32 	%rd219, %r170;

$L__BB33_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB33_10;

	or.b64  	%rd73, %rd219, %rd18;
	and.b64  	%rd74, %rd73, -4294967296;
	setp.eq.s64 	%p5, %rd74, 0;
	@%p5 bra 	$L__BB33_9;

	div.u64 	%rd219, %rd219, %rd18;
	bra.uni 	$L__BB33_10;

$L__BB33_9:
	cvt.u32.u64 	%r171, %rd18;
	cvt.u32.u64 	%r172, %rd219;
	div.u32 	%r173, %r172, %r171;
	cvt.u64.u32 	%rd219, %r173;

$L__BB33_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB33_14;

	or.b64  	%rd75, %rd219, %rd19;
	and.b64  	%rd76, %rd75, -4294967296;
	setp.eq.s64 	%p7, %rd76, 0;
	@%p7 bra 	$L__BB33_13;

	div.u64 	%rd219, %rd219, %rd19;
	bra.uni 	$L__BB33_14;

$L__BB33_13:
	cvt.u32.u64 	%r174, %rd19;
	cvt.u32.u64 	%r175, %rd219;
	div.u32 	%r176, %r175, %r174;
	cvt.u64.u32 	%rd219, %r176;

$L__BB33_14:
	cvt.s64.s32 	%rd215, %r112;
	ld.param.u64 	%rd214, [step_affine_y_cuda_kernel_backward_param_9];
	cvt.u32.u64 	%r177, %rd219;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b32 	%r178, %r177, 0, %p8;
	shl.b32 	%r179, %r178, 2;
	cvt.s64.s32 	%rd40, %r179;
	mul.lo.s64 	%rd41, %rd40, %rd20;
	add.s64 	%rd77, %rd16, %rd41;
	ld.global.f64 	%fd1, [%rd77];
	ld.global.f64 	%fd2, [%rd77+8];
	ld.global.f64 	%fd3, [%rd77+16];
	or.b64  	%rd78, %rd40, 1;
	mul.lo.s64 	%rd42, %rd78, %rd20;
	add.s64 	%rd79, %rd16, %rd42;
	ld.global.f64 	%fd4, [%rd79];
	ld.global.f64 	%fd5, [%rd79+8];
	ld.global.f64 	%fd6, [%rd79+16];
	or.b64  	%rd80, %rd40, 2;
	mul.lo.s64 	%rd43, %rd80, %rd20;
	add.s64 	%rd81, %rd16, %rd43;
	ld.global.f64 	%fd7, [%rd81];
	ld.global.f64 	%fd8, [%rd81+8];
	ld.global.f64 	%fd9, [%rd81+16];
	or.b64  	%rd82, %rd40, 3;
	mul.lo.s64 	%rd44, %rd82, %rd20;
	add.s64 	%rd83, %rd16, %rd44;
	ld.global.f64 	%fd10, [%rd83];
	ld.global.f64 	%fd11, [%rd83+8];
	ld.global.f64 	%fd12, [%rd83+16];
	cvt.s64.s32 	%rd45, %r178;
	mul.lo.s64 	%rd84, %rd45, %rd22;
	add.s64 	%rd85, %rd13, %rd84;
	ld.global.s32 	%rd46, [%rd85];
	mul.lo.s64 	%rd47, %rd46, %rd215;
	add.s64 	%rd86, %rd15, %rd47;
	ld.global.f64 	%fd13, [%rd86];
	setp.eq.s64 	%p9, %rd214, 0;
	@%p9 bra 	$L__BB33_16;

	mul.lo.s64 	%rd87, %rd45, %rd24;
	add.s64 	%rd88, %rd12, %rd87;
	ld.global.f64 	%fd75, [%rd88];
	add.f64 	%fd330, %fd75, 0d0000000000000000;
	ld.global.f64 	%fd76, [%rd88+8];
	add.f64 	%fd329, %fd76, 0d0000000000000000;
	ld.global.f64 	%fd77, [%rd88+16];
	add.f64 	%fd328, %fd77, 0d0000000000000000;
	ld.global.f64 	%fd78, [%rd88+24];
	add.f64 	%fd327, %fd78, 0d0000000000000000;
	ld.global.f64 	%fd79, [%rd88+32];
	add.f64 	%fd326, %fd79, 0d0000000000000000;
	ld.global.f64 	%fd80, [%rd88+40];
	add.f64 	%fd325, %fd80, 0d0000000000000000;
	ld.global.f64 	%fd81, [%rd88+48];
	add.f64 	%fd324, %fd81, 0d0000000000000000;
	ld.global.f64 	%fd82, [%rd88+56];
	add.f64 	%fd323, %fd82, 0d0000000000000000;
	ld.global.f64 	%fd83, [%rd88+64];
	add.f64 	%fd322, %fd83, 0d0000000000000000;
	ld.global.f64 	%fd84, [%rd88+72];
	add.f64 	%fd321, %fd84, 0d0000000000000000;
	ld.global.f64 	%fd85, [%rd88+80];
	add.f64 	%fd320, %fd85, 0d0000000000000000;
	ld.global.f64 	%fd86, [%rd88+88];
	add.f64 	%fd319, %fd86, 0d0000000000000000;
	bra.uni 	$L__BB33_18;

$L__BB33_16:
	ld.param.u64 	%rd216, [step_affine_y_cuda_kernel_backward_param_4+8];
	setp.eq.s64 	%p10, %rd216, 0;
	mov.f64 	%fd319, 0d0000000000000000;
	mov.f64 	%fd320, %fd319;
	mov.f64 	%fd321, %fd319;
	mov.f64 	%fd322, %fd319;
	mov.f64 	%fd323, %fd319;
	mov.f64 	%fd324, %fd319;
	mov.f64 	%fd325, %fd319;
	mov.f64 	%fd326, %fd319;
	mov.f64 	%fd327, %fd319;
	mov.f64 	%fd328, %fd319;
	mov.f64 	%fd329, %fd319;
	mov.f64 	%fd330, %fd319;
	@%p10 bra 	$L__BB33_18;

	mul.lo.s64 	%rd89, %rd45, %rd25;
	add.s64 	%rd90, %rd14, %rd89;
	ld.global.f64 	%fd99, [%rd90];
	add.f64 	%fd330, %fd99, 0d0000000000000000;
	ld.global.f64 	%fd100, [%rd90+8];
	add.f64 	%fd329, %fd100, 0d0000000000000000;
	ld.global.f64 	%fd101, [%rd90+16];
	add.f64 	%fd328, %fd101, 0d0000000000000000;
	ld.global.f64 	%fd102, [%rd90+24];
	add.f64 	%fd327, %fd102, 0d0000000000000000;
	ld.global.f64 	%fd103, [%rd90+32];
	add.f64 	%fd326, %fd103, 0d0000000000000000;
	ld.global.f64 	%fd104, [%rd90+40];
	add.f64 	%fd325, %fd104, 0d0000000000000000;
	ld.global.f64 	%fd105, [%rd90+48];
	add.f64 	%fd324, %fd105, 0d0000000000000000;
	ld.global.f64 	%fd106, [%rd90+56];
	add.f64 	%fd323, %fd106, 0d0000000000000000;
	ld.global.f64 	%fd107, [%rd90+64];
	add.f64 	%fd322, %fd107, 0d0000000000000000;
	ld.global.f64 	%fd108, [%rd90+72];
	add.f64 	%fd321, %fd108, 0d0000000000000000;
	ld.global.f64 	%fd109, [%rd90+80];
	add.f64 	%fd320, %fd109, 0d0000000000000000;
	ld.global.f64 	%fd110, [%rd90+88];
	add.f64 	%fd319, %fd110, 0d0000000000000000;

$L__BB33_18:
	add.f64 	%fd50, %fd330, 0d0000000000000000;
	fma.rn.f64 	%fd51, %fd13, %fd50, 0d0000000000000000;
	add.f64 	%fd52, %fd329, 0d0000000000000000;
	fma.rn.f64 	%fd53, %fd13, %fd52, 0d0000000000000000;
	add.f64 	%fd54, %fd328, 0d0000000000000000;
	fma.rn.f64 	%fd55, %fd13, %fd54, 0d0000000000000000;
	add.f64 	%fd56, %fd327, 0d0000000000000000;
	fma.rn.f64 	%fd57, %fd13, %fd56, 0d0000000000000000;
	add.f64 	%fd58, %fd326, 0d0000000000000000;
	fma.rn.f64 	%fd59, %fd13, %fd58, 0d0000000000000000;
	add.f64 	%fd60, %fd325, 0d0000000000000000;
	fma.rn.f64 	%fd61, %fd13, %fd60, 0d0000000000000000;
	add.f64 	%fd62, %fd324, 0d0000000000000000;
	fma.rn.f64 	%fd63, %fd13, %fd62, 0d0000000000000000;
	add.f64 	%fd64, %fd323, 0d0000000000000000;
	fma.rn.f64 	%fd65, %fd13, %fd64, 0d0000000000000000;
	add.f64 	%fd66, %fd322, 0d0000000000000000;
	fma.rn.f64 	%fd67, %fd13, %fd66, 0d0000000000000000;
	add.f64 	%fd68, %fd321, 0d0000000000000000;
	fma.rn.f64 	%fd69, %fd13, %fd68, 0d0000000000000000;
	add.f64 	%fd70, %fd320, 0d0000000000000000;
	fma.rn.f64 	%fd71, %fd13, %fd70, 0d0000000000000000;
	add.f64 	%fd72, %fd319, 0d0000000000000000;
	fma.rn.f64 	%fd73, %fd13, %fd72, 0d0000000000000000;
	fma.rn.f64 	%fd111, %fd1, %fd50, 0d0000000000000000;
	fma.rn.f64 	%fd112, %fd2, %fd52, %fd111;
	fma.rn.f64 	%fd113, %fd3, %fd54, %fd112;
	fma.rn.f64 	%fd114, %fd4, %fd56, %fd113;
	fma.rn.f64 	%fd115, %fd5, %fd58, %fd114;
	fma.rn.f64 	%fd116, %fd6, %fd60, %fd115;
	fma.rn.f64 	%fd117, %fd7, %fd62, %fd116;
	fma.rn.f64 	%fd118, %fd8, %fd64, %fd117;
	fma.rn.f64 	%fd119, %fd9, %fd66, %fd118;
	fma.rn.f64 	%fd120, %fd10, %fd68, %fd119;
	fma.rn.f64 	%fd121, %fd11, %fd70, %fd120;
	fma.rn.f64 	%fd122, %fd12, %fd72, %fd121;
	add.f64 	%fd74, %fd122, 0d0000000000000000;
	setp.eq.s64 	%p11, %rd64, 0;
	@%p11 bra 	$L__BB33_20;

	mul.lo.s64 	%rd92, %rd46, %rd26;
	add.s64 	%rd91, %rd64, %rd92;
	// begin inline asm
	{ atom.add.f64 %fd123,[%rd91],%fd74; }

	// end inline asm
	bra.uni 	$L__BB33_22;

$L__BB33_20:
	setp.eq.s64 	%p12, %rd55, 0;
	@%p12 bra 	$L__BB33_22;

	mul.lo.s64 	%rd217, %rd46, %rd215;
	add.s64 	%rd93, %rd55, %rd217;
	// begin inline asm
	{ atom.add.f64 %fd125,[%rd93],%fd74; }

	// end inline asm

$L__BB33_22:
	setp.eq.s64 	%p13, %rd60, 0;
	@%p13 bra 	$L__BB33_24;

	mul.lo.s64 	%rd106, %rd45, %rd27;
	add.s64 	%rd94, %rd60, %rd106;
	// begin inline asm
	{ atom.add.f64 %fd127,[%rd94],%fd50; }

	// end inline asm
	add.s64 	%rd95, %rd94, 8;
	// begin inline asm
	{ atom.add.f64 %fd129,[%rd95],%fd52; }

	// end inline asm
	add.s64 	%rd96, %rd94, 16;
	// begin inline asm
	{ atom.add.f64 %fd131,[%rd96],%fd54; }

	// end inline asm
	add.s64 	%rd97, %rd94, 24;
	// begin inline asm
	{ atom.add.f64 %fd133,[%rd97],%fd56; }

	// end inline asm
	add.s64 	%rd98, %rd94, 32;
	// begin inline asm
	{ atom.add.f64 %fd135,[%rd98],%fd58; }

	// end inline asm
	add.s64 	%rd99, %rd94, 40;
	// begin inline asm
	{ atom.add.f64 %fd137,[%rd99],%fd60; }

	// end inline asm
	add.s64 	%rd100, %rd94, 48;
	// begin inline asm
	{ atom.add.f64 %fd139,[%rd100],%fd62; }

	// end inline asm
	add.s64 	%rd101, %rd94, 56;
	// begin inline asm
	{ atom.add.f64 %fd141,[%rd101],%fd64; }

	// end inline asm
	add.s64 	%rd102, %rd94, 64;
	// begin inline asm
	{ atom.add.f64 %fd143,[%rd102],%fd66; }

	// end inline asm
	add.s64 	%rd103, %rd94, 72;
	// begin inline asm
	{ atom.add.f64 %fd145,[%rd103],%fd68; }

	// end inline asm
	add.s64 	%rd104, %rd94, 80;
	// begin inline asm
	{ atom.add.f64 %fd147,[%rd104],%fd70; }

	// end inline asm
	add.s64 	%rd105, %rd94, 88;
	// begin inline asm
	{ atom.add.f64 %fd149,[%rd105],%fd72; }

	// end inline asm
	bra.uni 	$L__BB33_26;

$L__BB33_24:
	setp.eq.s64 	%p14, %rd51, 0;
	@%p14 bra 	$L__BB33_26;

	mul.lo.s64 	%rd119, %rd45, %rd21;
	add.s64 	%rd107, %rd51, %rd119;
	// begin inline asm
	{ atom.add.f64 %fd151,[%rd107],%fd50; }

	// end inline asm
	add.s64 	%rd108, %rd107, 8;
	// begin inline asm
	{ atom.add.f64 %fd153,[%rd108],%fd52; }

	// end inline asm
	add.s64 	%rd109, %rd107, 16;
	// begin inline asm
	{ atom.add.f64 %fd155,[%rd109],%fd54; }

	// end inline asm
	add.s64 	%rd110, %rd107, 24;
	// begin inline asm
	{ atom.add.f64 %fd157,[%rd110],%fd56; }

	// end inline asm
	add.s64 	%rd111, %rd107, 32;
	// begin inline asm
	{ atom.add.f64 %fd159,[%rd111],%fd58; }

	// end inline asm
	add.s64 	%rd112, %rd107, 40;
	// begin inline asm
	{ atom.add.f64 %fd161,[%rd112],%fd60; }

	// end inline asm
	add.s64 	%rd113, %rd107, 48;
	// begin inline asm
	{ atom.add.f64 %fd163,[%rd113],%fd62; }

	// end inline asm
	add.s64 	%rd114, %rd107, 56;
	// begin inline asm
	{ atom.add.f64 %fd165,[%rd114],%fd64; }

	// end inline asm
	add.s64 	%rd115, %rd107, 64;
	// begin inline asm
	{ atom.add.f64 %fd167,[%rd115],%fd66; }

	// end inline asm
	add.s64 	%rd116, %rd107, 72;
	// begin inline asm
	{ atom.add.f64 %fd169,[%rd116],%fd68; }

	// end inline asm
	add.s64 	%rd117, %rd107, 80;
	// begin inline asm
	{ atom.add.f64 %fd171,[%rd117],%fd70; }

	// end inline asm
	add.s64 	%rd118, %rd107, 88;
	// begin inline asm
	{ atom.add.f64 %fd173,[%rd118],%fd72; }

	// end inline asm

$L__BB33_26:
	setp.eq.s64 	%p15, %rd62, 0;
	@%p15 bra 	$L__BB33_28;

	add.s64 	%rd123, %rd40, 3;
	mul.lo.s64 	%rd124, %rd123, %rd28;
	add.s64 	%rd120, %rd62, %rd124;
	mov.f64 	%fd178, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd175,[%rd120],%fd178; }

	// end inline asm
	add.s64 	%rd121, %rd120, 8;
	// begin inline asm
	{ atom.add.f64 %fd177,[%rd121],%fd178; }

	// end inline asm
	add.s64 	%rd122, %rd120, 16;
	// begin inline asm
	{ atom.add.f64 %fd179,[%rd122],%fd73; }

	// end inline asm
	bra.uni 	$L__BB33_30;

$L__BB33_28:
	setp.eq.s64 	%p16, %rd53, 0;
	@%p16 bra 	$L__BB33_30;

	add.s64 	%rd125, %rd53, %rd44;
	mov.f64 	%fd184, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd181,[%rd125],%fd184; }

	// end inline asm
	add.s64 	%rd126, %rd125, 8;
	// begin inline asm
	{ atom.add.f64 %fd183,[%rd126],%fd184; }

	// end inline asm
	add.s64 	%rd127, %rd125, 16;
	// begin inline asm
	{ atom.add.f64 %fd185,[%rd127],%fd73; }

	// end inline asm

$L__BB33_30:
	@%p15 bra 	$L__BB33_32;

	add.s64 	%rd131, %rd40, 3;
	mul.lo.s64 	%rd132, %rd131, %rd28;
	add.s64 	%rd128, %rd62, %rd132;
	mov.f64 	%fd192, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd187,[%rd128],%fd192; }

	// end inline asm
	add.s64 	%rd129, %rd128, 8;
	// begin inline asm
	{ atom.add.f64 %fd189,[%rd129],%fd71; }

	// end inline asm
	add.s64 	%rd130, %rd128, 16;
	// begin inline asm
	{ atom.add.f64 %fd191,[%rd130],%fd192; }

	// end inline asm
	bra.uni 	$L__BB33_34;

$L__BB33_32:
	setp.eq.s64 	%p18, %rd53, 0;
	@%p18 bra 	$L__BB33_34;

	add.s64 	%rd133, %rd53, %rd44;
	mov.f64 	%fd198, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd193,[%rd133],%fd198; }

	// end inline asm
	add.s64 	%rd134, %rd133, 8;
	// begin inline asm
	{ atom.add.f64 %fd195,[%rd134],%fd71; }

	// end inline asm
	add.s64 	%rd135, %rd133, 16;
	// begin inline asm
	{ atom.add.f64 %fd197,[%rd135],%fd198; }

	// end inline asm

$L__BB33_34:
	@%p15 bra 	$L__BB33_36;

	add.s64 	%rd139, %rd40, 3;
	mul.lo.s64 	%rd140, %rd139, %rd28;
	add.s64 	%rd136, %rd62, %rd140;
	// begin inline asm
	{ atom.add.f64 %fd199,[%rd136],%fd69; }

	// end inline asm
	add.s64 	%rd137, %rd136, 8;
	mov.f64 	%fd204, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd201,[%rd137],%fd204; }

	// end inline asm
	add.s64 	%rd138, %rd136, 16;
	// begin inline asm
	{ atom.add.f64 %fd203,[%rd138],%fd204; }

	// end inline asm
	bra.uni 	$L__BB33_38;

$L__BB33_36:
	setp.eq.s64 	%p20, %rd53, 0;
	@%p20 bra 	$L__BB33_38;

	add.s64 	%rd141, %rd53, %rd44;
	// begin inline asm
	{ atom.add.f64 %fd205,[%rd141],%fd69; }

	// end inline asm
	add.s64 	%rd142, %rd141, 8;
	mov.f64 	%fd210, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd207,[%rd142],%fd210; }

	// end inline asm
	add.s64 	%rd143, %rd141, 16;
	// begin inline asm
	{ atom.add.f64 %fd209,[%rd143],%fd210; }

	// end inline asm

$L__BB33_38:
	@%p15 bra 	$L__BB33_40;

	add.s64 	%rd147, %rd40, 2;
	mul.lo.s64 	%rd148, %rd147, %rd28;
	add.s64 	%rd144, %rd62, %rd148;
	mov.f64 	%fd214, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd211,[%rd144],%fd214; }

	// end inline asm
	add.s64 	%rd145, %rd144, 8;
	// begin inline asm
	{ atom.add.f64 %fd213,[%rd145],%fd214; }

	// end inline asm
	add.s64 	%rd146, %rd144, 16;
	// begin inline asm
	{ atom.add.f64 %fd215,[%rd146],%fd67; }

	// end inline asm
	bra.uni 	$L__BB33_42;

$L__BB33_40:
	setp.eq.s64 	%p22, %rd53, 0;
	@%p22 bra 	$L__BB33_42;

	add.s64 	%rd149, %rd53, %rd43;
	mov.f64 	%fd220, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd217,[%rd149],%fd220; }

	// end inline asm
	add.s64 	%rd150, %rd149, 8;
	// begin inline asm
	{ atom.add.f64 %fd219,[%rd150],%fd220; }

	// end inline asm
	add.s64 	%rd151, %rd149, 16;
	// begin inline asm
	{ atom.add.f64 %fd221,[%rd151],%fd67; }

	// end inline asm

$L__BB33_42:
	@%p15 bra 	$L__BB33_44;

	add.s64 	%rd155, %rd40, 2;
	mul.lo.s64 	%rd156, %rd155, %rd28;
	add.s64 	%rd152, %rd62, %rd156;
	mov.f64 	%fd228, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd223,[%rd152],%fd228; }

	// end inline asm
	add.s64 	%rd153, %rd152, 8;
	// begin inline asm
	{ atom.add.f64 %fd225,[%rd153],%fd65; }

	// end inline asm
	add.s64 	%rd154, %rd152, 16;
	// begin inline asm
	{ atom.add.f64 %fd227,[%rd154],%fd228; }

	// end inline asm
	bra.uni 	$L__BB33_46;

$L__BB33_44:
	setp.eq.s64 	%p24, %rd53, 0;
	@%p24 bra 	$L__BB33_46;

	add.s64 	%rd157, %rd53, %rd43;
	mov.f64 	%fd234, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd229,[%rd157],%fd234; }

	// end inline asm
	add.s64 	%rd158, %rd157, 8;
	// begin inline asm
	{ atom.add.f64 %fd231,[%rd158],%fd65; }

	// end inline asm
	add.s64 	%rd159, %rd157, 16;
	// begin inline asm
	{ atom.add.f64 %fd233,[%rd159],%fd234; }

	// end inline asm

$L__BB33_46:
	@%p15 bra 	$L__BB33_48;

	add.s64 	%rd163, %rd40, 2;
	mul.lo.s64 	%rd164, %rd163, %rd28;
	add.s64 	%rd160, %rd62, %rd164;
	// begin inline asm
	{ atom.add.f64 %fd235,[%rd160],%fd63; }

	// end inline asm
	add.s64 	%rd161, %rd160, 8;
	mov.f64 	%fd240, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd237,[%rd161],%fd240; }

	// end inline asm
	add.s64 	%rd162, %rd160, 16;
	// begin inline asm
	{ atom.add.f64 %fd239,[%rd162],%fd240; }

	// end inline asm
	bra.uni 	$L__BB33_50;

$L__BB33_48:
	setp.eq.s64 	%p26, %rd53, 0;
	@%p26 bra 	$L__BB33_50;

	add.s64 	%rd165, %rd53, %rd43;
	// begin inline asm
	{ atom.add.f64 %fd241,[%rd165],%fd63; }

	// end inline asm
	add.s64 	%rd166, %rd165, 8;
	mov.f64 	%fd246, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd243,[%rd166],%fd246; }

	// end inline asm
	add.s64 	%rd167, %rd165, 16;
	// begin inline asm
	{ atom.add.f64 %fd245,[%rd167],%fd246; }

	// end inline asm

$L__BB33_50:
	@%p15 bra 	$L__BB33_52;

	add.s64 	%rd171, %rd40, 1;
	mul.lo.s64 	%rd172, %rd171, %rd28;
	add.s64 	%rd168, %rd62, %rd172;
	mov.f64 	%fd250, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd247,[%rd168],%fd250; }

	// end inline asm
	add.s64 	%rd169, %rd168, 8;
	// begin inline asm
	{ atom.add.f64 %fd249,[%rd169],%fd250; }

	// end inline asm
	add.s64 	%rd170, %rd168, 16;
	// begin inline asm
	{ atom.add.f64 %fd251,[%rd170],%fd61; }

	// end inline asm
	bra.uni 	$L__BB33_54;

$L__BB33_52:
	setp.eq.s64 	%p28, %rd53, 0;
	@%p28 bra 	$L__BB33_54;

	add.s64 	%rd173, %rd53, %rd42;
	mov.f64 	%fd256, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd253,[%rd173],%fd256; }

	// end inline asm
	add.s64 	%rd174, %rd173, 8;
	// begin inline asm
	{ atom.add.f64 %fd255,[%rd174],%fd256; }

	// end inline asm
	add.s64 	%rd175, %rd173, 16;
	// begin inline asm
	{ atom.add.f64 %fd257,[%rd175],%fd61; }

	// end inline asm

$L__BB33_54:
	@%p15 bra 	$L__BB33_56;

	add.s64 	%rd179, %rd40, 1;
	mul.lo.s64 	%rd180, %rd179, %rd28;
	add.s64 	%rd176, %rd62, %rd180;
	mov.f64 	%fd264, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd259,[%rd176],%fd264; }

	// end inline asm
	add.s64 	%rd177, %rd176, 8;
	// begin inline asm
	{ atom.add.f64 %fd261,[%rd177],%fd59; }

	// end inline asm
	add.s64 	%rd178, %rd176, 16;
	// begin inline asm
	{ atom.add.f64 %fd263,[%rd178],%fd264; }

	// end inline asm
	bra.uni 	$L__BB33_58;

$L__BB33_56:
	setp.eq.s64 	%p30, %rd53, 0;
	@%p30 bra 	$L__BB33_58;

	add.s64 	%rd181, %rd53, %rd42;
	mov.f64 	%fd270, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd265,[%rd181],%fd270; }

	// end inline asm
	add.s64 	%rd182, %rd181, 8;
	// begin inline asm
	{ atom.add.f64 %fd267,[%rd182],%fd59; }

	// end inline asm
	add.s64 	%rd183, %rd181, 16;
	// begin inline asm
	{ atom.add.f64 %fd269,[%rd183],%fd270; }

	// end inline asm

$L__BB33_58:
	@%p15 bra 	$L__BB33_60;

	add.s64 	%rd187, %rd40, 1;
	mul.lo.s64 	%rd188, %rd187, %rd28;
	add.s64 	%rd184, %rd62, %rd188;
	// begin inline asm
	{ atom.add.f64 %fd271,[%rd184],%fd57; }

	// end inline asm
	add.s64 	%rd185, %rd184, 8;
	mov.f64 	%fd276, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd273,[%rd185],%fd276; }

	// end inline asm
	add.s64 	%rd186, %rd184, 16;
	// begin inline asm
	{ atom.add.f64 %fd275,[%rd186],%fd276; }

	// end inline asm
	bra.uni 	$L__BB33_62;

$L__BB33_60:
	setp.eq.s64 	%p32, %rd53, 0;
	@%p32 bra 	$L__BB33_62;

	add.s64 	%rd189, %rd53, %rd42;
	// begin inline asm
	{ atom.add.f64 %fd277,[%rd189],%fd57; }

	// end inline asm
	add.s64 	%rd190, %rd189, 8;
	mov.f64 	%fd282, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd279,[%rd190],%fd282; }

	// end inline asm
	add.s64 	%rd191, %rd189, 16;
	// begin inline asm
	{ atom.add.f64 %fd281,[%rd191],%fd282; }

	// end inline asm

$L__BB33_62:
	@%p15 bra 	$L__BB33_64;

	mul.lo.s64 	%rd195, %rd40, %rd28;
	add.s64 	%rd192, %rd62, %rd195;
	mov.f64 	%fd286, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd283,[%rd192],%fd286; }

	// end inline asm
	add.s64 	%rd193, %rd192, 8;
	// begin inline asm
	{ atom.add.f64 %fd285,[%rd193],%fd286; }

	// end inline asm
	add.s64 	%rd194, %rd192, 16;
	// begin inline asm
	{ atom.add.f64 %fd287,[%rd194],%fd55; }

	// end inline asm
	bra.uni 	$L__BB33_66;

$L__BB33_64:
	setp.eq.s64 	%p34, %rd53, 0;
	@%p34 bra 	$L__BB33_66;

	add.s64 	%rd196, %rd53, %rd41;
	mov.f64 	%fd292, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd289,[%rd196],%fd292; }

	// end inline asm
	add.s64 	%rd197, %rd196, 8;
	// begin inline asm
	{ atom.add.f64 %fd291,[%rd197],%fd292; }

	// end inline asm
	add.s64 	%rd198, %rd196, 16;
	// begin inline asm
	{ atom.add.f64 %fd293,[%rd198],%fd55; }

	// end inline asm

$L__BB33_66:
	@%p15 bra 	$L__BB33_68;

	mul.lo.s64 	%rd202, %rd40, %rd28;
	add.s64 	%rd199, %rd62, %rd202;
	mov.f64 	%fd300, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd295,[%rd199],%fd300; }

	// end inline asm
	add.s64 	%rd200, %rd199, 8;
	// begin inline asm
	{ atom.add.f64 %fd297,[%rd200],%fd53; }

	// end inline asm
	add.s64 	%rd201, %rd199, 16;
	// begin inline asm
	{ atom.add.f64 %fd299,[%rd201],%fd300; }

	// end inline asm
	bra.uni 	$L__BB33_70;

$L__BB33_68:
	setp.eq.s64 	%p36, %rd53, 0;
	@%p36 bra 	$L__BB33_70;

	add.s64 	%rd203, %rd53, %rd41;
	mov.f64 	%fd306, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd301,[%rd203],%fd306; }

	// end inline asm
	add.s64 	%rd204, %rd203, 8;
	// begin inline asm
	{ atom.add.f64 %fd303,[%rd204],%fd53; }

	// end inline asm
	add.s64 	%rd205, %rd203, 16;
	// begin inline asm
	{ atom.add.f64 %fd305,[%rd205],%fd306; }

	// end inline asm

$L__BB33_70:
	@%p15 bra 	$L__BB33_72;

	mul.lo.s64 	%rd209, %rd40, %rd28;
	add.s64 	%rd206, %rd62, %rd209;
	// begin inline asm
	{ atom.add.f64 %fd307,[%rd206],%fd51; }

	// end inline asm
	add.s64 	%rd207, %rd206, 8;
	mov.f64 	%fd312, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd309,[%rd207],%fd312; }

	// end inline asm
	add.s64 	%rd208, %rd206, 16;
	// begin inline asm
	{ atom.add.f64 %fd311,[%rd208],%fd312; }

	// end inline asm
	bra.uni 	$L__BB33_74;

$L__BB33_72:
	setp.eq.s64 	%p38, %rd53, 0;
	@%p38 bra 	$L__BB33_74;

	add.s64 	%rd210, %rd53, %rd41;
	// begin inline asm
	{ atom.add.f64 %fd313,[%rd210],%fd51; }

	// end inline asm
	add.s64 	%rd211, %rd210, 8;
	mov.f64 	%fd318, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd315,[%rd211],%fd318; }

	// end inline asm
	add.s64 	%rd212, %rd210, 16;
	// begin inline asm
	{ atom.add.f64 %fd317,[%rd212],%fd318; }

	// end inline asm

$L__BB33_74:
	ld.param.u64 	%rd213, [step_affine_y_cuda_kernel_backward_param_0+24];
	add.s64 	%rd218, %rd218, %rd29;
	setp.lt.u64 	%p39, %rd218, %rd213;
	@%p39 bra 	$L__BB33_2;

$L__BB33_75:
	ret;

}
	// .globl	init_affine_mass_matrix_kernel_cuda_kernel_forward
.visible .entry init_affine_mass_matrix_kernel_cuda_kernel_forward(
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_0[32],
	.param .u32 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_1,
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_8[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_9[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_10[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_11[56]
)
{
	.reg .pred 	%p<218>;
	.reg .b16 	%rs<81>;
	.reg .b32 	%r<409>;
	.reg .f64 	%fd<868>;
	.reg .b64 	%rd<119>;


	ld.param.v2.u32 	{%r118, %r119}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r120, %r121}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r126, %r127}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r134, %r135}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r142, %r143}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r150, %r151}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r158, %r159}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r166, %r167}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r174, %r175}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_8+32];
	ld.param.v2.u32 	{%r182, %r183}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_9+32];
	ld.param.v2.u32 	{%r190, %r191}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_10+32];
	ld.param.v2.u32 	{%r198, %r199}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_11+32];
	ld.param.u64 	%rd58, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_10];
	ld.param.u64 	%rd56, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd54, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd52, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd50, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd48, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd46, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd44, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd42, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd41, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r26, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_0+16];
	mov.u32 	%r202, %ntid.x;
	cvt.u64.u32 	%rd1, %r202;
	mov.u32 	%r203, %ctaid.x;
	mul.wide.u32 	%rd62, %r202, %r203;
	mov.u32 	%r204, %tid.x;
	cvt.u64.u32 	%rd63, %r204;
	add.s64 	%rd115, %rd62, %rd63;
	setp.ge.u64 	%p9, %rd115, %rd41;
	@%p9 bra 	$L__BB34_134;

	cvta.to.global.u64 	%rd5, %rd58;
	cvta.to.global.u64 	%rd6, %rd56;
	cvta.to.global.u64 	%rd7, %rd54;
	cvta.to.global.u64 	%rd8, %rd52;
	cvta.to.global.u64 	%rd9, %rd50;
	cvta.to.global.u64 	%rd10, %rd48;
	cvta.to.global.u64 	%rd11, %rd46;
	cvta.to.global.u64 	%rd12, %rd44;
	cvta.to.global.u64 	%rd13, %rd42;
	cvt.s64.s32 	%rd14, %r121;
	cvt.s64.s32 	%rd15, %r120;
	cvt.s64.s32 	%rd16, %r119;
	cvt.s64.s32 	%rd17, %r142;
	cvt.s64.s32 	%rd18, %r150;
	mov.u32 	%r205, %nctaid.x;
	cvt.u64.u32 	%rd64, %r205;
	mul.lo.s64 	%rd19, %rd1, %rd64;
	cvt.s64.s32 	%rd20, %r158;
	cvt.s64.s32 	%rd21, %r166;
	cvt.s64.s32 	%rd22, %r190;
	cvt.s64.s32 	%rd23, %r174;
	cvt.s64.s32 	%rd24, %r182;
	cvt.s64.s32 	%rd25, %r134;
	cvt.s64.s32 	%rd26, %r126;
	cvt.s64.s32 	%rd27, %r198;

$L__BB34_2:
	setp.lt.s32 	%p10, %r26, 4;
	mov.u64 	%rd116, %rd115;
	@%p10 bra 	$L__BB34_6;

	or.b64  	%rd65, %rd115, %rd14;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p11, %rd66, 0;
	@%p11 bra 	$L__BB34_5;

	div.u64 	%rd116, %rd115, %rd14;
	bra.uni 	$L__BB34_6;

$L__BB34_5:
	cvt.u32.u64 	%r206, %rd14;
	cvt.u32.u64 	%r207, %rd115;
	div.u32 	%r208, %r207, %r206;
	cvt.u64.u32 	%rd116, %r208;

$L__BB34_6:
	setp.lt.s32 	%p12, %r26, 3;
	@%p12 bra 	$L__BB34_10;

	or.b64  	%rd67, %rd116, %rd15;
	and.b64  	%rd68, %rd67, -4294967296;
	setp.eq.s64 	%p13, %rd68, 0;
	@%p13 bra 	$L__BB34_9;

	div.u64 	%rd116, %rd116, %rd15;
	bra.uni 	$L__BB34_10;

$L__BB34_9:
	cvt.u32.u64 	%r209, %rd15;
	cvt.u32.u64 	%r210, %rd116;
	div.u32 	%r211, %r210, %r209;
	cvt.u64.u32 	%rd116, %r211;

$L__BB34_10:
	setp.lt.s32 	%p14, %r26, 2;
	@%p14 bra 	$L__BB34_14;

	or.b64  	%rd69, %rd116, %rd16;
	and.b64  	%rd70, %rd69, -4294967296;
	setp.eq.s64 	%p15, %rd70, 0;
	@%p15 bra 	$L__BB34_13;

	div.u64 	%rd116, %rd116, %rd16;
	bra.uni 	$L__BB34_14;

$L__BB34_13:
	cvt.u32.u64 	%r212, %rd16;
	cvt.u32.u64 	%r213, %rd116;
	div.u32 	%r214, %r213, %r212;
	cvt.u64.u32 	%rd116, %r214;

$L__BB34_14:
	ld.param.u32 	%r387, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_1];
	cvt.u32.u64 	%r215, %rd116;
	setp.gt.s32 	%p16, %r26, 0;
	selp.b32 	%r2, %r215, 0, %p16;
	setp.ge.s32 	%p17, %r2, %r387;
	@%p17 bra 	$L__BB34_133;

	cvt.s64.s32 	%rd38, %r2;
	mul.lo.s64 	%rd71, %rd38, %rd17;
	add.s64 	%rd72, %rd11, %rd71;
	ld.global.s32 	%rd39, [%rd72];
	mul.lo.s64 	%rd73, %rd39, %rd18;
	add.s64 	%rd74, %rd10, %rd73;
	ld.global.u32 	%r216, [%rd74];
	setp.eq.s32 	%p18, %r216, 0;
	@%p18 bra 	$L__BB34_133;

	mul.lo.s64 	%rd75, %rd39, %rd20;
	add.s64 	%rd76, %rd9, %rd75;
	mul.lo.s64 	%rd77, %rd39, %rd22;
	add.s64 	%rd78, %rd5, %rd77;
	ld.global.f64 	%fd1, [%rd78];
	ld.global.f64 	%fd2, [%rd78+8];
	ld.global.f64 	%fd3, [%rd78+16];
	mul.lo.s64 	%rd79, %rd39, %rd23;
	add.s64 	%rd80, %rd7, %rd79;
	ld.global.f64 	%fd4, [%rd80];
	mul.lo.s64 	%rd81, %rd39, %rd24;
	add.s64 	%rd82, %rd6, %rd81;
	ld.global.f64 	%fd5, [%rd82];
	mul.lo.s64 	%rd83, %rd38, %rd25;
	add.s64 	%rd84, %rd12, %rd83;
	ld.global.s32 	%rd85, [%rd84];
	mul.lo.s64 	%rd86, %rd85, %rd26;
	add.s64 	%rd87, %rd13, %rd86;
	ld.global.s32 	%rd88, [%rd84+4];
	mul.lo.s64 	%rd89, %rd88, %rd26;
	add.s64 	%rd90, %rd13, %rd89;
	ld.global.s32 	%rd91, [%rd84+8];
	mul.lo.s64 	%rd92, %rd91, %rd26;
	add.s64 	%rd93, %rd13, %rd92;
	ld.global.f64 	%fd6, [%rd87];
	ld.global.f64 	%fd7, [%rd90];
	sub.f64 	%fd229, %fd7, %fd6;
	ld.global.f64 	%fd8, [%rd87+8];
	ld.global.f64 	%fd9, [%rd90+8];
	sub.f64 	%fd230, %fd9, %fd8;
	ld.global.f64 	%fd10, [%rd87+16];
	ld.global.f64 	%fd11, [%rd90+16];
	sub.f64 	%fd231, %fd11, %fd10;
	ld.global.f64 	%fd12, [%rd93];
	sub.f64 	%fd232, %fd12, %fd6;
	ld.global.f64 	%fd13, [%rd93+8];
	sub.f64 	%fd233, %fd13, %fd8;
	ld.global.f64 	%fd14, [%rd93+16];
	sub.f64 	%fd234, %fd14, %fd10;
	mul.f64 	%fd235, %fd230, %fd234;
	mul.f64 	%fd236, %fd231, %fd233;
	sub.f64 	%fd18, %fd235, %fd236;
	mul.f64 	%fd237, %fd231, %fd232;
	mul.f64 	%fd238, %fd229, %fd234;
	sub.f64 	%fd19, %fd237, %fd238;
	mul.f64 	%fd239, %fd229, %fd233;
	mul.f64 	%fd240, %fd230, %fd232;
	sub.f64 	%fd20, %fd239, %fd240;
	mul.f64 	%fd241, %fd19, %fd19;
	fma.rn.f64 	%fd242, %fd18, %fd18, %fd241;
	fma.rn.f64 	%fd243, %fd20, %fd20, %fd242;
	sqrt.rn.f64 	%fd244, %fd243;
	ld.global.u32 	%r3, [%rd76];
	setp.eq.s32 	%p19, %r3, 0;
	mov.f64 	%fd839, 0d0000000000000000;
	mov.f64 	%fd840, %fd839;
	mov.f64 	%fd841, %fd839;
	mov.f64 	%fd852, %fd839;
	mov.f64 	%fd853, %fd839;
	mov.f64 	%fd854, %fd839;
	mov.f64 	%fd855, %fd839;
	mov.f64 	%fd857, %fd839;
	mov.f64 	%fd858, %fd839;
	mov.f64 	%fd859, %fd839;
	mov.f64 	%fd862, %fd839;
	mov.f64 	%fd863, %fd839;
	mov.f64 	%fd867, %fd839;
	@%p19 bra 	$L__BB34_130;

	mul.f64 	%fd803, %fd244, 0d3FE0000000000000;
	mul.lo.s64 	%rd94, %rd39, %rd21;
	add.s64 	%rd95, %rd8, %rd94;
	ld.global.u32 	%r217, [%rd95];
	setp.eq.s32 	%p20, %r217, 0;
	neg.f64 	%fd245, %fd18;
	neg.f64 	%fd246, %fd19;
	neg.f64 	%fd247, %fd20;
	selp.f64 	%fd248, 0d0000000000000000, %fd245, %p20;
	selp.f64 	%fd249, 0d0000000000000000, %fd246, %p20;
	selp.f64 	%fd250, 0d0000000000000000, %fd247, %p20;
	sub.f64 	%fd251, %fd6, %fd1;
	div.rn.f64 	%fd252, %fd251, 0d4008000000000000;
	mov.f64 	%fd253, 0d4008000000000000;
	sub.f64 	%fd254, %fd7, %fd1;
	div.rn.f64 	%fd255, %fd254, 0d4008000000000000;
	add.f64 	%fd256, %fd252, %fd255;
	sub.f64 	%fd257, %fd12, %fd1;
	div.rn.f64 	%fd258, %fd257, 0d4008000000000000;
	add.f64 	%fd22, %fd256, %fd258;
	mul.f64 	%fd259, %fd251, 0d4008000000000000;
	div.rn.f64 	%fd260, %fd259, 0d4014000000000000;
	div.rn.f64 	%fd261, %fd254, 0d4014000000000000;
	add.f64 	%fd262, %fd260, %fd261;
	div.rn.f64 	%fd263, %fd257, 0d4014000000000000;
	add.f64 	%fd23, %fd262, %fd263;
	div.rn.f64 	%fd264, %fd251, 0d4014000000000000;
	add.f64 	%fd265, %fd264, %fd261;
	mul.f64 	%fd266, %fd257, 0d4008000000000000;
	div.rn.f64 	%fd267, %fd266, 0d4014000000000000;
	add.f64 	%fd24, %fd265, %fd267;
	mul.f64 	%fd268, %fd254, 0d4008000000000000;
	div.rn.f64 	%fd269, %fd268, 0d4014000000000000;
	add.f64 	%fd270, %fd264, %fd269;
	add.f64 	%fd25, %fd263, %fd270;
	sub.f64 	%fd271, %fd8, %fd2;
	div.rn.f64 	%fd272, %fd271, 0d4008000000000000;
	sub.f64 	%fd273, %fd9, %fd2;
	div.rn.f64 	%fd274, %fd273, 0d4008000000000000;
	add.f64 	%fd275, %fd272, %fd274;
	sub.f64 	%fd276, %fd13, %fd2;
	div.rn.f64 	%fd277, %fd276, 0d4008000000000000;
	add.f64 	%fd26, %fd275, %fd277;
	mul.f64 	%fd278, %fd271, 0d4008000000000000;
	div.rn.f64 	%fd279, %fd278, 0d4014000000000000;
	div.rn.f64 	%fd280, %fd273, 0d4014000000000000;
	add.f64 	%fd281, %fd279, %fd280;
	div.rn.f64 	%fd282, %fd276, 0d4014000000000000;
	add.f64 	%fd27, %fd281, %fd282;
	div.rn.f64 	%fd283, %fd271, 0d4014000000000000;
	add.f64 	%fd284, %fd283, %fd280;
	mul.f64 	%fd285, %fd276, 0d4008000000000000;
	div.rn.f64 	%fd286, %fd285, 0d4014000000000000;
	add.f64 	%fd28, %fd284, %fd286;
	mul.f64 	%fd287, %fd273, 0d4008000000000000;
	div.rn.f64 	%fd288, %fd287, 0d4014000000000000;
	add.f64 	%fd289, %fd283, %fd288;
	add.f64 	%fd29, %fd282, %fd289;
	sub.f64 	%fd290, %fd10, %fd3;
	div.rn.f64 	%fd291, %fd290, 0d4008000000000000;
	sub.f64 	%fd292, %fd11, %fd3;
	div.rn.f64 	%fd293, %fd292, 0d4008000000000000;
	add.f64 	%fd294, %fd291, %fd293;
	sub.f64 	%fd295, %fd14, %fd3;
	div.rn.f64 	%fd296, %fd295, 0d4008000000000000;
	add.f64 	%fd30, %fd294, %fd296;
	mul.f64 	%fd297, %fd290, 0d4008000000000000;
	div.rn.f64 	%fd298, %fd297, 0d4014000000000000;
	div.rn.f64 	%fd299, %fd292, 0d4014000000000000;
	add.f64 	%fd300, %fd298, %fd299;
	div.rn.f64 	%fd301, %fd295, 0d4014000000000000;
	add.f64 	%fd31, %fd300, %fd301;
	div.rn.f64 	%fd302, %fd290, 0d4014000000000000;
	add.f64 	%fd303, %fd302, %fd299;
	mul.f64 	%fd304, %fd295, 0d4008000000000000;
	div.rn.f64 	%fd305, %fd304, 0d4014000000000000;
	add.f64 	%fd32, %fd303, %fd305;
	mul.f64 	%fd306, %fd292, 0d4008000000000000;
	div.rn.f64 	%fd307, %fd306, 0d4014000000000000;
	add.f64 	%fd308, %fd302, %fd307;
	add.f64 	%fd33, %fd301, %fd308;
	selp.f64 	%fd309, %fd20, %fd250, %p20;
	selp.f64 	%fd310, %fd19, %fd249, %p20;
	selp.f64 	%fd311, %fd18, %fd248, %p20;
	add.f64 	%fd312, %fd803, %fd803;
	div.rn.f64 	%fd839, %fd311, %fd312;
	div.rn.f64 	%fd840, %fd310, %fd312;
	div.rn.f64 	%fd841, %fd309, %fd312;
	mul.f64 	%fd37, %fd5, %fd803;
	mov.f64 	%fd313, 0d3FF0000000000000;
	sub.f64 	%fd314, %fd313, %fd22;
	sub.f64 	%fd315, %fd314, %fd26;
	sub.f64 	%fd38, %fd315, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd38;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd253;
	}
	and.b32  	%r6, %r5, 2146435072;
	setp.eq.s32 	%p21, %r6, 1073741824;
	abs.f64 	%fd39, %fd38;
	{ // callseq 55, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd39;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd253;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd817, [retval0+0];
	} // callseq 55
	setp.lt.s32 	%p22, %r4, 0;
	and.pred  	%p1, %p22, %p21;
	not.pred 	%p23, %p1;
	@%p23 bra 	$L__BB34_19;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r218}, %fd817;
	}
	xor.b32  	%r219, %r218, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r220, %temp}, %fd817;
	}
	mov.b64 	%fd817, {%r220, %r219};

$L__BB34_19:
	setp.eq.f64 	%p24, %fd38, 0d0000000000000000;
	@%p24 bra 	$L__BB34_23;
	bra.uni 	$L__BB34_20;

$L__BB34_23:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r394}, %fd38;
	}
	selp.b32 	%r221, %r394, 0, %p21;
	mov.u32 	%r222, 0;
	or.b32  	%r223, %r221, 2146435072;
	setp.lt.s32 	%p28, %r5, 0;
	selp.b32 	%r224, %r223, %r221, %p28;
	mov.b64 	%fd817, {%r222, %r224};
	bra.uni 	$L__BB34_24;

$L__BB34_20:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r391}, %fd38;
	}
	setp.gt.s32 	%p25, %r391, -1;
	@%p25 bra 	$L__BB34_24;

	mov.f64 	%fd316, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd317, %fd316;
	setp.eq.f64 	%p26, %fd317, 0d4008000000000000;
	@%p26 bra 	$L__BB34_24;

	mov.f64 	%fd817, 0dFFF8000000000000;

$L__BB34_24:
	add.f64 	%fd319, %fd38, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r225}, %fd319;
	}
	and.b32  	%r226, %r225, 2146435072;
	setp.ne.s32 	%p29, %r226, 2146435072;
	@%p29 bra 	$L__BB34_31;

	abs.f64 	%fd807, %fd38;
	setp.gtu.f64 	%p30, %fd807, 0d7FF0000000000000;
	@%p30 bra 	$L__BB34_30;
	bra.uni 	$L__BB34_26;

$L__BB34_30:
	mov.f64 	%fd321, 0d4008000000000000;
	add.rn.f64 	%fd817, %fd38, %fd321;
	bra.uni 	$L__BB34_31;

$L__BB34_26:
	mov.f64 	%fd320, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r227, %temp}, %fd320;
	}
	and.b32  	%r7, %r5, 2147483647;
	setp.eq.s32 	%p31, %r7, 2146435072;
	setp.eq.s32 	%p32, %r227, 0;
	and.pred  	%p33, %p31, %p32;
	@%p33 bra 	$L__BB34_29;
	bra.uni 	$L__BB34_27;

$L__BB34_29:
	abs.f64 	%fd808, %fd38;
	setp.gt.f64 	%p40, %fd808, 0d3FF0000000000000;
	selp.b32 	%r234, 2146435072, 0, %p40;
	mov.u32 	%r235, 0;
	xor.b32  	%r236, %r234, 2146435072;
	setp.lt.s32 	%p41, %r5, 0;
	selp.b32 	%r237, %r236, %r234, %p41;
	setp.eq.f64 	%p42, %fd38, 0dBFF0000000000000;
	selp.b32 	%r238, 1072693248, %r237, %p42;
	mov.b64 	%fd817, {%r235, %r238};
	bra.uni 	$L__BB34_31;

$L__BB34_27:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r392}, %fd38;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r228, %temp}, %fd38;
	}
	and.b32  	%r229, %r392, 2147483647;
	setp.ne.s32 	%p34, %r229, 2146435072;
	setp.ne.s32 	%p35, %r228, 0;
	or.pred  	%p36, %p34, %p35;
	@%p36 bra 	$L__BB34_31;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r393}, %fd38;
	}
	setp.lt.s32 	%p211, %r393, 0;
	and.pred  	%p210, %p211, %p21;
	setp.gt.s32 	%p37, %r5, -1;
	selp.b32 	%r230, 2146435072, 0, %p37;
	mov.u32 	%r231, 0;
	setp.ne.s32 	%p38, %r7, 1071644672;
	and.pred  	%p39, %p38, %p210;
	or.b32  	%r232, %r230, -2147483648;
	selp.b32 	%r233, %r232, %r230, %p39;
	mov.b64 	%fd817, {%r231, %r233};

$L__BB34_31:
	mul.f64 	%fd49, %fd37, 0dBFE2000000000000;
	mov.f64 	%fd322, 0d0000000000000000;
	sub.f64 	%fd323, %fd322, %fd817;
	div.rn.f64 	%fd324, %fd323, 0d4008000000000000;
	mov.f64 	%fd325, 0d4008000000000000;
	setp.eq.f64 	%p43, %fd38, 0d3FF0000000000000;
	selp.f64 	%fd50, 0dBFD5555555555555, %fd324, %p43;
	abs.f64 	%fd51, %fd22;
	{ // callseq 56, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd51;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd325;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd820, [retval0+0];
	} // callseq 56
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r8}, %fd22;
	}
	setp.lt.s32 	%p44, %r8, 0;
	and.pred  	%p2, %p44, %p21;
	not.pred 	%p46, %p2;
	@%p46 bra 	$L__BB34_33;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r239}, %fd820;
	}
	xor.b32  	%r240, %r239, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r241, %temp}, %fd820;
	}
	mov.b64 	%fd820, {%r241, %r240};

$L__BB34_33:
	setp.eq.f64 	%p47, %fd22, 0d0000000000000000;
	@%p47 bra 	$L__BB34_37;
	bra.uni 	$L__BB34_34;

$L__BB34_37:
	selp.b32 	%r242, %r8, 0, %p21;
	mov.u32 	%r243, 0;
	or.b32  	%r244, %r242, 2146435072;
	setp.lt.s32 	%p51, %r5, 0;
	selp.b32 	%r245, %r244, %r242, %p51;
	mov.b64 	%fd820, {%r243, %r245};
	bra.uni 	$L__BB34_38;

$L__BB34_34:
	setp.gt.s32 	%p48, %r8, -1;
	@%p48 bra 	$L__BB34_38;

	mov.f64 	%fd326, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd327, %fd326;
	setp.eq.f64 	%p49, %fd327, 0d4008000000000000;
	@%p49 bra 	$L__BB34_38;

	mov.f64 	%fd820, 0dFFF8000000000000;

$L__BB34_38:
	add.f64 	%fd329, %fd22, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r246}, %fd329;
	}
	and.b32  	%r247, %r246, 2146435072;
	setp.ne.s32 	%p52, %r247, 2146435072;
	@%p52 bra 	$L__BB34_45;

	setp.gtu.f64 	%p53, %fd51, 0d7FF0000000000000;
	@%p53 bra 	$L__BB34_44;
	bra.uni 	$L__BB34_40;

$L__BB34_44:
	mov.f64 	%fd331, 0d4008000000000000;
	add.rn.f64 	%fd820, %fd22, %fd331;
	bra.uni 	$L__BB34_45;

$L__BB34_40:
	mov.f64 	%fd330, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r248, %temp}, %fd330;
	}
	and.b32  	%r9, %r5, 2147483647;
	setp.eq.s32 	%p54, %r9, 2146435072;
	setp.eq.s32 	%p55, %r248, 0;
	and.pred  	%p56, %p54, %p55;
	@%p56 bra 	$L__BB34_43;
	bra.uni 	$L__BB34_41;

$L__BB34_43:
	setp.gt.f64 	%p63, %fd51, 0d3FF0000000000000;
	selp.b32 	%r255, 2146435072, 0, %p63;
	mov.u32 	%r256, 0;
	xor.b32  	%r257, %r255, 2146435072;
	setp.lt.s32 	%p64, %r5, 0;
	selp.b32 	%r258, %r257, %r255, %p64;
	setp.eq.f64 	%p65, %fd22, 0dBFF0000000000000;
	selp.b32 	%r259, 1072693248, %r258, %p65;
	mov.b64 	%fd820, {%r256, %r259};
	bra.uni 	$L__BB34_45;

$L__BB34_41:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r249, %temp}, %fd22;
	}
	and.b32  	%r250, %r8, 2147483647;
	setp.ne.s32 	%p57, %r250, 2146435072;
	setp.ne.s32 	%p58, %r249, 0;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB34_45;

	setp.gt.s32 	%p60, %r5, -1;
	selp.b32 	%r251, 2146435072, 0, %p60;
	mov.u32 	%r252, 0;
	setp.ne.s32 	%p61, %r9, 1071644672;
	and.pred  	%p62, %p61, %p2;
	or.b32  	%r253, %r251, -2147483648;
	selp.b32 	%r254, %r253, %r251, %p62;
	mov.b64 	%fd820, {%r252, %r254};

$L__BB34_45:
	mul.f64 	%fd332, %fd22, %fd22;
	mul.f64 	%fd333, %fd26, 0d3FE0000000000000;
	mov.f64 	%fd334, 0d3FE0000000000000;
	mul.f64 	%fd335, %fd30, 0d3FE0000000000000;
	mul.f64 	%fd336, %fd22, 0d3FE0000000000000;
	div.rn.f64 	%fd337, %fd820, 0d4008000000000000;
	mov.f64 	%fd338, 0d4008000000000000;
	setp.eq.f64 	%p66, %fd22, 0d3FF0000000000000;
	mov.f64 	%fd339, 0d3FF0000000000000;
	selp.f64 	%fd340, 0d3FD5555555555555, %fd337, %p66;
	mul.f64 	%fd341, %fd839, %fd340;
	mul.f64 	%fd342, %fd332, 0d3FE0000000000000;
	mul.f64 	%fd343, %fd26, %fd342;
	mul.f64 	%fd344, %fd839, %fd343;
	mul.f64 	%fd345, %fd342, %fd30;
	mul.f64 	%fd346, %fd839, %fd345;
	mul.f64 	%fd347, %fd22, %fd26;
	mul.f64 	%fd348, %fd26, %fd347;
	mul.f64 	%fd349, %fd839, %fd348;
	mul.f64 	%fd350, %fd347, %fd30;
	mul.f64 	%fd351, %fd839, %fd350;
	mul.f64 	%fd352, %fd22, %fd30;
	mul.f64 	%fd353, %fd30, %fd352;
	mul.f64 	%fd354, %fd839, %fd353;
	mul.f64 	%fd355, %fd839, %fd50;
	fma.rn.f64 	%fd61, %fd49, %fd355, 0d0000000000000000;
	mul.f64 	%fd356, %fd26, %fd26;
	div.rn.f64 	%fd357, %fd26, 0d4008000000000000;
	sub.f64 	%fd358, %fd334, %fd357;
	sub.f64 	%fd359, %fd358, %fd336;
	sub.f64 	%fd360, %fd359, %fd335;
	mul.f64 	%fd361, %fd356, %fd360;
	mul.f64 	%fd362, %fd840, %fd361;
	fma.rn.f64 	%fd62, %fd49, %fd362, 0d0000000000000000;
	fma.rn.f64 	%fd63, %fd49, %fd341, 0d0000000000000000;
	fma.rn.f64 	%fd64, %fd49, %fd346, 0d0000000000000000;
	fma.rn.f64 	%fd65, %fd49, %fd351, 0d0000000000000000;
	div.rn.f64 	%fd363, %fd22, 0d4008000000000000;
	sub.f64 	%fd364, %fd334, %fd363;
	sub.f64 	%fd365, %fd364, %fd333;
	sub.f64 	%fd366, %fd365, %fd335;
	mul.f64 	%fd367, %fd332, %fd366;
	mul.f64 	%fd368, %fd839, %fd367;
	fma.rn.f64 	%fd66, %fd49, %fd368, 0d0000000000000000;
	mul.f64 	%fd369, %fd30, %fd30;
	div.rn.f64 	%fd370, %fd30, 0d4008000000000000;
	sub.f64 	%fd371, %fd334, %fd370;
	sub.f64 	%fd372, %fd371, %fd336;
	sub.f64 	%fd373, %fd372, %fd333;
	mul.f64 	%fd374, %fd369, %fd373;
	mul.f64 	%fd375, %fd841, %fd374;
	fma.rn.f64 	%fd67, %fd49, %fd375, 0d0000000000000000;
	fma.rn.f64 	%fd68, %fd49, %fd344, 0d0000000000000000;
	fma.rn.f64 	%fd69, %fd49, %fd349, 0d0000000000000000;
	fma.rn.f64 	%fd70, %fd49, %fd354, 0d0000000000000000;
	mul.f64 	%fd71, %fd37, 0d3FE0AAAAA0000000;
	sub.f64 	%fd376, %fd339, %fd23;
	sub.f64 	%fd377, %fd376, %fd27;
	sub.f64 	%fd72, %fd377, %fd31;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r10}, %fd72;
	}
	abs.f64 	%fd73, %fd72;
	{ // callseq 57, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd73;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd338;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd823, [retval0+0];
	} // callseq 57
	setp.lt.s32 	%p67, %r10, 0;
	and.pred  	%p3, %p67, %p21;
	not.pred 	%p69, %p3;
	@%p69 bra 	$L__BB34_47;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r260}, %fd823;
	}
	xor.b32  	%r261, %r260, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r262, %temp}, %fd823;
	}
	mov.b64 	%fd823, {%r262, %r261};

$L__BB34_47:
	setp.eq.f64 	%p70, %fd72, 0d0000000000000000;
	@%p70 bra 	$L__BB34_51;
	bra.uni 	$L__BB34_48;

$L__BB34_51:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r390}, %fd72;
	}
	selp.b32 	%r263, %r390, 0, %p21;
	mov.u32 	%r264, 0;
	or.b32  	%r265, %r263, 2146435072;
	setp.lt.s32 	%p74, %r5, 0;
	selp.b32 	%r266, %r265, %r263, %p74;
	mov.b64 	%fd823, {%r264, %r266};
	bra.uni 	$L__BB34_52;

$L__BB34_48:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r386}, %fd72;
	}
	setp.gt.s32 	%p71, %r386, -1;
	@%p71 bra 	$L__BB34_52;

	mov.f64 	%fd378, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd379, %fd378;
	setp.eq.f64 	%p72, %fd379, 0d4008000000000000;
	@%p72 bra 	$L__BB34_52;

	mov.f64 	%fd823, 0dFFF8000000000000;

$L__BB34_52:
	add.f64 	%fd381, %fd72, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r267}, %fd381;
	}
	and.b32  	%r268, %r267, 2146435072;
	setp.ne.s32 	%p75, %r268, 2146435072;
	@%p75 bra 	$L__BB34_59;

	abs.f64 	%fd805, %fd72;
	setp.gtu.f64 	%p76, %fd805, 0d7FF0000000000000;
	@%p76 bra 	$L__BB34_58;
	bra.uni 	$L__BB34_54;

$L__BB34_58:
	mov.f64 	%fd383, 0d4008000000000000;
	add.rn.f64 	%fd823, %fd72, %fd383;
	bra.uni 	$L__BB34_59;

$L__BB34_54:
	mov.f64 	%fd382, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r269, %temp}, %fd382;
	}
	and.b32  	%r11, %r5, 2147483647;
	setp.eq.s32 	%p77, %r11, 2146435072;
	setp.eq.s32 	%p78, %r269, 0;
	and.pred  	%p79, %p77, %p78;
	@%p79 bra 	$L__BB34_57;
	bra.uni 	$L__BB34_55;

$L__BB34_57:
	abs.f64 	%fd806, %fd72;
	setp.gt.f64 	%p86, %fd806, 0d3FF0000000000000;
	selp.b32 	%r276, 2146435072, 0, %p86;
	mov.u32 	%r277, 0;
	xor.b32  	%r278, %r276, 2146435072;
	setp.lt.s32 	%p87, %r5, 0;
	selp.b32 	%r279, %r278, %r276, %p87;
	setp.eq.f64 	%p88, %fd72, 0dBFF0000000000000;
	selp.b32 	%r280, 1072693248, %r279, %p88;
	mov.b64 	%fd823, {%r277, %r280};
	bra.uni 	$L__BB34_59;

$L__BB34_55:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r388}, %fd72;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r270, %temp}, %fd72;
	}
	and.b32  	%r271, %r388, 2147483647;
	setp.ne.s32 	%p80, %r271, 2146435072;
	setp.ne.s32 	%p81, %r270, 0;
	or.pred  	%p82, %p80, %p81;
	@%p82 bra 	$L__BB34_59;

	and.b32  	%r395, %r5, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r389}, %fd72;
	}
	setp.lt.s32 	%p209, %r389, 0;
	and.pred  	%p208, %p209, %p21;
	setp.gt.s32 	%p83, %r5, -1;
	selp.b32 	%r272, 2146435072, 0, %p83;
	mov.u32 	%r273, 0;
	setp.ne.s32 	%p84, %r395, 1071644672;
	and.pred  	%p85, %p84, %p208;
	or.b32  	%r274, %r272, -2147483648;
	selp.b32 	%r275, %r274, %r272, %p85;
	mov.b64 	%fd823, {%r273, %r275};

$L__BB34_59:
	mov.f64 	%fd384, 0d0000000000000000;
	sub.f64 	%fd385, %fd384, %fd823;
	div.rn.f64 	%fd386, %fd385, 0d4008000000000000;
	mov.f64 	%fd387, 0d4008000000000000;
	setp.eq.f64 	%p89, %fd72, 0d3FF0000000000000;
	selp.f64 	%fd83, 0dBFD5555555555555, %fd386, %p89;
	abs.f64 	%fd84, %fd23;
	{ // callseq 58, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd84;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd387;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd826, [retval0+0];
	} // callseq 58
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd23;
	}
	setp.lt.s32 	%p90, %r12, 0;
	and.pred  	%p4, %p90, %p21;
	not.pred 	%p92, %p4;
	@%p92 bra 	$L__BB34_61;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r281}, %fd826;
	}
	xor.b32  	%r282, %r281, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r283, %temp}, %fd826;
	}
	mov.b64 	%fd826, {%r283, %r282};

$L__BB34_61:
	setp.eq.f64 	%p93, %fd23, 0d0000000000000000;
	@%p93 bra 	$L__BB34_65;
	bra.uni 	$L__BB34_62;

$L__BB34_65:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r399}, %fd23;
	}
	selp.b32 	%r284, %r399, 0, %p21;
	mov.u32 	%r285, 0;
	or.b32  	%r286, %r284, 2146435072;
	setp.lt.s32 	%p97, %r5, 0;
	selp.b32 	%r287, %r286, %r284, %p97;
	mov.b64 	%fd826, {%r285, %r287};
	bra.uni 	$L__BB34_66;

$L__BB34_62:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r396}, %fd23;
	}
	setp.gt.s32 	%p94, %r396, -1;
	@%p94 bra 	$L__BB34_66;

	mov.f64 	%fd388, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd389, %fd388;
	setp.eq.f64 	%p95, %fd389, 0d4008000000000000;
	@%p95 bra 	$L__BB34_66;

	mov.f64 	%fd826, 0dFFF8000000000000;

$L__BB34_66:
	add.f64 	%fd391, %fd23, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r288}, %fd391;
	}
	and.b32  	%r289, %r288, 2146435072;
	setp.ne.s32 	%p98, %r289, 2146435072;
	@%p98 bra 	$L__BB34_73;

	abs.f64 	%fd809, %fd23;
	setp.gtu.f64 	%p99, %fd809, 0d7FF0000000000000;
	@%p99 bra 	$L__BB34_72;
	bra.uni 	$L__BB34_68;

$L__BB34_72:
	mov.f64 	%fd393, 0d4008000000000000;
	add.rn.f64 	%fd826, %fd23, %fd393;
	bra.uni 	$L__BB34_73;

$L__BB34_68:
	mov.f64 	%fd392, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r290, %temp}, %fd392;
	}
	and.b32  	%r13, %r5, 2147483647;
	setp.eq.s32 	%p100, %r13, 2146435072;
	setp.eq.s32 	%p101, %r290, 0;
	and.pred  	%p102, %p100, %p101;
	@%p102 bra 	$L__BB34_71;
	bra.uni 	$L__BB34_69;

$L__BB34_71:
	abs.f64 	%fd810, %fd23;
	setp.gt.f64 	%p109, %fd810, 0d3FF0000000000000;
	selp.b32 	%r297, 2146435072, 0, %p109;
	mov.u32 	%r298, 0;
	xor.b32  	%r299, %r297, 2146435072;
	setp.lt.s32 	%p110, %r5, 0;
	selp.b32 	%r300, %r299, %r297, %p110;
	setp.eq.f64 	%p111, %fd23, 0dBFF0000000000000;
	selp.b32 	%r301, 1072693248, %r300, %p111;
	mov.b64 	%fd826, {%r298, %r301};
	bra.uni 	$L__BB34_73;

$L__BB34_69:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r397}, %fd23;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r291, %temp}, %fd23;
	}
	and.b32  	%r292, %r397, 2147483647;
	setp.ne.s32 	%p103, %r292, 2146435072;
	setp.ne.s32 	%p104, %r291, 0;
	or.pred  	%p105, %p103, %p104;
	@%p105 bra 	$L__BB34_73;

	and.b32  	%r400, %r5, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r398}, %fd23;
	}
	setp.lt.s32 	%p213, %r398, 0;
	and.pred  	%p212, %p213, %p21;
	setp.gt.s32 	%p106, %r5, -1;
	selp.b32 	%r293, 2146435072, 0, %p106;
	mov.u32 	%r294, 0;
	setp.ne.s32 	%p107, %r400, 1071644672;
	and.pred  	%p108, %p107, %p212;
	or.b32  	%r295, %r293, -2147483648;
	selp.b32 	%r296, %r295, %r293, %p108;
	mov.b64 	%fd826, {%r294, %r296};

$L__BB34_73:
	mul.f64 	%fd394, %fd23, %fd23;
	mul.f64 	%fd395, %fd27, 0d3FE0000000000000;
	mov.f64 	%fd396, 0d3FE0000000000000;
	mul.f64 	%fd397, %fd31, 0d3FE0000000000000;
	mul.f64 	%fd398, %fd23, 0d3FE0000000000000;
	div.rn.f64 	%fd399, %fd826, 0d4008000000000000;
	mov.f64 	%fd400, 0d4008000000000000;
	setp.eq.f64 	%p112, %fd23, 0d3FF0000000000000;
	mov.f64 	%fd401, 0d3FF0000000000000;
	selp.f64 	%fd402, 0d3FD5555555555555, %fd399, %p112;
	mul.f64 	%fd403, %fd839, %fd402;
	mul.f64 	%fd404, %fd394, 0d3FE0000000000000;
	mul.f64 	%fd405, %fd27, %fd404;
	mul.f64 	%fd406, %fd839, %fd405;
	mul.f64 	%fd407, %fd404, %fd31;
	mul.f64 	%fd408, %fd839, %fd407;
	mul.f64 	%fd409, %fd23, %fd27;
	mul.f64 	%fd410, %fd27, %fd409;
	mul.f64 	%fd411, %fd839, %fd410;
	mul.f64 	%fd412, %fd409, %fd31;
	mul.f64 	%fd413, %fd839, %fd412;
	mul.f64 	%fd414, %fd23, %fd31;
	mul.f64 	%fd415, %fd31, %fd414;
	mul.f64 	%fd416, %fd839, %fd415;
	mul.f64 	%fd417, %fd839, %fd83;
	fma.rn.f64 	%fd94, %fd71, %fd417, %fd61;
	div.rn.f64 	%fd418, %fd23, 0d4008000000000000;
	sub.f64 	%fd419, %fd396, %fd418;
	sub.f64 	%fd420, %fd419, %fd395;
	sub.f64 	%fd421, %fd420, %fd397;
	mul.f64 	%fd422, %fd394, %fd421;
	mul.f64 	%fd423, %fd839, %fd422;
	fma.rn.f64 	%fd95, %fd71, %fd423, %fd66;
	mul.f64 	%fd424, %fd27, %fd27;
	div.rn.f64 	%fd425, %fd27, 0d4008000000000000;
	sub.f64 	%fd426, %fd396, %fd425;
	sub.f64 	%fd427, %fd426, %fd398;
	sub.f64 	%fd428, %fd427, %fd397;
	mul.f64 	%fd429, %fd424, %fd428;
	mul.f64 	%fd430, %fd840, %fd429;
	fma.rn.f64 	%fd96, %fd71, %fd430, %fd62;
	mul.f64 	%fd431, %fd31, %fd31;
	div.rn.f64 	%fd432, %fd31, 0d4008000000000000;
	sub.f64 	%fd433, %fd396, %fd432;
	sub.f64 	%fd434, %fd433, %fd398;
	sub.f64 	%fd435, %fd434, %fd395;
	mul.f64 	%fd436, %fd431, %fd435;
	mul.f64 	%fd437, %fd841, %fd436;
	fma.rn.f64 	%fd97, %fd71, %fd437, %fd67;
	fma.rn.f64 	%fd98, %fd71, %fd403, %fd63;
	fma.rn.f64 	%fd99, %fd71, %fd406, %fd68;
	fma.rn.f64 	%fd100, %fd71, %fd408, %fd64;
	fma.rn.f64 	%fd101, %fd71, %fd411, %fd69;
	fma.rn.f64 	%fd102, %fd71, %fd413, %fd65;
	fma.rn.f64 	%fd103, %fd71, %fd416, %fd70;
	sub.f64 	%fd438, %fd401, %fd24;
	sub.f64 	%fd439, %fd438, %fd28;
	sub.f64 	%fd104, %fd439, %fd32;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd104;
	}
	abs.f64 	%fd105, %fd104;
	{ // callseq 59, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd105;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd400;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd829, [retval0+0];
	} // callseq 59
	setp.lt.s32 	%p113, %r14, 0;
	and.pred  	%p5, %p113, %p21;
	not.pred 	%p115, %p5;
	@%p115 bra 	$L__BB34_75;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r302}, %fd829;
	}
	xor.b32  	%r303, %r302, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r304, %temp}, %fd829;
	}
	mov.b64 	%fd829, {%r304, %r303};

$L__BB34_75:
	setp.eq.f64 	%p116, %fd104, 0d0000000000000000;
	@%p116 bra 	$L__BB34_79;
	bra.uni 	$L__BB34_76;

$L__BB34_79:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r404}, %fd104;
	}
	selp.b32 	%r305, %r404, 0, %p21;
	mov.u32 	%r306, 0;
	or.b32  	%r307, %r305, 2146435072;
	setp.lt.s32 	%p120, %r5, 0;
	selp.b32 	%r308, %r307, %r305, %p120;
	mov.b64 	%fd829, {%r306, %r308};
	bra.uni 	$L__BB34_80;

$L__BB34_76:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r401}, %fd104;
	}
	setp.gt.s32 	%p117, %r401, -1;
	@%p117 bra 	$L__BB34_80;

	mov.f64 	%fd440, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd441, %fd440;
	setp.eq.f64 	%p118, %fd441, 0d4008000000000000;
	@%p118 bra 	$L__BB34_80;

	mov.f64 	%fd829, 0dFFF8000000000000;

$L__BB34_80:
	add.f64 	%fd443, %fd104, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r309}, %fd443;
	}
	and.b32  	%r310, %r309, 2146435072;
	setp.ne.s32 	%p121, %r310, 2146435072;
	@%p121 bra 	$L__BB34_87;

	abs.f64 	%fd811, %fd104;
	setp.gtu.f64 	%p122, %fd811, 0d7FF0000000000000;
	@%p122 bra 	$L__BB34_86;
	bra.uni 	$L__BB34_82;

$L__BB34_86:
	mov.f64 	%fd445, 0d4008000000000000;
	add.rn.f64 	%fd829, %fd104, %fd445;
	bra.uni 	$L__BB34_87;

$L__BB34_82:
	mov.f64 	%fd444, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r311, %temp}, %fd444;
	}
	and.b32  	%r15, %r5, 2147483647;
	setp.eq.s32 	%p123, %r15, 2146435072;
	setp.eq.s32 	%p124, %r311, 0;
	and.pred  	%p125, %p123, %p124;
	@%p125 bra 	$L__BB34_85;
	bra.uni 	$L__BB34_83;

$L__BB34_85:
	abs.f64 	%fd812, %fd104;
	setp.gt.f64 	%p132, %fd812, 0d3FF0000000000000;
	selp.b32 	%r318, 2146435072, 0, %p132;
	mov.u32 	%r319, 0;
	xor.b32  	%r320, %r318, 2146435072;
	setp.lt.s32 	%p133, %r5, 0;
	selp.b32 	%r321, %r320, %r318, %p133;
	setp.eq.f64 	%p134, %fd104, 0dBFF0000000000000;
	selp.b32 	%r322, 1072693248, %r321, %p134;
	mov.b64 	%fd829, {%r319, %r322};
	bra.uni 	$L__BB34_87;

$L__BB34_83:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r402}, %fd104;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r312, %temp}, %fd104;
	}
	and.b32  	%r313, %r402, 2147483647;
	setp.ne.s32 	%p126, %r313, 2146435072;
	setp.ne.s32 	%p127, %r312, 0;
	or.pred  	%p128, %p126, %p127;
	@%p128 bra 	$L__BB34_87;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r403}, %fd104;
	}
	setp.lt.s32 	%p215, %r403, 0;
	and.pred  	%p214, %p215, %p21;
	setp.gt.s32 	%p129, %r5, -1;
	selp.b32 	%r314, 2146435072, 0, %p129;
	mov.u32 	%r315, 0;
	setp.ne.s32 	%p130, %r15, 1071644672;
	and.pred  	%p131, %p130, %p214;
	or.b32  	%r316, %r314, -2147483648;
	selp.b32 	%r317, %r316, %r314, %p131;
	mov.b64 	%fd829, {%r315, %r317};

$L__BB34_87:
	mov.f64 	%fd446, 0d0000000000000000;
	sub.f64 	%fd447, %fd446, %fd829;
	div.rn.f64 	%fd448, %fd447, 0d4008000000000000;
	mov.f64 	%fd449, 0d4008000000000000;
	setp.eq.f64 	%p135, %fd104, 0d3FF0000000000000;
	selp.f64 	%fd115, 0dBFD5555555555555, %fd448, %p135;
	abs.f64 	%fd116, %fd24;
	{ // callseq 60, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd116;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd449;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd832, [retval0+0];
	} // callseq 60
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd24;
	}
	setp.lt.s32 	%p136, %r16, 0;
	and.pred  	%p6, %p136, %p21;
	not.pred 	%p138, %p6;
	@%p138 bra 	$L__BB34_89;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r323}, %fd832;
	}
	xor.b32  	%r324, %r323, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r325, %temp}, %fd832;
	}
	mov.b64 	%fd832, {%r325, %r324};

$L__BB34_89:
	setp.eq.f64 	%p139, %fd24, 0d0000000000000000;
	@%p139 bra 	$L__BB34_93;
	bra.uni 	$L__BB34_90;

$L__BB34_93:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r408}, %fd24;
	}
	selp.b32 	%r326, %r408, 0, %p21;
	mov.u32 	%r327, 0;
	or.b32  	%r328, %r326, 2146435072;
	setp.lt.s32 	%p143, %r5, 0;
	selp.b32 	%r329, %r328, %r326, %p143;
	mov.b64 	%fd832, {%r327, %r329};
	bra.uni 	$L__BB34_94;

$L__BB34_90:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r405}, %fd24;
	}
	setp.gt.s32 	%p140, %r405, -1;
	@%p140 bra 	$L__BB34_94;

	mov.f64 	%fd450, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd451, %fd450;
	setp.eq.f64 	%p141, %fd451, 0d4008000000000000;
	@%p141 bra 	$L__BB34_94;

	mov.f64 	%fd832, 0dFFF8000000000000;

$L__BB34_94:
	add.f64 	%fd453, %fd24, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r330}, %fd453;
	}
	and.b32  	%r331, %r330, 2146435072;
	setp.ne.s32 	%p144, %r331, 2146435072;
	@%p144 bra 	$L__BB34_101;

	abs.f64 	%fd813, %fd24;
	setp.gtu.f64 	%p145, %fd813, 0d7FF0000000000000;
	@%p145 bra 	$L__BB34_100;
	bra.uni 	$L__BB34_96;

$L__BB34_100:
	mov.f64 	%fd455, 0d4008000000000000;
	add.rn.f64 	%fd832, %fd24, %fd455;
	bra.uni 	$L__BB34_101;

$L__BB34_96:
	mov.f64 	%fd454, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r332, %temp}, %fd454;
	}
	and.b32  	%r17, %r5, 2147483647;
	setp.eq.s32 	%p146, %r17, 2146435072;
	setp.eq.s32 	%p147, %r332, 0;
	and.pred  	%p148, %p146, %p147;
	@%p148 bra 	$L__BB34_99;
	bra.uni 	$L__BB34_97;

$L__BB34_99:
	abs.f64 	%fd814, %fd24;
	setp.gt.f64 	%p155, %fd814, 0d3FF0000000000000;
	selp.b32 	%r339, 2146435072, 0, %p155;
	mov.u32 	%r340, 0;
	xor.b32  	%r341, %r339, 2146435072;
	setp.lt.s32 	%p156, %r5, 0;
	selp.b32 	%r342, %r341, %r339, %p156;
	setp.eq.f64 	%p157, %fd24, 0dBFF0000000000000;
	selp.b32 	%r343, 1072693248, %r342, %p157;
	mov.b64 	%fd832, {%r340, %r343};
	bra.uni 	$L__BB34_101;

$L__BB34_97:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r406}, %fd24;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r333, %temp}, %fd24;
	}
	and.b32  	%r334, %r406, 2147483647;
	setp.ne.s32 	%p149, %r334, 2146435072;
	setp.ne.s32 	%p150, %r333, 0;
	or.pred  	%p151, %p149, %p150;
	@%p151 bra 	$L__BB34_101;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r407}, %fd24;
	}
	setp.lt.s32 	%p217, %r407, 0;
	and.pred  	%p216, %p217, %p21;
	setp.gt.s32 	%p152, %r5, -1;
	selp.b32 	%r335, 2146435072, 0, %p152;
	mov.u32 	%r336, 0;
	setp.ne.s32 	%p153, %r17, 1071644672;
	and.pred  	%p154, %p153, %p216;
	or.b32  	%r337, %r335, -2147483648;
	selp.b32 	%r338, %r337, %r335, %p154;
	mov.b64 	%fd832, {%r336, %r338};

$L__BB34_101:
	mul.f64 	%fd456, %fd24, %fd24;
	mul.f64 	%fd457, %fd28, 0d3FE0000000000000;
	mov.f64 	%fd458, 0d3FE0000000000000;
	mul.f64 	%fd459, %fd32, 0d3FE0000000000000;
	mul.f64 	%fd460, %fd24, 0d3FE0000000000000;
	div.rn.f64 	%fd461, %fd832, 0d4008000000000000;
	mov.f64 	%fd462, 0d4008000000000000;
	setp.eq.f64 	%p158, %fd24, 0d3FF0000000000000;
	mov.f64 	%fd463, 0d3FF0000000000000;
	selp.f64 	%fd464, 0d3FD5555555555555, %fd461, %p158;
	mul.f64 	%fd465, %fd839, %fd464;
	mul.f64 	%fd466, %fd456, 0d3FE0000000000000;
	mul.f64 	%fd467, %fd28, %fd466;
	mul.f64 	%fd468, %fd839, %fd467;
	mul.f64 	%fd469, %fd466, %fd32;
	mul.f64 	%fd470, %fd839, %fd469;
	mul.f64 	%fd471, %fd24, %fd28;
	mul.f64 	%fd472, %fd28, %fd471;
	mul.f64 	%fd473, %fd839, %fd472;
	mul.f64 	%fd474, %fd471, %fd32;
	mul.f64 	%fd475, %fd839, %fd474;
	mul.f64 	%fd476, %fd24, %fd32;
	mul.f64 	%fd477, %fd32, %fd476;
	mul.f64 	%fd478, %fd839, %fd477;
	mul.f64 	%fd479, %fd839, %fd115;
	fma.rn.f64 	%fd126, %fd71, %fd479, %fd94;
	div.rn.f64 	%fd480, %fd24, 0d4008000000000000;
	sub.f64 	%fd481, %fd458, %fd480;
	sub.f64 	%fd482, %fd481, %fd457;
	sub.f64 	%fd483, %fd482, %fd459;
	mul.f64 	%fd484, %fd456, %fd483;
	mul.f64 	%fd485, %fd839, %fd484;
	fma.rn.f64 	%fd127, %fd71, %fd485, %fd95;
	mul.f64 	%fd486, %fd28, %fd28;
	div.rn.f64 	%fd487, %fd28, 0d4008000000000000;
	sub.f64 	%fd488, %fd458, %fd487;
	sub.f64 	%fd489, %fd488, %fd460;
	sub.f64 	%fd490, %fd489, %fd459;
	mul.f64 	%fd491, %fd486, %fd490;
	mul.f64 	%fd492, %fd840, %fd491;
	fma.rn.f64 	%fd128, %fd71, %fd492, %fd96;
	mul.f64 	%fd493, %fd32, %fd32;
	div.rn.f64 	%fd494, %fd32, 0d4008000000000000;
	sub.f64 	%fd495, %fd458, %fd494;
	sub.f64 	%fd496, %fd495, %fd460;
	sub.f64 	%fd497, %fd496, %fd457;
	mul.f64 	%fd498, %fd493, %fd497;
	mul.f64 	%fd499, %fd841, %fd498;
	fma.rn.f64 	%fd129, %fd71, %fd499, %fd97;
	fma.rn.f64 	%fd130, %fd71, %fd465, %fd98;
	fma.rn.f64 	%fd131, %fd71, %fd468, %fd99;
	fma.rn.f64 	%fd132, %fd71, %fd470, %fd100;
	fma.rn.f64 	%fd133, %fd71, %fd473, %fd101;
	fma.rn.f64 	%fd134, %fd71, %fd475, %fd102;
	fma.rn.f64 	%fd135, %fd71, %fd478, %fd103;
	sub.f64 	%fd500, %fd463, %fd25;
	sub.f64 	%fd501, %fd500, %fd29;
	sub.f64 	%fd136, %fd501, %fd33;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd136;
	}
	abs.f64 	%fd137, %fd136;
	{ // callseq 61, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd137;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd462;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd835, [retval0+0];
	} // callseq 61
	setp.lt.s32 	%p159, %r18, 0;
	and.pred  	%p7, %p159, %p21;
	not.pred 	%p161, %p7;
	@%p161 bra 	$L__BB34_103;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r344}, %fd835;
	}
	xor.b32  	%r345, %r344, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r346, %temp}, %fd835;
	}
	mov.b64 	%fd835, {%r346, %r345};

$L__BB34_103:
	setp.eq.f64 	%p162, %fd136, 0d0000000000000000;
	@%p162 bra 	$L__BB34_107;
	bra.uni 	$L__BB34_104;

$L__BB34_107:
	selp.b32 	%r347, %r18, 0, %p21;
	mov.u32 	%r348, 0;
	or.b32  	%r349, %r347, 2146435072;
	setp.lt.s32 	%p166, %r5, 0;
	selp.b32 	%r350, %r349, %r347, %p166;
	mov.b64 	%fd835, {%r348, %r350};
	bra.uni 	$L__BB34_108;

$L__BB34_104:
	setp.gt.s32 	%p163, %r18, -1;
	@%p163 bra 	$L__BB34_108;

	mov.f64 	%fd502, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd503, %fd502;
	setp.eq.f64 	%p164, %fd503, 0d4008000000000000;
	@%p164 bra 	$L__BB34_108;

	mov.f64 	%fd835, 0dFFF8000000000000;

$L__BB34_108:
	add.f64 	%fd505, %fd136, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r351}, %fd505;
	}
	and.b32  	%r352, %r351, 2146435072;
	setp.ne.s32 	%p167, %r352, 2146435072;
	@%p167 bra 	$L__BB34_115;

	setp.gtu.f64 	%p168, %fd137, 0d7FF0000000000000;
	@%p168 bra 	$L__BB34_114;
	bra.uni 	$L__BB34_110;

$L__BB34_114:
	mov.f64 	%fd507, 0d4008000000000000;
	add.rn.f64 	%fd835, %fd136, %fd507;
	bra.uni 	$L__BB34_115;

$L__BB34_110:
	mov.f64 	%fd506, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r353, %temp}, %fd506;
	}
	and.b32  	%r19, %r5, 2147483647;
	setp.eq.s32 	%p169, %r19, 2146435072;
	setp.eq.s32 	%p170, %r353, 0;
	and.pred  	%p171, %p169, %p170;
	@%p171 bra 	$L__BB34_113;
	bra.uni 	$L__BB34_111;

$L__BB34_113:
	setp.gt.f64 	%p178, %fd137, 0d3FF0000000000000;
	selp.b32 	%r360, 2146435072, 0, %p178;
	mov.u32 	%r361, 0;
	xor.b32  	%r362, %r360, 2146435072;
	setp.lt.s32 	%p179, %r5, 0;
	selp.b32 	%r363, %r362, %r360, %p179;
	setp.eq.f64 	%p180, %fd136, 0dBFF0000000000000;
	selp.b32 	%r364, 1072693248, %r363, %p180;
	mov.b64 	%fd835, {%r361, %r364};
	bra.uni 	$L__BB34_115;

$L__BB34_111:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r354, %temp}, %fd136;
	}
	and.b32  	%r355, %r18, 2147483647;
	setp.ne.s32 	%p172, %r355, 2146435072;
	setp.ne.s32 	%p173, %r354, 0;
	or.pred  	%p174, %p172, %p173;
	@%p174 bra 	$L__BB34_115;

	setp.gt.s32 	%p175, %r5, -1;
	selp.b32 	%r356, 2146435072, 0, %p175;
	mov.u32 	%r357, 0;
	setp.ne.s32 	%p176, %r19, 1071644672;
	and.pred  	%p177, %p176, %p7;
	or.b32  	%r358, %r356, -2147483648;
	selp.b32 	%r359, %r358, %r356, %p177;
	mov.b64 	%fd835, {%r357, %r359};

$L__BB34_115:
	mov.f64 	%fd508, 0d0000000000000000;
	sub.f64 	%fd509, %fd508, %fd835;
	div.rn.f64 	%fd510, %fd509, 0d4008000000000000;
	mov.f64 	%fd511, 0d4008000000000000;
	setp.eq.f64 	%p181, %fd136, 0d3FF0000000000000;
	selp.f64 	%fd147, 0dBFD5555555555555, %fd510, %p181;
	abs.f64 	%fd148, %fd25;
	{ // callseq 62, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd148;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd511;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd838, [retval0+0];
	} // callseq 62
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd25;
	}
	setp.lt.s32 	%p182, %r20, 0;
	and.pred  	%p8, %p182, %p21;
	not.pred 	%p184, %p8;
	@%p184 bra 	$L__BB34_117;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r365}, %fd838;
	}
	xor.b32  	%r366, %r365, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r367, %temp}, %fd838;
	}
	mov.b64 	%fd838, {%r367, %r366};

$L__BB34_117:
	setp.eq.f64 	%p185, %fd25, 0d0000000000000000;
	@%p185 bra 	$L__BB34_121;
	bra.uni 	$L__BB34_118;

$L__BB34_121:
	selp.b32 	%r368, %r20, 0, %p21;
	mov.u32 	%r369, 0;
	or.b32  	%r370, %r368, 2146435072;
	setp.lt.s32 	%p189, %r5, 0;
	selp.b32 	%r371, %r370, %r368, %p189;
	mov.b64 	%fd838, {%r369, %r371};
	bra.uni 	$L__BB34_122;

$L__BB34_118:
	setp.gt.s32 	%p186, %r20, -1;
	@%p186 bra 	$L__BB34_122;

	mov.f64 	%fd512, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd513, %fd512;
	setp.eq.f64 	%p187, %fd513, 0d4008000000000000;
	@%p187 bra 	$L__BB34_122;

	mov.f64 	%fd838, 0dFFF8000000000000;

$L__BB34_122:
	add.f64 	%fd515, %fd25, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r372}, %fd515;
	}
	and.b32  	%r373, %r372, 2146435072;
	setp.ne.s32 	%p190, %r373, 2146435072;
	@%p190 bra 	$L__BB34_129;

	setp.gtu.f64 	%p191, %fd148, 0d7FF0000000000000;
	@%p191 bra 	$L__BB34_128;
	bra.uni 	$L__BB34_124;

$L__BB34_128:
	mov.f64 	%fd517, 0d4008000000000000;
	add.rn.f64 	%fd838, %fd25, %fd517;
	bra.uni 	$L__BB34_129;

$L__BB34_124:
	mov.f64 	%fd516, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r374, %temp}, %fd516;
	}
	and.b32  	%r21, %r5, 2147483647;
	setp.eq.s32 	%p192, %r21, 2146435072;
	setp.eq.s32 	%p193, %r374, 0;
	and.pred  	%p194, %p192, %p193;
	@%p194 bra 	$L__BB34_127;
	bra.uni 	$L__BB34_125;

$L__BB34_127:
	setp.gt.f64 	%p201, %fd148, 0d3FF0000000000000;
	selp.b32 	%r381, 2146435072, 0, %p201;
	mov.u32 	%r382, 0;
	xor.b32  	%r383, %r381, 2146435072;
	setp.lt.s32 	%p202, %r5, 0;
	selp.b32 	%r384, %r383, %r381, %p202;
	setp.eq.f64 	%p203, %fd25, 0dBFF0000000000000;
	selp.b32 	%r385, 1072693248, %r384, %p203;
	mov.b64 	%fd838, {%r382, %r385};
	bra.uni 	$L__BB34_129;

$L__BB34_125:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r375, %temp}, %fd25;
	}
	and.b32  	%r376, %r20, 2147483647;
	setp.ne.s32 	%p195, %r376, 2146435072;
	setp.ne.s32 	%p196, %r375, 0;
	or.pred  	%p197, %p195, %p196;
	@%p197 bra 	$L__BB34_129;

	setp.gt.s32 	%p198, %r5, -1;
	selp.b32 	%r377, 2146435072, 0, %p198;
	mov.u32 	%r378, 0;
	setp.ne.s32 	%p199, %r21, 1071644672;
	and.pred  	%p200, %p199, %p8;
	or.b32  	%r379, %r377, -2147483648;
	selp.b32 	%r380, %r379, %r377, %p200;
	mov.b64 	%fd838, {%r378, %r380};

$L__BB34_129:
	mul.f64 	%fd518, %fd25, %fd25;
	mul.f64 	%fd519, %fd29, 0d3FE0000000000000;
	mov.f64 	%fd520, 0d3FE0000000000000;
	mul.f64 	%fd521, %fd33, 0d3FE0000000000000;
	mul.f64 	%fd522, %fd25, 0d3FE0000000000000;
	div.rn.f64 	%fd523, %fd838, 0d4008000000000000;
	setp.eq.f64 	%p204, %fd25, 0d3FF0000000000000;
	selp.f64 	%fd524, 0d3FD5555555555555, %fd523, %p204;
	mul.f64 	%fd525, %fd839, %fd524;
	mul.f64 	%fd526, %fd518, 0d3FE0000000000000;
	mul.f64 	%fd527, %fd29, %fd526;
	mul.f64 	%fd528, %fd839, %fd527;
	mul.f64 	%fd529, %fd526, %fd33;
	mul.f64 	%fd530, %fd839, %fd529;
	mul.f64 	%fd531, %fd25, %fd29;
	mul.f64 	%fd532, %fd29, %fd531;
	mul.f64 	%fd533, %fd839, %fd532;
	mul.f64 	%fd534, %fd531, %fd33;
	mul.f64 	%fd535, %fd839, %fd534;
	mul.f64 	%fd536, %fd25, %fd33;
	mul.f64 	%fd537, %fd33, %fd536;
	mul.f64 	%fd538, %fd839, %fd537;
	mul.f64 	%fd539, %fd839, %fd147;
	fma.rn.f64 	%fd852, %fd71, %fd539, %fd126;
	div.rn.f64 	%fd540, %fd25, 0d4008000000000000;
	sub.f64 	%fd541, %fd520, %fd540;
	sub.f64 	%fd542, %fd541, %fd519;
	sub.f64 	%fd543, %fd542, %fd521;
	mul.f64 	%fd544, %fd518, %fd543;
	mul.f64 	%fd545, %fd839, %fd544;
	fma.rn.f64 	%fd853, %fd71, %fd545, %fd127;
	mul.f64 	%fd546, %fd29, %fd29;
	div.rn.f64 	%fd547, %fd29, 0d4008000000000000;
	sub.f64 	%fd548, %fd520, %fd547;
	sub.f64 	%fd549, %fd548, %fd522;
	sub.f64 	%fd550, %fd549, %fd521;
	mul.f64 	%fd551, %fd546, %fd550;
	mul.f64 	%fd552, %fd840, %fd551;
	fma.rn.f64 	%fd854, %fd71, %fd552, %fd128;
	mul.f64 	%fd553, %fd33, %fd33;
	div.rn.f64 	%fd554, %fd33, 0d4008000000000000;
	sub.f64 	%fd555, %fd520, %fd554;
	sub.f64 	%fd556, %fd555, %fd522;
	sub.f64 	%fd557, %fd556, %fd519;
	mul.f64 	%fd558, %fd553, %fd557;
	mul.f64 	%fd559, %fd841, %fd558;
	fma.rn.f64 	%fd855, %fd71, %fd559, %fd129;
	fma.rn.f64 	%fd857, %fd71, %fd525, %fd130;
	fma.rn.f64 	%fd858, %fd71, %fd528, %fd131;
	fma.rn.f64 	%fd859, %fd71, %fd530, %fd132;
	fma.rn.f64 	%fd862, %fd71, %fd533, %fd133;
	fma.rn.f64 	%fd863, %fd71, %fd535, %fd134;
	fma.rn.f64 	%fd867, %fd71, %fd538, %fd135;

$L__BB34_130:
	setp.ne.s32 	%p205, %r3, 0;
	selp.f64 	%fd181, %fd18, %fd839, %p19;
	selp.f64 	%fd182, %fd19, %fd840, %p19;
	selp.f64 	%fd183, %fd20, %fd841, %p19;
	mov.f64 	%fd856, %fd853;
	mov.f64 	%fd860, %fd854;
	mov.f64 	%fd861, %fd858;
	mov.f64 	%fd864, %fd855;
	mov.f64 	%fd865, %fd859;
	mov.f64 	%fd866, %fd863;
	@%p205 bra 	$L__BB34_132;

	mul.f64 	%fd804, %fd244, 0d3FE0000000000000;
	mul.f64 	%fd560, %fd4, %fd804;
	mul.f64 	%fd561, %fd182, %fd182;
	fma.rn.f64 	%fd562, %fd181, %fd181, %fd561;
	fma.rn.f64 	%fd563, %fd183, %fd183, %fd562;
	sqrt.rn.f64 	%fd564, %fd563;
	div.rn.f64 	%fd565, %fd181, %fd564;
	div.rn.f64 	%fd566, %fd182, %fd564;
	div.rn.f64 	%fd567, %fd183, %fd564;
	mul.f64 	%fd568, %fd4, %fd565;
	mul.f64 	%fd569, %fd4, %fd566;
	mul.f64 	%fd570, %fd4, %fd567;
	div.rn.f64 	%fd571, %fd568, 0d400BB67AE8584CAA;
	div.rn.f64 	%fd572, %fd569, 0d400BB67AE8584CAA;
	div.rn.f64 	%fd573, %fd570, 0d400BB67AE8584CAA;
	sub.f64 	%fd574, %fd6, %fd1;
	add.f64 	%fd575, %fd574, %fd574;
	div.rn.f64 	%fd576, %fd575, 0d4008000000000000;
	sub.f64 	%fd577, %fd7, %fd1;
	div.rn.f64 	%fd578, %fd577, 0d4018000000000000;
	add.f64 	%fd579, %fd576, %fd578;
	sub.f64 	%fd580, %fd12, %fd1;
	div.rn.f64 	%fd581, %fd580, 0d4018000000000000;
	add.f64 	%fd582, %fd579, %fd581;
	sub.f64 	%fd583, %fd582, %fd571;
	fma.rn.f64 	%fd584, %fd574, 0d4018000000000000, %fd578;
	add.f64 	%fd585, %fd580, %fd580;
	div.rn.f64 	%fd586, %fd585, 0d4008000000000000;
	add.f64 	%fd587, %fd584, %fd586;
	sub.f64 	%fd588, %fd587, %fd571;
	div.rn.f64 	%fd589, %fd574, 0d4018000000000000;
	add.f64 	%fd590, %fd577, %fd577;
	div.rn.f64 	%fd591, %fd590, 0d4008000000000000;
	add.f64 	%fd592, %fd589, %fd591;
	add.f64 	%fd593, %fd581, %fd592;
	sub.f64 	%fd594, %fd593, %fd571;
	add.f64 	%fd595, %fd571, %fd582;
	add.f64 	%fd596, %fd571, %fd587;
	add.f64 	%fd597, %fd571, %fd593;
	sub.f64 	%fd598, %fd8, %fd2;
	add.f64 	%fd599, %fd598, %fd598;
	div.rn.f64 	%fd600, %fd599, 0d4008000000000000;
	sub.f64 	%fd601, %fd9, %fd2;
	div.rn.f64 	%fd602, %fd601, 0d4018000000000000;
	add.f64 	%fd603, %fd600, %fd602;
	sub.f64 	%fd604, %fd13, %fd2;
	div.rn.f64 	%fd605, %fd604, 0d4018000000000000;
	add.f64 	%fd606, %fd603, %fd605;
	sub.f64 	%fd607, %fd606, %fd572;
	fma.rn.f64 	%fd608, %fd598, 0d4018000000000000, %fd602;
	add.f64 	%fd609, %fd604, %fd604;
	div.rn.f64 	%fd610, %fd609, 0d4008000000000000;
	add.f64 	%fd611, %fd608, %fd610;
	sub.f64 	%fd612, %fd611, %fd572;
	div.rn.f64 	%fd613, %fd598, 0d4018000000000000;
	add.f64 	%fd614, %fd601, %fd601;
	div.rn.f64 	%fd615, %fd614, 0d4008000000000000;
	add.f64 	%fd616, %fd613, %fd615;
	add.f64 	%fd617, %fd605, %fd616;
	sub.f64 	%fd618, %fd617, %fd572;
	add.f64 	%fd619, %fd572, %fd606;
	add.f64 	%fd620, %fd572, %fd611;
	add.f64 	%fd621, %fd572, %fd617;
	sub.f64 	%fd622, %fd10, %fd3;
	add.f64 	%fd623, %fd622, %fd622;
	div.rn.f64 	%fd624, %fd623, 0d4008000000000000;
	sub.f64 	%fd625, %fd11, %fd3;
	div.rn.f64 	%fd626, %fd625, 0d4018000000000000;
	add.f64 	%fd627, %fd624, %fd626;
	sub.f64 	%fd628, %fd14, %fd3;
	div.rn.f64 	%fd629, %fd628, 0d4018000000000000;
	add.f64 	%fd630, %fd627, %fd629;
	sub.f64 	%fd631, %fd630, %fd573;
	fma.rn.f64 	%fd632, %fd622, 0d4018000000000000, %fd626;
	add.f64 	%fd633, %fd628, %fd628;
	div.rn.f64 	%fd634, %fd633, 0d4008000000000000;
	add.f64 	%fd635, %fd632, %fd634;
	sub.f64 	%fd636, %fd635, %fd573;
	div.rn.f64 	%fd637, %fd622, 0d4018000000000000;
	add.f64 	%fd638, %fd625, %fd625;
	div.rn.f64 	%fd639, %fd638, 0d4008000000000000;
	add.f64 	%fd640, %fd637, %fd639;
	add.f64 	%fd641, %fd629, %fd640;
	sub.f64 	%fd642, %fd641, %fd573;
	add.f64 	%fd643, %fd573, %fd630;
	add.f64 	%fd644, %fd573, %fd635;
	add.f64 	%fd645, %fd573, %fd641;
	mul.f64 	%fd646, %fd5, %fd560;
	div.rn.f64 	%fd647, %fd646, 0d4018000000000000;
	mov.f64 	%fd648, 0d3FF0000000000000;
	sub.f64 	%fd649, %fd648, %fd583;
	sub.f64 	%fd650, %fd649, %fd607;
	sub.f64 	%fd651, %fd650, %fd631;
	mul.f64 	%fd652, %fd647, %fd651;
	fma.rn.f64 	%fd653, %fd651, %fd652, %fd852;
	mul.f64 	%fd654, %fd647, %fd583;
	fma.rn.f64 	%fd655, %fd654, %fd651, %fd853;
	mul.f64 	%fd656, %fd647, %fd607;
	fma.rn.f64 	%fd657, %fd656, %fd651, %fd854;
	mul.f64 	%fd658, %fd647, %fd631;
	fma.rn.f64 	%fd659, %fd658, %fd651, %fd855;
	fma.rn.f64 	%fd660, %fd583, %fd652, %fd853;
	fma.rn.f64 	%fd661, %fd583, %fd654, %fd857;
	fma.rn.f64 	%fd662, %fd583, %fd656, %fd858;
	fma.rn.f64 	%fd663, %fd583, %fd658, %fd859;
	fma.rn.f64 	%fd664, %fd607, %fd652, %fd854;
	fma.rn.f64 	%fd665, %fd654, %fd607, %fd858;
	fma.rn.f64 	%fd666, %fd607, %fd656, %fd862;
	fma.rn.f64 	%fd667, %fd607, %fd658, %fd863;
	fma.rn.f64 	%fd668, %fd631, %fd652, %fd855;
	fma.rn.f64 	%fd669, %fd654, %fd631, %fd859;
	fma.rn.f64 	%fd670, %fd656, %fd631, %fd863;
	fma.rn.f64 	%fd671, %fd631, %fd658, %fd867;
	sub.f64 	%fd672, %fd648, %fd588;
	sub.f64 	%fd673, %fd672, %fd612;
	sub.f64 	%fd674, %fd673, %fd636;
	mul.f64 	%fd675, %fd647, %fd674;
	fma.rn.f64 	%fd676, %fd674, %fd675, %fd653;
	mul.f64 	%fd677, %fd647, %fd588;
	fma.rn.f64 	%fd678, %fd677, %fd674, %fd655;
	mul.f64 	%fd679, %fd647, %fd612;
	fma.rn.f64 	%fd680, %fd679, %fd674, %fd657;
	mul.f64 	%fd681, %fd647, %fd636;
	fma.rn.f64 	%fd682, %fd681, %fd674, %fd659;
	fma.rn.f64 	%fd683, %fd588, %fd675, %fd660;
	fma.rn.f64 	%fd684, %fd588, %fd677, %fd661;
	fma.rn.f64 	%fd685, %fd588, %fd679, %fd662;
	fma.rn.f64 	%fd686, %fd588, %fd681, %fd663;
	fma.rn.f64 	%fd687, %fd612, %fd675, %fd664;
	fma.rn.f64 	%fd688, %fd677, %fd612, %fd665;
	fma.rn.f64 	%fd689, %fd612, %fd679, %fd666;
	fma.rn.f64 	%fd690, %fd612, %fd681, %fd667;
	fma.rn.f64 	%fd691, %fd636, %fd675, %fd668;
	fma.rn.f64 	%fd692, %fd677, %fd636, %fd669;
	fma.rn.f64 	%fd693, %fd679, %fd636, %fd670;
	fma.rn.f64 	%fd694, %fd636, %fd681, %fd671;
	sub.f64 	%fd695, %fd648, %fd594;
	sub.f64 	%fd696, %fd695, %fd618;
	sub.f64 	%fd697, %fd696, %fd642;
	mul.f64 	%fd698, %fd647, %fd697;
	fma.rn.f64 	%fd699, %fd697, %fd698, %fd676;
	mul.f64 	%fd700, %fd647, %fd594;
	fma.rn.f64 	%fd701, %fd700, %fd697, %fd678;
	mul.f64 	%fd702, %fd647, %fd618;
	fma.rn.f64 	%fd703, %fd702, %fd697, %fd680;
	mul.f64 	%fd704, %fd647, %fd642;
	fma.rn.f64 	%fd705, %fd704, %fd697, %fd682;
	fma.rn.f64 	%fd706, %fd594, %fd698, %fd683;
	fma.rn.f64 	%fd707, %fd594, %fd700, %fd684;
	fma.rn.f64 	%fd708, %fd594, %fd702, %fd685;
	fma.rn.f64 	%fd709, %fd594, %fd704, %fd686;
	fma.rn.f64 	%fd710, %fd618, %fd698, %fd687;
	fma.rn.f64 	%fd711, %fd700, %fd618, %fd688;
	fma.rn.f64 	%fd712, %fd618, %fd702, %fd689;
	fma.rn.f64 	%fd713, %fd618, %fd704, %fd690;
	fma.rn.f64 	%fd714, %fd642, %fd698, %fd691;
	fma.rn.f64 	%fd715, %fd700, %fd642, %fd692;
	fma.rn.f64 	%fd716, %fd702, %fd642, %fd693;
	fma.rn.f64 	%fd717, %fd642, %fd704, %fd694;
	sub.f64 	%fd718, %fd648, %fd595;
	sub.f64 	%fd719, %fd718, %fd619;
	sub.f64 	%fd720, %fd719, %fd643;
	mul.f64 	%fd721, %fd647, %fd720;
	fma.rn.f64 	%fd722, %fd720, %fd721, %fd699;
	mul.f64 	%fd723, %fd647, %fd595;
	fma.rn.f64 	%fd724, %fd723, %fd720, %fd701;
	mul.f64 	%fd725, %fd647, %fd619;
	fma.rn.f64 	%fd726, %fd725, %fd720, %fd703;
	mul.f64 	%fd727, %fd647, %fd643;
	fma.rn.f64 	%fd728, %fd727, %fd720, %fd705;
	fma.rn.f64 	%fd729, %fd595, %fd721, %fd706;
	fma.rn.f64 	%fd730, %fd595, %fd723, %fd707;
	fma.rn.f64 	%fd731, %fd595, %fd725, %fd708;
	fma.rn.f64 	%fd732, %fd595, %fd727, %fd709;
	fma.rn.f64 	%fd733, %fd619, %fd721, %fd710;
	fma.rn.f64 	%fd734, %fd723, %fd619, %fd711;
	fma.rn.f64 	%fd735, %fd619, %fd725, %fd712;
	fma.rn.f64 	%fd736, %fd619, %fd727, %fd713;
	fma.rn.f64 	%fd737, %fd643, %fd721, %fd714;
	fma.rn.f64 	%fd738, %fd723, %fd643, %fd715;
	fma.rn.f64 	%fd739, %fd725, %fd643, %fd716;
	fma.rn.f64 	%fd740, %fd643, %fd727, %fd717;
	sub.f64 	%fd741, %fd648, %fd596;
	sub.f64 	%fd742, %fd741, %fd620;
	sub.f64 	%fd743, %fd742, %fd644;
	mul.f64 	%fd744, %fd647, %fd743;
	fma.rn.f64 	%fd745, %fd743, %fd744, %fd722;
	mul.f64 	%fd746, %fd647, %fd596;
	fma.rn.f64 	%fd747, %fd746, %fd743, %fd724;
	mul.f64 	%fd748, %fd647, %fd620;
	fma.rn.f64 	%fd749, %fd748, %fd743, %fd726;
	mul.f64 	%fd750, %fd647, %fd644;
	fma.rn.f64 	%fd751, %fd750, %fd743, %fd728;
	fma.rn.f64 	%fd752, %fd596, %fd744, %fd729;
	fma.rn.f64 	%fd753, %fd596, %fd746, %fd730;
	fma.rn.f64 	%fd754, %fd596, %fd748, %fd731;
	fma.rn.f64 	%fd755, %fd596, %fd750, %fd732;
	fma.rn.f64 	%fd756, %fd620, %fd744, %fd733;
	fma.rn.f64 	%fd757, %fd746, %fd620, %fd734;
	fma.rn.f64 	%fd758, %fd620, %fd748, %fd735;
	fma.rn.f64 	%fd759, %fd620, %fd750, %fd736;
	fma.rn.f64 	%fd760, %fd644, %fd744, %fd737;
	fma.rn.f64 	%fd761, %fd746, %fd644, %fd738;
	fma.rn.f64 	%fd762, %fd748, %fd644, %fd739;
	fma.rn.f64 	%fd763, %fd644, %fd750, %fd740;
	sub.f64 	%fd764, %fd648, %fd597;
	sub.f64 	%fd765, %fd764, %fd621;
	sub.f64 	%fd766, %fd765, %fd645;
	mul.f64 	%fd767, %fd647, %fd766;
	fma.rn.f64 	%fd852, %fd766, %fd767, %fd745;
	mul.f64 	%fd768, %fd647, %fd597;
	fma.rn.f64 	%fd853, %fd768, %fd766, %fd747;
	mul.f64 	%fd769, %fd647, %fd621;
	fma.rn.f64 	%fd854, %fd769, %fd766, %fd749;
	mul.f64 	%fd770, %fd647, %fd645;
	fma.rn.f64 	%fd855, %fd770, %fd766, %fd751;
	fma.rn.f64 	%fd856, %fd597, %fd767, %fd752;
	fma.rn.f64 	%fd857, %fd597, %fd768, %fd753;
	fma.rn.f64 	%fd858, %fd597, %fd769, %fd754;
	fma.rn.f64 	%fd859, %fd597, %fd770, %fd755;
	fma.rn.f64 	%fd860, %fd621, %fd767, %fd756;
	fma.rn.f64 	%fd861, %fd768, %fd621, %fd757;
	fma.rn.f64 	%fd862, %fd621, %fd769, %fd758;
	fma.rn.f64 	%fd863, %fd621, %fd770, %fd759;
	fma.rn.f64 	%fd864, %fd645, %fd767, %fd760;
	fma.rn.f64 	%fd865, %fd768, %fd645, %fd761;
	fma.rn.f64 	%fd866, %fd769, %fd645, %fd762;
	fma.rn.f64 	%fd867, %fd645, %fd770, %fd763;

$L__BB34_132:
	ld.param.u64 	%rd113, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_11];
	mul.lo.s64 	%rd112, %rd39, %rd27;
	add.s64 	%rd96, %rd113, %rd112;
	// begin inline asm
	{ atom.add.f64 %fd771,[%rd96],%fd852; }

	// end inline asm
	add.s64 	%rd97, %rd96, 8;
	// begin inline asm
	{ atom.add.f64 %fd773,[%rd97],%fd853; }

	// end inline asm
	add.s64 	%rd98, %rd96, 16;
	// begin inline asm
	{ atom.add.f64 %fd775,[%rd98],%fd854; }

	// end inline asm
	add.s64 	%rd99, %rd96, 24;
	// begin inline asm
	{ atom.add.f64 %fd777,[%rd99],%fd855; }

	// end inline asm
	add.s64 	%rd100, %rd96, 32;
	// begin inline asm
	{ atom.add.f64 %fd779,[%rd100],%fd856; }

	// end inline asm
	add.s64 	%rd101, %rd96, 40;
	// begin inline asm
	{ atom.add.f64 %fd781,[%rd101],%fd857; }

	// end inline asm
	add.s64 	%rd102, %rd96, 48;
	// begin inline asm
	{ atom.add.f64 %fd783,[%rd102],%fd858; }

	// end inline asm
	add.s64 	%rd103, %rd96, 56;
	// begin inline asm
	{ atom.add.f64 %fd785,[%rd103],%fd859; }

	// end inline asm
	add.s64 	%rd104, %rd96, 64;
	// begin inline asm
	{ atom.add.f64 %fd787,[%rd104],%fd860; }

	// end inline asm
	add.s64 	%rd105, %rd96, 72;
	// begin inline asm
	{ atom.add.f64 %fd789,[%rd105],%fd861; }

	// end inline asm
	add.s64 	%rd106, %rd96, 80;
	// begin inline asm
	{ atom.add.f64 %fd791,[%rd106],%fd862; }

	// end inline asm
	add.s64 	%rd107, %rd96, 88;
	// begin inline asm
	{ atom.add.f64 %fd793,[%rd107],%fd863; }

	// end inline asm
	add.s64 	%rd108, %rd96, 96;
	// begin inline asm
	{ atom.add.f64 %fd795,[%rd108],%fd864; }

	// end inline asm
	add.s64 	%rd109, %rd96, 104;
	// begin inline asm
	{ atom.add.f64 %fd797,[%rd109],%fd865; }

	// end inline asm
	add.s64 	%rd110, %rd96, 112;
	// begin inline asm
	{ atom.add.f64 %fd799,[%rd110],%fd866; }

	// end inline asm
	add.s64 	%rd111, %rd96, 120;
	// begin inline asm
	{ atom.add.f64 %fd801,[%rd111],%fd867; }

	// end inline asm

$L__BB34_133:
	ld.param.u64 	%rd114, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_0+24];
	add.s64 	%rd115, %rd115, %rd19;
	setp.lt.u64 	%p207, %rd115, %rd114;
	@%p207 bra 	$L__BB34_2;

$L__BB34_134:
	ret;

}
	// .globl	init_affine_mass_matrix_kernel_cuda_kernel_backward
.visible .entry init_affine_mass_matrix_kernel_cuda_kernel_backward(
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_0[32],
	.param .u32 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_1,
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_11[56],
	.param .u32 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_12,
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_14[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_15[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_16[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_17[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_18[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_19[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_20[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_21[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_22[56]
)
{
	.reg .pred 	%p<423>;
	.reg .b16 	%rs<121>;
	.reg .b32 	%r<707>;
	.reg .f64 	%fd<5763>;
	.reg .b64 	%rd<183>;


	ld.param.v2.u32 	{%r207, %r208}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r209, %r210}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r215, %r216}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r223, %r224}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r231, %r232}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r239, %r240}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r247, %r248}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r255, %r256}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r263, %r264}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r271, %r272}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r279, %r280}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r287, %r288}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r295, %r296}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_13+32];
	ld.param.v2.u32 	{%r303, %r304}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_19+32];
	ld.param.v2.u32 	{%r311, %r312}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_20+32];
	ld.param.v2.u32 	{%r319, %r320}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_21+32];
	ld.param.v2.u32 	{%r327, %r328}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_22+32];
	ld.param.u64 	%rd82, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_22];
	ld.param.u64 	%rd80, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_21];
	ld.param.u64 	%rd78, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_20];
	ld.param.u64 	%rd76, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_19];
	ld.param.u64 	%rd74, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_13];
	ld.param.u64 	%rd73, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_11+8];
	ld.param.u64 	%rd71, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_10+8];
	ld.param.u64 	%rd70, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd69, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_9+8];
	ld.param.u64 	%rd68, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd67, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_8+8];
	ld.param.u64 	%rd66, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd64, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd62, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd60, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd58, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd56, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd55, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd54, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd53, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r70, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_0+16];
	mov.u32 	%r331, %ntid.x;
	mov.u32 	%r332, %ctaid.x;
	mul.wide.u32 	%rd84, %r331, %r332;
	mov.u32 	%r333, %tid.x;
	cvt.u64.u32 	%rd85, %r333;
	add.s64 	%rd179, %rd84, %rd85;
	setp.ge.u64 	%p17, %rd179, %rd53;
	@%p17 bra 	$L__BB35_284;

	cvta.to.global.u64 	%rd13, %rd70;
	cvta.to.global.u64 	%rd14, %rd68;
	cvta.to.global.u64 	%rd15, %rd66;
	cvta.to.global.u64 	%rd16, %rd64;
	cvta.to.global.u64 	%rd17, %rd62;
	cvta.to.global.u64 	%rd18, %rd60;
	cvta.to.global.u64 	%rd19, %rd58;
	cvta.to.global.u64 	%rd20, %rd56;
	cvta.to.global.u64 	%rd21, %rd54;
	cvt.s64.s32 	%rd22, %r210;
	cvt.s64.s32 	%rd23, %r209;
	cvt.s64.s32 	%rd24, %r208;
	cvt.s64.s32 	%rd25, %r231;
	cvt.s64.s32 	%rd26, %r239;
	cvt.s64.s32 	%rd27, %r247;
	cvt.s64.s32 	%rd28, %r255;
	cvt.s64.s32 	%rd29, %r279;
	cvt.s64.s32 	%rd30, %r263;
	cvt.s64.s32 	%rd31, %r271;
	cvt.s64.s32 	%rd32, %r223;
	cvt.s64.s32 	%rd33, %r215;
	cvt.s64.s32 	%rd34, %r327;
	cvt.s64.s32 	%rd35, %r287;
	cvt.s64.s32 	%rd36, %r295;
	cvt.s64.s32 	%rd37, %r311;
	cvt.s64.s32 	%rd38, %r303;
	cvt.s64.s32 	%rd39, %r319;

$L__BB35_2:
	setp.lt.s32 	%p18, %r70, 4;
	mov.u64 	%rd180, %rd179;
	@%p18 bra 	$L__BB35_6;

	or.b64  	%rd86, %rd179, %rd22;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.eq.s64 	%p19, %rd87, 0;
	@%p19 bra 	$L__BB35_5;

	div.u64 	%rd180, %rd179, %rd22;
	bra.uni 	$L__BB35_6;

$L__BB35_5:
	cvt.u32.u64 	%r335, %rd22;
	cvt.u32.u64 	%r336, %rd179;
	div.u32 	%r337, %r336, %r335;
	cvt.u64.u32 	%rd180, %r337;

$L__BB35_6:
	setp.lt.s32 	%p20, %r70, 3;
	@%p20 bra 	$L__BB35_10;

	or.b64  	%rd88, %rd180, %rd23;
	and.b64  	%rd89, %rd88, -4294967296;
	setp.eq.s64 	%p21, %rd89, 0;
	@%p21 bra 	$L__BB35_9;

	div.u64 	%rd180, %rd180, %rd23;
	bra.uni 	$L__BB35_10;

$L__BB35_9:
	cvt.u32.u64 	%r338, %rd23;
	cvt.u32.u64 	%r339, %rd180;
	div.u32 	%r340, %r339, %r338;
	cvt.u64.u32 	%rd180, %r340;

$L__BB35_10:
	setp.lt.s32 	%p22, %r70, 2;
	@%p22 bra 	$L__BB35_14;

	or.b64  	%rd90, %rd180, %rd24;
	and.b64  	%rd91, %rd90, -4294967296;
	setp.eq.s64 	%p23, %rd91, 0;
	@%p23 bra 	$L__BB35_13;

	div.u64 	%rd180, %rd180, %rd24;
	bra.uni 	$L__BB35_14;

$L__BB35_13:
	cvt.u32.u64 	%r341, %rd24;
	cvt.u32.u64 	%r342, %rd180;
	div.u32 	%r343, %r342, %r341;
	cvt.u64.u32 	%rd180, %r343;

$L__BB35_14:
	ld.param.u32 	%r686, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_1];
	cvt.u32.u64 	%r344, %rd180;
	setp.gt.s32 	%p24, %r70, 0;
	selp.b32 	%r12, %r344, 0, %p24;
	setp.ge.s32 	%p25, %r12, %r686;
	mov.f64 	%fd5091, 0d0000000000000000;
	mov.f64 	%fd5092, 0d0000000000000000;
	mov.f64 	%fd5093, 0d0000000000000000;
	mov.f64 	%fd5095, 0d0000000000000000;
	mov.f64 	%fd5096, 0d0000000000000000;
	mov.f64 	%fd5097, 0d0000000000000000;
	mov.f64 	%fd5099, 0d0000000000000000;
	mov.f64 	%fd5100, 0d0000000000000000;
	mov.f64 	%fd5101, 0d0000000000000000;
	mov.f64 	%fd5102, 0d0000000000000000;
	mov.f64 	%fd5103, 0d0000000000000000;
	mov.f64 	%fd5104, 0d0000000000000000;
	mov.f64 	%fd5105, 0d0000000000000000;
	mov.f64 	%fd5106, 0d0000000000000000;
	mov.f64 	%fd5107, 0d0000000000000000;
	mov.f64 	%fd5108, 0d0000000000000000;
	mov.f64 	%fd5109, 0d0000000000000000;
	mov.f64 	%fd5110, 0d0000000000000000;
	mov.f64 	%fd5111, 0d0000000000000000;
	mov.f64 	%fd5112, 0d0000000000000000;
	mov.f64 	%fd5113, 0d0000000000000000;
	mov.f64 	%fd5114, 0d0000000000000000;
	mov.f64 	%fd5115, 0d0000000000000000;
	mov.f64 	%fd5116, 0d0000000000000000;
	mov.f64 	%fd5117, 0d0000000000000000;
	mov.f64 	%fd5118, 0d0000000000000000;
	mov.f64 	%fd5119, 0d0000000000000000;
	mov.f64 	%fd5120, 0d0000000000000000;
	mov.f64 	%fd5121, 0d0000000000000000;
	mov.f64 	%fd5122, 0d0000000000000000;
	@%p25 bra 	$L__BB35_132;

	cvt.s64.s32 	%rd50, %r12;
	mul.lo.s64 	%rd92, %rd50, %rd25;
	add.s64 	%rd93, %rd19, %rd92;
	ld.global.u32 	%r13, [%rd93];
	cvt.s64.s32 	%rd51, %r13;
	mul.lo.s64 	%rd94, %rd51, %rd26;
	add.s64 	%rd95, %rd18, %rd94;
	ld.global.u32 	%r14, [%rd95];
	setp.eq.s32 	%p26, %r14, 0;
	mov.u32 	%r706, 0;
	@%p26 bra 	$L__BB35_132;

	mul.lo.s64 	%rd96, %rd51, %rd27;
	add.s64 	%rd97, %rd17, %rd96;
	mul.lo.s64 	%rd98, %rd51, %rd28;
	add.s64 	%rd99, %rd16, %rd98;
	ld.global.u32 	%r697, [%rd99];
	mul.lo.s64 	%rd100, %rd51, %rd29;
	add.s64 	%rd101, %rd13, %rd100;
	ld.global.f64 	%fd557, [%rd101];
	ld.global.f64 	%fd558, [%rd101+8];
	ld.global.f64 	%fd559, [%rd101+16];
	mul.lo.s64 	%rd102, %rd51, %rd30;
	add.s64 	%rd103, %rd15, %rd102;
	ld.global.f64 	%fd5123, [%rd103];
	mul.lo.s64 	%rd104, %rd51, %rd31;
	add.s64 	%rd105, %rd14, %rd104;
	ld.global.f64 	%fd5124, [%rd105];
	mul.lo.s64 	%rd106, %rd50, %rd32;
	add.s64 	%rd107, %rd20, %rd106;
	ld.global.u32 	%r702, [%rd107];
	cvt.s64.s32 	%rd108, %r702;
	mul.lo.s64 	%rd109, %rd108, %rd33;
	add.s64 	%rd110, %rd21, %rd109;
	ld.global.u32 	%r703, [%rd107+4];
	cvt.s64.s32 	%rd111, %r703;
	mul.lo.s64 	%rd112, %rd111, %rd33;
	add.s64 	%rd113, %rd21, %rd112;
	ld.global.u32 	%r704, [%rd107+8];
	cvt.s64.s32 	%rd114, %r704;
	mul.lo.s64 	%rd115, %rd114, %rd33;
	add.s64 	%rd116, %rd21, %rd115;
	ld.global.f64 	%fd562, [%rd110];
	ld.global.f64 	%fd563, [%rd113];
	sub.f64 	%fd5120, %fd563, %fd562;
	ld.global.f64 	%fd565, [%rd110+8];
	ld.global.f64 	%fd566, [%rd113+8];
	sub.f64 	%fd5121, %fd566, %fd565;
	ld.global.f64 	%fd568, [%rd110+16];
	ld.global.f64 	%fd569, [%rd113+16];
	sub.f64 	%fd5122, %fd569, %fd568;
	ld.global.f64 	%fd571, [%rd116];
	sub.f64 	%fd5117, %fd571, %fd562;
	ld.global.f64 	%fd573, [%rd116+8];
	sub.f64 	%fd5118, %fd573, %fd565;
	ld.global.f64 	%fd575, [%rd116+16];
	sub.f64 	%fd5119, %fd575, %fd568;
	mul.f64 	%fd2148, %fd5121, %fd5119;
	mul.f64 	%fd2149, %fd5122, %fd5118;
	sub.f64 	%fd5114, %fd2148, %fd2149;
	mul.f64 	%fd2150, %fd5122, %fd5117;
	mul.f64 	%fd2151, %fd5120, %fd5119;
	sub.f64 	%fd5115, %fd2150, %fd2151;
	mul.f64 	%fd2152, %fd5120, %fd5118;
	mul.f64 	%fd2153, %fd5121, %fd5117;
	sub.f64 	%fd5116, %fd2152, %fd2153;
	mul.f64 	%fd2154, %fd5115, %fd5115;
	fma.rn.f64 	%fd2155, %fd5114, %fd5114, %fd2154;
	fma.rn.f64 	%fd2156, %fd5116, %fd5116, %fd2155;
	sqrt.rn.f64 	%fd5125, %fd2156;
	mul.f64 	%fd5126, %fd5125, 0d3FE0000000000000;
	ld.global.u32 	%r698, [%rd97];
	setp.eq.s32 	%p27, %r698, 0;
	mov.f64 	%fd4812, 0d0000000000000000;
	mov.f64 	%fd4813, %fd4812;
	mov.f64 	%fd4814, %fd4812;
	mov.f64 	%fd5099, %fd4812;
	mov.f64 	%fd5100, %fd4812;
	mov.f64 	%fd5101, %fd4812;
	mov.f64 	%fd5102, %fd4812;
	mov.f64 	%fd5103, %fd4812;
	mov.f64 	%fd5104, %fd4812;
	mov.f64 	%fd5105, %fd4812;
	mov.f64 	%fd5106, %fd4812;
	mov.f64 	%fd5107, %fd4812;
	mov.f64 	%fd5108, %fd4812;
	mov.f64 	%fd5109, %fd4812;
	mov.f64 	%fd5110, %fd4812;
	mov.f64 	%fd5111, %fd4812;
	mov.f64 	%fd5112, %fd4812;
	mov.f64 	%fd5113, %fd4812;
	@%p27 bra 	$L__BB35_130;

	neg.f64 	%fd2157, %fd5114;
	setp.eq.s32 	%p28, %r697, 0;
	neg.f64 	%fd2158, %fd5115;
	neg.f64 	%fd2159, %fd5116;
	selp.f64 	%fd2160, 0d0000000000000000, %fd2157, %p28;
	selp.f64 	%fd2161, 0d0000000000000000, %fd2158, %p28;
	selp.f64 	%fd2162, 0d0000000000000000, %fd2159, %p28;
	sub.f64 	%fd2163, %fd562, %fd557;
	div.rn.f64 	%fd2164, %fd2163, 0d4008000000000000;
	mov.f64 	%fd2165, 0d4008000000000000;
	sub.f64 	%fd2166, %fd563, %fd557;
	div.rn.f64 	%fd2167, %fd2166, 0d4008000000000000;
	add.f64 	%fd2168, %fd2164, %fd2167;
	sub.f64 	%fd2169, %fd571, %fd557;
	div.rn.f64 	%fd2170, %fd2169, 0d4008000000000000;
	add.f64 	%fd5186, %fd2168, %fd2170;
	mul.f64 	%fd2171, %fd2163, 0d4008000000000000;
	div.rn.f64 	%fd2172, %fd2171, 0d4014000000000000;
	div.rn.f64 	%fd2173, %fd2166, 0d4014000000000000;
	add.f64 	%fd2174, %fd2172, %fd2173;
	div.rn.f64 	%fd2175, %fd2169, 0d4014000000000000;
	add.f64 	%fd5251, %fd2174, %fd2175;
	div.rn.f64 	%fd2176, %fd2163, 0d4014000000000000;
	add.f64 	%fd2177, %fd2176, %fd2173;
	mul.f64 	%fd2178, %fd2169, 0d4008000000000000;
	div.rn.f64 	%fd2179, %fd2178, 0d4014000000000000;
	add.f64 	%fd5316, %fd2177, %fd2179;
	mul.f64 	%fd2180, %fd2166, 0d4008000000000000;
	div.rn.f64 	%fd2181, %fd2180, 0d4014000000000000;
	add.f64 	%fd2182, %fd2176, %fd2181;
	add.f64 	%fd5380, %fd2175, %fd2182;
	sub.f64 	%fd2183, %fd565, %fd558;
	div.rn.f64 	%fd2184, %fd2183, 0d4008000000000000;
	sub.f64 	%fd2185, %fd566, %fd558;
	div.rn.f64 	%fd2186, %fd2185, 0d4008000000000000;
	add.f64 	%fd2187, %fd2184, %fd2186;
	sub.f64 	%fd2188, %fd573, %fd558;
	div.rn.f64 	%fd2189, %fd2188, 0d4008000000000000;
	add.f64 	%fd5180, %fd2187, %fd2189;
	mul.f64 	%fd2190, %fd2183, 0d4008000000000000;
	div.rn.f64 	%fd2191, %fd2190, 0d4014000000000000;
	div.rn.f64 	%fd2192, %fd2185, 0d4014000000000000;
	add.f64 	%fd2193, %fd2191, %fd2192;
	div.rn.f64 	%fd2194, %fd2188, 0d4014000000000000;
	add.f64 	%fd5245, %fd2193, %fd2194;
	div.rn.f64 	%fd2195, %fd2183, 0d4014000000000000;
	add.f64 	%fd2196, %fd2195, %fd2192;
	mul.f64 	%fd2197, %fd2188, 0d4008000000000000;
	div.rn.f64 	%fd2198, %fd2197, 0d4014000000000000;
	add.f64 	%fd5310, %fd2196, %fd2198;
	mul.f64 	%fd2199, %fd2185, 0d4008000000000000;
	div.rn.f64 	%fd2200, %fd2199, 0d4014000000000000;
	add.f64 	%fd2201, %fd2195, %fd2200;
	add.f64 	%fd5374, %fd2194, %fd2201;
	sub.f64 	%fd2202, %fd568, %fd559;
	div.rn.f64 	%fd2203, %fd2202, 0d4008000000000000;
	sub.f64 	%fd2204, %fd569, %fd559;
	div.rn.f64 	%fd2205, %fd2204, 0d4008000000000000;
	add.f64 	%fd2206, %fd2203, %fd2205;
	sub.f64 	%fd2207, %fd575, %fd559;
	div.rn.f64 	%fd2208, %fd2207, 0d4008000000000000;
	add.f64 	%fd5189, %fd2206, %fd2208;
	mul.f64 	%fd2209, %fd2202, 0d4008000000000000;
	div.rn.f64 	%fd2210, %fd2209, 0d4014000000000000;
	div.rn.f64 	%fd2211, %fd2204, 0d4014000000000000;
	add.f64 	%fd2212, %fd2210, %fd2211;
	div.rn.f64 	%fd2213, %fd2207, 0d4014000000000000;
	add.f64 	%fd5254, %fd2212, %fd2213;
	div.rn.f64 	%fd2214, %fd2202, 0d4014000000000000;
	add.f64 	%fd2215, %fd2214, %fd2211;
	mul.f64 	%fd2216, %fd2207, 0d4008000000000000;
	div.rn.f64 	%fd2217, %fd2216, 0d4014000000000000;
	add.f64 	%fd5319, %fd2215, %fd2217;
	mul.f64 	%fd2218, %fd2204, 0d4008000000000000;
	div.rn.f64 	%fd2219, %fd2218, 0d4014000000000000;
	add.f64 	%fd2220, %fd2214, %fd2219;
	add.f64 	%fd5383, %fd2213, %fd2220;
	add.f64 	%fd5127, %fd5126, %fd5126;
	selp.f64 	%fd5101, %fd5116, %fd2162, %p28;
	selp.f64 	%fd5100, %fd5115, %fd2161, %p28;
	selp.f64 	%fd5099, %fd5114, %fd2160, %p28;
	div.rn.f64 	%fd4812, %fd5099, %fd5127;
	div.rn.f64 	%fd4813, %fd5100, %fd5127;
	div.rn.f64 	%fd4814, %fd5101, %fd5127;
	mov.f64 	%fd2221, 0d3FF0000000000000;
	sub.f64 	%fd2222, %fd2221, %fd5186;
	sub.f64 	%fd2223, %fd2222, %fd5180;
	sub.f64 	%fd5129, %fd2223, %fd5189;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd5129;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd2165;
	}
	and.b32  	%r22, %r21, 2146435072;
	setp.eq.s32 	%p29, %r22, 1073741824;
	abs.f64 	%fd605, %fd5129;
	{ // callseq 63, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd605;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd2165;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd4789, [retval0+0];
	} // callseq 63
	setp.lt.s32 	%p30, %r20, 0;
	and.pred  	%p1, %p30, %p29;
	not.pred 	%p31, %p1;
	@%p31 bra 	$L__BB35_19;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r346}, %fd4789;
	}
	xor.b32  	%r347, %r346, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r348, %temp}, %fd4789;
	}
	mov.b64 	%fd4789, {%r348, %r347};

$L__BB35_19:
	setp.eq.f64 	%p32, %fd5129, 0d0000000000000000;
	@%p32 bra 	$L__BB35_23;
	bra.uni 	$L__BB35_20;

$L__BB35_23:
	selp.b32 	%r349, %r20, 0, %p29;
	mov.u32 	%r350, 0;
	or.b32  	%r351, %r349, 2146435072;
	setp.lt.s32 	%p36, %r21, 0;
	selp.b32 	%r352, %r351, %r349, %p36;
	mov.b64 	%fd4789, {%r350, %r352};
	bra.uni 	$L__BB35_24;

$L__BB35_20:
	setp.gt.s32 	%p33, %r20, -1;
	@%p33 bra 	$L__BB35_24;

	mov.f64 	%fd2224, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2225, %fd2224;
	setp.eq.f64 	%p34, %fd2225, 0d4008000000000000;
	@%p34 bra 	$L__BB35_24;

	mov.f64 	%fd4789, 0dFFF8000000000000;

$L__BB35_24:
	add.f64 	%fd2227, %fd5129, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r353}, %fd2227;
	}
	and.b32  	%r354, %r353, 2146435072;
	setp.ne.s32 	%p37, %r354, 2146435072;
	@%p37 bra 	$L__BB35_31;

	setp.gtu.f64 	%p38, %fd605, 0d7FF0000000000000;
	@%p38 bra 	$L__BB35_30;
	bra.uni 	$L__BB35_26;

$L__BB35_30:
	mov.f64 	%fd2229, 0d4008000000000000;
	add.rn.f64 	%fd4789, %fd5129, %fd2229;
	bra.uni 	$L__BB35_31;

$L__BB35_26:
	mov.f64 	%fd2228, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r355, %temp}, %fd2228;
	}
	and.b32  	%r23, %r21, 2147483647;
	setp.eq.s32 	%p39, %r23, 2146435072;
	setp.eq.s32 	%p40, %r355, 0;
	and.pred  	%p41, %p39, %p40;
	@%p41 bra 	$L__BB35_29;
	bra.uni 	$L__BB35_27;

$L__BB35_29:
	setp.gt.f64 	%p48, %fd605, 0d3FF0000000000000;
	selp.b32 	%r362, 2146435072, 0, %p48;
	mov.u32 	%r363, 0;
	xor.b32  	%r364, %r362, 2146435072;
	setp.lt.s32 	%p49, %r21, 0;
	selp.b32 	%r365, %r364, %r362, %p49;
	setp.eq.f64 	%p50, %fd5129, 0dBFF0000000000000;
	selp.b32 	%r366, 1072693248, %r365, %p50;
	mov.b64 	%fd4789, {%r363, %r366};
	bra.uni 	$L__BB35_31;

$L__BB35_27:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r356, %temp}, %fd5129;
	}
	and.b32  	%r357, %r20, 2147483647;
	setp.ne.s32 	%p42, %r357, 2146435072;
	setp.ne.s32 	%p43, %r356, 0;
	or.pred  	%p44, %p42, %p43;
	@%p44 bra 	$L__BB35_31;

	setp.gt.s32 	%p45, %r21, -1;
	selp.b32 	%r358, 2146435072, 0, %p45;
	mov.u32 	%r359, 0;
	setp.ne.s32 	%p46, %r23, 1071644672;
	and.pred  	%p47, %p46, %p1;
	or.b32  	%r360, %r358, -2147483648;
	selp.b32 	%r361, %r360, %r358, %p47;
	mov.b64 	%fd4789, {%r359, %r361};

$L__BB35_31:
	mov.f64 	%fd2230, 0d0000000000000000;
	sub.f64 	%fd2231, %fd2230, %fd4789;
	div.rn.f64 	%fd2232, %fd2231, 0d4008000000000000;
	mov.f64 	%fd2233, 0d4008000000000000;
	setp.eq.f64 	%p51, %fd5129, 0d3FF0000000000000;
	selp.f64 	%fd5130, 0dBFD5555555555555, %fd2232, %p51;
	div.rn.f64 	%fd2234, %fd5186, 0d4008000000000000;
	mov.f64 	%fd2235, 0d3FE0000000000000;
	sub.f64 	%fd2236, %fd2235, %fd2234;
	mul.f64 	%fd2237, %fd5180, 0d3FE0000000000000;
	sub.f64 	%fd2238, %fd2236, %fd2237;
	mul.f64 	%fd2239, %fd5189, 0d3FE0000000000000;
	sub.f64 	%fd5136, %fd2238, %fd2239;
	mul.f64 	%fd5135, %fd5186, %fd5186;
	mul.f64 	%fd5137, %fd5135, %fd5136;
	div.rn.f64 	%fd2240, %fd5180, 0d4008000000000000;
	sub.f64 	%fd2241, %fd2235, %fd2240;
	mul.f64 	%fd2242, %fd5186, 0d3FE0000000000000;
	sub.f64 	%fd2243, %fd2241, %fd2242;
	sub.f64 	%fd5143, %fd2243, %fd2239;
	mul.f64 	%fd5142, %fd5180, %fd5180;
	mul.f64 	%fd5144, %fd5142, %fd5143;
	div.rn.f64 	%fd2244, %fd5189, 0d4008000000000000;
	sub.f64 	%fd2245, %fd2235, %fd2244;
	sub.f64 	%fd2246, %fd2245, %fd2242;
	sub.f64 	%fd5150, %fd2246, %fd2237;
	mul.f64 	%fd5149, %fd5189, %fd5189;
	mul.f64 	%fd5151, %fd5149, %fd5150;
	abs.f64 	%fd625, %fd5186;
	{ // callseq 64, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd625;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd2233;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd4792, [retval0+0];
	} // callseq 64
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r24}, %fd5186;
	}
	setp.lt.s32 	%p52, %r24, 0;
	and.pred  	%p2, %p52, %p29;
	not.pred 	%p54, %p2;
	@%p54 bra 	$L__BB35_33;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r367}, %fd4792;
	}
	xor.b32  	%r368, %r367, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r369, %temp}, %fd4792;
	}
	mov.b64 	%fd4792, {%r369, %r368};

$L__BB35_33:
	setp.eq.f64 	%p55, %fd5186, 0d0000000000000000;
	@%p55 bra 	$L__BB35_37;
	bra.uni 	$L__BB35_34;

$L__BB35_37:
	selp.b32 	%r370, %r24, 0, %p29;
	mov.u32 	%r371, 0;
	or.b32  	%r372, %r370, 2146435072;
	setp.lt.s32 	%p59, %r21, 0;
	selp.b32 	%r373, %r372, %r370, %p59;
	mov.b64 	%fd4792, {%r371, %r373};
	bra.uni 	$L__BB35_38;

$L__BB35_34:
	setp.gt.s32 	%p56, %r24, -1;
	@%p56 bra 	$L__BB35_38;

	mov.f64 	%fd2247, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2248, %fd2247;
	setp.eq.f64 	%p57, %fd2248, 0d4008000000000000;
	@%p57 bra 	$L__BB35_38;

	mov.f64 	%fd4792, 0dFFF8000000000000;

$L__BB35_38:
	add.f64 	%fd2250, %fd5186, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r374}, %fd2250;
	}
	and.b32  	%r375, %r374, 2146435072;
	setp.ne.s32 	%p60, %r375, 2146435072;
	@%p60 bra 	$L__BB35_45;

	setp.gtu.f64 	%p61, %fd625, 0d7FF0000000000000;
	@%p61 bra 	$L__BB35_44;
	bra.uni 	$L__BB35_40;

$L__BB35_44:
	mov.f64 	%fd2252, 0d4008000000000000;
	add.rn.f64 	%fd4792, %fd5186, %fd2252;
	bra.uni 	$L__BB35_45;

$L__BB35_40:
	mov.f64 	%fd2251, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r376, %temp}, %fd2251;
	}
	and.b32  	%r25, %r21, 2147483647;
	setp.eq.s32 	%p62, %r25, 2146435072;
	setp.eq.s32 	%p63, %r376, 0;
	and.pred  	%p64, %p62, %p63;
	@%p64 bra 	$L__BB35_43;
	bra.uni 	$L__BB35_41;

$L__BB35_43:
	setp.gt.f64 	%p71, %fd625, 0d3FF0000000000000;
	selp.b32 	%r383, 2146435072, 0, %p71;
	mov.u32 	%r384, 0;
	xor.b32  	%r385, %r383, 2146435072;
	setp.lt.s32 	%p72, %r21, 0;
	selp.b32 	%r386, %r385, %r383, %p72;
	setp.eq.f64 	%p73, %fd5186, 0dBFF0000000000000;
	selp.b32 	%r387, 1072693248, %r386, %p73;
	mov.b64 	%fd4792, {%r384, %r387};
	bra.uni 	$L__BB35_45;

$L__BB35_41:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r377, %temp}, %fd5186;
	}
	and.b32  	%r378, %r24, 2147483647;
	setp.ne.s32 	%p65, %r378, 2146435072;
	setp.ne.s32 	%p66, %r377, 0;
	or.pred  	%p67, %p65, %p66;
	@%p67 bra 	$L__BB35_45;

	setp.gt.s32 	%p68, %r21, -1;
	selp.b32 	%r379, 2146435072, 0, %p68;
	mov.u32 	%r380, 0;
	setp.ne.s32 	%p69, %r25, 1071644672;
	and.pred  	%p70, %p69, %p2;
	or.b32  	%r381, %r379, -2147483648;
	selp.b32 	%r382, %r381, %r379, %p70;
	mov.b64 	%fd4792, {%r380, %r382};

$L__BB35_45:
	div.rn.f64 	%fd2253, %fd4792, 0d4008000000000000;
	mov.f64 	%fd2254, 0d4008000000000000;
	setp.eq.f64 	%p74, %fd5186, 0d3FF0000000000000;
	mov.f64 	%fd2255, 0d3FF0000000000000;
	selp.f64 	%fd5155, 0d3FD5555555555555, %fd2253, %p74;
	mul.f64 	%fd5167, %fd5135, 0d3FE0000000000000;
	mul.f64 	%fd5162, %fd5180, %fd5167;
	mul.f64 	%fd5169, %fd5167, %fd5189;
	mul.f64 	%fd5181, %fd5186, %fd5180;
	mul.f64 	%fd5176, %fd5180, %fd5181;
	mul.f64 	%fd5183, %fd5181, %fd5189;
	mul.f64 	%fd5188, %fd5186, %fd5189;
	mul.f64 	%fd5190, %fd5189, %fd5188;
	sub.f64 	%fd2256, %fd2255, %fd5251;
	sub.f64 	%fd2257, %fd2256, %fd5245;
	sub.f64 	%fd5194, %fd2257, %fd5254;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r26}, %fd5194;
	}
	abs.f64 	%fd645, %fd5194;
	{ // callseq 65, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd645;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd2254;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd4795, [retval0+0];
	} // callseq 65
	setp.lt.s32 	%p75, %r26, 0;
	and.pred  	%p3, %p75, %p29;
	not.pred 	%p77, %p3;
	@%p77 bra 	$L__BB35_47;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r388}, %fd4795;
	}
	xor.b32  	%r389, %r388, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r390, %temp}, %fd4795;
	}
	mov.b64 	%fd4795, {%r390, %r389};

$L__BB35_47:
	setp.eq.f64 	%p78, %fd5194, 0d0000000000000000;
	@%p78 bra 	$L__BB35_51;
	bra.uni 	$L__BB35_48;

$L__BB35_51:
	selp.b32 	%r391, %r26, 0, %p29;
	mov.u32 	%r392, 0;
	or.b32  	%r393, %r391, 2146435072;
	setp.lt.s32 	%p82, %r21, 0;
	selp.b32 	%r394, %r393, %r391, %p82;
	mov.b64 	%fd4795, {%r392, %r394};
	bra.uni 	$L__BB35_52;

$L__BB35_48:
	setp.gt.s32 	%p79, %r26, -1;
	@%p79 bra 	$L__BB35_52;

	mov.f64 	%fd2258, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2259, %fd2258;
	setp.eq.f64 	%p80, %fd2259, 0d4008000000000000;
	@%p80 bra 	$L__BB35_52;

	mov.f64 	%fd4795, 0dFFF8000000000000;

$L__BB35_52:
	add.f64 	%fd2261, %fd5194, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r395}, %fd2261;
	}
	and.b32  	%r396, %r395, 2146435072;
	setp.ne.s32 	%p83, %r396, 2146435072;
	@%p83 bra 	$L__BB35_59;

	setp.gtu.f64 	%p84, %fd645, 0d7FF0000000000000;
	@%p84 bra 	$L__BB35_58;
	bra.uni 	$L__BB35_54;

$L__BB35_58:
	mov.f64 	%fd2263, 0d4008000000000000;
	add.rn.f64 	%fd4795, %fd5194, %fd2263;
	bra.uni 	$L__BB35_59;

$L__BB35_54:
	mov.f64 	%fd2262, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r397, %temp}, %fd2262;
	}
	and.b32  	%r27, %r21, 2147483647;
	setp.eq.s32 	%p85, %r27, 2146435072;
	setp.eq.s32 	%p86, %r397, 0;
	and.pred  	%p87, %p85, %p86;
	@%p87 bra 	$L__BB35_57;
	bra.uni 	$L__BB35_55;

$L__BB35_57:
	setp.gt.f64 	%p94, %fd645, 0d3FF0000000000000;
	selp.b32 	%r404, 2146435072, 0, %p94;
	mov.u32 	%r405, 0;
	xor.b32  	%r406, %r404, 2146435072;
	setp.lt.s32 	%p95, %r21, 0;
	selp.b32 	%r407, %r406, %r404, %p95;
	setp.eq.f64 	%p96, %fd5194, 0dBFF0000000000000;
	selp.b32 	%r408, 1072693248, %r407, %p96;
	mov.b64 	%fd4795, {%r405, %r408};
	bra.uni 	$L__BB35_59;

$L__BB35_55:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r398, %temp}, %fd5194;
	}
	and.b32  	%r399, %r26, 2147483647;
	setp.ne.s32 	%p88, %r399, 2146435072;
	setp.ne.s32 	%p89, %r398, 0;
	or.pred  	%p90, %p88, %p89;
	@%p90 bra 	$L__BB35_59;

	setp.gt.s32 	%p91, %r21, -1;
	selp.b32 	%r400, 2146435072, 0, %p91;
	mov.u32 	%r401, 0;
	setp.ne.s32 	%p92, %r27, 1071644672;
	and.pred  	%p93, %p92, %p3;
	or.b32  	%r402, %r400, -2147483648;
	selp.b32 	%r403, %r402, %r400, %p93;
	mov.b64 	%fd4795, {%r401, %r403};

$L__BB35_59:
	mov.f64 	%fd2264, 0d0000000000000000;
	sub.f64 	%fd2265, %fd2264, %fd4795;
	div.rn.f64 	%fd2266, %fd2265, 0d4008000000000000;
	mov.f64 	%fd2267, 0d4008000000000000;
	setp.eq.f64 	%p97, %fd5194, 0d3FF0000000000000;
	selp.f64 	%fd5195, 0dBFD5555555555555, %fd2266, %p97;
	div.rn.f64 	%fd2268, %fd5251, 0d4008000000000000;
	mov.f64 	%fd2269, 0d3FE0000000000000;
	sub.f64 	%fd2270, %fd2269, %fd2268;
	mul.f64 	%fd2271, %fd5245, 0d3FE0000000000000;
	sub.f64 	%fd2272, %fd2270, %fd2271;
	mul.f64 	%fd2273, %fd5254, 0d3FE0000000000000;
	sub.f64 	%fd5201, %fd2272, %fd2273;
	mul.f64 	%fd5200, %fd5251, %fd5251;
	mul.f64 	%fd5202, %fd5200, %fd5201;
	div.rn.f64 	%fd2274, %fd5245, 0d4008000000000000;
	sub.f64 	%fd2275, %fd2269, %fd2274;
	mul.f64 	%fd2276, %fd5251, 0d3FE0000000000000;
	sub.f64 	%fd2277, %fd2275, %fd2276;
	sub.f64 	%fd5208, %fd2277, %fd2273;
	mul.f64 	%fd5207, %fd5245, %fd5245;
	mul.f64 	%fd5209, %fd5207, %fd5208;
	div.rn.f64 	%fd2278, %fd5254, 0d4008000000000000;
	sub.f64 	%fd2279, %fd2269, %fd2278;
	sub.f64 	%fd2280, %fd2279, %fd2276;
	sub.f64 	%fd5215, %fd2280, %fd2271;
	mul.f64 	%fd5214, %fd5254, %fd5254;
	mul.f64 	%fd5216, %fd5214, %fd5215;
	abs.f64 	%fd665, %fd5251;
	{ // callseq 66, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd665;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd2267;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd4798, [retval0+0];
	} // callseq 66
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd5251;
	}
	setp.lt.s32 	%p98, %r28, 0;
	and.pred  	%p4, %p98, %p29;
	not.pred 	%p100, %p4;
	@%p100 bra 	$L__BB35_61;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r409}, %fd4798;
	}
	xor.b32  	%r410, %r409, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r411, %temp}, %fd4798;
	}
	mov.b64 	%fd4798, {%r411, %r410};

$L__BB35_61:
	setp.eq.f64 	%p101, %fd5251, 0d0000000000000000;
	@%p101 bra 	$L__BB35_65;
	bra.uni 	$L__BB35_62;

$L__BB35_65:
	selp.b32 	%r412, %r28, 0, %p29;
	mov.u32 	%r413, 0;
	or.b32  	%r414, %r412, 2146435072;
	setp.lt.s32 	%p105, %r21, 0;
	selp.b32 	%r415, %r414, %r412, %p105;
	mov.b64 	%fd4798, {%r413, %r415};
	bra.uni 	$L__BB35_66;

$L__BB35_62:
	setp.gt.s32 	%p102, %r28, -1;
	@%p102 bra 	$L__BB35_66;

	mov.f64 	%fd2281, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2282, %fd2281;
	setp.eq.f64 	%p103, %fd2282, 0d4008000000000000;
	@%p103 bra 	$L__BB35_66;

	mov.f64 	%fd4798, 0dFFF8000000000000;

$L__BB35_66:
	add.f64 	%fd2284, %fd5251, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r416}, %fd2284;
	}
	and.b32  	%r417, %r416, 2146435072;
	setp.ne.s32 	%p106, %r417, 2146435072;
	@%p106 bra 	$L__BB35_73;

	setp.gtu.f64 	%p107, %fd665, 0d7FF0000000000000;
	@%p107 bra 	$L__BB35_72;
	bra.uni 	$L__BB35_68;

$L__BB35_72:
	mov.f64 	%fd2286, 0d4008000000000000;
	add.rn.f64 	%fd4798, %fd5251, %fd2286;
	bra.uni 	$L__BB35_73;

$L__BB35_68:
	mov.f64 	%fd2285, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r418, %temp}, %fd2285;
	}
	and.b32  	%r29, %r21, 2147483647;
	setp.eq.s32 	%p108, %r29, 2146435072;
	setp.eq.s32 	%p109, %r418, 0;
	and.pred  	%p110, %p108, %p109;
	@%p110 bra 	$L__BB35_71;
	bra.uni 	$L__BB35_69;

$L__BB35_71:
	setp.gt.f64 	%p117, %fd665, 0d3FF0000000000000;
	selp.b32 	%r425, 2146435072, 0, %p117;
	mov.u32 	%r426, 0;
	xor.b32  	%r427, %r425, 2146435072;
	setp.lt.s32 	%p118, %r21, 0;
	selp.b32 	%r428, %r427, %r425, %p118;
	setp.eq.f64 	%p119, %fd5251, 0dBFF0000000000000;
	selp.b32 	%r429, 1072693248, %r428, %p119;
	mov.b64 	%fd4798, {%r426, %r429};
	bra.uni 	$L__BB35_73;

$L__BB35_69:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r419, %temp}, %fd5251;
	}
	and.b32  	%r420, %r28, 2147483647;
	setp.ne.s32 	%p111, %r420, 2146435072;
	setp.ne.s32 	%p112, %r419, 0;
	or.pred  	%p113, %p111, %p112;
	@%p113 bra 	$L__BB35_73;

	setp.gt.s32 	%p114, %r21, -1;
	selp.b32 	%r421, 2146435072, 0, %p114;
	mov.u32 	%r422, 0;
	setp.ne.s32 	%p115, %r29, 1071644672;
	and.pred  	%p116, %p115, %p4;
	or.b32  	%r423, %r421, -2147483648;
	selp.b32 	%r424, %r423, %r421, %p116;
	mov.b64 	%fd4798, {%r422, %r424};

$L__BB35_73:
	div.rn.f64 	%fd2287, %fd4798, 0d4008000000000000;
	mov.f64 	%fd2288, 0d4008000000000000;
	setp.eq.f64 	%p120, %fd5251, 0d3FF0000000000000;
	mov.f64 	%fd2289, 0d3FF0000000000000;
	selp.f64 	%fd5220, 0d3FD5555555555555, %fd2287, %p120;
	mul.f64 	%fd5232, %fd5200, 0d3FE0000000000000;
	mul.f64 	%fd5227, %fd5245, %fd5232;
	mul.f64 	%fd5234, %fd5232, %fd5254;
	mul.f64 	%fd5246, %fd5251, %fd5245;
	mul.f64 	%fd5241, %fd5245, %fd5246;
	mul.f64 	%fd5248, %fd5246, %fd5254;
	mul.f64 	%fd5253, %fd5251, %fd5254;
	mul.f64 	%fd5255, %fd5254, %fd5253;
	sub.f64 	%fd2290, %fd2289, %fd5316;
	sub.f64 	%fd2291, %fd2290, %fd5310;
	sub.f64 	%fd5259, %fd2291, %fd5319;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd5259;
	}
	abs.f64 	%fd685, %fd5259;
	{ // callseq 67, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd685;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd2288;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd4801, [retval0+0];
	} // callseq 67
	setp.lt.s32 	%p121, %r30, 0;
	and.pred  	%p5, %p121, %p29;
	not.pred 	%p123, %p5;
	@%p123 bra 	$L__BB35_75;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r430}, %fd4801;
	}
	xor.b32  	%r431, %r430, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r432, %temp}, %fd4801;
	}
	mov.b64 	%fd4801, {%r432, %r431};

$L__BB35_75:
	setp.eq.f64 	%p124, %fd5259, 0d0000000000000000;
	@%p124 bra 	$L__BB35_79;
	bra.uni 	$L__BB35_76;

$L__BB35_79:
	selp.b32 	%r433, %r30, 0, %p29;
	mov.u32 	%r434, 0;
	or.b32  	%r435, %r433, 2146435072;
	setp.lt.s32 	%p128, %r21, 0;
	selp.b32 	%r436, %r435, %r433, %p128;
	mov.b64 	%fd4801, {%r434, %r436};
	bra.uni 	$L__BB35_80;

$L__BB35_76:
	setp.gt.s32 	%p125, %r30, -1;
	@%p125 bra 	$L__BB35_80;

	mov.f64 	%fd2292, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2293, %fd2292;
	setp.eq.f64 	%p126, %fd2293, 0d4008000000000000;
	@%p126 bra 	$L__BB35_80;

	mov.f64 	%fd4801, 0dFFF8000000000000;

$L__BB35_80:
	add.f64 	%fd2295, %fd5259, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r437}, %fd2295;
	}
	and.b32  	%r438, %r437, 2146435072;
	setp.ne.s32 	%p129, %r438, 2146435072;
	@%p129 bra 	$L__BB35_87;

	setp.gtu.f64 	%p130, %fd685, 0d7FF0000000000000;
	@%p130 bra 	$L__BB35_86;
	bra.uni 	$L__BB35_82;

$L__BB35_86:
	mov.f64 	%fd2297, 0d4008000000000000;
	add.rn.f64 	%fd4801, %fd5259, %fd2297;
	bra.uni 	$L__BB35_87;

$L__BB35_82:
	mov.f64 	%fd2296, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r439, %temp}, %fd2296;
	}
	and.b32  	%r31, %r21, 2147483647;
	setp.eq.s32 	%p131, %r31, 2146435072;
	setp.eq.s32 	%p132, %r439, 0;
	and.pred  	%p133, %p131, %p132;
	@%p133 bra 	$L__BB35_85;
	bra.uni 	$L__BB35_83;

$L__BB35_85:
	setp.gt.f64 	%p140, %fd685, 0d3FF0000000000000;
	selp.b32 	%r446, 2146435072, 0, %p140;
	mov.u32 	%r447, 0;
	xor.b32  	%r448, %r446, 2146435072;
	setp.lt.s32 	%p141, %r21, 0;
	selp.b32 	%r449, %r448, %r446, %p141;
	setp.eq.f64 	%p142, %fd5259, 0dBFF0000000000000;
	selp.b32 	%r450, 1072693248, %r449, %p142;
	mov.b64 	%fd4801, {%r447, %r450};
	bra.uni 	$L__BB35_87;

$L__BB35_83:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r440, %temp}, %fd5259;
	}
	and.b32  	%r441, %r30, 2147483647;
	setp.ne.s32 	%p134, %r441, 2146435072;
	setp.ne.s32 	%p135, %r440, 0;
	or.pred  	%p136, %p134, %p135;
	@%p136 bra 	$L__BB35_87;

	setp.gt.s32 	%p137, %r21, -1;
	selp.b32 	%r442, 2146435072, 0, %p137;
	mov.u32 	%r443, 0;
	setp.ne.s32 	%p138, %r31, 1071644672;
	and.pred  	%p139, %p138, %p5;
	or.b32  	%r444, %r442, -2147483648;
	selp.b32 	%r445, %r444, %r442, %p139;
	mov.b64 	%fd4801, {%r443, %r445};

$L__BB35_87:
	mov.f64 	%fd2298, 0d0000000000000000;
	sub.f64 	%fd2299, %fd2298, %fd4801;
	div.rn.f64 	%fd2300, %fd2299, 0d4008000000000000;
	mov.f64 	%fd2301, 0d4008000000000000;
	setp.eq.f64 	%p143, %fd5259, 0d3FF0000000000000;
	selp.f64 	%fd5260, 0dBFD5555555555555, %fd2300, %p143;
	div.rn.f64 	%fd2302, %fd5316, 0d4008000000000000;
	mov.f64 	%fd2303, 0d3FE0000000000000;
	sub.f64 	%fd2304, %fd2303, %fd2302;
	mul.f64 	%fd2305, %fd5310, 0d3FE0000000000000;
	sub.f64 	%fd2306, %fd2304, %fd2305;
	mul.f64 	%fd2307, %fd5319, 0d3FE0000000000000;
	sub.f64 	%fd5266, %fd2306, %fd2307;
	mul.f64 	%fd5265, %fd5316, %fd5316;
	mul.f64 	%fd5267, %fd5265, %fd5266;
	div.rn.f64 	%fd2308, %fd5310, 0d4008000000000000;
	sub.f64 	%fd2309, %fd2303, %fd2308;
	mul.f64 	%fd2310, %fd5316, 0d3FE0000000000000;
	sub.f64 	%fd2311, %fd2309, %fd2310;
	sub.f64 	%fd5273, %fd2311, %fd2307;
	mul.f64 	%fd5272, %fd5310, %fd5310;
	mul.f64 	%fd5274, %fd5272, %fd5273;
	div.rn.f64 	%fd2312, %fd5319, 0d4008000000000000;
	sub.f64 	%fd2313, %fd2303, %fd2312;
	sub.f64 	%fd2314, %fd2313, %fd2310;
	sub.f64 	%fd5280, %fd2314, %fd2305;
	mul.f64 	%fd5279, %fd5319, %fd5319;
	mul.f64 	%fd5281, %fd5279, %fd5280;
	abs.f64 	%fd705, %fd5316;
	{ // callseq 68, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd705;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd2301;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd4804, [retval0+0];
	} // callseq 68
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd5316;
	}
	setp.lt.s32 	%p144, %r32, 0;
	and.pred  	%p6, %p144, %p29;
	not.pred 	%p146, %p6;
	@%p146 bra 	$L__BB35_89;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r451}, %fd4804;
	}
	xor.b32  	%r452, %r451, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r453, %temp}, %fd4804;
	}
	mov.b64 	%fd4804, {%r453, %r452};

$L__BB35_89:
	setp.eq.f64 	%p147, %fd5316, 0d0000000000000000;
	@%p147 bra 	$L__BB35_93;
	bra.uni 	$L__BB35_90;

$L__BB35_93:
	selp.b32 	%r454, %r32, 0, %p29;
	mov.u32 	%r455, 0;
	or.b32  	%r456, %r454, 2146435072;
	setp.lt.s32 	%p151, %r21, 0;
	selp.b32 	%r457, %r456, %r454, %p151;
	mov.b64 	%fd4804, {%r455, %r457};
	bra.uni 	$L__BB35_94;

$L__BB35_90:
	setp.gt.s32 	%p148, %r32, -1;
	@%p148 bra 	$L__BB35_94;

	mov.f64 	%fd2315, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2316, %fd2315;
	setp.eq.f64 	%p149, %fd2316, 0d4008000000000000;
	@%p149 bra 	$L__BB35_94;

	mov.f64 	%fd4804, 0dFFF8000000000000;

$L__BB35_94:
	add.f64 	%fd2318, %fd5316, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r458}, %fd2318;
	}
	and.b32  	%r459, %r458, 2146435072;
	setp.ne.s32 	%p152, %r459, 2146435072;
	@%p152 bra 	$L__BB35_101;

	setp.gtu.f64 	%p153, %fd705, 0d7FF0000000000000;
	@%p153 bra 	$L__BB35_100;
	bra.uni 	$L__BB35_96;

$L__BB35_100:
	mov.f64 	%fd2320, 0d4008000000000000;
	add.rn.f64 	%fd4804, %fd5316, %fd2320;
	bra.uni 	$L__BB35_101;

$L__BB35_96:
	mov.f64 	%fd2319, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r460, %temp}, %fd2319;
	}
	and.b32  	%r33, %r21, 2147483647;
	setp.eq.s32 	%p154, %r33, 2146435072;
	setp.eq.s32 	%p155, %r460, 0;
	and.pred  	%p156, %p154, %p155;
	@%p156 bra 	$L__BB35_99;
	bra.uni 	$L__BB35_97;

$L__BB35_99:
	setp.gt.f64 	%p163, %fd705, 0d3FF0000000000000;
	selp.b32 	%r467, 2146435072, 0, %p163;
	mov.u32 	%r468, 0;
	xor.b32  	%r469, %r467, 2146435072;
	setp.lt.s32 	%p164, %r21, 0;
	selp.b32 	%r470, %r469, %r467, %p164;
	setp.eq.f64 	%p165, %fd5316, 0dBFF0000000000000;
	selp.b32 	%r471, 1072693248, %r470, %p165;
	mov.b64 	%fd4804, {%r468, %r471};
	bra.uni 	$L__BB35_101;

$L__BB35_97:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r461, %temp}, %fd5316;
	}
	and.b32  	%r462, %r32, 2147483647;
	setp.ne.s32 	%p157, %r462, 2146435072;
	setp.ne.s32 	%p158, %r461, 0;
	or.pred  	%p159, %p157, %p158;
	@%p159 bra 	$L__BB35_101;

	setp.gt.s32 	%p160, %r21, -1;
	selp.b32 	%r463, 2146435072, 0, %p160;
	mov.u32 	%r464, 0;
	setp.ne.s32 	%p161, %r33, 1071644672;
	and.pred  	%p162, %p161, %p6;
	or.b32  	%r465, %r463, -2147483648;
	selp.b32 	%r466, %r465, %r463, %p162;
	mov.b64 	%fd4804, {%r464, %r466};

$L__BB35_101:
	div.rn.f64 	%fd2321, %fd4804, 0d4008000000000000;
	mov.f64 	%fd2322, 0d4008000000000000;
	setp.eq.f64 	%p166, %fd5316, 0d3FF0000000000000;
	mov.f64 	%fd2323, 0d3FF0000000000000;
	selp.f64 	%fd5285, 0d3FD5555555555555, %fd2321, %p166;
	mul.f64 	%fd5297, %fd5265, 0d3FE0000000000000;
	mul.f64 	%fd5292, %fd5310, %fd5297;
	mul.f64 	%fd5299, %fd5297, %fd5319;
	mul.f64 	%fd5311, %fd5316, %fd5310;
	mul.f64 	%fd5306, %fd5310, %fd5311;
	mul.f64 	%fd5313, %fd5311, %fd5319;
	mul.f64 	%fd5318, %fd5316, %fd5319;
	mul.f64 	%fd5320, %fd5319, %fd5318;
	sub.f64 	%fd2324, %fd2323, %fd5380;
	sub.f64 	%fd2325, %fd2324, %fd5374;
	sub.f64 	%fd5323, %fd2325, %fd5383;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r34}, %fd5323;
	}
	abs.f64 	%fd725, %fd5323;
	{ // callseq 69, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd725;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd2322;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd4807, [retval0+0];
	} // callseq 69
	setp.lt.s32 	%p167, %r34, 0;
	and.pred  	%p7, %p167, %p29;
	not.pred 	%p169, %p7;
	@%p169 bra 	$L__BB35_103;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r472}, %fd4807;
	}
	xor.b32  	%r473, %r472, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r474, %temp}, %fd4807;
	}
	mov.b64 	%fd4807, {%r474, %r473};

$L__BB35_103:
	setp.eq.f64 	%p170, %fd5323, 0d0000000000000000;
	@%p170 bra 	$L__BB35_107;
	bra.uni 	$L__BB35_104;

$L__BB35_107:
	selp.b32 	%r475, %r34, 0, %p29;
	mov.u32 	%r476, 0;
	or.b32  	%r477, %r475, 2146435072;
	setp.lt.s32 	%p174, %r21, 0;
	selp.b32 	%r478, %r477, %r475, %p174;
	mov.b64 	%fd4807, {%r476, %r478};
	bra.uni 	$L__BB35_108;

$L__BB35_104:
	setp.gt.s32 	%p171, %r34, -1;
	@%p171 bra 	$L__BB35_108;

	mov.f64 	%fd2326, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2327, %fd2326;
	setp.eq.f64 	%p172, %fd2327, 0d4008000000000000;
	@%p172 bra 	$L__BB35_108;

	mov.f64 	%fd4807, 0dFFF8000000000000;

$L__BB35_108:
	add.f64 	%fd2329, %fd5323, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r479}, %fd2329;
	}
	and.b32  	%r480, %r479, 2146435072;
	setp.ne.s32 	%p175, %r480, 2146435072;
	@%p175 bra 	$L__BB35_115;

	setp.gtu.f64 	%p176, %fd725, 0d7FF0000000000000;
	@%p176 bra 	$L__BB35_114;
	bra.uni 	$L__BB35_110;

$L__BB35_114:
	mov.f64 	%fd2331, 0d4008000000000000;
	add.rn.f64 	%fd4807, %fd5323, %fd2331;
	bra.uni 	$L__BB35_115;

$L__BB35_110:
	mov.f64 	%fd2330, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r481, %temp}, %fd2330;
	}
	and.b32  	%r35, %r21, 2147483647;
	setp.eq.s32 	%p177, %r35, 2146435072;
	setp.eq.s32 	%p178, %r481, 0;
	and.pred  	%p179, %p177, %p178;
	@%p179 bra 	$L__BB35_113;
	bra.uni 	$L__BB35_111;

$L__BB35_113:
	setp.gt.f64 	%p186, %fd725, 0d3FF0000000000000;
	selp.b32 	%r488, 2146435072, 0, %p186;
	mov.u32 	%r489, 0;
	xor.b32  	%r490, %r488, 2146435072;
	setp.lt.s32 	%p187, %r21, 0;
	selp.b32 	%r491, %r490, %r488, %p187;
	setp.eq.f64 	%p188, %fd5323, 0dBFF0000000000000;
	selp.b32 	%r492, 1072693248, %r491, %p188;
	mov.b64 	%fd4807, {%r489, %r492};
	bra.uni 	$L__BB35_115;

$L__BB35_111:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r482, %temp}, %fd5323;
	}
	and.b32  	%r483, %r34, 2147483647;
	setp.ne.s32 	%p180, %r483, 2146435072;
	setp.ne.s32 	%p181, %r482, 0;
	or.pred  	%p182, %p180, %p181;
	@%p182 bra 	$L__BB35_115;

	setp.gt.s32 	%p183, %r21, -1;
	selp.b32 	%r484, 2146435072, 0, %p183;
	mov.u32 	%r485, 0;
	setp.ne.s32 	%p184, %r35, 1071644672;
	and.pred  	%p185, %p184, %p7;
	or.b32  	%r486, %r484, -2147483648;
	selp.b32 	%r487, %r486, %r484, %p185;
	mov.b64 	%fd4807, {%r485, %r487};

$L__BB35_115:
	mov.f64 	%fd2332, 0d0000000000000000;
	sub.f64 	%fd2333, %fd2332, %fd4807;
	div.rn.f64 	%fd2334, %fd2333, 0d4008000000000000;
	mov.f64 	%fd2335, 0d4008000000000000;
	setp.eq.f64 	%p189, %fd5323, 0d3FF0000000000000;
	selp.f64 	%fd5324, 0dBFD5555555555555, %fd2334, %p189;
	div.rn.f64 	%fd2336, %fd5380, 0d4008000000000000;
	mov.f64 	%fd2337, 0d3FE0000000000000;
	sub.f64 	%fd2338, %fd2337, %fd2336;
	mul.f64 	%fd2339, %fd5374, 0d3FE0000000000000;
	sub.f64 	%fd2340, %fd2338, %fd2339;
	mul.f64 	%fd2341, %fd5383, 0d3FE0000000000000;
	sub.f64 	%fd5330, %fd2340, %fd2341;
	mul.f64 	%fd5329, %fd5380, %fd5380;
	mul.f64 	%fd5331, %fd5329, %fd5330;
	div.rn.f64 	%fd2342, %fd5374, 0d4008000000000000;
	sub.f64 	%fd2343, %fd2337, %fd2342;
	mul.f64 	%fd2344, %fd5380, 0d3FE0000000000000;
	sub.f64 	%fd2345, %fd2343, %fd2344;
	sub.f64 	%fd5337, %fd2345, %fd2341;
	mul.f64 	%fd5336, %fd5374, %fd5374;
	mul.f64 	%fd5338, %fd5336, %fd5337;
	div.rn.f64 	%fd2346, %fd5383, 0d4008000000000000;
	sub.f64 	%fd2347, %fd2337, %fd2346;
	sub.f64 	%fd2348, %fd2347, %fd2344;
	sub.f64 	%fd5344, %fd2348, %fd2339;
	mul.f64 	%fd5343, %fd5383, %fd5383;
	mul.f64 	%fd5345, %fd5343, %fd5344;
	abs.f64 	%fd745, %fd5380;
	{ // callseq 70, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd745;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd2335;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd4810, [retval0+0];
	} // callseq 70
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd5380;
	}
	setp.lt.s32 	%p190, %r36, 0;
	and.pred  	%p8, %p190, %p29;
	not.pred 	%p192, %p8;
	@%p192 bra 	$L__BB35_117;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r493}, %fd4810;
	}
	xor.b32  	%r494, %r493, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r495, %temp}, %fd4810;
	}
	mov.b64 	%fd4810, {%r495, %r494};

$L__BB35_117:
	setp.eq.f64 	%p193, %fd5380, 0d0000000000000000;
	@%p193 bra 	$L__BB35_121;
	bra.uni 	$L__BB35_118;

$L__BB35_121:
	selp.b32 	%r496, %r36, 0, %p29;
	mov.u32 	%r497, 0;
	or.b32  	%r498, %r496, 2146435072;
	setp.lt.s32 	%p197, %r21, 0;
	selp.b32 	%r499, %r498, %r496, %p197;
	mov.b64 	%fd4810, {%r497, %r499};
	bra.uni 	$L__BB35_122;

$L__BB35_118:
	setp.gt.s32 	%p194, %r36, -1;
	@%p194 bra 	$L__BB35_122;

	mov.f64 	%fd2349, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2350, %fd2349;
	setp.eq.f64 	%p195, %fd2350, 0d4008000000000000;
	@%p195 bra 	$L__BB35_122;

	mov.f64 	%fd4810, 0dFFF8000000000000;

$L__BB35_122:
	add.f64 	%fd2352, %fd5380, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r500}, %fd2352;
	}
	and.b32  	%r501, %r500, 2146435072;
	setp.ne.s32 	%p198, %r501, 2146435072;
	@%p198 bra 	$L__BB35_129;

	setp.gtu.f64 	%p199, %fd745, 0d7FF0000000000000;
	@%p199 bra 	$L__BB35_128;
	bra.uni 	$L__BB35_124;

$L__BB35_128:
	mov.f64 	%fd2354, 0d4008000000000000;
	add.rn.f64 	%fd4810, %fd5380, %fd2354;
	bra.uni 	$L__BB35_129;

$L__BB35_124:
	mov.f64 	%fd2353, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r502, %temp}, %fd2353;
	}
	and.b32  	%r37, %r21, 2147483647;
	setp.eq.s32 	%p200, %r37, 2146435072;
	setp.eq.s32 	%p201, %r502, 0;
	and.pred  	%p202, %p200, %p201;
	@%p202 bra 	$L__BB35_127;
	bra.uni 	$L__BB35_125;

$L__BB35_127:
	setp.gt.f64 	%p209, %fd745, 0d3FF0000000000000;
	selp.b32 	%r509, 2146435072, 0, %p209;
	mov.u32 	%r510, 0;
	xor.b32  	%r511, %r509, 2146435072;
	setp.lt.s32 	%p210, %r21, 0;
	selp.b32 	%r512, %r511, %r509, %p210;
	setp.eq.f64 	%p211, %fd5380, 0dBFF0000000000000;
	selp.b32 	%r513, 1072693248, %r512, %p211;
	mov.b64 	%fd4810, {%r510, %r513};
	bra.uni 	$L__BB35_129;

$L__BB35_125:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r503, %temp}, %fd5380;
	}
	and.b32  	%r504, %r36, 2147483647;
	setp.ne.s32 	%p203, %r504, 2146435072;
	setp.ne.s32 	%p204, %r503, 0;
	or.pred  	%p205, %p203, %p204;
	@%p205 bra 	$L__BB35_129;

	setp.gt.s32 	%p206, %r21, -1;
	selp.b32 	%r505, 2146435072, 0, %p206;
	mov.u32 	%r506, 0;
	setp.ne.s32 	%p207, %r37, 1071644672;
	and.pred  	%p208, %p207, %p8;
	or.b32  	%r507, %r505, -2147483648;
	selp.b32 	%r508, %r507, %r505, %p208;
	mov.b64 	%fd4810, {%r506, %r508};

$L__BB35_129:
	mul.f64 	%fd2355, %fd5124, %fd5126;
	mul.f64 	%fd5258, %fd2355, 0d3FE0AAAAA0000000;
	div.rn.f64 	%fd2356, %fd4810, 0d4008000000000000;
	setp.eq.f64 	%p212, %fd5380, 0d3FF0000000000000;
	selp.f64 	%fd5349, 0d3FD5555555555555, %fd2356, %p212;
	mul.f64 	%fd5351, %fd4812, %fd5349;
	mul.f64 	%fd5361, %fd5329, 0d3FE0000000000000;
	mul.f64 	%fd5356, %fd5374, %fd5361;
	mul.f64 	%fd5358, %fd4812, %fd5356;
	mul.f64 	%fd5363, %fd5361, %fd5383;
	mul.f64 	%fd5365, %fd4812, %fd5363;
	mul.f64 	%fd5375, %fd5380, %fd5374;
	mul.f64 	%fd5370, %fd5374, %fd5375;
	mul.f64 	%fd5372, %fd4812, %fd5370;
	mul.f64 	%fd5377, %fd5375, %fd5383;
	mul.f64 	%fd5379, %fd4812, %fd5377;
	mul.f64 	%fd5382, %fd5380, %fd5383;
	mul.f64 	%fd5384, %fd5383, %fd5382;
	mul.f64 	%fd5386, %fd4812, %fd5384;
	mul.f64 	%fd5287, %fd4812, %fd5285;
	mul.f64 	%fd5294, %fd4812, %fd5292;
	mul.f64 	%fd5301, %fd4812, %fd5299;
	mul.f64 	%fd5308, %fd4812, %fd5306;
	mul.f64 	%fd5315, %fd4812, %fd5313;
	mul.f64 	%fd5322, %fd4812, %fd5320;
	mul.f64 	%fd5222, %fd4812, %fd5220;
	mul.f64 	%fd5229, %fd4812, %fd5227;
	mul.f64 	%fd5236, %fd4812, %fd5234;
	mul.f64 	%fd5243, %fd4812, %fd5241;
	mul.f64 	%fd5250, %fd4812, %fd5248;
	mul.f64 	%fd5257, %fd4812, %fd5255;
	mul.f64 	%fd5157, %fd4812, %fd5155;
	mul.f64 	%fd5164, %fd4812, %fd5162;
	mul.f64 	%fd5171, %fd4812, %fd5169;
	mul.f64 	%fd5178, %fd4812, %fd5176;
	mul.f64 	%fd5185, %fd4812, %fd5183;
	mul.f64 	%fd5192, %fd4812, %fd5190;
	mul.f64 	%fd2357, %fd565, %fd569;
	mul.f64 	%fd2358, %fd568, %fd566;
	sub.f64 	%fd5111, %fd2357, %fd2358;
	mul.f64 	%fd2359, %fd562, %fd569;
	mul.f64 	%fd2360, %fd568, %fd563;
	sub.f64 	%fd5112, %fd2360, %fd2359;
	mul.f64 	%fd2361, %fd565, %fd563;
	mul.f64 	%fd2362, %fd562, %fd566;
	sub.f64 	%fd5113, %fd2362, %fd2361;
	mul.f64 	%fd5128, %fd2355, 0dBFE2000000000000;
	mul.f64 	%fd5153, %fd4814, %fd5151;
	mul.f64 	%fd5146, %fd4813, %fd5144;
	mul.f64 	%fd5139, %fd4812, %fd5137;
	mul.f64 	%fd5132, %fd4812, %fd5130;
	mul.f64 	%fd5218, %fd4814, %fd5216;
	mul.f64 	%fd5211, %fd4813, %fd5209;
	mul.f64 	%fd5204, %fd4812, %fd5202;
	mul.f64 	%fd5197, %fd4812, %fd5195;
	mul.f64 	%fd5283, %fd4814, %fd5281;
	mul.f64 	%fd5276, %fd4813, %fd5274;
	mul.f64 	%fd5269, %fd4812, %fd5267;
	mul.f64 	%fd5262, %fd4812, %fd5260;
	mul.f64 	%fd5347, %fd4814, %fd5345;
	mul.f64 	%fd5340, %fd4813, %fd5338;
	mul.f64 	%fd5333, %fd4812, %fd5331;
	mul.f64 	%fd5326, %fd4812, %fd5324;
	mov.f64 	%fd5102, %fd571;
	mov.f64 	%fd5103, %fd573;
	mov.f64 	%fd5104, %fd575;
	mov.f64 	%fd5105, %fd563;
	mov.f64 	%fd5106, %fd566;
	mov.f64 	%fd5107, %fd569;
	mov.f64 	%fd5108, %fd562;
	mov.f64 	%fd5109, %fd565;
	mov.f64 	%fd5110, %fd568;
	mov.f64 	%fd5131, %fd4812;
	mov.f64 	%fd5138, %fd4812;
	mov.f64 	%fd5145, %fd4813;
	mov.f64 	%fd5152, %fd4814;
	mov.f64 	%fd5156, %fd4812;
	mov.f64 	%fd5163, %fd4812;
	mov.f64 	%fd5170, %fd4812;
	mov.f64 	%fd5177, %fd4812;
	mov.f64 	%fd5184, %fd4812;
	mov.f64 	%fd5191, %fd4812;
	mov.f64 	%fd5196, %fd4812;
	mov.f64 	%fd5203, %fd4812;
	mov.f64 	%fd5210, %fd4813;
	mov.f64 	%fd5217, %fd4814;
	mov.f64 	%fd5221, %fd4812;
	mov.f64 	%fd5228, %fd4812;
	mov.f64 	%fd5235, %fd4812;
	mov.f64 	%fd5242, %fd4812;
	mov.f64 	%fd5249, %fd4812;
	mov.f64 	%fd5256, %fd4812;
	mov.f64 	%fd5261, %fd4812;
	mov.f64 	%fd5268, %fd4812;
	mov.f64 	%fd5275, %fd4813;
	mov.f64 	%fd5282, %fd4814;
	mov.f64 	%fd5286, %fd4812;
	mov.f64 	%fd5293, %fd4812;
	mov.f64 	%fd5300, %fd4812;
	mov.f64 	%fd5307, %fd4812;
	mov.f64 	%fd5314, %fd4812;
	mov.f64 	%fd5321, %fd4812;
	mov.f64 	%fd5325, %fd4812;
	mov.f64 	%fd5332, %fd4812;
	mov.f64 	%fd5339, %fd4813;
	mov.f64 	%fd5346, %fd4814;
	mov.f64 	%fd5350, %fd4812;
	mov.f64 	%fd5357, %fd4812;
	mov.f64 	%fd5364, %fd4812;
	mov.f64 	%fd5371, %fd4812;
	mov.f64 	%fd5378, %fd4812;
	mov.f64 	%fd5385, %fd4812;

$L__BB35_130:
	setp.ne.s32 	%p213, %r698, 0;
	selp.f64 	%fd5095, %fd5114, %fd4812, %p27;
	selp.f64 	%fd5096, %fd5115, %fd4813, %p27;
	selp.f64 	%fd5097, %fd5116, %fd4814, %p27;
	mov.u32 	%r699, %r13;
	mov.u32 	%r700, %r13;
	mov.u32 	%r701, %r13;
	mov.u32 	%r705, %r13;
	mov.u32 	%r706, %r14;
	@%p213 bra 	$L__BB35_132;

	mul.f64 	%fd5094, %fd5123, %fd5126;
	mul.f64 	%fd2366, %fd5096, %fd5096;
	fma.rn.f64 	%fd2367, %fd5095, %fd5095, %fd2366;
	fma.rn.f64 	%fd2368, %fd5097, %fd5097, %fd2367;
	sqrt.rn.f64 	%fd5387, %fd2368;
	div.rn.f64 	%fd5091, %fd5095, %fd5387;
	div.rn.f64 	%fd5092, %fd5096, %fd5387;
	div.rn.f64 	%fd5093, %fd5097, %fd5387;
	mul.f64 	%fd2369, %fd5123, %fd5091;
	mul.f64 	%fd2370, %fd5123, %fd5092;
	mul.f64 	%fd2371, %fd5123, %fd5093;
	div.rn.f64 	%fd2372, %fd2369, 0d400BB67AE8584CAA;
	div.rn.f64 	%fd2373, %fd2370, 0d400BB67AE8584CAA;
	div.rn.f64 	%fd2374, %fd2371, 0d400BB67AE8584CAA;
	sub.f64 	%fd2375, %fd562, %fd557;
	add.f64 	%fd2376, %fd2375, %fd2375;
	div.rn.f64 	%fd2377, %fd2376, 0d4008000000000000;
	sub.f64 	%fd2378, %fd563, %fd557;
	div.rn.f64 	%fd2379, %fd2378, 0d4018000000000000;
	add.f64 	%fd2380, %fd2377, %fd2379;
	sub.f64 	%fd2381, %fd571, %fd557;
	div.rn.f64 	%fd2382, %fd2381, 0d4018000000000000;
	add.f64 	%fd2383, %fd2380, %fd2382;
	sub.f64 	%fd5427, %fd2383, %fd2372;
	fma.rn.f64 	%fd2384, %fd2375, 0d4018000000000000, %fd2379;
	add.f64 	%fd2385, %fd2381, %fd2381;
	div.rn.f64 	%fd2386, %fd2385, 0d4008000000000000;
	add.f64 	%fd2387, %fd2384, %fd2386;
	sub.f64 	%fd5475, %fd2387, %fd2372;
	div.rn.f64 	%fd2388, %fd2375, 0d4018000000000000;
	add.f64 	%fd2389, %fd2378, %fd2378;
	div.rn.f64 	%fd2390, %fd2389, 0d4008000000000000;
	add.f64 	%fd2391, %fd2388, %fd2390;
	add.f64 	%fd2392, %fd2382, %fd2391;
	sub.f64 	%fd5523, %fd2392, %fd2372;
	add.f64 	%fd5571, %fd2372, %fd2383;
	add.f64 	%fd5619, %fd2372, %fd2387;
	add.f64 	%fd5667, %fd2372, %fd2392;
	sub.f64 	%fd2393, %fd565, %fd558;
	add.f64 	%fd2394, %fd2393, %fd2393;
	div.rn.f64 	%fd2395, %fd2394, 0d4008000000000000;
	sub.f64 	%fd2396, %fd566, %fd558;
	div.rn.f64 	%fd2397, %fd2396, 0d4018000000000000;
	add.f64 	%fd2398, %fd2395, %fd2397;
	sub.f64 	%fd2399, %fd573, %fd558;
	div.rn.f64 	%fd2400, %fd2399, 0d4018000000000000;
	add.f64 	%fd2401, %fd2398, %fd2400;
	sub.f64 	%fd5430, %fd2401, %fd2373;
	fma.rn.f64 	%fd2402, %fd2393, 0d4018000000000000, %fd2397;
	add.f64 	%fd2403, %fd2399, %fd2399;
	div.rn.f64 	%fd2404, %fd2403, 0d4008000000000000;
	add.f64 	%fd2405, %fd2402, %fd2404;
	sub.f64 	%fd5478, %fd2405, %fd2373;
	div.rn.f64 	%fd2406, %fd2393, 0d4018000000000000;
	add.f64 	%fd2407, %fd2396, %fd2396;
	div.rn.f64 	%fd2408, %fd2407, 0d4008000000000000;
	add.f64 	%fd2409, %fd2406, %fd2408;
	add.f64 	%fd2410, %fd2400, %fd2409;
	sub.f64 	%fd5526, %fd2410, %fd2373;
	add.f64 	%fd5574, %fd2373, %fd2401;
	add.f64 	%fd5622, %fd2373, %fd2405;
	add.f64 	%fd5670, %fd2373, %fd2410;
	sub.f64 	%fd2411, %fd568, %fd559;
	add.f64 	%fd2412, %fd2411, %fd2411;
	div.rn.f64 	%fd2413, %fd2412, 0d4008000000000000;
	sub.f64 	%fd2414, %fd569, %fd559;
	div.rn.f64 	%fd2415, %fd2414, 0d4018000000000000;
	add.f64 	%fd2416, %fd2413, %fd2415;
	sub.f64 	%fd2417, %fd575, %fd559;
	div.rn.f64 	%fd2418, %fd2417, 0d4018000000000000;
	add.f64 	%fd2419, %fd2416, %fd2418;
	sub.f64 	%fd5435, %fd2419, %fd2374;
	fma.rn.f64 	%fd2420, %fd2411, 0d4018000000000000, %fd2415;
	add.f64 	%fd2421, %fd2417, %fd2417;
	div.rn.f64 	%fd2422, %fd2421, 0d4008000000000000;
	add.f64 	%fd2423, %fd2420, %fd2422;
	sub.f64 	%fd5483, %fd2423, %fd2374;
	div.rn.f64 	%fd2424, %fd2411, 0d4018000000000000;
	add.f64 	%fd2425, %fd2414, %fd2414;
	div.rn.f64 	%fd2426, %fd2425, 0d4008000000000000;
	add.f64 	%fd2427, %fd2424, %fd2426;
	add.f64 	%fd2428, %fd2418, %fd2427;
	sub.f64 	%fd5531, %fd2428, %fd2374;
	add.f64 	%fd5579, %fd2374, %fd2419;
	add.f64 	%fd5627, %fd2374, %fd2423;
	add.f64 	%fd5675, %fd2374, %fd2428;
	mul.f64 	%fd2429, %fd5124, %fd5094;
	div.rn.f64 	%fd5090, %fd2429, 0d4018000000000000;
	mov.f64 	%fd2430, 0d3FF0000000000000;
	sub.f64 	%fd2431, %fd2430, %fd5427;
	sub.f64 	%fd2432, %fd2431, %fd5430;
	sub.f64 	%fd5424, %fd2432, %fd5435;
	mul.f64 	%fd5425, %fd5090, %fd5424;
	mul.f64 	%fd5428, %fd5090, %fd5427;
	mul.f64 	%fd5431, %fd5090, %fd5430;
	mul.f64 	%fd5434, %fd5090, %fd5435;
	sub.f64 	%fd2433, %fd2430, %fd5475;
	sub.f64 	%fd2434, %fd2433, %fd5478;
	sub.f64 	%fd5472, %fd2434, %fd5483;
	mul.f64 	%fd5473, %fd5090, %fd5472;
	mul.f64 	%fd5476, %fd5090, %fd5475;
	mul.f64 	%fd5479, %fd5090, %fd5478;
	mul.f64 	%fd5482, %fd5090, %fd5483;
	sub.f64 	%fd2435, %fd2430, %fd5523;
	sub.f64 	%fd2436, %fd2435, %fd5526;
	sub.f64 	%fd5520, %fd2436, %fd5531;
	mul.f64 	%fd5521, %fd5090, %fd5520;
	mul.f64 	%fd5524, %fd5090, %fd5523;
	mul.f64 	%fd5527, %fd5090, %fd5526;
	mul.f64 	%fd5530, %fd5090, %fd5531;
	sub.f64 	%fd2437, %fd2430, %fd5571;
	sub.f64 	%fd2438, %fd2437, %fd5574;
	sub.f64 	%fd5568, %fd2438, %fd5579;
	mul.f64 	%fd5569, %fd5090, %fd5568;
	mul.f64 	%fd5572, %fd5090, %fd5571;
	mul.f64 	%fd5575, %fd5090, %fd5574;
	mul.f64 	%fd5578, %fd5090, %fd5579;
	sub.f64 	%fd2439, %fd2430, %fd5619;
	sub.f64 	%fd2440, %fd2439, %fd5622;
	sub.f64 	%fd5616, %fd2440, %fd5627;
	mul.f64 	%fd5617, %fd5090, %fd5616;
	mul.f64 	%fd5620, %fd5090, %fd5619;
	mul.f64 	%fd5623, %fd5090, %fd5622;
	mul.f64 	%fd5626, %fd5090, %fd5627;
	sub.f64 	%fd2441, %fd2430, %fd5667;
	sub.f64 	%fd2442, %fd2441, %fd5670;
	sub.f64 	%fd5664, %fd2442, %fd5675;
	mul.f64 	%fd5665, %fd5090, %fd5664;
	mul.f64 	%fd5668, %fd5090, %fd5667;
	mul.f64 	%fd5671, %fd5090, %fd5670;
	mul.f64 	%fd5674, %fd5090, %fd5675;
	mov.u32 	%r698, 0;
	mov.u32 	%r699, %r13;
	mov.u32 	%r700, %r13;
	mov.u32 	%r701, %r13;
	mov.u32 	%r705, %r13;
	mov.u32 	%r706, %r14;

$L__BB35_132:
	setp.eq.s32 	%p216, %r706, 0;
	or.pred  	%p217, %p216, %p25;
	@%p217 bra 	$L__BB35_283;

	setp.eq.s64 	%p218, %rd82, 0;
	@%p218 bra 	$L__BB35_135;

	cvta.to.global.u64 	%rd117, %rd82;
	cvt.s64.s32 	%rd118, %r705;
	mul.lo.s64 	%rd119, %rd118, %rd34;
	add.s64 	%rd120, %rd117, %rd119;
	ld.global.f64 	%fd2443, [%rd120];
	add.f64 	%fd5691, %fd2443, 0d0000000000000000;
	ld.global.f64 	%fd2444, [%rd120+8];
	add.f64 	%fd5690, %fd2444, 0d0000000000000000;
	ld.global.f64 	%fd2445, [%rd120+16];
	add.f64 	%fd5689, %fd2445, 0d0000000000000000;
	ld.global.f64 	%fd2446, [%rd120+24];
	add.f64 	%fd5688, %fd2446, 0d0000000000000000;
	ld.global.f64 	%fd2447, [%rd120+32];
	add.f64 	%fd5687, %fd2447, 0d0000000000000000;
	ld.global.f64 	%fd2448, [%rd120+40];
	add.f64 	%fd5686, %fd2448, 0d0000000000000000;
	ld.global.f64 	%fd2449, [%rd120+48];
	add.f64 	%fd5685, %fd2449, 0d0000000000000000;
	ld.global.f64 	%fd2450, [%rd120+56];
	add.f64 	%fd5684, %fd2450, 0d0000000000000000;
	ld.global.f64 	%fd2451, [%rd120+64];
	add.f64 	%fd5683, %fd2451, 0d0000000000000000;
	ld.global.f64 	%fd2452, [%rd120+72];
	add.f64 	%fd5682, %fd2452, 0d0000000000000000;
	ld.global.f64 	%fd2453, [%rd120+80];
	add.f64 	%fd5681, %fd2453, 0d0000000000000000;
	ld.global.f64 	%fd2454, [%rd120+88];
	add.f64 	%fd5680, %fd2454, 0d0000000000000000;
	ld.global.f64 	%fd2455, [%rd120+96];
	add.f64 	%fd5679, %fd2455, 0d0000000000000000;
	ld.global.f64 	%fd2456, [%rd120+104];
	add.f64 	%fd5678, %fd2456, 0d0000000000000000;
	ld.global.f64 	%fd2457, [%rd120+112];
	add.f64 	%fd5677, %fd2457, 0d0000000000000000;
	ld.global.f64 	%fd2458, [%rd120+120];
	add.f64 	%fd5676, %fd2458, 0d0000000000000000;
	bra.uni 	$L__BB35_137;

$L__BB35_135:
	setp.eq.s64 	%p219, %rd73, 0;
	mov.f64 	%fd5676, 0d0000000000000000;
	mov.f64 	%fd5677, %fd5676;
	mov.f64 	%fd5678, %fd5676;
	mov.f64 	%fd5679, %fd5676;
	mov.f64 	%fd5680, %fd5676;
	mov.f64 	%fd5681, %fd5676;
	mov.f64 	%fd5682, %fd5676;
	mov.f64 	%fd5683, %fd5676;
	mov.f64 	%fd5684, %fd5676;
	mov.f64 	%fd5685, %fd5676;
	mov.f64 	%fd5686, %fd5676;
	mov.f64 	%fd5687, %fd5676;
	mov.f64 	%fd5688, %fd5676;
	mov.f64 	%fd5689, %fd5676;
	mov.f64 	%fd5690, %fd5676;
	mov.f64 	%fd5691, %fd5676;
	@%p219 bra 	$L__BB35_137;

	cvta.to.global.u64 	%rd121, %rd73;
	cvt.s64.s32 	%rd122, %r705;
	mul.lo.s64 	%rd123, %rd122, %rd35;
	add.s64 	%rd124, %rd121, %rd123;
	ld.global.f64 	%fd2475, [%rd124];
	add.f64 	%fd5691, %fd2475, 0d0000000000000000;
	ld.global.f64 	%fd2476, [%rd124+8];
	add.f64 	%fd5690, %fd2476, 0d0000000000000000;
	ld.global.f64 	%fd2477, [%rd124+16];
	add.f64 	%fd5689, %fd2477, 0d0000000000000000;
	ld.global.f64 	%fd2478, [%rd124+24];
	add.f64 	%fd5688, %fd2478, 0d0000000000000000;
	ld.global.f64 	%fd2479, [%rd124+32];
	add.f64 	%fd5687, %fd2479, 0d0000000000000000;
	ld.global.f64 	%fd2480, [%rd124+40];
	add.f64 	%fd5686, %fd2480, 0d0000000000000000;
	ld.global.f64 	%fd2481, [%rd124+48];
	add.f64 	%fd5685, %fd2481, 0d0000000000000000;
	ld.global.f64 	%fd2482, [%rd124+56];
	add.f64 	%fd5684, %fd2482, 0d0000000000000000;
	ld.global.f64 	%fd2483, [%rd124+64];
	add.f64 	%fd5683, %fd2483, 0d0000000000000000;
	ld.global.f64 	%fd2484, [%rd124+72];
	add.f64 	%fd5682, %fd2484, 0d0000000000000000;
	ld.global.f64 	%fd2485, [%rd124+80];
	add.f64 	%fd5681, %fd2485, 0d0000000000000000;
	ld.global.f64 	%fd2486, [%rd124+88];
	add.f64 	%fd5680, %fd2486, 0d0000000000000000;
	ld.global.f64 	%fd2487, [%rd124+96];
	add.f64 	%fd5679, %fd2487, 0d0000000000000000;
	ld.global.f64 	%fd2488, [%rd124+104];
	add.f64 	%fd5678, %fd2488, 0d0000000000000000;
	ld.global.f64 	%fd2489, [%rd124+112];
	add.f64 	%fd5677, %fd2489, 0d0000000000000000;
	ld.global.f64 	%fd2490, [%rd124+120];
	add.f64 	%fd5676, %fd2490, 0d0000000000000000;

$L__BB35_137:
	setp.ne.s32 	%p220, %r698, 0;
	mov.f64 	%fd5695, 0d0000000000000000;
	mov.f64 	%fd5696, %fd5695;
	mov.f64 	%fd5697, %fd5695;
	mov.f64 	%fd5743, %fd5695;
	mov.f64 	%fd5744, %fd5695;
	mov.f64 	%fd5745, %fd5695;
	mov.f64 	%fd5746, %fd5695;
	mov.f64 	%fd5747, %fd5695;
	mov.f64 	%fd5748, %fd5695;
	mov.f64 	%fd5749, %fd5695;
	mov.f64 	%fd5750, %fd5695;
	mov.f64 	%fd5751, %fd5695;
	mov.f64 	%fd5752, %fd5695;
	mov.f64 	%fd5753, %fd5695;
	mov.f64 	%fd5754, %fd5695;
	mov.f64 	%fd5755, %fd5695;
	mov.f64 	%fd5759, %fd5695;
	mov.f64 	%fd5712, %fd5695;
	@%p220 bra 	$L__BB35_141;

	add.f64 	%fd2509, %fd5676, 0d0000000000000000;
	mov.f64 	%fd2510, 0d0000000000000000;
	fma.rn.f64 	%fd2511, %fd5675, %fd2509, 0d0000000000000000;
	fma.rn.f64 	%fd2512, %fd5674, %fd2509, 0d0000000000000000;
	fma.rn.f64 	%fd2513, %fd5675, %fd2511, 0d0000000000000000;
	fma.rn.f64 	%fd2514, %fd5090, %fd2511, 0d0000000000000000;
	add.f64 	%fd2515, %fd2512, %fd2514;
	add.f64 	%fd2516, %fd5677, 0d0000000000000000;
	fma.rn.f64 	%fd2517, %fd5675, %fd2516, 0d0000000000000000;
	fma.rn.f64 	%fd2518, %fd5671, %fd2516, 0d0000000000000000;
	add.f64 	%fd2519, %fd2518, %fd2515;
	fma.rn.f64 	%fd2520, %fd5670, %fd2517, %fd2513;
	fma.rn.f64 	%fd2521, %fd5090, %fd2517, 0d0000000000000000;
	add.f64 	%fd2522, %fd5678, 0d0000000000000000;
	fma.rn.f64 	%fd2523, %fd5675, %fd2522, 0d0000000000000000;
	fma.rn.f64 	%fd2524, %fd5668, %fd2522, 0d0000000000000000;
	add.f64 	%fd2525, %fd2524, %fd2519;
	fma.rn.f64 	%fd2526, %fd5667, %fd2523, %fd2520;
	fma.rn.f64 	%fd2527, %fd5090, %fd2523, 0d0000000000000000;
	add.f64 	%fd2528, %fd5679, 0d0000000000000000;
	fma.rn.f64 	%fd2529, %fd5675, %fd2528, 0d0000000000000000;
	fma.rn.f64 	%fd2530, %fd5665, %fd2528, 0d0000000000000000;
	add.f64 	%fd2531, %fd2530, %fd2525;
	fma.rn.f64 	%fd2532, %fd5664, %fd2529, %fd2526;
	fma.rn.f64 	%fd2533, %fd5090, %fd2529, 0d0000000000000000;
	add.f64 	%fd2534, %fd5680, 0d0000000000000000;
	fma.rn.f64 	%fd2535, %fd5670, %fd2534, 0d0000000000000000;
	fma.rn.f64 	%fd2536, %fd5674, %fd2534, 0d0000000000000000;
	add.f64 	%fd2537, %fd2521, %fd2536;
	fma.rn.f64 	%fd2538, %fd5675, %fd2535, %fd2532;
	fma.rn.f64 	%fd2539, %fd5090, %fd2535, 0d0000000000000000;
	add.f64 	%fd2540, %fd2531, %fd2539;
	add.f64 	%fd2541, %fd5681, 0d0000000000000000;
	fma.rn.f64 	%fd2542, %fd5670, %fd2541, 0d0000000000000000;
	fma.rn.f64 	%fd2543, %fd5671, %fd2541, 0d0000000000000000;
	add.f64 	%fd2544, %fd2537, %fd2543;
	fma.rn.f64 	%fd2545, %fd5670, %fd2542, %fd2538;
	fma.rn.f64 	%fd2546, %fd5090, %fd2542, 0d0000000000000000;
	add.f64 	%fd2547, %fd2544, %fd2546;
	add.f64 	%fd2548, %fd5682, 0d0000000000000000;
	fma.rn.f64 	%fd2549, %fd5670, %fd2548, 0d0000000000000000;
	fma.rn.f64 	%fd2550, %fd5668, %fd2548, 0d0000000000000000;
	add.f64 	%fd2551, %fd2550, %fd2547;
	fma.rn.f64 	%fd2552, %fd5667, %fd2549, %fd2545;
	fma.rn.f64 	%fd2553, %fd5090, %fd2549, 0d0000000000000000;
	add.f64 	%fd2554, %fd2527, %fd2553;
	add.f64 	%fd2555, %fd5683, 0d0000000000000000;
	fma.rn.f64 	%fd2556, %fd5670, %fd2555, 0d0000000000000000;
	fma.rn.f64 	%fd2557, %fd5665, %fd2555, 0d0000000000000000;
	add.f64 	%fd2558, %fd2557, %fd2551;
	fma.rn.f64 	%fd2559, %fd5664, %fd2556, %fd2552;
	fma.rn.f64 	%fd2560, %fd5090, %fd2556, 0d0000000000000000;
	add.f64 	%fd2561, %fd2533, %fd2560;
	add.f64 	%fd2562, %fd5684, 0d0000000000000000;
	fma.rn.f64 	%fd2563, %fd5667, %fd2562, 0d0000000000000000;
	fma.rn.f64 	%fd2564, %fd5674, %fd2562, 0d0000000000000000;
	add.f64 	%fd2565, %fd2564, %fd2554;
	fma.rn.f64 	%fd2566, %fd5675, %fd2563, %fd2559;
	fma.rn.f64 	%fd2567, %fd5090, %fd2563, 0d0000000000000000;
	add.f64 	%fd2568, %fd2540, %fd2567;
	add.f64 	%fd2569, %fd5685, 0d0000000000000000;
	fma.rn.f64 	%fd2570, %fd5667, %fd2569, 0d0000000000000000;
	fma.rn.f64 	%fd2571, %fd5671, %fd2569, 0d0000000000000000;
	add.f64 	%fd2572, %fd2571, %fd2565;
	fma.rn.f64 	%fd2573, %fd5670, %fd2570, %fd2566;
	fma.rn.f64 	%fd2574, %fd5090, %fd2570, 0d0000000000000000;
	add.f64 	%fd2575, %fd2558, %fd2574;
	add.f64 	%fd2576, %fd5686, 0d0000000000000000;
	fma.rn.f64 	%fd2577, %fd5667, %fd2576, 0d0000000000000000;
	fma.rn.f64 	%fd2578, %fd5668, %fd2576, 0d0000000000000000;
	add.f64 	%fd2579, %fd2578, %fd2572;
	fma.rn.f64 	%fd2580, %fd5667, %fd2577, %fd2573;
	fma.rn.f64 	%fd2581, %fd5090, %fd2577, 0d0000000000000000;
	add.f64 	%fd2582, %fd2579, %fd2581;
	add.f64 	%fd2583, %fd5687, 0d0000000000000000;
	fma.rn.f64 	%fd2584, %fd5667, %fd2583, 0d0000000000000000;
	fma.rn.f64 	%fd2585, %fd5665, %fd2583, 0d0000000000000000;
	add.f64 	%fd2586, %fd2585, %fd2582;
	fma.rn.f64 	%fd2587, %fd5664, %fd2584, %fd2580;
	fma.rn.f64 	%fd2588, %fd5090, %fd2584, 0d0000000000000000;
	add.f64 	%fd2589, %fd2561, %fd2588;
	add.f64 	%fd2590, %fd5688, 0d0000000000000000;
	fma.rn.f64 	%fd2591, %fd5664, %fd2590, 0d0000000000000000;
	fma.rn.f64 	%fd2592, %fd5674, %fd2590, 0d0000000000000000;
	add.f64 	%fd2593, %fd2592, %fd2589;
	fma.rn.f64 	%fd2594, %fd5675, %fd2591, %fd2587;
	fma.rn.f64 	%fd2595, %fd5090, %fd2591, 0d0000000000000000;
	add.f64 	%fd2596, %fd2568, %fd2595;
	add.f64 	%fd2597, %fd5689, 0d0000000000000000;
	fma.rn.f64 	%fd2598, %fd5664, %fd2597, 0d0000000000000000;
	fma.rn.f64 	%fd2599, %fd5671, %fd2597, 0d0000000000000000;
	add.f64 	%fd2600, %fd2599, %fd2593;
	fma.rn.f64 	%fd2601, %fd5670, %fd2598, %fd2594;
	fma.rn.f64 	%fd2602, %fd5090, %fd2598, 0d0000000000000000;
	add.f64 	%fd2603, %fd2575, %fd2602;
	add.f64 	%fd2604, %fd5690, 0d0000000000000000;
	fma.rn.f64 	%fd2605, %fd5664, %fd2604, 0d0000000000000000;
	fma.rn.f64 	%fd2606, %fd5668, %fd2604, 0d0000000000000000;
	add.f64 	%fd2607, %fd2606, %fd2600;
	fma.rn.f64 	%fd2608, %fd5667, %fd2605, %fd2601;
	fma.rn.f64 	%fd2609, %fd5090, %fd2605, 0d0000000000000000;
	add.f64 	%fd2610, %fd2586, %fd2609;
	add.f64 	%fd2611, %fd5691, 0d0000000000000000;
	fma.rn.f64 	%fd2612, %fd5664, %fd2611, 0d0000000000000000;
	fma.rn.f64 	%fd2613, %fd5665, %fd2611, 0d0000000000000000;
	add.f64 	%fd2614, %fd2613, %fd2607;
	fma.rn.f64 	%fd2615, %fd5664, %fd2612, %fd2608;
	fma.rn.f64 	%fd2616, %fd5090, %fd2612, 0d0000000000000000;
	add.f64 	%fd2617, %fd2616, %fd2614;
	add.f64 	%fd2618, %fd2617, 0d0000000000000000;
	add.f64 	%fd2619, %fd2610, 0d0000000000000000;
	add.f64 	%fd2620, %fd2603, 0d0000000000000000;
	add.f64 	%fd2621, %fd2596, 0d0000000000000000;
	sub.f64 	%fd2622, %fd2510, %fd2618;
	add.f64 	%fd2623, %fd2621, %fd2622;
	add.f64 	%fd2624, %fd2620, %fd2622;
	add.f64 	%fd2625, %fd2619, %fd2622;
	fma.rn.f64 	%fd2626, %fd5627, %fd2509, 0d0000000000000000;
	fma.rn.f64 	%fd2627, %fd5626, %fd2509, 0d0000000000000000;
	fma.rn.f64 	%fd2628, %fd5627, %fd2626, %fd2615;
	fma.rn.f64 	%fd2629, %fd5090, %fd2626, 0d0000000000000000;
	add.f64 	%fd2630, %fd2627, %fd2629;
	fma.rn.f64 	%fd2631, %fd5627, %fd2516, 0d0000000000000000;
	fma.rn.f64 	%fd2632, %fd5623, %fd2516, 0d0000000000000000;
	add.f64 	%fd2633, %fd2632, %fd2630;
	fma.rn.f64 	%fd2634, %fd5622, %fd2631, %fd2628;
	fma.rn.f64 	%fd2635, %fd5090, %fd2631, 0d0000000000000000;
	fma.rn.f64 	%fd2636, %fd5627, %fd2522, 0d0000000000000000;
	fma.rn.f64 	%fd2637, %fd5620, %fd2522, 0d0000000000000000;
	add.f64 	%fd2638, %fd2637, %fd2633;
	fma.rn.f64 	%fd2639, %fd5619, %fd2636, %fd2634;
	fma.rn.f64 	%fd2640, %fd5090, %fd2636, 0d0000000000000000;
	fma.rn.f64 	%fd2641, %fd5627, %fd2528, 0d0000000000000000;
	fma.rn.f64 	%fd2642, %fd5617, %fd2528, 0d0000000000000000;
	add.f64 	%fd2643, %fd2642, %fd2638;
	fma.rn.f64 	%fd2644, %fd5616, %fd2641, %fd2639;
	fma.rn.f64 	%fd2645, %fd5090, %fd2641, 0d0000000000000000;
	fma.rn.f64 	%fd2646, %fd5622, %fd2534, 0d0000000000000000;
	fma.rn.f64 	%fd2647, %fd5626, %fd2534, 0d0000000000000000;
	add.f64 	%fd2648, %fd2635, %fd2647;
	fma.rn.f64 	%fd2649, %fd5627, %fd2646, %fd2644;
	fma.rn.f64 	%fd2650, %fd5090, %fd2646, 0d0000000000000000;
	add.f64 	%fd2651, %fd2643, %fd2650;
	fma.rn.f64 	%fd2652, %fd5622, %fd2541, 0d0000000000000000;
	fma.rn.f64 	%fd2653, %fd5623, %fd2541, 0d0000000000000000;
	add.f64 	%fd2654, %fd2648, %fd2653;
	fma.rn.f64 	%fd2655, %fd5622, %fd2652, %fd2649;
	fma.rn.f64 	%fd2656, %fd5090, %fd2652, 0d0000000000000000;
	add.f64 	%fd2657, %fd2654, %fd2656;
	fma.rn.f64 	%fd2658, %fd5622, %fd2548, 0d0000000000000000;
	fma.rn.f64 	%fd2659, %fd5620, %fd2548, 0d0000000000000000;
	add.f64 	%fd2660, %fd2659, %fd2657;
	fma.rn.f64 	%fd2661, %fd5619, %fd2658, %fd2655;
	fma.rn.f64 	%fd2662, %fd5090, %fd2658, 0d0000000000000000;
	add.f64 	%fd2663, %fd2640, %fd2662;
	fma.rn.f64 	%fd2664, %fd5622, %fd2555, 0d0000000000000000;
	fma.rn.f64 	%fd2665, %fd5617, %fd2555, 0d0000000000000000;
	add.f64 	%fd2666, %fd2665, %fd2660;
	fma.rn.f64 	%fd2667, %fd5616, %fd2664, %fd2661;
	fma.rn.f64 	%fd2668, %fd5090, %fd2664, 0d0000000000000000;
	add.f64 	%fd2669, %fd2645, %fd2668;
	fma.rn.f64 	%fd2670, %fd5619, %fd2562, 0d0000000000000000;
	fma.rn.f64 	%fd2671, %fd5626, %fd2562, 0d0000000000000000;
	add.f64 	%fd2672, %fd2671, %fd2663;
	fma.rn.f64 	%fd2673, %fd5627, %fd2670, %fd2667;
	fma.rn.f64 	%fd2674, %fd5090, %fd2670, 0d0000000000000000;
	add.f64 	%fd2675, %fd2651, %fd2674;
	fma.rn.f64 	%fd2676, %fd5619, %fd2569, 0d0000000000000000;
	fma.rn.f64 	%fd2677, %fd5623, %fd2569, 0d0000000000000000;
	add.f64 	%fd2678, %fd2677, %fd2672;
	fma.rn.f64 	%fd2679, %fd5622, %fd2676, %fd2673;
	fma.rn.f64 	%fd2680, %fd5090, %fd2676, 0d0000000000000000;
	add.f64 	%fd2681, %fd2666, %fd2680;
	fma.rn.f64 	%fd2682, %fd5619, %fd2576, 0d0000000000000000;
	fma.rn.f64 	%fd2683, %fd5620, %fd2576, 0d0000000000000000;
	add.f64 	%fd2684, %fd2683, %fd2678;
	fma.rn.f64 	%fd2685, %fd5619, %fd2682, %fd2679;
	fma.rn.f64 	%fd2686, %fd5090, %fd2682, 0d0000000000000000;
	add.f64 	%fd2687, %fd2684, %fd2686;
	fma.rn.f64 	%fd2688, %fd5619, %fd2583, 0d0000000000000000;
	fma.rn.f64 	%fd2689, %fd5617, %fd2583, 0d0000000000000000;
	add.f64 	%fd2690, %fd2689, %fd2687;
	fma.rn.f64 	%fd2691, %fd5616, %fd2688, %fd2685;
	fma.rn.f64 	%fd2692, %fd5090, %fd2688, 0d0000000000000000;
	add.f64 	%fd2693, %fd2669, %fd2692;
	fma.rn.f64 	%fd2694, %fd5616, %fd2590, 0d0000000000000000;
	fma.rn.f64 	%fd2695, %fd5626, %fd2590, 0d0000000000000000;
	add.f64 	%fd2696, %fd2695, %fd2693;
	fma.rn.f64 	%fd2697, %fd5627, %fd2694, %fd2691;
	fma.rn.f64 	%fd2698, %fd5090, %fd2694, 0d0000000000000000;
	add.f64 	%fd2699, %fd2675, %fd2698;
	fma.rn.f64 	%fd2700, %fd5616, %fd2597, 0d0000000000000000;
	fma.rn.f64 	%fd2701, %fd5623, %fd2597, 0d0000000000000000;
	add.f64 	%fd2702, %fd2701, %fd2696;
	fma.rn.f64 	%fd2703, %fd5622, %fd2700, %fd2697;
	fma.rn.f64 	%fd2704, %fd5090, %fd2700, 0d0000000000000000;
	add.f64 	%fd2705, %fd2681, %fd2704;
	fma.rn.f64 	%fd2706, %fd5616, %fd2604, 0d0000000000000000;
	fma.rn.f64 	%fd2707, %fd5620, %fd2604, 0d0000000000000000;
	add.f64 	%fd2708, %fd2707, %fd2702;
	fma.rn.f64 	%fd2709, %fd5619, %fd2706, %fd2703;
	fma.rn.f64 	%fd2710, %fd5090, %fd2706, 0d0000000000000000;
	add.f64 	%fd2711, %fd2690, %fd2710;
	fma.rn.f64 	%fd2712, %fd5616, %fd2611, 0d0000000000000000;
	fma.rn.f64 	%fd2713, %fd5617, %fd2611, 0d0000000000000000;
	add.f64 	%fd2714, %fd2713, %fd2708;
	fma.rn.f64 	%fd2715, %fd5616, %fd2712, %fd2709;
	fma.rn.f64 	%fd2716, %fd5090, %fd2712, 0d0000000000000000;
	add.f64 	%fd2717, %fd2716, %fd2714;
	add.f64 	%fd2718, %fd2717, 0d0000000000000000;
	add.f64 	%fd2719, %fd2711, 0d0000000000000000;
	add.f64 	%fd2720, %fd2705, 0d0000000000000000;
	add.f64 	%fd2721, %fd2699, 0d0000000000000000;
	sub.f64 	%fd2722, %fd2510, %fd2718;
	add.f64 	%fd2723, %fd2721, %fd2722;
	add.f64 	%fd2724, %fd2720, %fd2722;
	add.f64 	%fd2725, %fd2719, %fd2722;
	fma.rn.f64 	%fd2726, %fd5579, %fd2509, 0d0000000000000000;
	fma.rn.f64 	%fd2727, %fd5578, %fd2509, 0d0000000000000000;
	fma.rn.f64 	%fd2728, %fd5579, %fd2726, %fd2715;
	fma.rn.f64 	%fd2729, %fd5090, %fd2726, 0d0000000000000000;
	add.f64 	%fd2730, %fd2727, %fd2729;
	fma.rn.f64 	%fd2731, %fd5579, %fd2516, 0d0000000000000000;
	fma.rn.f64 	%fd2732, %fd5575, %fd2516, 0d0000000000000000;
	add.f64 	%fd2733, %fd2732, %fd2730;
	fma.rn.f64 	%fd2734, %fd5574, %fd2731, %fd2728;
	fma.rn.f64 	%fd2735, %fd5090, %fd2731, 0d0000000000000000;
	fma.rn.f64 	%fd2736, %fd5579, %fd2522, 0d0000000000000000;
	fma.rn.f64 	%fd2737, %fd5572, %fd2522, 0d0000000000000000;
	add.f64 	%fd2738, %fd2737, %fd2733;
	fma.rn.f64 	%fd2739, %fd5571, %fd2736, %fd2734;
	fma.rn.f64 	%fd2740, %fd5090, %fd2736, 0d0000000000000000;
	fma.rn.f64 	%fd2741, %fd5579, %fd2528, 0d0000000000000000;
	fma.rn.f64 	%fd2742, %fd5569, %fd2528, 0d0000000000000000;
	add.f64 	%fd2743, %fd2742, %fd2738;
	fma.rn.f64 	%fd2744, %fd5568, %fd2741, %fd2739;
	fma.rn.f64 	%fd2745, %fd5090, %fd2741, 0d0000000000000000;
	fma.rn.f64 	%fd2746, %fd5574, %fd2534, 0d0000000000000000;
	fma.rn.f64 	%fd2747, %fd5578, %fd2534, 0d0000000000000000;
	add.f64 	%fd2748, %fd2735, %fd2747;
	fma.rn.f64 	%fd2749, %fd5579, %fd2746, %fd2744;
	fma.rn.f64 	%fd2750, %fd5090, %fd2746, 0d0000000000000000;
	add.f64 	%fd2751, %fd2743, %fd2750;
	fma.rn.f64 	%fd2752, %fd5574, %fd2541, 0d0000000000000000;
	fma.rn.f64 	%fd2753, %fd5575, %fd2541, 0d0000000000000000;
	add.f64 	%fd2754, %fd2748, %fd2753;
	fma.rn.f64 	%fd2755, %fd5574, %fd2752, %fd2749;
	fma.rn.f64 	%fd2756, %fd5090, %fd2752, 0d0000000000000000;
	add.f64 	%fd2757, %fd2754, %fd2756;
	fma.rn.f64 	%fd2758, %fd5574, %fd2548, 0d0000000000000000;
	fma.rn.f64 	%fd2759, %fd5572, %fd2548, 0d0000000000000000;
	add.f64 	%fd2760, %fd2759, %fd2757;
	fma.rn.f64 	%fd2761, %fd5571, %fd2758, %fd2755;
	fma.rn.f64 	%fd2762, %fd5090, %fd2758, 0d0000000000000000;
	add.f64 	%fd2763, %fd2740, %fd2762;
	fma.rn.f64 	%fd2764, %fd5574, %fd2555, 0d0000000000000000;
	fma.rn.f64 	%fd2765, %fd5569, %fd2555, 0d0000000000000000;
	add.f64 	%fd2766, %fd2765, %fd2760;
	fma.rn.f64 	%fd2767, %fd5568, %fd2764, %fd2761;
	fma.rn.f64 	%fd2768, %fd5090, %fd2764, 0d0000000000000000;
	add.f64 	%fd2769, %fd2745, %fd2768;
	fma.rn.f64 	%fd2770, %fd5571, %fd2562, 0d0000000000000000;
	fma.rn.f64 	%fd2771, %fd5578, %fd2562, 0d0000000000000000;
	add.f64 	%fd2772, %fd2771, %fd2763;
	fma.rn.f64 	%fd2773, %fd5579, %fd2770, %fd2767;
	fma.rn.f64 	%fd2774, %fd5090, %fd2770, 0d0000000000000000;
	add.f64 	%fd2775, %fd2751, %fd2774;
	fma.rn.f64 	%fd2776, %fd5571, %fd2569, 0d0000000000000000;
	fma.rn.f64 	%fd2777, %fd5575, %fd2569, 0d0000000000000000;
	add.f64 	%fd2778, %fd2777, %fd2772;
	fma.rn.f64 	%fd2779, %fd5574, %fd2776, %fd2773;
	fma.rn.f64 	%fd2780, %fd5090, %fd2776, 0d0000000000000000;
	add.f64 	%fd2781, %fd2766, %fd2780;
	fma.rn.f64 	%fd2782, %fd5571, %fd2576, 0d0000000000000000;
	fma.rn.f64 	%fd2783, %fd5572, %fd2576, 0d0000000000000000;
	add.f64 	%fd2784, %fd2783, %fd2778;
	fma.rn.f64 	%fd2785, %fd5571, %fd2782, %fd2779;
	fma.rn.f64 	%fd2786, %fd5090, %fd2782, 0d0000000000000000;
	add.f64 	%fd2787, %fd2784, %fd2786;
	fma.rn.f64 	%fd2788, %fd5571, %fd2583, 0d0000000000000000;
	fma.rn.f64 	%fd2789, %fd5569, %fd2583, 0d0000000000000000;
	add.f64 	%fd2790, %fd2789, %fd2787;
	fma.rn.f64 	%fd2791, %fd5568, %fd2788, %fd2785;
	fma.rn.f64 	%fd2792, %fd5090, %fd2788, 0d0000000000000000;
	add.f64 	%fd2793, %fd2769, %fd2792;
	fma.rn.f64 	%fd2794, %fd5568, %fd2590, 0d0000000000000000;
	fma.rn.f64 	%fd2795, %fd5578, %fd2590, 0d0000000000000000;
	add.f64 	%fd2796, %fd2795, %fd2793;
	fma.rn.f64 	%fd2797, %fd5579, %fd2794, %fd2791;
	fma.rn.f64 	%fd2798, %fd5090, %fd2794, 0d0000000000000000;
	add.f64 	%fd2799, %fd2775, %fd2798;
	fma.rn.f64 	%fd2800, %fd5568, %fd2597, 0d0000000000000000;
	fma.rn.f64 	%fd2801, %fd5575, %fd2597, 0d0000000000000000;
	add.f64 	%fd2802, %fd2801, %fd2796;
	fma.rn.f64 	%fd2803, %fd5574, %fd2800, %fd2797;
	fma.rn.f64 	%fd2804, %fd5090, %fd2800, 0d0000000000000000;
	add.f64 	%fd2805, %fd2781, %fd2804;
	fma.rn.f64 	%fd2806, %fd5568, %fd2604, 0d0000000000000000;
	fma.rn.f64 	%fd2807, %fd5572, %fd2604, 0d0000000000000000;
	add.f64 	%fd2808, %fd2807, %fd2802;
	fma.rn.f64 	%fd2809, %fd5571, %fd2806, %fd2803;
	fma.rn.f64 	%fd2810, %fd5090, %fd2806, 0d0000000000000000;
	add.f64 	%fd2811, %fd2790, %fd2810;
	fma.rn.f64 	%fd2812, %fd5568, %fd2611, 0d0000000000000000;
	fma.rn.f64 	%fd2813, %fd5569, %fd2611, 0d0000000000000000;
	add.f64 	%fd2814, %fd2813, %fd2808;
	fma.rn.f64 	%fd2815, %fd5568, %fd2812, %fd2809;
	fma.rn.f64 	%fd2816, %fd5090, %fd2812, 0d0000000000000000;
	add.f64 	%fd2817, %fd2816, %fd2814;
	add.f64 	%fd2818, %fd2817, 0d0000000000000000;
	add.f64 	%fd2819, %fd2811, 0d0000000000000000;
	add.f64 	%fd2820, %fd2805, 0d0000000000000000;
	add.f64 	%fd2821, %fd2799, 0d0000000000000000;
	sub.f64 	%fd2822, %fd2510, %fd2818;
	add.f64 	%fd2823, %fd2821, %fd2822;
	add.f64 	%fd2824, %fd2820, %fd2822;
	add.f64 	%fd2825, %fd2819, %fd2822;
	fma.rn.f64 	%fd2826, %fd5531, %fd2509, 0d0000000000000000;
	fma.rn.f64 	%fd2827, %fd5530, %fd2509, 0d0000000000000000;
	fma.rn.f64 	%fd2828, %fd5531, %fd2826, %fd2815;
	fma.rn.f64 	%fd2829, %fd5090, %fd2826, 0d0000000000000000;
	add.f64 	%fd2830, %fd2827, %fd2829;
	fma.rn.f64 	%fd2831, %fd5531, %fd2516, 0d0000000000000000;
	fma.rn.f64 	%fd2832, %fd5527, %fd2516, 0d0000000000000000;
	add.f64 	%fd2833, %fd2832, %fd2830;
	fma.rn.f64 	%fd2834, %fd5526, %fd2831, %fd2828;
	fma.rn.f64 	%fd2835, %fd5090, %fd2831, 0d0000000000000000;
	fma.rn.f64 	%fd2836, %fd5531, %fd2522, 0d0000000000000000;
	fma.rn.f64 	%fd2837, %fd5524, %fd2522, 0d0000000000000000;
	add.f64 	%fd2838, %fd2837, %fd2833;
	fma.rn.f64 	%fd2839, %fd5523, %fd2836, %fd2834;
	fma.rn.f64 	%fd2840, %fd5090, %fd2836, 0d0000000000000000;
	fma.rn.f64 	%fd2841, %fd5531, %fd2528, 0d0000000000000000;
	fma.rn.f64 	%fd2842, %fd5521, %fd2528, 0d0000000000000000;
	add.f64 	%fd2843, %fd2842, %fd2838;
	fma.rn.f64 	%fd2844, %fd5520, %fd2841, %fd2839;
	fma.rn.f64 	%fd2845, %fd5090, %fd2841, 0d0000000000000000;
	fma.rn.f64 	%fd2846, %fd5526, %fd2534, 0d0000000000000000;
	fma.rn.f64 	%fd2847, %fd5530, %fd2534, 0d0000000000000000;
	add.f64 	%fd2848, %fd2835, %fd2847;
	fma.rn.f64 	%fd2849, %fd5531, %fd2846, %fd2844;
	fma.rn.f64 	%fd2850, %fd5090, %fd2846, 0d0000000000000000;
	add.f64 	%fd2851, %fd2843, %fd2850;
	fma.rn.f64 	%fd2852, %fd5526, %fd2541, 0d0000000000000000;
	fma.rn.f64 	%fd2853, %fd5527, %fd2541, 0d0000000000000000;
	add.f64 	%fd2854, %fd2848, %fd2853;
	fma.rn.f64 	%fd2855, %fd5526, %fd2852, %fd2849;
	fma.rn.f64 	%fd2856, %fd5090, %fd2852, 0d0000000000000000;
	add.f64 	%fd2857, %fd2854, %fd2856;
	fma.rn.f64 	%fd2858, %fd5526, %fd2548, 0d0000000000000000;
	fma.rn.f64 	%fd2859, %fd5524, %fd2548, 0d0000000000000000;
	add.f64 	%fd2860, %fd2859, %fd2857;
	fma.rn.f64 	%fd2861, %fd5523, %fd2858, %fd2855;
	fma.rn.f64 	%fd2862, %fd5090, %fd2858, 0d0000000000000000;
	add.f64 	%fd2863, %fd2840, %fd2862;
	fma.rn.f64 	%fd2864, %fd5526, %fd2555, 0d0000000000000000;
	fma.rn.f64 	%fd2865, %fd5521, %fd2555, 0d0000000000000000;
	add.f64 	%fd2866, %fd2865, %fd2860;
	fma.rn.f64 	%fd2867, %fd5520, %fd2864, %fd2861;
	fma.rn.f64 	%fd2868, %fd5090, %fd2864, 0d0000000000000000;
	add.f64 	%fd2869, %fd2845, %fd2868;
	fma.rn.f64 	%fd2870, %fd5523, %fd2562, 0d0000000000000000;
	fma.rn.f64 	%fd2871, %fd5530, %fd2562, 0d0000000000000000;
	add.f64 	%fd2872, %fd2871, %fd2863;
	fma.rn.f64 	%fd2873, %fd5531, %fd2870, %fd2867;
	fma.rn.f64 	%fd2874, %fd5090, %fd2870, 0d0000000000000000;
	add.f64 	%fd2875, %fd2851, %fd2874;
	fma.rn.f64 	%fd2876, %fd5523, %fd2569, 0d0000000000000000;
	fma.rn.f64 	%fd2877, %fd5527, %fd2569, 0d0000000000000000;
	add.f64 	%fd2878, %fd2877, %fd2872;
	fma.rn.f64 	%fd2879, %fd5526, %fd2876, %fd2873;
	fma.rn.f64 	%fd2880, %fd5090, %fd2876, 0d0000000000000000;
	add.f64 	%fd2881, %fd2866, %fd2880;
	fma.rn.f64 	%fd2882, %fd5523, %fd2576, 0d0000000000000000;
	fma.rn.f64 	%fd2883, %fd5524, %fd2576, 0d0000000000000000;
	add.f64 	%fd2884, %fd2883, %fd2878;
	fma.rn.f64 	%fd2885, %fd5523, %fd2882, %fd2879;
	fma.rn.f64 	%fd2886, %fd5090, %fd2882, 0d0000000000000000;
	add.f64 	%fd2887, %fd2884, %fd2886;
	fma.rn.f64 	%fd2888, %fd5523, %fd2583, 0d0000000000000000;
	fma.rn.f64 	%fd2889, %fd5521, %fd2583, 0d0000000000000000;
	add.f64 	%fd2890, %fd2889, %fd2887;
	fma.rn.f64 	%fd2891, %fd5520, %fd2888, %fd2885;
	fma.rn.f64 	%fd2892, %fd5090, %fd2888, 0d0000000000000000;
	add.f64 	%fd2893, %fd2869, %fd2892;
	fma.rn.f64 	%fd2894, %fd5520, %fd2590, 0d0000000000000000;
	fma.rn.f64 	%fd2895, %fd5530, %fd2590, 0d0000000000000000;
	add.f64 	%fd2896, %fd2895, %fd2893;
	fma.rn.f64 	%fd2897, %fd5531, %fd2894, %fd2891;
	fma.rn.f64 	%fd2898, %fd5090, %fd2894, 0d0000000000000000;
	add.f64 	%fd2899, %fd2875, %fd2898;
	fma.rn.f64 	%fd2900, %fd5520, %fd2597, 0d0000000000000000;
	fma.rn.f64 	%fd2901, %fd5527, %fd2597, 0d0000000000000000;
	add.f64 	%fd2902, %fd2901, %fd2896;
	fma.rn.f64 	%fd2903, %fd5526, %fd2900, %fd2897;
	fma.rn.f64 	%fd2904, %fd5090, %fd2900, 0d0000000000000000;
	add.f64 	%fd2905, %fd2881, %fd2904;
	fma.rn.f64 	%fd2906, %fd5520, %fd2604, 0d0000000000000000;
	fma.rn.f64 	%fd2907, %fd5524, %fd2604, 0d0000000000000000;
	add.f64 	%fd2908, %fd2907, %fd2902;
	fma.rn.f64 	%fd2909, %fd5523, %fd2906, %fd2903;
	fma.rn.f64 	%fd2910, %fd5090, %fd2906, 0d0000000000000000;
	add.f64 	%fd2911, %fd2890, %fd2910;
	fma.rn.f64 	%fd2912, %fd5520, %fd2611, 0d0000000000000000;
	fma.rn.f64 	%fd2913, %fd5521, %fd2611, 0d0000000000000000;
	add.f64 	%fd2914, %fd2913, %fd2908;
	fma.rn.f64 	%fd2915, %fd5520, %fd2912, %fd2909;
	fma.rn.f64 	%fd2916, %fd5090, %fd2912, 0d0000000000000000;
	add.f64 	%fd2917, %fd2916, %fd2914;
	add.f64 	%fd2918, %fd2917, 0d0000000000000000;
	add.f64 	%fd2919, %fd2911, 0d0000000000000000;
	add.f64 	%fd2920, %fd2905, 0d0000000000000000;
	add.f64 	%fd2921, %fd2899, 0d0000000000000000;
	sub.f64 	%fd2922, %fd2510, %fd2918;
	add.f64 	%fd2923, %fd2921, %fd2922;
	add.f64 	%fd2924, %fd2920, %fd2922;
	add.f64 	%fd2925, %fd2919, %fd2922;
	fma.rn.f64 	%fd2926, %fd5483, %fd2509, 0d0000000000000000;
	fma.rn.f64 	%fd2927, %fd5482, %fd2509, 0d0000000000000000;
	fma.rn.f64 	%fd2928, %fd5483, %fd2926, %fd2915;
	fma.rn.f64 	%fd2929, %fd5090, %fd2926, 0d0000000000000000;
	add.f64 	%fd2930, %fd2927, %fd2929;
	fma.rn.f64 	%fd2931, %fd5483, %fd2516, 0d0000000000000000;
	fma.rn.f64 	%fd2932, %fd5479, %fd2516, 0d0000000000000000;
	add.f64 	%fd2933, %fd2932, %fd2930;
	fma.rn.f64 	%fd2934, %fd5478, %fd2931, %fd2928;
	fma.rn.f64 	%fd2935, %fd5090, %fd2931, 0d0000000000000000;
	fma.rn.f64 	%fd2936, %fd5483, %fd2522, 0d0000000000000000;
	fma.rn.f64 	%fd2937, %fd5476, %fd2522, 0d0000000000000000;
	add.f64 	%fd2938, %fd2937, %fd2933;
	fma.rn.f64 	%fd2939, %fd5475, %fd2936, %fd2934;
	fma.rn.f64 	%fd2940, %fd5090, %fd2936, 0d0000000000000000;
	fma.rn.f64 	%fd2941, %fd5483, %fd2528, 0d0000000000000000;
	fma.rn.f64 	%fd2942, %fd5473, %fd2528, 0d0000000000000000;
	add.f64 	%fd2943, %fd2942, %fd2938;
	fma.rn.f64 	%fd2944, %fd5472, %fd2941, %fd2939;
	fma.rn.f64 	%fd2945, %fd5090, %fd2941, 0d0000000000000000;
	fma.rn.f64 	%fd2946, %fd5478, %fd2534, 0d0000000000000000;
	fma.rn.f64 	%fd2947, %fd5482, %fd2534, 0d0000000000000000;
	add.f64 	%fd2948, %fd2935, %fd2947;
	fma.rn.f64 	%fd2949, %fd5483, %fd2946, %fd2944;
	fma.rn.f64 	%fd2950, %fd5090, %fd2946, 0d0000000000000000;
	add.f64 	%fd2951, %fd2943, %fd2950;
	fma.rn.f64 	%fd2952, %fd5478, %fd2541, 0d0000000000000000;
	fma.rn.f64 	%fd2953, %fd5479, %fd2541, 0d0000000000000000;
	add.f64 	%fd2954, %fd2948, %fd2953;
	fma.rn.f64 	%fd2955, %fd5478, %fd2952, %fd2949;
	fma.rn.f64 	%fd2956, %fd5090, %fd2952, 0d0000000000000000;
	add.f64 	%fd2957, %fd2954, %fd2956;
	fma.rn.f64 	%fd2958, %fd5478, %fd2548, 0d0000000000000000;
	fma.rn.f64 	%fd2959, %fd5476, %fd2548, 0d0000000000000000;
	add.f64 	%fd2960, %fd2959, %fd2957;
	fma.rn.f64 	%fd2961, %fd5475, %fd2958, %fd2955;
	fma.rn.f64 	%fd2962, %fd5090, %fd2958, 0d0000000000000000;
	add.f64 	%fd2963, %fd2940, %fd2962;
	fma.rn.f64 	%fd2964, %fd5478, %fd2555, 0d0000000000000000;
	fma.rn.f64 	%fd2965, %fd5473, %fd2555, 0d0000000000000000;
	add.f64 	%fd2966, %fd2965, %fd2960;
	fma.rn.f64 	%fd2967, %fd5472, %fd2964, %fd2961;
	fma.rn.f64 	%fd2968, %fd5090, %fd2964, 0d0000000000000000;
	add.f64 	%fd2969, %fd2945, %fd2968;
	fma.rn.f64 	%fd2970, %fd5475, %fd2562, 0d0000000000000000;
	fma.rn.f64 	%fd2971, %fd5482, %fd2562, 0d0000000000000000;
	add.f64 	%fd2972, %fd2971, %fd2963;
	fma.rn.f64 	%fd2973, %fd5483, %fd2970, %fd2967;
	fma.rn.f64 	%fd2974, %fd5090, %fd2970, 0d0000000000000000;
	add.f64 	%fd2975, %fd2951, %fd2974;
	fma.rn.f64 	%fd2976, %fd5475, %fd2569, 0d0000000000000000;
	fma.rn.f64 	%fd2977, %fd5479, %fd2569, 0d0000000000000000;
	add.f64 	%fd2978, %fd2977, %fd2972;
	fma.rn.f64 	%fd2979, %fd5478, %fd2976, %fd2973;
	fma.rn.f64 	%fd2980, %fd5090, %fd2976, 0d0000000000000000;
	add.f64 	%fd2981, %fd2966, %fd2980;
	fma.rn.f64 	%fd2982, %fd5475, %fd2576, 0d0000000000000000;
	fma.rn.f64 	%fd2983, %fd5476, %fd2576, 0d0000000000000000;
	add.f64 	%fd2984, %fd2983, %fd2978;
	fma.rn.f64 	%fd2985, %fd5475, %fd2982, %fd2979;
	fma.rn.f64 	%fd2986, %fd5090, %fd2982, 0d0000000000000000;
	add.f64 	%fd2987, %fd2984, %fd2986;
	fma.rn.f64 	%fd2988, %fd5475, %fd2583, 0d0000000000000000;
	fma.rn.f64 	%fd2989, %fd5473, %fd2583, 0d0000000000000000;
	add.f64 	%fd2990, %fd2989, %fd2987;
	fma.rn.f64 	%fd2991, %fd5472, %fd2988, %fd2985;
	fma.rn.f64 	%fd2992, %fd5090, %fd2988, 0d0000000000000000;
	add.f64 	%fd2993, %fd2969, %fd2992;
	fma.rn.f64 	%fd2994, %fd5472, %fd2590, 0d0000000000000000;
	fma.rn.f64 	%fd2995, %fd5482, %fd2590, 0d0000000000000000;
	add.f64 	%fd2996, %fd2995, %fd2993;
	fma.rn.f64 	%fd2997, %fd5483, %fd2994, %fd2991;
	fma.rn.f64 	%fd2998, %fd5090, %fd2994, 0d0000000000000000;
	add.f64 	%fd2999, %fd2975, %fd2998;
	fma.rn.f64 	%fd3000, %fd5472, %fd2597, 0d0000000000000000;
	fma.rn.f64 	%fd3001, %fd5479, %fd2597, 0d0000000000000000;
	add.f64 	%fd3002, %fd3001, %fd2996;
	fma.rn.f64 	%fd3003, %fd5478, %fd3000, %fd2997;
	fma.rn.f64 	%fd3004, %fd5090, %fd3000, 0d0000000000000000;
	add.f64 	%fd3005, %fd2981, %fd3004;
	fma.rn.f64 	%fd3006, %fd5472, %fd2604, 0d0000000000000000;
	fma.rn.f64 	%fd3007, %fd5476, %fd2604, 0d0000000000000000;
	add.f64 	%fd3008, %fd3007, %fd3002;
	fma.rn.f64 	%fd3009, %fd5475, %fd3006, %fd3003;
	fma.rn.f64 	%fd3010, %fd5090, %fd3006, 0d0000000000000000;
	add.f64 	%fd3011, %fd2990, %fd3010;
	fma.rn.f64 	%fd3012, %fd5472, %fd2611, 0d0000000000000000;
	fma.rn.f64 	%fd3013, %fd5473, %fd2611, 0d0000000000000000;
	add.f64 	%fd3014, %fd3013, %fd3008;
	fma.rn.f64 	%fd3015, %fd5472, %fd3012, %fd3009;
	fma.rn.f64 	%fd3016, %fd5090, %fd3012, 0d0000000000000000;
	add.f64 	%fd3017, %fd3016, %fd3014;
	add.f64 	%fd3018, %fd3017, 0d0000000000000000;
	add.f64 	%fd3019, %fd3011, 0d0000000000000000;
	add.f64 	%fd3020, %fd3005, 0d0000000000000000;
	add.f64 	%fd3021, %fd2999, 0d0000000000000000;
	sub.f64 	%fd3022, %fd2510, %fd3018;
	add.f64 	%fd3023, %fd3021, %fd3022;
	add.f64 	%fd3024, %fd3020, %fd3022;
	add.f64 	%fd3025, %fd3019, %fd3022;
	fma.rn.f64 	%fd3026, %fd5435, %fd2509, 0d0000000000000000;
	fma.rn.f64 	%fd3027, %fd5434, %fd2509, 0d0000000000000000;
	fma.rn.f64 	%fd3028, %fd5435, %fd3026, %fd3015;
	fma.rn.f64 	%fd3029, %fd5090, %fd3026, 0d0000000000000000;
	add.f64 	%fd3030, %fd3027, %fd3029;
	fma.rn.f64 	%fd3031, %fd5435, %fd2516, 0d0000000000000000;
	fma.rn.f64 	%fd3032, %fd5431, %fd2516, 0d0000000000000000;
	add.f64 	%fd3033, %fd3032, %fd3030;
	fma.rn.f64 	%fd3034, %fd5430, %fd3031, %fd3028;
	fma.rn.f64 	%fd3035, %fd5090, %fd3031, 0d0000000000000000;
	fma.rn.f64 	%fd3036, %fd5435, %fd2522, 0d0000000000000000;
	fma.rn.f64 	%fd3037, %fd5428, %fd2522, 0d0000000000000000;
	add.f64 	%fd3038, %fd3037, %fd3033;
	fma.rn.f64 	%fd3039, %fd5427, %fd3036, %fd3034;
	fma.rn.f64 	%fd3040, %fd5090, %fd3036, 0d0000000000000000;
	fma.rn.f64 	%fd3041, %fd5435, %fd2528, 0d0000000000000000;
	fma.rn.f64 	%fd3042, %fd5425, %fd2528, 0d0000000000000000;
	add.f64 	%fd3043, %fd3042, %fd3038;
	fma.rn.f64 	%fd3044, %fd5424, %fd3041, %fd3039;
	fma.rn.f64 	%fd3045, %fd5090, %fd3041, 0d0000000000000000;
	fma.rn.f64 	%fd3046, %fd5430, %fd2534, 0d0000000000000000;
	fma.rn.f64 	%fd3047, %fd5434, %fd2534, 0d0000000000000000;
	add.f64 	%fd3048, %fd3035, %fd3047;
	fma.rn.f64 	%fd3049, %fd5435, %fd3046, %fd3044;
	fma.rn.f64 	%fd3050, %fd5090, %fd3046, 0d0000000000000000;
	add.f64 	%fd3051, %fd3043, %fd3050;
	fma.rn.f64 	%fd3052, %fd5430, %fd2541, 0d0000000000000000;
	fma.rn.f64 	%fd3053, %fd5431, %fd2541, 0d0000000000000000;
	add.f64 	%fd3054, %fd3048, %fd3053;
	fma.rn.f64 	%fd3055, %fd5430, %fd3052, %fd3049;
	fma.rn.f64 	%fd3056, %fd5090, %fd3052, 0d0000000000000000;
	add.f64 	%fd3057, %fd3054, %fd3056;
	fma.rn.f64 	%fd3058, %fd5430, %fd2548, 0d0000000000000000;
	fma.rn.f64 	%fd3059, %fd5428, %fd2548, 0d0000000000000000;
	add.f64 	%fd3060, %fd3059, %fd3057;
	fma.rn.f64 	%fd3061, %fd5427, %fd3058, %fd3055;
	fma.rn.f64 	%fd3062, %fd5090, %fd3058, 0d0000000000000000;
	add.f64 	%fd3063, %fd3040, %fd3062;
	fma.rn.f64 	%fd3064, %fd5430, %fd2555, 0d0000000000000000;
	fma.rn.f64 	%fd3065, %fd5425, %fd2555, 0d0000000000000000;
	add.f64 	%fd3066, %fd3065, %fd3060;
	fma.rn.f64 	%fd3067, %fd5424, %fd3064, %fd3061;
	fma.rn.f64 	%fd3068, %fd5090, %fd3064, 0d0000000000000000;
	add.f64 	%fd3069, %fd3045, %fd3068;
	fma.rn.f64 	%fd3070, %fd5427, %fd2562, 0d0000000000000000;
	fma.rn.f64 	%fd3071, %fd5434, %fd2562, 0d0000000000000000;
	add.f64 	%fd3072, %fd3071, %fd3063;
	fma.rn.f64 	%fd3073, %fd5435, %fd3070, %fd3067;
	fma.rn.f64 	%fd3074, %fd5090, %fd3070, 0d0000000000000000;
	add.f64 	%fd3075, %fd3051, %fd3074;
	fma.rn.f64 	%fd3076, %fd5427, %fd2569, 0d0000000000000000;
	fma.rn.f64 	%fd3077, %fd5431, %fd2569, 0d0000000000000000;
	add.f64 	%fd3078, %fd3077, %fd3072;
	fma.rn.f64 	%fd3079, %fd5430, %fd3076, %fd3073;
	fma.rn.f64 	%fd3080, %fd5090, %fd3076, 0d0000000000000000;
	add.f64 	%fd3081, %fd3066, %fd3080;
	fma.rn.f64 	%fd3082, %fd5427, %fd2576, 0d0000000000000000;
	fma.rn.f64 	%fd3083, %fd5428, %fd2576, 0d0000000000000000;
	add.f64 	%fd3084, %fd3083, %fd3078;
	fma.rn.f64 	%fd3085, %fd5427, %fd3082, %fd3079;
	fma.rn.f64 	%fd3086, %fd5090, %fd3082, 0d0000000000000000;
	add.f64 	%fd3087, %fd3084, %fd3086;
	fma.rn.f64 	%fd3088, %fd5427, %fd2583, 0d0000000000000000;
	fma.rn.f64 	%fd3089, %fd5425, %fd2583, 0d0000000000000000;
	add.f64 	%fd3090, %fd3089, %fd3087;
	fma.rn.f64 	%fd3091, %fd5424, %fd3088, %fd3085;
	fma.rn.f64 	%fd3092, %fd5090, %fd3088, 0d0000000000000000;
	add.f64 	%fd3093, %fd3069, %fd3092;
	fma.rn.f64 	%fd3094, %fd5424, %fd2590, 0d0000000000000000;
	fma.rn.f64 	%fd3095, %fd5434, %fd2590, 0d0000000000000000;
	add.f64 	%fd3096, %fd3095, %fd3093;
	fma.rn.f64 	%fd3097, %fd5435, %fd3094, %fd3091;
	fma.rn.f64 	%fd3098, %fd5090, %fd3094, 0d0000000000000000;
	add.f64 	%fd3099, %fd3075, %fd3098;
	fma.rn.f64 	%fd3100, %fd5424, %fd2597, 0d0000000000000000;
	fma.rn.f64 	%fd3101, %fd5431, %fd2597, 0d0000000000000000;
	add.f64 	%fd3102, %fd3101, %fd3096;
	fma.rn.f64 	%fd3103, %fd5430, %fd3100, %fd3097;
	fma.rn.f64 	%fd3104, %fd5090, %fd3100, 0d0000000000000000;
	add.f64 	%fd3105, %fd3081, %fd3104;
	fma.rn.f64 	%fd3106, %fd5424, %fd2604, 0d0000000000000000;
	fma.rn.f64 	%fd3107, %fd5428, %fd2604, 0d0000000000000000;
	add.f64 	%fd3108, %fd3107, %fd3102;
	fma.rn.f64 	%fd3109, %fd5427, %fd3106, %fd3103;
	fma.rn.f64 	%fd3110, %fd5090, %fd3106, 0d0000000000000000;
	add.f64 	%fd3111, %fd3090, %fd3110;
	fma.rn.f64 	%fd3112, %fd5424, %fd2611, 0d0000000000000000;
	fma.rn.f64 	%fd3113, %fd5425, %fd2611, 0d0000000000000000;
	add.f64 	%fd3114, %fd3113, %fd3108;
	fma.rn.f64 	%fd3115, %fd5424, %fd3112, %fd3109;
	fma.rn.f64 	%fd3116, %fd5090, %fd3112, 0d0000000000000000;
	add.f64 	%fd3117, %fd3116, %fd3114;
	add.f64 	%fd3118, %fd3117, 0d0000000000000000;
	add.f64 	%fd3119, %fd3111, 0d0000000000000000;
	add.f64 	%fd3120, %fd3105, 0d0000000000000000;
	add.f64 	%fd3121, %fd3099, 0d0000000000000000;
	sub.f64 	%fd3122, %fd2510, %fd3118;
	add.f64 	%fd3123, %fd3121, %fd3122;
	add.f64 	%fd3124, %fd3120, %fd3122;
	add.f64 	%fd3125, %fd3119, %fd3122;
	div.rn.f64 	%fd3126, %fd3115, 0d4018000000000000;
	add.f64 	%fd3128, %fd2623, 0d0000000000000000;
	div.rn.f64 	%fd3129, %fd3128, 0d4018000000000000;
	add.f64 	%fd3130, %fd3129, 0d0000000000000000;
	sub.f64 	%fd3131, %fd2510, %fd3130;
	div.rn.f64 	%fd3132, %fd3128, 0d4008000000000000;
	add.f64 	%fd3133, %fd3132, 0d0000000000000000;
	fma.rn.f64 	%fd3134, %fd3133, 0d4000000000000000, 0d0000000000000000;
	sub.f64 	%fd3135, %fd3131, %fd3134;
	sub.f64 	%fd3136, %fd3135, %fd3130;
	add.f64 	%fd3137, %fd2723, 0d0000000000000000;
	add.f64 	%fd3138, %fd3137, %fd3128;
	div.rn.f64 	%fd3139, %fd3137, 0d4008000000000000;
	add.f64 	%fd3140, %fd3139, 0d0000000000000000;
	fma.rn.f64 	%fd3141, %fd3140, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3142, %fd3130, %fd3141;
	sub.f64 	%fd3143, %fd3136, %fd3141;
	div.rn.f64 	%fd3144, %fd3137, 0d4018000000000000;
	add.f64 	%fd3145, %fd3144, 0d0000000000000000;
	add.f64 	%fd3146, %fd3134, %fd3145;
	sub.f64 	%fd3147, %fd3143, %fd3145;
	fma.rn.f64 	%fd3148, %fd3137, 0d4018000000000000, 0d0000000000000000;
	add.f64 	%fd3149, %fd3130, %fd3148;
	sub.f64 	%fd3150, %fd3147, %fd3148;
	add.f64 	%fd3151, %fd2823, 0d0000000000000000;
	add.f64 	%fd3152, %fd3151, %fd3138;
	div.rn.f64 	%fd3153, %fd3151, 0d4018000000000000;
	add.f64 	%fd3154, %fd3153, 0d0000000000000000;
	add.f64 	%fd3155, %fd3142, %fd3154;
	sub.f64 	%fd3156, %fd3150, %fd3154;
	add.f64 	%fd3157, %fd3146, %fd3154;
	sub.f64 	%fd3158, %fd3156, %fd3154;
	div.rn.f64 	%fd3159, %fd3151, 0d4008000000000000;
	add.f64 	%fd3160, %fd3159, 0d0000000000000000;
	fma.rn.f64 	%fd3161, %fd3160, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3162, %fd3149, %fd3161;
	sub.f64 	%fd3163, %fd3158, %fd3161;
	add.f64 	%fd3164, %fd2923, 0d0000000000000000;
	sub.f64 	%fd3165, %fd2510, %fd3164;
	add.f64 	%fd3166, %fd3165, %fd3152;
	div.rn.f64 	%fd3167, %fd3164, 0d4018000000000000;
	add.f64 	%fd3168, %fd3167, 0d0000000000000000;
	add.f64 	%fd3169, %fd3155, %fd3168;
	sub.f64 	%fd3170, %fd3163, %fd3168;
	div.rn.f64 	%fd3171, %fd3164, 0d4008000000000000;
	add.f64 	%fd3172, %fd3171, 0d0000000000000000;
	fma.rn.f64 	%fd3173, %fd3172, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3174, %fd3157, %fd3173;
	sub.f64 	%fd3175, %fd3170, %fd3173;
	add.f64 	%fd3176, %fd3162, %fd3168;
	sub.f64 	%fd3177, %fd3175, %fd3168;
	add.f64 	%fd3178, %fd3023, 0d0000000000000000;
	sub.f64 	%fd3179, %fd2510, %fd3178;
	add.f64 	%fd3180, %fd3179, %fd3166;
	div.rn.f64 	%fd3181, %fd3178, 0d4008000000000000;
	add.f64 	%fd3182, %fd3181, 0d0000000000000000;
	fma.rn.f64 	%fd3183, %fd3182, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3184, %fd3169, %fd3183;
	sub.f64 	%fd3185, %fd3177, %fd3183;
	div.rn.f64 	%fd3186, %fd3178, 0d4018000000000000;
	add.f64 	%fd3187, %fd3186, 0d0000000000000000;
	add.f64 	%fd3188, %fd3174, %fd3187;
	sub.f64 	%fd3189, %fd3185, %fd3187;
	fma.rn.f64 	%fd3190, %fd3178, 0d4018000000000000, 0d0000000000000000;
	add.f64 	%fd3191, %fd3176, %fd3190;
	sub.f64 	%fd3192, %fd3189, %fd3190;
	add.f64 	%fd3193, %fd3123, 0d0000000000000000;
	sub.f64 	%fd3194, %fd2510, %fd3193;
	add.f64 	%fd3195, %fd3180, %fd3194;
	div.rn.f64 	%fd3196, %fd3193, 0d4018000000000000;
	add.f64 	%fd3197, %fd3196, 0d0000000000000000;
	sub.f64 	%fd3198, %fd3192, %fd3197;
	sub.f64 	%fd3199, %fd3198, %fd3197;
	div.rn.f64 	%fd3200, %fd3193, 0d4008000000000000;
	add.f64 	%fd3201, %fd3200, 0d0000000000000000;
	fma.rn.f64 	%fd3202, %fd3201, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3203, %fd2624, 0d0000000000000000;
	div.rn.f64 	%fd3204, %fd3203, 0d4018000000000000;
	add.f64 	%fd3205, %fd3204, 0d0000000000000000;
	sub.f64 	%fd3206, %fd2510, %fd3205;
	div.rn.f64 	%fd3207, %fd3203, 0d4008000000000000;
	add.f64 	%fd3208, %fd3207, 0d0000000000000000;
	fma.rn.f64 	%fd3209, %fd3208, 0d4000000000000000, 0d0000000000000000;
	sub.f64 	%fd3210, %fd3206, %fd3209;
	sub.f64 	%fd3211, %fd3210, %fd3205;
	add.f64 	%fd3212, %fd2724, 0d0000000000000000;
	add.f64 	%fd3213, %fd3212, %fd3203;
	div.rn.f64 	%fd3214, %fd3212, 0d4008000000000000;
	add.f64 	%fd3215, %fd3214, 0d0000000000000000;
	fma.rn.f64 	%fd3216, %fd3215, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3217, %fd3205, %fd3216;
	sub.f64 	%fd3218, %fd3211, %fd3216;
	div.rn.f64 	%fd3219, %fd3212, 0d4018000000000000;
	add.f64 	%fd3220, %fd3219, 0d0000000000000000;
	add.f64 	%fd3221, %fd3209, %fd3220;
	sub.f64 	%fd3222, %fd3218, %fd3220;
	fma.rn.f64 	%fd3223, %fd3212, 0d4018000000000000, 0d0000000000000000;
	add.f64 	%fd3224, %fd3205, %fd3223;
	sub.f64 	%fd3225, %fd3222, %fd3223;
	add.f64 	%fd3226, %fd2824, 0d0000000000000000;
	add.f64 	%fd3227, %fd3226, %fd3213;
	div.rn.f64 	%fd3228, %fd3226, 0d4018000000000000;
	add.f64 	%fd3229, %fd3228, 0d0000000000000000;
	add.f64 	%fd3230, %fd3217, %fd3229;
	sub.f64 	%fd3231, %fd3225, %fd3229;
	add.f64 	%fd3232, %fd3221, %fd3229;
	sub.f64 	%fd3233, %fd3231, %fd3229;
	div.rn.f64 	%fd3234, %fd3226, 0d4008000000000000;
	add.f64 	%fd3235, %fd3234, 0d0000000000000000;
	fma.rn.f64 	%fd3236, %fd3235, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3237, %fd3224, %fd3236;
	sub.f64 	%fd3238, %fd3233, %fd3236;
	add.f64 	%fd3239, %fd2924, 0d0000000000000000;
	sub.f64 	%fd3240, %fd2510, %fd3239;
	add.f64 	%fd3241, %fd3240, %fd3227;
	div.rn.f64 	%fd3242, %fd3239, 0d4018000000000000;
	add.f64 	%fd3243, %fd3242, 0d0000000000000000;
	add.f64 	%fd3244, %fd3230, %fd3243;
	sub.f64 	%fd3245, %fd3238, %fd3243;
	div.rn.f64 	%fd3246, %fd3239, 0d4008000000000000;
	add.f64 	%fd3247, %fd3246, 0d0000000000000000;
	fma.rn.f64 	%fd3248, %fd3247, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3249, %fd3232, %fd3248;
	sub.f64 	%fd3250, %fd3245, %fd3248;
	add.f64 	%fd3251, %fd3237, %fd3243;
	sub.f64 	%fd3252, %fd3250, %fd3243;
	add.f64 	%fd3253, %fd3024, 0d0000000000000000;
	sub.f64 	%fd3254, %fd2510, %fd3253;
	add.f64 	%fd3255, %fd3254, %fd3241;
	div.rn.f64 	%fd3256, %fd3253, 0d4008000000000000;
	add.f64 	%fd3257, %fd3256, 0d0000000000000000;
	fma.rn.f64 	%fd3258, %fd3257, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3259, %fd3244, %fd3258;
	sub.f64 	%fd3260, %fd3252, %fd3258;
	div.rn.f64 	%fd3261, %fd3253, 0d4018000000000000;
	add.f64 	%fd3262, %fd3261, 0d0000000000000000;
	add.f64 	%fd3263, %fd3249, %fd3262;
	sub.f64 	%fd3264, %fd3260, %fd3262;
	fma.rn.f64 	%fd3265, %fd3253, 0d4018000000000000, 0d0000000000000000;
	add.f64 	%fd3266, %fd3251, %fd3265;
	sub.f64 	%fd3267, %fd3264, %fd3265;
	add.f64 	%fd3268, %fd3124, 0d0000000000000000;
	sub.f64 	%fd3269, %fd2510, %fd3268;
	add.f64 	%fd3270, %fd3269, %fd3255;
	div.rn.f64 	%fd3271, %fd3268, 0d4018000000000000;
	add.f64 	%fd3272, %fd3271, 0d0000000000000000;
	sub.f64 	%fd3273, %fd3267, %fd3272;
	sub.f64 	%fd3274, %fd3273, %fd3272;
	div.rn.f64 	%fd3275, %fd3268, 0d4008000000000000;
	add.f64 	%fd3276, %fd3275, 0d0000000000000000;
	fma.rn.f64 	%fd3277, %fd3276, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3278, %fd2625, 0d0000000000000000;
	div.rn.f64 	%fd3279, %fd3278, 0d4018000000000000;
	add.f64 	%fd3280, %fd3279, 0d0000000000000000;
	sub.f64 	%fd3281, %fd2510, %fd3280;
	div.rn.f64 	%fd3282, %fd3278, 0d4008000000000000;
	add.f64 	%fd3283, %fd3282, 0d0000000000000000;
	fma.rn.f64 	%fd3284, %fd3283, 0d4000000000000000, 0d0000000000000000;
	sub.f64 	%fd3285, %fd3281, %fd3284;
	sub.f64 	%fd3286, %fd3285, %fd3280;
	add.f64 	%fd3287, %fd2725, 0d0000000000000000;
	add.f64 	%fd3288, %fd3287, %fd3278;
	div.rn.f64 	%fd3289, %fd3287, 0d4008000000000000;
	add.f64 	%fd3290, %fd3289, 0d0000000000000000;
	fma.rn.f64 	%fd3291, %fd3290, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3292, %fd3280, %fd3291;
	sub.f64 	%fd3293, %fd3286, %fd3291;
	div.rn.f64 	%fd3294, %fd3287, 0d4018000000000000;
	add.f64 	%fd3295, %fd3294, 0d0000000000000000;
	add.f64 	%fd3296, %fd3284, %fd3295;
	sub.f64 	%fd3297, %fd3293, %fd3295;
	fma.rn.f64 	%fd3298, %fd3287, 0d4018000000000000, 0d0000000000000000;
	add.f64 	%fd3299, %fd3280, %fd3298;
	sub.f64 	%fd3300, %fd3297, %fd3298;
	add.f64 	%fd3301, %fd2825, 0d0000000000000000;
	add.f64 	%fd3302, %fd3301, %fd3288;
	div.rn.f64 	%fd3303, %fd3301, 0d4018000000000000;
	add.f64 	%fd3304, %fd3303, 0d0000000000000000;
	add.f64 	%fd3305, %fd3292, %fd3304;
	sub.f64 	%fd3306, %fd3300, %fd3304;
	add.f64 	%fd3307, %fd3296, %fd3304;
	sub.f64 	%fd3308, %fd3306, %fd3304;
	div.rn.f64 	%fd3309, %fd3301, 0d4008000000000000;
	add.f64 	%fd3310, %fd3309, 0d0000000000000000;
	fma.rn.f64 	%fd3311, %fd3310, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3312, %fd3299, %fd3311;
	sub.f64 	%fd3313, %fd3308, %fd3311;
	add.f64 	%fd3314, %fd2925, 0d0000000000000000;
	sub.f64 	%fd3315, %fd2510, %fd3314;
	add.f64 	%fd3316, %fd3315, %fd3302;
	div.rn.f64 	%fd3317, %fd3314, 0d4018000000000000;
	add.f64 	%fd3318, %fd3317, 0d0000000000000000;
	add.f64 	%fd3319, %fd3305, %fd3318;
	sub.f64 	%fd3320, %fd3313, %fd3318;
	div.rn.f64 	%fd3321, %fd3314, 0d4008000000000000;
	add.f64 	%fd3322, %fd3321, 0d0000000000000000;
	fma.rn.f64 	%fd3323, %fd3322, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3324, %fd3307, %fd3323;
	sub.f64 	%fd3325, %fd3320, %fd3323;
	add.f64 	%fd3326, %fd3312, %fd3318;
	sub.f64 	%fd3327, %fd3325, %fd3318;
	add.f64 	%fd3328, %fd3025, 0d0000000000000000;
	sub.f64 	%fd3329, %fd2510, %fd3328;
	add.f64 	%fd3330, %fd3329, %fd3316;
	div.rn.f64 	%fd3331, %fd3328, 0d4008000000000000;
	add.f64 	%fd3332, %fd3331, 0d0000000000000000;
	fma.rn.f64 	%fd3333, %fd3332, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3334, %fd3319, %fd3333;
	sub.f64 	%fd3335, %fd3327, %fd3333;
	div.rn.f64 	%fd3336, %fd3328, 0d4018000000000000;
	add.f64 	%fd3337, %fd3336, 0d0000000000000000;
	add.f64 	%fd3338, %fd3324, %fd3337;
	sub.f64 	%fd3339, %fd3335, %fd3337;
	fma.rn.f64 	%fd3340, %fd3328, 0d4018000000000000, 0d0000000000000000;
	add.f64 	%fd3341, %fd3326, %fd3340;
	sub.f64 	%fd3342, %fd3339, %fd3340;
	add.f64 	%fd3343, %fd3125, 0d0000000000000000;
	sub.f64 	%fd3344, %fd2510, %fd3343;
	add.f64 	%fd3345, %fd3344, %fd3330;
	div.rn.f64 	%fd3346, %fd3343, 0d4018000000000000;
	add.f64 	%fd3347, %fd3346, 0d0000000000000000;
	sub.f64 	%fd3348, %fd3342, %fd3347;
	sub.f64 	%fd3349, %fd3348, %fd3347;
	div.rn.f64 	%fd3350, %fd3343, 0d4008000000000000;
	add.f64 	%fd3351, %fd3350, 0d0000000000000000;
	fma.rn.f64 	%fd3352, %fd3351, 0d4000000000000000, 0d0000000000000000;
	div.rn.f64 	%fd3353, %fd3345, 0d400BB67AE8584CAA;
	add.f64 	%fd3354, %fd3353, 0d0000000000000000;
	div.rn.f64 	%fd3355, %fd3270, 0d400BB67AE8584CAA;
	add.f64 	%fd3356, %fd3355, 0d0000000000000000;
	div.rn.f64 	%fd3357, %fd3195, 0d400BB67AE8584CAA;
	add.f64 	%fd3358, %fd3357, 0d0000000000000000;
	add.f64 	%fd3359, %fd3184, %fd3197;
	add.f64 	%fd3360, %fd3191, %fd3202;
	add.f64 	%fd3361, %fd3263, %fd3272;
	add.f64 	%fd5745, %fd3359, 0d0000000000000000;
	add.f64 	%fd5751, %fd3360, 0d0000000000000000;
	add.f64 	%fd5747, %fd3361, 0d0000000000000000;
	fma.rn.f64 	%fd3362, %fd5123, %fd3354, 0d0000000000000000;
	fma.rn.f64 	%fd3363, %fd5123, %fd3356, 0d0000000000000000;
	fma.rn.f64 	%fd3364, %fd5123, %fd3358, 0d0000000000000000;
	add.f64 	%fd3365, %fd3259, %fd3272;
	add.f64 	%fd5744, %fd3365, 0d0000000000000000;
	mul.f64 	%fd3366, %fd5092, %fd3356;
	fma.rn.f64 	%fd3367, %fd5091, %fd3354, %fd3366;
	fma.rn.f64 	%fd1785, %fd5093, %fd3358, %fd3367;
	mul.f64 	%fd3368, %fd5096, %fd3363;
	fma.rn.f64 	%fd3369, %fd5095, %fd3362, %fd3368;
	fma.rn.f64 	%fd3370, %fd5097, %fd3364, %fd3369;
	mul.f64 	%fd3371, %fd5387, %fd5387;
	div.rn.f64 	%fd1786, %fd3370, %fd3371;
	div.rn.f64 	%fd3372, %fd3362, %fd5387;
	add.f64 	%fd5697, %fd3372, 0d0000000000000000;
	div.rn.f64 	%fd3373, %fd3363, %fd5387;
	add.f64 	%fd5696, %fd3373, 0d0000000000000000;
	div.rn.f64 	%fd3374, %fd3364, %fd5387;
	add.f64 	%fd5695, %fd3374, 0d0000000000000000;
	add.f64 	%fd1790, %fd3188, %fd3197;
	sub.f64 	%fd5754, %fd3199, %fd3202;
	add.f64 	%fd1792, %fd3266, %fd3277;
	sub.f64 	%fd5753, %fd3274, %fd3277;
	add.f64 	%fd5743, %fd3334, %fd3347;
	add.f64 	%fd5746, %fd3338, %fd3347;
	add.f64 	%fd5749, %fd3341, %fd3352;
	sub.f64 	%fd5752, %fd3349, %fd3352;
	setp.leu.f64 	%p221, %fd5387, 0d0000000000000000;
	@%p221 bra 	$L__BB35_140;

	sub.f64 	%fd3376, %fd2510, %fd1786;
	div.rn.f64 	%fd3377, %fd5095, %fd5387;
	div.rn.f64 	%fd3378, %fd5096, %fd5387;
	div.rn.f64 	%fd3379, %fd5097, %fd5387;
	fma.rn.f64 	%fd5697, %fd3377, %fd3376, %fd5697;
	fma.rn.f64 	%fd5696, %fd3378, %fd3376, %fd5696;
	fma.rn.f64 	%fd5695, %fd3379, %fd3376, %fd5695;

$L__BB35_140:
	add.f64 	%fd4230, %fd3126, 0d0000000000000000;
	fma.rn.f64 	%fd5759, %fd5094, %fd4230, 0d0000000000000000;
	fma.rn.f64 	%fd4228, %fd5124, %fd4230, 0d0000000000000000;
	fma.rn.f64 	%fd5755, %fd5123, %fd4228, 0d0000000000000000;
	add.f64 	%fd3380, %fd1785, 0d0000000000000000;
	fma.rn.f64 	%fd5712, %fd5126, %fd4228, %fd3380;
	add.f64 	%fd5750, %fd1792, 0d0000000000000000;
	add.f64 	%fd5748, %fd1790, 0d0000000000000000;

$L__BB35_141:
	add.f64 	%fd3381, %fd5695, 0d0000000000000000;
	setp.eq.s32 	%p222, %r698, 0;
	selp.f64 	%fd1826, 0d0000000000000000, %fd3381, %p222;
	add.f64 	%fd3382, %fd5696, 0d0000000000000000;
	selp.f64 	%fd1827, 0d0000000000000000, %fd3382, %p222;
	add.f64 	%fd3383, %fd5697, 0d0000000000000000;
	selp.f64 	%fd1828, 0d0000000000000000, %fd3383, %p222;
	selp.f64 	%fd5756, %fd3383, 0d0000000000000000, %p222;
	selp.f64 	%fd5757, %fd3382, 0d0000000000000000, %p222;
	selp.f64 	%fd5758, %fd3381, 0d0000000000000000, %p222;
	@%p222 bra 	$L__BB35_257;

	add.f64 	%fd1832, %fd5676, 0d0000000000000000;
	add.f64 	%fd3384, %fd5677, 0d0000000000000000;
	add.f64 	%fd1833, %fd3384, %fd5680;
	add.f64 	%fd3385, %fd5678, 0d0000000000000000;
	add.f64 	%fd1834, %fd3385, %fd5684;
	add.f64 	%fd3386, %fd5682, 0d0000000000000000;
	add.f64 	%fd1835, %fd3386, %fd5685;
	add.f64 	%fd3387, %fd5679, 0d0000000000000000;
	add.f64 	%fd1836, %fd3387, %fd5688;
	add.f64 	%fd3388, %fd5683, 0d0000000000000000;
	add.f64 	%fd1837, %fd3388, %fd5689;
	add.f64 	%fd3389, %fd5687, 0d0000000000000000;
	add.f64 	%fd1838, %fd3389, %fd5690;
	add.f64 	%fd1839, %fd5691, 0d0000000000000000;
	fma.rn.f64 	%fd3390, %fd5258, %fd1832, 0d0000000000000000;
	fma.rn.f64 	%fd3391, %fd5386, %fd1832, 0d0000000000000000;
	fma.rn.f64 	%fd3392, %fd5385, %fd3390, 0d0000000000000000;
	fma.rn.f64 	%fd3393, %fd5384, %fd3390, 0d0000000000000000;
	add.f64 	%fd3394, %fd3393, %fd1828;
	fma.rn.f64 	%fd3395, %fd5383, %fd3392, 0d0000000000000000;
	fma.rn.f64 	%fd3396, %fd5382, %fd3392, 0d0000000000000000;
	fma.rn.f64 	%fd3397, %fd5383, %fd3395, 0d0000000000000000;
	fma.rn.f64 	%fd3398, %fd5380, %fd3395, 0d0000000000000000;
	add.f64 	%fd3399, %fd3398, %fd3396;
	fma.rn.f64 	%fd3400, %fd5258, %fd1833, 0d0000000000000000;
	fma.rn.f64 	%fd3401, %fd5379, %fd1833, %fd3391;
	fma.rn.f64 	%fd3402, %fd5378, %fd3400, 0d0000000000000000;
	fma.rn.f64 	%fd3403, %fd5377, %fd3400, 0d0000000000000000;
	add.f64 	%fd3404, %fd3403, %fd3394;
	fma.rn.f64 	%fd3405, %fd5383, %fd3402, 0d0000000000000000;
	fma.rn.f64 	%fd3406, %fd5375, %fd3402, 0d0000000000000000;
	add.f64 	%fd3407, %fd3406, %fd3399;
	fma.rn.f64 	%fd3408, %fd5374, %fd3405, 0d0000000000000000;
	fma.rn.f64 	%fd3409, %fd5380, %fd3405, 0d0000000000000000;
	add.f64 	%fd3410, %fd3408, %fd3397;
	add.f64 	%fd1840, %fd5681, 0d0000000000000000;
	fma.rn.f64 	%fd3411, %fd5258, %fd1840, 0d0000000000000000;
	fma.rn.f64 	%fd3412, %fd5372, %fd1840, %fd3401;
	fma.rn.f64 	%fd3413, %fd5371, %fd3411, 0d0000000000000000;
	fma.rn.f64 	%fd3414, %fd5370, %fd3411, 0d0000000000000000;
	add.f64 	%fd3415, %fd3414, %fd3404;
	fma.rn.f64 	%fd3416, %fd5374, %fd3413, 0d0000000000000000;
	fma.rn.f64 	%fd3417, %fd5375, %fd3413, 0d0000000000000000;
	add.f64 	%fd3418, %fd3417, %fd3409;
	fma.rn.f64 	%fd3419, %fd5374, %fd3416, 0d0000000000000000;
	fma.rn.f64 	%fd3420, %fd5380, %fd3416, 0d0000000000000000;
	add.f64 	%fd3421, %fd3420, %fd3418;
	add.f64 	%fd3422, %fd3419, %fd3410;
	fma.rn.f64 	%fd3423, %fd5258, %fd1834, 0d0000000000000000;
	fma.rn.f64 	%fd3424, %fd5365, %fd1834, %fd3412;
	fma.rn.f64 	%fd3425, %fd5364, %fd3423, 0d0000000000000000;
	fma.rn.f64 	%fd3426, %fd5363, %fd3423, 0d0000000000000000;
	add.f64 	%fd3427, %fd3426, %fd3415;
	fma.rn.f64 	%fd3428, %fd5383, %fd3425, 0d0000000000000000;
	fma.rn.f64 	%fd3429, %fd5361, %fd3425, 0d0000000000000000;
	add.f64 	%fd1841, %fd3429, %fd3407;
	fma.rn.f64 	%fd3430, %fd3428, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3431, %fd5380, %fd3430, 0d0000000000000000;
	fma.rn.f64 	%fd3432, %fd5380, %fd3430, 0d0000000000000000;
	add.f64 	%fd3433, %fd3432, %fd3422;
	add.f64 	%fd3434, %fd3431, %fd3433;
	fma.rn.f64 	%fd3435, %fd5258, %fd1835, 0d0000000000000000;
	fma.rn.f64 	%fd3436, %fd5358, %fd1835, %fd3424;
	fma.rn.f64 	%fd3437, %fd5357, %fd3435, 0d0000000000000000;
	fma.rn.f64 	%fd3438, %fd5356, %fd3435, 0d0000000000000000;
	add.f64 	%fd3439, %fd3438, %fd3427;
	fma.rn.f64 	%fd3440, %fd5374, %fd3437, 0d0000000000000000;
	fma.rn.f64 	%fd3441, %fd5361, %fd3437, 0d0000000000000000;
	add.f64 	%fd1842, %fd3441, %fd3421;
	fma.rn.f64 	%fd3442, %fd3440, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3443, %fd5380, %fd3442, 0d0000000000000000;
	fma.rn.f64 	%fd3444, %fd5380, %fd3442, 0d0000000000000000;
	add.f64 	%fd3445, %fd3444, %fd3434;
	add.f64 	%fd1843, %fd3443, %fd3445;
	add.f64 	%fd1844, %fd5686, 0d0000000000000000;
	fma.rn.f64 	%fd3446, %fd5258, %fd1844, 0d0000000000000000;
	fma.rn.f64 	%fd1845, %fd5351, %fd1844, %fd3436;
	fma.rn.f64 	%fd1846, %fd5350, %fd3446, 0d0000000000000000;
	fma.rn.f64 	%fd3447, %fd5349, %fd3446, 0d0000000000000000;
	add.f64 	%fd1847, %fd3447, %fd3439;
	mov.f64 	%fd3448, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r48}, %fd3448;
	}
	and.b32  	%r49, %r48, 2146435072;
	setp.eq.s32 	%p223, %r49, 1062207488;
	abs.f64 	%fd1848, %fd5380;
	{ // callseq 71, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1848;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd3448;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd5715, [retval0+0];
	} // callseq 71
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd5380;
	}
	setp.lt.s32 	%p224, %r50, 0;
	and.pred  	%p9, %p224, %p223;
	not.pred 	%p225, %p9;
	@%p225 bra 	$L__BB35_144;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r515}, %fd5715;
	}
	xor.b32  	%r516, %r515, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r517, %temp}, %fd5715;
	}
	mov.b64 	%fd5715, {%r517, %r516};

$L__BB35_144:
	setp.eq.f64 	%p226, %fd5380, 0d0000000000000000;
	@%p226 bra 	$L__BB35_148;
	bra.uni 	$L__BB35_145;

$L__BB35_148:
	selp.b32 	%r518, %r50, 0, %p223;
	mov.u32 	%r519, 0;
	or.b32  	%r520, %r518, 2146435072;
	setp.lt.s32 	%p230, %r48, 0;
	selp.b32 	%r521, %r520, %r518, %p230;
	mov.b64 	%fd5715, {%r519, %r521};
	bra.uni 	$L__BB35_149;

$L__BB35_145:
	setp.gt.s32 	%p227, %r50, -1;
	@%p227 bra 	$L__BB35_149;

	mov.f64 	%fd3449, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd3450, %fd3449;
	setp.eq.f64 	%p228, %fd3450, 0d4000000000000000;
	@%p228 bra 	$L__BB35_149;

	mov.f64 	%fd5715, 0dFFF8000000000000;

$L__BB35_149:
	add.f64 	%fd3452, %fd5380, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r522}, %fd3452;
	}
	and.b32  	%r523, %r522, 2146435072;
	setp.ne.s32 	%p231, %r523, 2146435072;
	@%p231 bra 	$L__BB35_156;

	setp.gtu.f64 	%p232, %fd1848, 0d7FF0000000000000;
	@%p232 bra 	$L__BB35_155;
	bra.uni 	$L__BB35_151;

$L__BB35_155:
	mov.f64 	%fd3454, 0d4000000000000000;
	add.rn.f64 	%fd5715, %fd5380, %fd3454;
	bra.uni 	$L__BB35_156;

$L__BB35_151:
	mov.f64 	%fd3453, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r524, %temp}, %fd3453;
	}
	and.b32  	%r51, %r48, 2147483647;
	setp.eq.s32 	%p233, %r51, 2146435072;
	setp.eq.s32 	%p234, %r524, 0;
	and.pred  	%p235, %p233, %p234;
	@%p235 bra 	$L__BB35_154;
	bra.uni 	$L__BB35_152;

$L__BB35_154:
	setp.gt.f64 	%p242, %fd1848, 0d3FF0000000000000;
	selp.b32 	%r531, 2146435072, 0, %p242;
	mov.u32 	%r532, 0;
	xor.b32  	%r533, %r531, 2146435072;
	setp.lt.s32 	%p243, %r48, 0;
	selp.b32 	%r534, %r533, %r531, %p243;
	setp.eq.f64 	%p244, %fd5380, 0dBFF0000000000000;
	selp.b32 	%r535, 1072693248, %r534, %p244;
	mov.b64 	%fd5715, {%r532, %r535};
	bra.uni 	$L__BB35_156;

$L__BB35_152:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r525, %temp}, %fd5380;
	}
	and.b32  	%r526, %r50, 2147483647;
	setp.ne.s32 	%p236, %r526, 2146435072;
	setp.ne.s32 	%p237, %r525, 0;
	or.pred  	%p238, %p236, %p237;
	@%p238 bra 	$L__BB35_156;

	setp.gt.s32 	%p239, %r48, -1;
	selp.b32 	%r527, 2146435072, 0, %p239;
	mov.u32 	%r528, 0;
	setp.ne.s32 	%p240, %r51, 1071644672;
	and.pred  	%p241, %p240, %p9;
	or.b32  	%r529, %r527, -2147483648;
	selp.b32 	%r530, %r529, %r527, %p241;
	mov.b64 	%fd5715, {%r528, %r530};

$L__BB35_156:
	mul.f64 	%fd3455, %fd5715, 0d4008000000000000;
	setp.eq.f64 	%p245, %fd5380, 0d3FF0000000000000;
	selp.f64 	%fd3456, 0d4008000000000000, %fd3455, %p245;
	div.rn.f64 	%fd3457, %fd1846, 0d4008000000000000;
	add.f64 	%fd3458, %fd3457, 0d0000000000000000;
	mov.f64 	%fd3459, 0d0000000000000000;
	fma.rn.f64 	%fd3460, %fd3458, %fd3456, 0d0000000000000000;
	add.f64 	%fd3461, %fd1843, %fd3460;
	fma.rn.f64 	%fd3462, %fd5258, %fd1836, 0d0000000000000000;
	fma.rn.f64 	%fd3463, %fd5347, %fd1836, %fd1845;
	fma.rn.f64 	%fd3464, %fd5346, %fd3462, 0d0000000000000000;
	fma.rn.f64 	%fd3465, %fd5345, %fd3462, 0d0000000000000000;
	add.f64 	%fd1858, %fd3465, %fd1826;
	fma.rn.f64 	%fd3466, %fd5344, %fd3464, 0d0000000000000000;
	fma.rn.f64 	%fd3467, %fd5343, %fd3464, 0d0000000000000000;
	sub.f64 	%fd3468, %fd3459, %fd3467;
	fma.rn.f64 	%fd3469, %fd3468, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd3470, %fd3469, %fd1842;
	add.f64 	%fd3471, %fd3469, %fd3461;
	div.rn.f64 	%fd3472, %fd3468, 0d4008000000000000;
	add.f64 	%fd3473, %fd3472, 0d0000000000000000;
	add.f64 	%fd3474, %fd3473, %fd1841;
	fma.rn.f64 	%fd3475, %fd5383, %fd3466, 0d0000000000000000;
	fma.rn.f64 	%fd3476, %fd5383, %fd3466, 0d0000000000000000;
	add.f64 	%fd3477, %fd3476, %fd3474;
	add.f64 	%fd3478, %fd3475, %fd3477;
	fma.rn.f64 	%fd3479, %fd5258, %fd1837, 0d0000000000000000;
	fma.rn.f64 	%fd3480, %fd5340, %fd1837, %fd3463;
	fma.rn.f64 	%fd3481, %fd5339, %fd3479, 0d0000000000000000;
	fma.rn.f64 	%fd3482, %fd5338, %fd3479, 0d0000000000000000;
	add.f64 	%fd1859, %fd3482, %fd1827;
	fma.rn.f64 	%fd3483, %fd5337, %fd3481, 0d0000000000000000;
	fma.rn.f64 	%fd3484, %fd5336, %fd3481, 0d0000000000000000;
	sub.f64 	%fd3485, %fd3459, %fd3484;
	fma.rn.f64 	%fd3486, %fd3485, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd3487, %fd3486, %fd3478;
	add.f64 	%fd3488, %fd3486, %fd3471;
	div.rn.f64 	%fd3489, %fd3485, 0d4008000000000000;
	add.f64 	%fd3490, %fd3489, 0d0000000000000000;
	add.f64 	%fd3491, %fd3490, %fd3470;
	fma.rn.f64 	%fd3492, %fd5374, %fd3483, 0d0000000000000000;
	fma.rn.f64 	%fd3493, %fd5374, %fd3483, 0d0000000000000000;
	add.f64 	%fd3494, %fd3493, %fd3491;
	add.f64 	%fd3495, %fd3492, %fd3494;
	fma.rn.f64 	%fd3496, %fd5258, %fd1838, 0d0000000000000000;
	fma.rn.f64 	%fd3497, %fd5333, %fd1838, %fd3480;
	fma.rn.f64 	%fd3498, %fd5332, %fd3496, 0d0000000000000000;
	fma.rn.f64 	%fd3499, %fd5331, %fd3496, 0d0000000000000000;
	add.f64 	%fd3500, %fd3499, %fd1847;
	fma.rn.f64 	%fd3501, %fd5330, %fd3498, 0d0000000000000000;
	fma.rn.f64 	%fd3502, %fd5329, %fd3498, 0d0000000000000000;
	sub.f64 	%fd3503, %fd3459, %fd3502;
	fma.rn.f64 	%fd3504, %fd3503, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd1860, %fd3504, %fd3487;
	add.f64 	%fd1861, %fd3504, %fd3495;
	div.rn.f64 	%fd3505, %fd3503, 0d4008000000000000;
	add.f64 	%fd3506, %fd3505, 0d0000000000000000;
	add.f64 	%fd3507, %fd3506, %fd3488;
	fma.rn.f64 	%fd3508, %fd5380, %fd3501, 0d0000000000000000;
	fma.rn.f64 	%fd3509, %fd5380, %fd3501, 0d0000000000000000;
	add.f64 	%fd3510, %fd3509, %fd3507;
	add.f64 	%fd1862, %fd3508, %fd3510;
	fma.rn.f64 	%fd3511, %fd5258, %fd1839, 0d0000000000000000;
	fma.rn.f64 	%fd1863, %fd5326, %fd1839, %fd3497;
	fma.rn.f64 	%fd1864, %fd5325, %fd3511, 0d0000000000000000;
	fma.rn.f64 	%fd3512, %fd5324, %fd3511, 0d0000000000000000;
	add.f64 	%fd1865, %fd3512, %fd3500;
	abs.f64 	%fd1866, %fd5323;
	mov.f64 	%fd3513, 0d4000000000000000;
	{ // callseq 72, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1866;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd3513;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd5718, [retval0+0];
	} // callseq 72
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd5323;
	}
	setp.lt.s32 	%p246, %r52, 0;
	and.pred  	%p10, %p246, %p223;
	not.pred 	%p248, %p10;
	@%p248 bra 	$L__BB35_158;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r536}, %fd5718;
	}
	xor.b32  	%r537, %r536, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r538, %temp}, %fd5718;
	}
	mov.b64 	%fd5718, {%r538, %r537};

$L__BB35_158:
	setp.eq.f64 	%p249, %fd5323, 0d0000000000000000;
	@%p249 bra 	$L__BB35_162;
	bra.uni 	$L__BB35_159;

$L__BB35_162:
	selp.b32 	%r539, %r52, 0, %p223;
	mov.u32 	%r540, 0;
	or.b32  	%r541, %r539, 2146435072;
	setp.lt.s32 	%p253, %r48, 0;
	selp.b32 	%r542, %r541, %r539, %p253;
	mov.b64 	%fd5718, {%r540, %r542};
	bra.uni 	$L__BB35_163;

$L__BB35_159:
	setp.gt.s32 	%p250, %r52, -1;
	@%p250 bra 	$L__BB35_163;

	mov.f64 	%fd3514, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd3515, %fd3514;
	setp.eq.f64 	%p251, %fd3515, 0d4000000000000000;
	@%p251 bra 	$L__BB35_163;

	mov.f64 	%fd5718, 0dFFF8000000000000;

$L__BB35_163:
	add.f64 	%fd3517, %fd5323, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r543}, %fd3517;
	}
	and.b32  	%r544, %r543, 2146435072;
	setp.ne.s32 	%p254, %r544, 2146435072;
	@%p254 bra 	$L__BB35_170;

	setp.gtu.f64 	%p255, %fd1866, 0d7FF0000000000000;
	@%p255 bra 	$L__BB35_169;
	bra.uni 	$L__BB35_165;

$L__BB35_169:
	mov.f64 	%fd3519, 0d4000000000000000;
	add.rn.f64 	%fd5718, %fd5323, %fd3519;
	bra.uni 	$L__BB35_170;

$L__BB35_165:
	mov.f64 	%fd3518, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r545, %temp}, %fd3518;
	}
	and.b32  	%r53, %r48, 2147483647;
	setp.eq.s32 	%p256, %r53, 2146435072;
	setp.eq.s32 	%p257, %r545, 0;
	and.pred  	%p258, %p256, %p257;
	@%p258 bra 	$L__BB35_168;
	bra.uni 	$L__BB35_166;

$L__BB35_168:
	setp.gt.f64 	%p265, %fd1866, 0d3FF0000000000000;
	selp.b32 	%r552, 2146435072, 0, %p265;
	mov.u32 	%r553, 0;
	xor.b32  	%r554, %r552, 2146435072;
	setp.lt.s32 	%p266, %r48, 0;
	selp.b32 	%r555, %r554, %r552, %p266;
	setp.eq.f64 	%p267, %fd5323, 0dBFF0000000000000;
	selp.b32 	%r556, 1072693248, %r555, %p267;
	mov.b64 	%fd5718, {%r553, %r556};
	bra.uni 	$L__BB35_170;

$L__BB35_166:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r546, %temp}, %fd5323;
	}
	and.b32  	%r547, %r52, 2147483647;
	setp.ne.s32 	%p259, %r547, 2146435072;
	setp.ne.s32 	%p260, %r546, 0;
	or.pred  	%p261, %p259, %p260;
	@%p261 bra 	$L__BB35_170;

	setp.gt.s32 	%p262, %r48, -1;
	selp.b32 	%r548, 2146435072, 0, %p262;
	mov.u32 	%r549, 0;
	setp.ne.s32 	%p263, %r53, 1071644672;
	and.pred  	%p264, %p263, %p10;
	or.b32  	%r550, %r548, -2147483648;
	selp.b32 	%r551, %r550, %r548, %p264;
	mov.b64 	%fd5718, {%r549, %r551};

$L__BB35_170:
	mul.f64 	%fd3520, %fd5718, 0d4008000000000000;
	setp.eq.f64 	%p268, %fd5323, 0d3FF0000000000000;
	selp.f64 	%fd3521, 0d4008000000000000, %fd3520, %p268;
	div.rn.f64 	%fd3522, %fd1864, 0d4008000000000000;
	add.f64 	%fd3523, %fd3522, 0d0000000000000000;
	mov.f64 	%fd3524, 0d0000000000000000;
	sub.f64 	%fd3525, %fd3524, %fd3523;
	fma.rn.f64 	%fd3526, %fd3525, %fd3521, 0d0000000000000000;
	sub.f64 	%fd3527, %fd3524, %fd3526;
	add.f64 	%fd3528, %fd1860, %fd3527;
	add.f64 	%fd3529, %fd1861, %fd3527;
	add.f64 	%fd3530, %fd1862, %fd3527;
	fma.rn.f64 	%fd3531, %fd1863, 0d3FE0AAAAA0000000, 0d0000000000000000;
	add.f64 	%fd1876, %fd3530, 0d0000000000000000;
	add.f64 	%fd1877, %fd3529, 0d0000000000000000;
	add.f64 	%fd1878, %fd3528, 0d0000000000000000;
	fma.rn.f64 	%fd1879, %fd5124, %fd3531, %fd5755;
	fma.rn.f64 	%fd1880, %fd5126, %fd3531, %fd5759;
	fma.rn.f64 	%fd3532, %fd5258, %fd1832, 0d0000000000000000;
	fma.rn.f64 	%fd3533, %fd5322, %fd1832, 0d0000000000000000;
	fma.rn.f64 	%fd3534, %fd5321, %fd3532, 0d0000000000000000;
	fma.rn.f64 	%fd3535, %fd5320, %fd3532, 0d0000000000000000;
	add.f64 	%fd3536, %fd3535, %fd1865;
	fma.rn.f64 	%fd3537, %fd5319, %fd3534, 0d0000000000000000;
	fma.rn.f64 	%fd3538, %fd5318, %fd3534, 0d0000000000000000;
	fma.rn.f64 	%fd3539, %fd5319, %fd3537, 0d0000000000000000;
	fma.rn.f64 	%fd3540, %fd5316, %fd3537, 0d0000000000000000;
	add.f64 	%fd3541, %fd3540, %fd3538;
	fma.rn.f64 	%fd3542, %fd5258, %fd1833, 0d0000000000000000;
	fma.rn.f64 	%fd3543, %fd5315, %fd1833, %fd3533;
	fma.rn.f64 	%fd3544, %fd5314, %fd3542, 0d0000000000000000;
	fma.rn.f64 	%fd3545, %fd5313, %fd3542, 0d0000000000000000;
	add.f64 	%fd3546, %fd3545, %fd3536;
	fma.rn.f64 	%fd3547, %fd5319, %fd3544, 0d0000000000000000;
	fma.rn.f64 	%fd3548, %fd5311, %fd3544, 0d0000000000000000;
	add.f64 	%fd3549, %fd3548, %fd3541;
	fma.rn.f64 	%fd3550, %fd5310, %fd3547, 0d0000000000000000;
	fma.rn.f64 	%fd3551, %fd5316, %fd3547, 0d0000000000000000;
	add.f64 	%fd3552, %fd3550, %fd3539;
	fma.rn.f64 	%fd3553, %fd5258, %fd1840, 0d0000000000000000;
	fma.rn.f64 	%fd3554, %fd5308, %fd1840, %fd3543;
	fma.rn.f64 	%fd3555, %fd5307, %fd3553, 0d0000000000000000;
	fma.rn.f64 	%fd3556, %fd5306, %fd3553, 0d0000000000000000;
	add.f64 	%fd3557, %fd3556, %fd3546;
	fma.rn.f64 	%fd3558, %fd5310, %fd3555, 0d0000000000000000;
	fma.rn.f64 	%fd3559, %fd5311, %fd3555, 0d0000000000000000;
	add.f64 	%fd3560, %fd3559, %fd3551;
	fma.rn.f64 	%fd3561, %fd5310, %fd3558, 0d0000000000000000;
	fma.rn.f64 	%fd3562, %fd5316, %fd3558, 0d0000000000000000;
	add.f64 	%fd3563, %fd3562, %fd3560;
	add.f64 	%fd3564, %fd3561, %fd3552;
	fma.rn.f64 	%fd3565, %fd5258, %fd1834, 0d0000000000000000;
	fma.rn.f64 	%fd3566, %fd5301, %fd1834, %fd3554;
	fma.rn.f64 	%fd3567, %fd5300, %fd3565, 0d0000000000000000;
	fma.rn.f64 	%fd3568, %fd5299, %fd3565, 0d0000000000000000;
	add.f64 	%fd3569, %fd3568, %fd3557;
	fma.rn.f64 	%fd3570, %fd5319, %fd3567, 0d0000000000000000;
	fma.rn.f64 	%fd3571, %fd5297, %fd3567, 0d0000000000000000;
	add.f64 	%fd1881, %fd3571, %fd3549;
	fma.rn.f64 	%fd3572, %fd3570, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3573, %fd5316, %fd3572, 0d0000000000000000;
	fma.rn.f64 	%fd3574, %fd5316, %fd3572, 0d0000000000000000;
	add.f64 	%fd3575, %fd3574, %fd3564;
	add.f64 	%fd3576, %fd3573, %fd3575;
	fma.rn.f64 	%fd3577, %fd5258, %fd1835, 0d0000000000000000;
	fma.rn.f64 	%fd3578, %fd5294, %fd1835, %fd3566;
	fma.rn.f64 	%fd3579, %fd5293, %fd3577, 0d0000000000000000;
	fma.rn.f64 	%fd3580, %fd5292, %fd3577, 0d0000000000000000;
	add.f64 	%fd3581, %fd3580, %fd3569;
	fma.rn.f64 	%fd3582, %fd5310, %fd3579, 0d0000000000000000;
	fma.rn.f64 	%fd3583, %fd5297, %fd3579, 0d0000000000000000;
	add.f64 	%fd1882, %fd3583, %fd3563;
	fma.rn.f64 	%fd3584, %fd3582, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3585, %fd5316, %fd3584, 0d0000000000000000;
	fma.rn.f64 	%fd3586, %fd5316, %fd3584, 0d0000000000000000;
	add.f64 	%fd3587, %fd3586, %fd3576;
	add.f64 	%fd1883, %fd3585, %fd3587;
	fma.rn.f64 	%fd3588, %fd5258, %fd1844, 0d0000000000000000;
	fma.rn.f64 	%fd1884, %fd5287, %fd1844, %fd3578;
	fma.rn.f64 	%fd1885, %fd5286, %fd3588, 0d0000000000000000;
	fma.rn.f64 	%fd3589, %fd5285, %fd3588, 0d0000000000000000;
	add.f64 	%fd1886, %fd3589, %fd3581;
	abs.f64 	%fd1887, %fd5316;
	mov.f64 	%fd3590, 0d4000000000000000;
	{ // callseq 73, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1887;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd3590;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd5721, [retval0+0];
	} // callseq 73
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd5316;
	}
	setp.lt.s32 	%p269, %r54, 0;
	and.pred  	%p11, %p269, %p223;
	not.pred 	%p271, %p11;
	@%p271 bra 	$L__BB35_172;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r557}, %fd5721;
	}
	xor.b32  	%r558, %r557, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r559, %temp}, %fd5721;
	}
	mov.b64 	%fd5721, {%r559, %r558};

$L__BB35_172:
	setp.eq.f64 	%p272, %fd5316, 0d0000000000000000;
	@%p272 bra 	$L__BB35_176;
	bra.uni 	$L__BB35_173;

$L__BB35_176:
	selp.b32 	%r560, %r54, 0, %p223;
	mov.u32 	%r561, 0;
	or.b32  	%r562, %r560, 2146435072;
	setp.lt.s32 	%p276, %r48, 0;
	selp.b32 	%r563, %r562, %r560, %p276;
	mov.b64 	%fd5721, {%r561, %r563};
	bra.uni 	$L__BB35_177;

$L__BB35_173:
	setp.gt.s32 	%p273, %r54, -1;
	@%p273 bra 	$L__BB35_177;

	mov.f64 	%fd3591, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd3592, %fd3591;
	setp.eq.f64 	%p274, %fd3592, 0d4000000000000000;
	@%p274 bra 	$L__BB35_177;

	mov.f64 	%fd5721, 0dFFF8000000000000;

$L__BB35_177:
	add.f64 	%fd3594, %fd5316, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r564}, %fd3594;
	}
	and.b32  	%r565, %r564, 2146435072;
	setp.ne.s32 	%p277, %r565, 2146435072;
	@%p277 bra 	$L__BB35_184;

	setp.gtu.f64 	%p278, %fd1887, 0d7FF0000000000000;
	@%p278 bra 	$L__BB35_183;
	bra.uni 	$L__BB35_179;

$L__BB35_183:
	mov.f64 	%fd3596, 0d4000000000000000;
	add.rn.f64 	%fd5721, %fd5316, %fd3596;
	bra.uni 	$L__BB35_184;

$L__BB35_179:
	mov.f64 	%fd3595, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r566, %temp}, %fd3595;
	}
	and.b32  	%r55, %r48, 2147483647;
	setp.eq.s32 	%p279, %r55, 2146435072;
	setp.eq.s32 	%p280, %r566, 0;
	and.pred  	%p281, %p279, %p280;
	@%p281 bra 	$L__BB35_182;
	bra.uni 	$L__BB35_180;

$L__BB35_182:
	setp.gt.f64 	%p288, %fd1887, 0d3FF0000000000000;
	selp.b32 	%r573, 2146435072, 0, %p288;
	mov.u32 	%r574, 0;
	xor.b32  	%r575, %r573, 2146435072;
	setp.lt.s32 	%p289, %r48, 0;
	selp.b32 	%r576, %r575, %r573, %p289;
	setp.eq.f64 	%p290, %fd5316, 0dBFF0000000000000;
	selp.b32 	%r577, 1072693248, %r576, %p290;
	mov.b64 	%fd5721, {%r574, %r577};
	bra.uni 	$L__BB35_184;

$L__BB35_180:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r567, %temp}, %fd5316;
	}
	and.b32  	%r568, %r54, 2147483647;
	setp.ne.s32 	%p282, %r568, 2146435072;
	setp.ne.s32 	%p283, %r567, 0;
	or.pred  	%p284, %p282, %p283;
	@%p284 bra 	$L__BB35_184;

	setp.gt.s32 	%p285, %r48, -1;
	selp.b32 	%r569, 2146435072, 0, %p285;
	mov.u32 	%r570, 0;
	setp.ne.s32 	%p286, %r55, 1071644672;
	and.pred  	%p287, %p286, %p11;
	or.b32  	%r571, %r569, -2147483648;
	selp.b32 	%r572, %r571, %r569, %p287;
	mov.b64 	%fd5721, {%r570, %r572};

$L__BB35_184:
	mul.f64 	%fd3597, %fd5721, 0d4008000000000000;
	setp.eq.f64 	%p291, %fd5316, 0d3FF0000000000000;
	selp.f64 	%fd3598, 0d4008000000000000, %fd3597, %p291;
	div.rn.f64 	%fd3599, %fd1885, 0d4008000000000000;
	add.f64 	%fd3600, %fd3599, 0d0000000000000000;
	mov.f64 	%fd3601, 0d0000000000000000;
	fma.rn.f64 	%fd3602, %fd3600, %fd3598, 0d0000000000000000;
	add.f64 	%fd3603, %fd1883, %fd3602;
	fma.rn.f64 	%fd3604, %fd5258, %fd1836, 0d0000000000000000;
	fma.rn.f64 	%fd3605, %fd5283, %fd1836, %fd1884;
	fma.rn.f64 	%fd3606, %fd5282, %fd3604, 0d0000000000000000;
	fma.rn.f64 	%fd3607, %fd5281, %fd3604, 0d0000000000000000;
	add.f64 	%fd1897, %fd3607, %fd1858;
	fma.rn.f64 	%fd3608, %fd5280, %fd3606, 0d0000000000000000;
	fma.rn.f64 	%fd3609, %fd5279, %fd3606, 0d0000000000000000;
	sub.f64 	%fd3610, %fd3601, %fd3609;
	fma.rn.f64 	%fd3611, %fd3610, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd3612, %fd3611, %fd1882;
	add.f64 	%fd3613, %fd3611, %fd3603;
	div.rn.f64 	%fd3614, %fd3610, 0d4008000000000000;
	add.f64 	%fd3615, %fd3614, 0d0000000000000000;
	add.f64 	%fd3616, %fd3615, %fd1881;
	fma.rn.f64 	%fd3617, %fd5319, %fd3608, 0d0000000000000000;
	fma.rn.f64 	%fd3618, %fd5319, %fd3608, 0d0000000000000000;
	add.f64 	%fd3619, %fd3618, %fd3616;
	add.f64 	%fd3620, %fd3617, %fd3619;
	fma.rn.f64 	%fd3621, %fd5258, %fd1837, 0d0000000000000000;
	fma.rn.f64 	%fd3622, %fd5276, %fd1837, %fd3605;
	fma.rn.f64 	%fd3623, %fd5275, %fd3621, 0d0000000000000000;
	fma.rn.f64 	%fd3624, %fd5274, %fd3621, 0d0000000000000000;
	add.f64 	%fd1898, %fd3624, %fd1859;
	fma.rn.f64 	%fd3625, %fd5273, %fd3623, 0d0000000000000000;
	fma.rn.f64 	%fd3626, %fd5272, %fd3623, 0d0000000000000000;
	sub.f64 	%fd3627, %fd3601, %fd3626;
	fma.rn.f64 	%fd3628, %fd3627, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd3629, %fd3628, %fd3620;
	add.f64 	%fd3630, %fd3628, %fd3613;
	div.rn.f64 	%fd3631, %fd3627, 0d4008000000000000;
	add.f64 	%fd3632, %fd3631, 0d0000000000000000;
	add.f64 	%fd3633, %fd3632, %fd3612;
	fma.rn.f64 	%fd3634, %fd5310, %fd3625, 0d0000000000000000;
	fma.rn.f64 	%fd3635, %fd5310, %fd3625, 0d0000000000000000;
	add.f64 	%fd3636, %fd3635, %fd3633;
	add.f64 	%fd3637, %fd3634, %fd3636;
	fma.rn.f64 	%fd3638, %fd5258, %fd1838, 0d0000000000000000;
	fma.rn.f64 	%fd3639, %fd5269, %fd1838, %fd3622;
	fma.rn.f64 	%fd3640, %fd5268, %fd3638, 0d0000000000000000;
	fma.rn.f64 	%fd3641, %fd5267, %fd3638, 0d0000000000000000;
	add.f64 	%fd3642, %fd3641, %fd1886;
	fma.rn.f64 	%fd3643, %fd5266, %fd3640, 0d0000000000000000;
	fma.rn.f64 	%fd3644, %fd5265, %fd3640, 0d0000000000000000;
	sub.f64 	%fd3645, %fd3601, %fd3644;
	fma.rn.f64 	%fd3646, %fd3645, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd1899, %fd3646, %fd3629;
	add.f64 	%fd1900, %fd3646, %fd3637;
	div.rn.f64 	%fd3647, %fd3645, 0d4008000000000000;
	add.f64 	%fd3648, %fd3647, 0d0000000000000000;
	add.f64 	%fd3649, %fd3648, %fd3630;
	fma.rn.f64 	%fd3650, %fd5316, %fd3643, 0d0000000000000000;
	fma.rn.f64 	%fd3651, %fd5316, %fd3643, 0d0000000000000000;
	add.f64 	%fd3652, %fd3651, %fd3649;
	add.f64 	%fd1901, %fd3650, %fd3652;
	fma.rn.f64 	%fd3653, %fd5258, %fd1839, 0d0000000000000000;
	fma.rn.f64 	%fd1902, %fd5262, %fd1839, %fd3639;
	fma.rn.f64 	%fd1903, %fd5261, %fd3653, 0d0000000000000000;
	fma.rn.f64 	%fd3654, %fd5260, %fd3653, 0d0000000000000000;
	add.f64 	%fd1904, %fd3654, %fd3642;
	abs.f64 	%fd1905, %fd5259;
	mov.f64 	%fd3655, 0d4000000000000000;
	{ // callseq 74, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1905;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd3655;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd5724, [retval0+0];
	} // callseq 74
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r56}, %fd5259;
	}
	setp.lt.s32 	%p292, %r56, 0;
	and.pred  	%p12, %p292, %p223;
	not.pred 	%p294, %p12;
	@%p294 bra 	$L__BB35_186;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r578}, %fd5724;
	}
	xor.b32  	%r579, %r578, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r580, %temp}, %fd5724;
	}
	mov.b64 	%fd5724, {%r580, %r579};

$L__BB35_186:
	setp.eq.f64 	%p295, %fd5259, 0d0000000000000000;
	@%p295 bra 	$L__BB35_190;
	bra.uni 	$L__BB35_187;

$L__BB35_190:
	selp.b32 	%r581, %r56, 0, %p223;
	mov.u32 	%r582, 0;
	or.b32  	%r583, %r581, 2146435072;
	setp.lt.s32 	%p299, %r48, 0;
	selp.b32 	%r584, %r583, %r581, %p299;
	mov.b64 	%fd5724, {%r582, %r584};
	bra.uni 	$L__BB35_191;

$L__BB35_187:
	setp.gt.s32 	%p296, %r56, -1;
	@%p296 bra 	$L__BB35_191;

	mov.f64 	%fd3656, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd3657, %fd3656;
	setp.eq.f64 	%p297, %fd3657, 0d4000000000000000;
	@%p297 bra 	$L__BB35_191;

	mov.f64 	%fd5724, 0dFFF8000000000000;

$L__BB35_191:
	add.f64 	%fd3659, %fd5259, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r585}, %fd3659;
	}
	and.b32  	%r586, %r585, 2146435072;
	setp.ne.s32 	%p300, %r586, 2146435072;
	@%p300 bra 	$L__BB35_198;

	setp.gtu.f64 	%p301, %fd1905, 0d7FF0000000000000;
	@%p301 bra 	$L__BB35_197;
	bra.uni 	$L__BB35_193;

$L__BB35_197:
	mov.f64 	%fd3661, 0d4000000000000000;
	add.rn.f64 	%fd5724, %fd5259, %fd3661;
	bra.uni 	$L__BB35_198;

$L__BB35_193:
	mov.f64 	%fd3660, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r587, %temp}, %fd3660;
	}
	and.b32  	%r57, %r48, 2147483647;
	setp.eq.s32 	%p302, %r57, 2146435072;
	setp.eq.s32 	%p303, %r587, 0;
	and.pred  	%p304, %p302, %p303;
	@%p304 bra 	$L__BB35_196;
	bra.uni 	$L__BB35_194;

$L__BB35_196:
	setp.gt.f64 	%p311, %fd1905, 0d3FF0000000000000;
	selp.b32 	%r594, 2146435072, 0, %p311;
	mov.u32 	%r595, 0;
	xor.b32  	%r596, %r594, 2146435072;
	setp.lt.s32 	%p312, %r48, 0;
	selp.b32 	%r597, %r596, %r594, %p312;
	setp.eq.f64 	%p313, %fd5259, 0dBFF0000000000000;
	selp.b32 	%r598, 1072693248, %r597, %p313;
	mov.b64 	%fd5724, {%r595, %r598};
	bra.uni 	$L__BB35_198;

$L__BB35_194:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r588, %temp}, %fd5259;
	}
	and.b32  	%r589, %r56, 2147483647;
	setp.ne.s32 	%p305, %r589, 2146435072;
	setp.ne.s32 	%p306, %r588, 0;
	or.pred  	%p307, %p305, %p306;
	@%p307 bra 	$L__BB35_198;

	setp.gt.s32 	%p308, %r48, -1;
	selp.b32 	%r590, 2146435072, 0, %p308;
	mov.u32 	%r591, 0;
	setp.ne.s32 	%p309, %r57, 1071644672;
	and.pred  	%p310, %p309, %p12;
	or.b32  	%r592, %r590, -2147483648;
	selp.b32 	%r593, %r592, %r590, %p310;
	mov.b64 	%fd5724, {%r591, %r593};

$L__BB35_198:
	mul.f64 	%fd3662, %fd5724, 0d4008000000000000;
	setp.eq.f64 	%p314, %fd5259, 0d3FF0000000000000;
	selp.f64 	%fd3663, 0d4008000000000000, %fd3662, %p314;
	div.rn.f64 	%fd3664, %fd1903, 0d4008000000000000;
	add.f64 	%fd3665, %fd3664, 0d0000000000000000;
	mov.f64 	%fd3666, 0d0000000000000000;
	sub.f64 	%fd3667, %fd3666, %fd3665;
	fma.rn.f64 	%fd3668, %fd3667, %fd3663, 0d0000000000000000;
	sub.f64 	%fd3669, %fd3666, %fd3668;
	add.f64 	%fd3670, %fd1899, %fd3669;
	add.f64 	%fd3671, %fd1900, %fd3669;
	add.f64 	%fd3672, %fd1901, %fd3669;
	fma.rn.f64 	%fd3673, %fd1902, 0d3FE0AAAAA0000000, 0d0000000000000000;
	add.f64 	%fd1915, %fd3672, 0d0000000000000000;
	add.f64 	%fd1916, %fd3671, 0d0000000000000000;
	add.f64 	%fd1917, %fd3670, 0d0000000000000000;
	fma.rn.f64 	%fd1918, %fd5124, %fd3673, %fd1879;
	fma.rn.f64 	%fd1919, %fd5126, %fd3673, %fd1880;
	fma.rn.f64 	%fd3674, %fd5258, %fd1832, 0d0000000000000000;
	fma.rn.f64 	%fd3675, %fd5257, %fd1832, 0d0000000000000000;
	fma.rn.f64 	%fd3676, %fd5256, %fd3674, 0d0000000000000000;
	fma.rn.f64 	%fd3677, %fd5255, %fd3674, 0d0000000000000000;
	add.f64 	%fd3678, %fd3677, %fd1904;
	fma.rn.f64 	%fd3679, %fd5254, %fd3676, 0d0000000000000000;
	fma.rn.f64 	%fd3680, %fd5253, %fd3676, 0d0000000000000000;
	fma.rn.f64 	%fd3681, %fd5254, %fd3679, 0d0000000000000000;
	fma.rn.f64 	%fd3682, %fd5251, %fd3679, 0d0000000000000000;
	add.f64 	%fd3683, %fd3682, %fd3680;
	fma.rn.f64 	%fd3684, %fd5258, %fd1833, 0d0000000000000000;
	fma.rn.f64 	%fd3685, %fd5250, %fd1833, %fd3675;
	fma.rn.f64 	%fd3686, %fd5249, %fd3684, 0d0000000000000000;
	fma.rn.f64 	%fd3687, %fd5248, %fd3684, 0d0000000000000000;
	add.f64 	%fd3688, %fd3687, %fd3678;
	fma.rn.f64 	%fd3689, %fd5254, %fd3686, 0d0000000000000000;
	fma.rn.f64 	%fd3690, %fd5246, %fd3686, 0d0000000000000000;
	add.f64 	%fd3691, %fd3690, %fd3683;
	fma.rn.f64 	%fd3692, %fd5245, %fd3689, 0d0000000000000000;
	fma.rn.f64 	%fd3693, %fd5251, %fd3689, 0d0000000000000000;
	add.f64 	%fd3694, %fd3692, %fd3681;
	fma.rn.f64 	%fd3695, %fd5258, %fd1840, 0d0000000000000000;
	fma.rn.f64 	%fd3696, %fd5243, %fd1840, %fd3685;
	fma.rn.f64 	%fd3697, %fd5242, %fd3695, 0d0000000000000000;
	fma.rn.f64 	%fd3698, %fd5241, %fd3695, 0d0000000000000000;
	add.f64 	%fd3699, %fd3698, %fd3688;
	fma.rn.f64 	%fd3700, %fd5245, %fd3697, 0d0000000000000000;
	fma.rn.f64 	%fd3701, %fd5246, %fd3697, 0d0000000000000000;
	add.f64 	%fd3702, %fd3701, %fd3693;
	fma.rn.f64 	%fd3703, %fd5245, %fd3700, 0d0000000000000000;
	fma.rn.f64 	%fd3704, %fd5251, %fd3700, 0d0000000000000000;
	add.f64 	%fd3705, %fd3704, %fd3702;
	add.f64 	%fd3706, %fd3703, %fd3694;
	fma.rn.f64 	%fd3707, %fd5258, %fd1834, 0d0000000000000000;
	fma.rn.f64 	%fd3708, %fd5236, %fd1834, %fd3696;
	fma.rn.f64 	%fd3709, %fd5235, %fd3707, 0d0000000000000000;
	fma.rn.f64 	%fd3710, %fd5234, %fd3707, 0d0000000000000000;
	add.f64 	%fd3711, %fd3710, %fd3699;
	fma.rn.f64 	%fd3712, %fd5254, %fd3709, 0d0000000000000000;
	fma.rn.f64 	%fd3713, %fd5232, %fd3709, 0d0000000000000000;
	add.f64 	%fd1920, %fd3713, %fd3691;
	fma.rn.f64 	%fd3714, %fd3712, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3715, %fd5251, %fd3714, 0d0000000000000000;
	fma.rn.f64 	%fd3716, %fd5251, %fd3714, 0d0000000000000000;
	add.f64 	%fd3717, %fd3716, %fd3706;
	add.f64 	%fd3718, %fd3715, %fd3717;
	fma.rn.f64 	%fd3719, %fd5258, %fd1835, 0d0000000000000000;
	fma.rn.f64 	%fd3720, %fd5229, %fd1835, %fd3708;
	fma.rn.f64 	%fd3721, %fd5228, %fd3719, 0d0000000000000000;
	fma.rn.f64 	%fd3722, %fd5227, %fd3719, 0d0000000000000000;
	add.f64 	%fd3723, %fd3722, %fd3711;
	fma.rn.f64 	%fd3724, %fd5245, %fd3721, 0d0000000000000000;
	fma.rn.f64 	%fd3725, %fd5232, %fd3721, 0d0000000000000000;
	add.f64 	%fd1921, %fd3725, %fd3705;
	fma.rn.f64 	%fd3726, %fd3724, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3727, %fd5251, %fd3726, 0d0000000000000000;
	fma.rn.f64 	%fd3728, %fd5251, %fd3726, 0d0000000000000000;
	add.f64 	%fd3729, %fd3728, %fd3718;
	add.f64 	%fd1922, %fd3727, %fd3729;
	fma.rn.f64 	%fd3730, %fd5258, %fd1844, 0d0000000000000000;
	fma.rn.f64 	%fd1923, %fd5222, %fd1844, %fd3720;
	fma.rn.f64 	%fd1924, %fd5221, %fd3730, 0d0000000000000000;
	fma.rn.f64 	%fd3731, %fd5220, %fd3730, 0d0000000000000000;
	add.f64 	%fd1925, %fd3731, %fd3723;
	abs.f64 	%fd1926, %fd5251;
	mov.f64 	%fd3732, 0d4000000000000000;
	{ // callseq 75, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1926;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd3732;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd5727, [retval0+0];
	} // callseq 75
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r58}, %fd5251;
	}
	setp.lt.s32 	%p315, %r58, 0;
	and.pred  	%p13, %p315, %p223;
	not.pred 	%p317, %p13;
	@%p317 bra 	$L__BB35_200;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r599}, %fd5727;
	}
	xor.b32  	%r600, %r599, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r601, %temp}, %fd5727;
	}
	mov.b64 	%fd5727, {%r601, %r600};

$L__BB35_200:
	setp.eq.f64 	%p318, %fd5251, 0d0000000000000000;
	@%p318 bra 	$L__BB35_204;
	bra.uni 	$L__BB35_201;

$L__BB35_204:
	selp.b32 	%r602, %r58, 0, %p223;
	mov.u32 	%r603, 0;
	or.b32  	%r604, %r602, 2146435072;
	setp.lt.s32 	%p322, %r48, 0;
	selp.b32 	%r605, %r604, %r602, %p322;
	mov.b64 	%fd5727, {%r603, %r605};
	bra.uni 	$L__BB35_205;

$L__BB35_201:
	setp.gt.s32 	%p319, %r58, -1;
	@%p319 bra 	$L__BB35_205;

	mov.f64 	%fd3733, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd3734, %fd3733;
	setp.eq.f64 	%p320, %fd3734, 0d4000000000000000;
	@%p320 bra 	$L__BB35_205;

	mov.f64 	%fd5727, 0dFFF8000000000000;

$L__BB35_205:
	add.f64 	%fd3736, %fd5251, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r606}, %fd3736;
	}
	and.b32  	%r607, %r606, 2146435072;
	setp.ne.s32 	%p323, %r607, 2146435072;
	@%p323 bra 	$L__BB35_212;

	setp.gtu.f64 	%p324, %fd1926, 0d7FF0000000000000;
	@%p324 bra 	$L__BB35_211;
	bra.uni 	$L__BB35_207;

$L__BB35_211:
	mov.f64 	%fd3738, 0d4000000000000000;
	add.rn.f64 	%fd5727, %fd5251, %fd3738;
	bra.uni 	$L__BB35_212;

$L__BB35_207:
	mov.f64 	%fd3737, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r608, %temp}, %fd3737;
	}
	and.b32  	%r59, %r48, 2147483647;
	setp.eq.s32 	%p325, %r59, 2146435072;
	setp.eq.s32 	%p326, %r608, 0;
	and.pred  	%p327, %p325, %p326;
	@%p327 bra 	$L__BB35_210;
	bra.uni 	$L__BB35_208;

$L__BB35_210:
	setp.gt.f64 	%p334, %fd1926, 0d3FF0000000000000;
	selp.b32 	%r615, 2146435072, 0, %p334;
	mov.u32 	%r616, 0;
	xor.b32  	%r617, %r615, 2146435072;
	setp.lt.s32 	%p335, %r48, 0;
	selp.b32 	%r618, %r617, %r615, %p335;
	setp.eq.f64 	%p336, %fd5251, 0dBFF0000000000000;
	selp.b32 	%r619, 1072693248, %r618, %p336;
	mov.b64 	%fd5727, {%r616, %r619};
	bra.uni 	$L__BB35_212;

$L__BB35_208:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r609, %temp}, %fd5251;
	}
	and.b32  	%r610, %r58, 2147483647;
	setp.ne.s32 	%p328, %r610, 2146435072;
	setp.ne.s32 	%p329, %r609, 0;
	or.pred  	%p330, %p328, %p329;
	@%p330 bra 	$L__BB35_212;

	setp.gt.s32 	%p331, %r48, -1;
	selp.b32 	%r611, 2146435072, 0, %p331;
	mov.u32 	%r612, 0;
	setp.ne.s32 	%p332, %r59, 1071644672;
	and.pred  	%p333, %p332, %p13;
	or.b32  	%r613, %r611, -2147483648;
	selp.b32 	%r614, %r613, %r611, %p333;
	mov.b64 	%fd5727, {%r612, %r614};

$L__BB35_212:
	mul.f64 	%fd3739, %fd5727, 0d4008000000000000;
	setp.eq.f64 	%p337, %fd5251, 0d3FF0000000000000;
	selp.f64 	%fd3740, 0d4008000000000000, %fd3739, %p337;
	div.rn.f64 	%fd3741, %fd1924, 0d4008000000000000;
	add.f64 	%fd3742, %fd3741, 0d0000000000000000;
	mov.f64 	%fd3743, 0d0000000000000000;
	fma.rn.f64 	%fd3744, %fd3742, %fd3740, 0d0000000000000000;
	add.f64 	%fd3745, %fd1922, %fd3744;
	fma.rn.f64 	%fd3746, %fd5258, %fd1836, 0d0000000000000000;
	fma.rn.f64 	%fd3747, %fd5218, %fd1836, %fd1923;
	fma.rn.f64 	%fd3748, %fd5217, %fd3746, 0d0000000000000000;
	fma.rn.f64 	%fd3749, %fd5216, %fd3746, 0d0000000000000000;
	add.f64 	%fd1936, %fd3749, %fd1897;
	fma.rn.f64 	%fd3750, %fd5215, %fd3748, 0d0000000000000000;
	fma.rn.f64 	%fd3751, %fd5214, %fd3748, 0d0000000000000000;
	sub.f64 	%fd3752, %fd3743, %fd3751;
	fma.rn.f64 	%fd3753, %fd3752, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd3754, %fd3753, %fd1921;
	add.f64 	%fd3755, %fd3753, %fd3745;
	div.rn.f64 	%fd3756, %fd3752, 0d4008000000000000;
	add.f64 	%fd3757, %fd3756, 0d0000000000000000;
	add.f64 	%fd3758, %fd3757, %fd1920;
	fma.rn.f64 	%fd3759, %fd5254, %fd3750, 0d0000000000000000;
	fma.rn.f64 	%fd3760, %fd5254, %fd3750, 0d0000000000000000;
	add.f64 	%fd3761, %fd3760, %fd3758;
	add.f64 	%fd3762, %fd3759, %fd3761;
	fma.rn.f64 	%fd3763, %fd5258, %fd1837, 0d0000000000000000;
	fma.rn.f64 	%fd3764, %fd5211, %fd1837, %fd3747;
	fma.rn.f64 	%fd3765, %fd5210, %fd3763, 0d0000000000000000;
	fma.rn.f64 	%fd3766, %fd5209, %fd3763, 0d0000000000000000;
	add.f64 	%fd1937, %fd3766, %fd1898;
	fma.rn.f64 	%fd3767, %fd5208, %fd3765, 0d0000000000000000;
	fma.rn.f64 	%fd3768, %fd5207, %fd3765, 0d0000000000000000;
	sub.f64 	%fd3769, %fd3743, %fd3768;
	fma.rn.f64 	%fd3770, %fd3769, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd3771, %fd3770, %fd3762;
	add.f64 	%fd3772, %fd3770, %fd3755;
	div.rn.f64 	%fd3773, %fd3769, 0d4008000000000000;
	add.f64 	%fd3774, %fd3773, 0d0000000000000000;
	add.f64 	%fd3775, %fd3774, %fd3754;
	fma.rn.f64 	%fd3776, %fd5245, %fd3767, 0d0000000000000000;
	fma.rn.f64 	%fd3777, %fd5245, %fd3767, 0d0000000000000000;
	add.f64 	%fd3778, %fd3777, %fd3775;
	add.f64 	%fd3779, %fd3776, %fd3778;
	fma.rn.f64 	%fd3780, %fd5258, %fd1838, 0d0000000000000000;
	fma.rn.f64 	%fd3781, %fd5204, %fd1838, %fd3764;
	fma.rn.f64 	%fd3782, %fd5203, %fd3780, 0d0000000000000000;
	fma.rn.f64 	%fd3783, %fd5202, %fd3780, 0d0000000000000000;
	add.f64 	%fd3784, %fd3783, %fd1925;
	fma.rn.f64 	%fd3785, %fd5201, %fd3782, 0d0000000000000000;
	fma.rn.f64 	%fd3786, %fd5200, %fd3782, 0d0000000000000000;
	sub.f64 	%fd3787, %fd3743, %fd3786;
	fma.rn.f64 	%fd3788, %fd3787, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd1938, %fd3788, %fd3771;
	add.f64 	%fd1939, %fd3788, %fd3779;
	div.rn.f64 	%fd3789, %fd3787, 0d4008000000000000;
	add.f64 	%fd3790, %fd3789, 0d0000000000000000;
	add.f64 	%fd3791, %fd3790, %fd3772;
	fma.rn.f64 	%fd3792, %fd5251, %fd3785, 0d0000000000000000;
	fma.rn.f64 	%fd3793, %fd5251, %fd3785, 0d0000000000000000;
	add.f64 	%fd3794, %fd3793, %fd3791;
	add.f64 	%fd1940, %fd3792, %fd3794;
	fma.rn.f64 	%fd3795, %fd5258, %fd1839, 0d0000000000000000;
	fma.rn.f64 	%fd1941, %fd5197, %fd1839, %fd3781;
	fma.rn.f64 	%fd1942, %fd5196, %fd3795, 0d0000000000000000;
	fma.rn.f64 	%fd3796, %fd5195, %fd3795, 0d0000000000000000;
	add.f64 	%fd1943, %fd3796, %fd3784;
	abs.f64 	%fd1944, %fd5194;
	mov.f64 	%fd3797, 0d4000000000000000;
	{ // callseq 76, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1944;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd3797;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd5730, [retval0+0];
	} // callseq 76
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r60}, %fd5194;
	}
	setp.lt.s32 	%p338, %r60, 0;
	and.pred  	%p14, %p338, %p223;
	not.pred 	%p340, %p14;
	@%p340 bra 	$L__BB35_214;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r620}, %fd5730;
	}
	xor.b32  	%r621, %r620, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r622, %temp}, %fd5730;
	}
	mov.b64 	%fd5730, {%r622, %r621};

$L__BB35_214:
	setp.eq.f64 	%p341, %fd5194, 0d0000000000000000;
	@%p341 bra 	$L__BB35_218;
	bra.uni 	$L__BB35_215;

$L__BB35_218:
	selp.b32 	%r623, %r60, 0, %p223;
	mov.u32 	%r624, 0;
	or.b32  	%r625, %r623, 2146435072;
	setp.lt.s32 	%p345, %r48, 0;
	selp.b32 	%r626, %r625, %r623, %p345;
	mov.b64 	%fd5730, {%r624, %r626};
	bra.uni 	$L__BB35_219;

$L__BB35_215:
	setp.gt.s32 	%p342, %r60, -1;
	@%p342 bra 	$L__BB35_219;

	mov.f64 	%fd3798, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd3799, %fd3798;
	setp.eq.f64 	%p343, %fd3799, 0d4000000000000000;
	@%p343 bra 	$L__BB35_219;

	mov.f64 	%fd5730, 0dFFF8000000000000;

$L__BB35_219:
	add.f64 	%fd3801, %fd5194, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r627}, %fd3801;
	}
	and.b32  	%r628, %r627, 2146435072;
	setp.ne.s32 	%p346, %r628, 2146435072;
	@%p346 bra 	$L__BB35_226;

	setp.gtu.f64 	%p347, %fd1944, 0d7FF0000000000000;
	@%p347 bra 	$L__BB35_225;
	bra.uni 	$L__BB35_221;

$L__BB35_225:
	mov.f64 	%fd3803, 0d4000000000000000;
	add.rn.f64 	%fd5730, %fd5194, %fd3803;
	bra.uni 	$L__BB35_226;

$L__BB35_221:
	mov.f64 	%fd3802, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r629, %temp}, %fd3802;
	}
	and.b32  	%r61, %r48, 2147483647;
	setp.eq.s32 	%p348, %r61, 2146435072;
	setp.eq.s32 	%p349, %r629, 0;
	and.pred  	%p350, %p348, %p349;
	@%p350 bra 	$L__BB35_224;
	bra.uni 	$L__BB35_222;

$L__BB35_224:
	setp.gt.f64 	%p357, %fd1944, 0d3FF0000000000000;
	selp.b32 	%r636, 2146435072, 0, %p357;
	mov.u32 	%r637, 0;
	xor.b32  	%r638, %r636, 2146435072;
	setp.lt.s32 	%p358, %r48, 0;
	selp.b32 	%r639, %r638, %r636, %p358;
	setp.eq.f64 	%p359, %fd5194, 0dBFF0000000000000;
	selp.b32 	%r640, 1072693248, %r639, %p359;
	mov.b64 	%fd5730, {%r637, %r640};
	bra.uni 	$L__BB35_226;

$L__BB35_222:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r630, %temp}, %fd5194;
	}
	and.b32  	%r631, %r60, 2147483647;
	setp.ne.s32 	%p351, %r631, 2146435072;
	setp.ne.s32 	%p352, %r630, 0;
	or.pred  	%p353, %p351, %p352;
	@%p353 bra 	$L__BB35_226;

	setp.gt.s32 	%p354, %r48, -1;
	selp.b32 	%r632, 2146435072, 0, %p354;
	mov.u32 	%r633, 0;
	setp.ne.s32 	%p355, %r61, 1071644672;
	and.pred  	%p356, %p355, %p14;
	or.b32  	%r634, %r632, -2147483648;
	selp.b32 	%r635, %r634, %r632, %p356;
	mov.b64 	%fd5730, {%r633, %r635};

$L__BB35_226:
	mul.f64 	%fd3804, %fd5730, 0d4008000000000000;
	setp.eq.f64 	%p360, %fd5194, 0d3FF0000000000000;
	selp.f64 	%fd3805, 0d4008000000000000, %fd3804, %p360;
	div.rn.f64 	%fd3806, %fd1942, 0d4008000000000000;
	add.f64 	%fd3807, %fd3806, 0d0000000000000000;
	mov.f64 	%fd3808, 0d0000000000000000;
	sub.f64 	%fd3809, %fd3808, %fd3807;
	fma.rn.f64 	%fd3810, %fd3809, %fd3805, 0d0000000000000000;
	sub.f64 	%fd3811, %fd3808, %fd3810;
	add.f64 	%fd3812, %fd1938, %fd3811;
	add.f64 	%fd3813, %fd1939, %fd3811;
	add.f64 	%fd3814, %fd1940, %fd3811;
	fma.rn.f64 	%fd3815, %fd1941, 0d3FE0AAAAA0000000, 0d0000000000000000;
	add.f64 	%fd1954, %fd3814, 0d0000000000000000;
	add.f64 	%fd1955, %fd3813, 0d0000000000000000;
	add.f64 	%fd1956, %fd3812, 0d0000000000000000;
	fma.rn.f64 	%fd1957, %fd5124, %fd3815, %fd1918;
	fma.rn.f64 	%fd1958, %fd5126, %fd3815, %fd1919;
	fma.rn.f64 	%fd3816, %fd5128, %fd1832, 0d0000000000000000;
	fma.rn.f64 	%fd3817, %fd5192, %fd1832, 0d0000000000000000;
	fma.rn.f64 	%fd3818, %fd5191, %fd3816, 0d0000000000000000;
	fma.rn.f64 	%fd3819, %fd5190, %fd3816, 0d0000000000000000;
	add.f64 	%fd3820, %fd3819, %fd1943;
	fma.rn.f64 	%fd3821, %fd5189, %fd3818, 0d0000000000000000;
	fma.rn.f64 	%fd3822, %fd5188, %fd3818, 0d0000000000000000;
	fma.rn.f64 	%fd3823, %fd5189, %fd3821, 0d0000000000000000;
	fma.rn.f64 	%fd3824, %fd5186, %fd3821, 0d0000000000000000;
	add.f64 	%fd3825, %fd3824, %fd3822;
	fma.rn.f64 	%fd3826, %fd5128, %fd1833, 0d0000000000000000;
	fma.rn.f64 	%fd3827, %fd5185, %fd1833, %fd3817;
	fma.rn.f64 	%fd3828, %fd5184, %fd3826, 0d0000000000000000;
	fma.rn.f64 	%fd3829, %fd5183, %fd3826, 0d0000000000000000;
	add.f64 	%fd3830, %fd3829, %fd3820;
	fma.rn.f64 	%fd3831, %fd5189, %fd3828, 0d0000000000000000;
	fma.rn.f64 	%fd3832, %fd5181, %fd3828, 0d0000000000000000;
	add.f64 	%fd3833, %fd3832, %fd3825;
	fma.rn.f64 	%fd3834, %fd5180, %fd3831, 0d0000000000000000;
	fma.rn.f64 	%fd3835, %fd5186, %fd3831, 0d0000000000000000;
	add.f64 	%fd3836, %fd3834, %fd3823;
	fma.rn.f64 	%fd3837, %fd5128, %fd1840, 0d0000000000000000;
	fma.rn.f64 	%fd3838, %fd5178, %fd1840, %fd3827;
	fma.rn.f64 	%fd3839, %fd5177, %fd3837, 0d0000000000000000;
	fma.rn.f64 	%fd3840, %fd5176, %fd3837, 0d0000000000000000;
	add.f64 	%fd3841, %fd3840, %fd3830;
	fma.rn.f64 	%fd3842, %fd5180, %fd3839, 0d0000000000000000;
	fma.rn.f64 	%fd3843, %fd5181, %fd3839, 0d0000000000000000;
	add.f64 	%fd3844, %fd3843, %fd3835;
	fma.rn.f64 	%fd3845, %fd5180, %fd3842, 0d0000000000000000;
	fma.rn.f64 	%fd3846, %fd5186, %fd3842, 0d0000000000000000;
	add.f64 	%fd3847, %fd3846, %fd3844;
	add.f64 	%fd3848, %fd3845, %fd3836;
	fma.rn.f64 	%fd3849, %fd5128, %fd1834, 0d0000000000000000;
	fma.rn.f64 	%fd3850, %fd5171, %fd1834, %fd3838;
	fma.rn.f64 	%fd3851, %fd5170, %fd3849, 0d0000000000000000;
	fma.rn.f64 	%fd3852, %fd5169, %fd3849, 0d0000000000000000;
	add.f64 	%fd3853, %fd3852, %fd3841;
	fma.rn.f64 	%fd3854, %fd5189, %fd3851, 0d0000000000000000;
	fma.rn.f64 	%fd3855, %fd5167, %fd3851, 0d0000000000000000;
	add.f64 	%fd1959, %fd3855, %fd3833;
	fma.rn.f64 	%fd3856, %fd3854, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3857, %fd5186, %fd3856, 0d0000000000000000;
	fma.rn.f64 	%fd3858, %fd5186, %fd3856, 0d0000000000000000;
	add.f64 	%fd3859, %fd3858, %fd3848;
	add.f64 	%fd3860, %fd3857, %fd3859;
	fma.rn.f64 	%fd3861, %fd5128, %fd1835, 0d0000000000000000;
	fma.rn.f64 	%fd3862, %fd5164, %fd1835, %fd3850;
	fma.rn.f64 	%fd3863, %fd5163, %fd3861, 0d0000000000000000;
	fma.rn.f64 	%fd3864, %fd5162, %fd3861, 0d0000000000000000;
	add.f64 	%fd3865, %fd3864, %fd3853;
	fma.rn.f64 	%fd3866, %fd5180, %fd3863, 0d0000000000000000;
	fma.rn.f64 	%fd3867, %fd5167, %fd3863, 0d0000000000000000;
	add.f64 	%fd1960, %fd3867, %fd3847;
	fma.rn.f64 	%fd3868, %fd3866, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3869, %fd5186, %fd3868, 0d0000000000000000;
	fma.rn.f64 	%fd3870, %fd5186, %fd3868, 0d0000000000000000;
	add.f64 	%fd3871, %fd3870, %fd3860;
	add.f64 	%fd1961, %fd3869, %fd3871;
	fma.rn.f64 	%fd3872, %fd5128, %fd1844, 0d0000000000000000;
	fma.rn.f64 	%fd1962, %fd5157, %fd1844, %fd3862;
	fma.rn.f64 	%fd1963, %fd5156, %fd3872, 0d0000000000000000;
	fma.rn.f64 	%fd3873, %fd5155, %fd3872, 0d0000000000000000;
	add.f64 	%fd1964, %fd3873, %fd3865;
	abs.f64 	%fd1965, %fd5186;
	mov.f64 	%fd3874, 0d4000000000000000;
	{ // callseq 77, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1965;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd3874;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd5733, [retval0+0];
	} // callseq 77
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd5186;
	}
	setp.lt.s32 	%p361, %r62, 0;
	and.pred  	%p15, %p361, %p223;
	not.pred 	%p363, %p15;
	@%p363 bra 	$L__BB35_228;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r641}, %fd5733;
	}
	xor.b32  	%r642, %r641, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r643, %temp}, %fd5733;
	}
	mov.b64 	%fd5733, {%r643, %r642};

$L__BB35_228:
	setp.eq.f64 	%p364, %fd5186, 0d0000000000000000;
	@%p364 bra 	$L__BB35_232;
	bra.uni 	$L__BB35_229;

$L__BB35_232:
	selp.b32 	%r644, %r62, 0, %p223;
	mov.u32 	%r645, 0;
	or.b32  	%r646, %r644, 2146435072;
	setp.lt.s32 	%p368, %r48, 0;
	selp.b32 	%r647, %r646, %r644, %p368;
	mov.b64 	%fd5733, {%r645, %r647};
	bra.uni 	$L__BB35_233;

$L__BB35_229:
	setp.gt.s32 	%p365, %r62, -1;
	@%p365 bra 	$L__BB35_233;

	mov.f64 	%fd3875, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd3876, %fd3875;
	setp.eq.f64 	%p366, %fd3876, 0d4000000000000000;
	@%p366 bra 	$L__BB35_233;

	mov.f64 	%fd5733, 0dFFF8000000000000;

$L__BB35_233:
	add.f64 	%fd3878, %fd5186, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r648}, %fd3878;
	}
	and.b32  	%r649, %r648, 2146435072;
	setp.ne.s32 	%p369, %r649, 2146435072;
	@%p369 bra 	$L__BB35_240;

	setp.gtu.f64 	%p370, %fd1965, 0d7FF0000000000000;
	@%p370 bra 	$L__BB35_239;
	bra.uni 	$L__BB35_235;

$L__BB35_239:
	mov.f64 	%fd3880, 0d4000000000000000;
	add.rn.f64 	%fd5733, %fd5186, %fd3880;
	bra.uni 	$L__BB35_240;

$L__BB35_235:
	mov.f64 	%fd3879, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r650, %temp}, %fd3879;
	}
	and.b32  	%r63, %r48, 2147483647;
	setp.eq.s32 	%p371, %r63, 2146435072;
	setp.eq.s32 	%p372, %r650, 0;
	and.pred  	%p373, %p371, %p372;
	@%p373 bra 	$L__BB35_238;
	bra.uni 	$L__BB35_236;

$L__BB35_238:
	setp.gt.f64 	%p380, %fd1965, 0d3FF0000000000000;
	selp.b32 	%r657, 2146435072, 0, %p380;
	mov.u32 	%r658, 0;
	xor.b32  	%r659, %r657, 2146435072;
	setp.lt.s32 	%p381, %r48, 0;
	selp.b32 	%r660, %r659, %r657, %p381;
	setp.eq.f64 	%p382, %fd5186, 0dBFF0000000000000;
	selp.b32 	%r661, 1072693248, %r660, %p382;
	mov.b64 	%fd5733, {%r658, %r661};
	bra.uni 	$L__BB35_240;

$L__BB35_236:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r651, %temp}, %fd5186;
	}
	and.b32  	%r652, %r62, 2147483647;
	setp.ne.s32 	%p374, %r652, 2146435072;
	setp.ne.s32 	%p375, %r651, 0;
	or.pred  	%p376, %p374, %p375;
	@%p376 bra 	$L__BB35_240;

	setp.gt.s32 	%p377, %r48, -1;
	selp.b32 	%r653, 2146435072, 0, %p377;
	mov.u32 	%r654, 0;
	setp.ne.s32 	%p378, %r63, 1071644672;
	and.pred  	%p379, %p378, %p15;
	or.b32  	%r655, %r653, -2147483648;
	selp.b32 	%r656, %r655, %r653, %p379;
	mov.b64 	%fd5733, {%r654, %r656};

$L__BB35_240:
	mul.f64 	%fd3881, %fd5733, 0d4008000000000000;
	setp.eq.f64 	%p383, %fd5186, 0d3FF0000000000000;
	selp.f64 	%fd3882, 0d4008000000000000, %fd3881, %p383;
	div.rn.f64 	%fd3883, %fd1963, 0d4008000000000000;
	add.f64 	%fd3884, %fd3883, 0d0000000000000000;
	mov.f64 	%fd3885, 0d0000000000000000;
	fma.rn.f64 	%fd3886, %fd3884, %fd3882, 0d0000000000000000;
	add.f64 	%fd3887, %fd1961, %fd3886;
	fma.rn.f64 	%fd3888, %fd5128, %fd1836, 0d0000000000000000;
	fma.rn.f64 	%fd3889, %fd5153, %fd1836, %fd1962;
	fma.rn.f64 	%fd3890, %fd5152, %fd3888, 0d0000000000000000;
	fma.rn.f64 	%fd3891, %fd5151, %fd3888, 0d0000000000000000;
	add.f64 	%fd1975, %fd3891, %fd1936;
	fma.rn.f64 	%fd3892, %fd5150, %fd3890, 0d0000000000000000;
	fma.rn.f64 	%fd3893, %fd5149, %fd3890, 0d0000000000000000;
	sub.f64 	%fd3894, %fd3885, %fd3893;
	fma.rn.f64 	%fd3895, %fd3894, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd3896, %fd3895, %fd1960;
	add.f64 	%fd3897, %fd3895, %fd3887;
	div.rn.f64 	%fd3898, %fd3894, 0d4008000000000000;
	add.f64 	%fd3899, %fd3898, 0d0000000000000000;
	add.f64 	%fd3900, %fd3899, %fd1959;
	fma.rn.f64 	%fd3901, %fd5189, %fd3892, 0d0000000000000000;
	fma.rn.f64 	%fd3902, %fd5189, %fd3892, 0d0000000000000000;
	add.f64 	%fd3903, %fd3902, %fd3900;
	add.f64 	%fd3904, %fd3901, %fd3903;
	fma.rn.f64 	%fd3905, %fd5128, %fd1837, 0d0000000000000000;
	fma.rn.f64 	%fd3906, %fd5146, %fd1837, %fd3889;
	fma.rn.f64 	%fd3907, %fd5145, %fd3905, 0d0000000000000000;
	fma.rn.f64 	%fd3908, %fd5144, %fd3905, 0d0000000000000000;
	add.f64 	%fd1976, %fd3908, %fd1937;
	fma.rn.f64 	%fd3909, %fd5143, %fd3907, 0d0000000000000000;
	fma.rn.f64 	%fd3910, %fd5142, %fd3907, 0d0000000000000000;
	sub.f64 	%fd3911, %fd3885, %fd3910;
	fma.rn.f64 	%fd3912, %fd3911, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd3913, %fd3912, %fd3904;
	add.f64 	%fd3914, %fd3912, %fd3897;
	div.rn.f64 	%fd3915, %fd3911, 0d4008000000000000;
	add.f64 	%fd3916, %fd3915, 0d0000000000000000;
	add.f64 	%fd3917, %fd3916, %fd3896;
	fma.rn.f64 	%fd3918, %fd5180, %fd3909, 0d0000000000000000;
	fma.rn.f64 	%fd3919, %fd5180, %fd3909, 0d0000000000000000;
	add.f64 	%fd3920, %fd3919, %fd3917;
	add.f64 	%fd3921, %fd3918, %fd3920;
	fma.rn.f64 	%fd3922, %fd5128, %fd1838, 0d0000000000000000;
	fma.rn.f64 	%fd3923, %fd5139, %fd1838, %fd3906;
	fma.rn.f64 	%fd3924, %fd5138, %fd3922, 0d0000000000000000;
	fma.rn.f64 	%fd3925, %fd5137, %fd3922, 0d0000000000000000;
	add.f64 	%fd3926, %fd3925, %fd1964;
	fma.rn.f64 	%fd3927, %fd5136, %fd3924, 0d0000000000000000;
	fma.rn.f64 	%fd3928, %fd5135, %fd3924, 0d0000000000000000;
	sub.f64 	%fd3929, %fd3885, %fd3928;
	fma.rn.f64 	%fd3930, %fd3929, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd1977, %fd3930, %fd3913;
	add.f64 	%fd1978, %fd3930, %fd3921;
	div.rn.f64 	%fd3931, %fd3929, 0d4008000000000000;
	add.f64 	%fd3932, %fd3931, 0d0000000000000000;
	add.f64 	%fd3933, %fd3932, %fd3914;
	fma.rn.f64 	%fd3934, %fd5186, %fd3927, 0d0000000000000000;
	fma.rn.f64 	%fd3935, %fd5186, %fd3927, 0d0000000000000000;
	add.f64 	%fd3936, %fd3935, %fd3933;
	add.f64 	%fd1979, %fd3934, %fd3936;
	fma.rn.f64 	%fd3937, %fd5128, %fd1839, 0d0000000000000000;
	fma.rn.f64 	%fd1980, %fd5132, %fd1839, %fd3923;
	fma.rn.f64 	%fd1981, %fd5131, %fd3937, 0d0000000000000000;
	fma.rn.f64 	%fd3938, %fd5130, %fd3937, 0d0000000000000000;
	add.f64 	%fd1982, %fd3938, %fd3926;
	abs.f64 	%fd1983, %fd5129;
	mov.f64 	%fd3939, 0d4000000000000000;
	{ // callseq 78, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1983;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd3939;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd5736, [retval0+0];
	} // callseq 78
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r64}, %fd5129;
	}
	setp.lt.s32 	%p384, %r64, 0;
	and.pred  	%p16, %p384, %p223;
	not.pred 	%p386, %p16;
	@%p386 bra 	$L__BB35_242;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r662}, %fd5736;
	}
	xor.b32  	%r663, %r662, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r664, %temp}, %fd5736;
	}
	mov.b64 	%fd5736, {%r664, %r663};

$L__BB35_242:
	setp.eq.f64 	%p387, %fd5129, 0d0000000000000000;
	@%p387 bra 	$L__BB35_246;
	bra.uni 	$L__BB35_243;

$L__BB35_246:
	selp.b32 	%r665, %r64, 0, %p223;
	mov.u32 	%r666, 0;
	or.b32  	%r667, %r665, 2146435072;
	setp.lt.s32 	%p391, %r48, 0;
	selp.b32 	%r668, %r667, %r665, %p391;
	mov.b64 	%fd5736, {%r666, %r668};
	bra.uni 	$L__BB35_247;

$L__BB35_243:
	setp.gt.s32 	%p388, %r64, -1;
	@%p388 bra 	$L__BB35_247;

	mov.f64 	%fd3940, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd3941, %fd3940;
	setp.eq.f64 	%p389, %fd3941, 0d4000000000000000;
	@%p389 bra 	$L__BB35_247;

	mov.f64 	%fd5736, 0dFFF8000000000000;

$L__BB35_247:
	add.f64 	%fd3943, %fd5129, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r669}, %fd3943;
	}
	and.b32  	%r670, %r669, 2146435072;
	setp.ne.s32 	%p392, %r670, 2146435072;
	@%p392 bra 	$L__BB35_254;

	setp.gtu.f64 	%p393, %fd1983, 0d7FF0000000000000;
	@%p393 bra 	$L__BB35_253;
	bra.uni 	$L__BB35_249;

$L__BB35_253:
	mov.f64 	%fd3945, 0d4000000000000000;
	add.rn.f64 	%fd5736, %fd5129, %fd3945;
	bra.uni 	$L__BB35_254;

$L__BB35_249:
	mov.f64 	%fd3944, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r671, %temp}, %fd3944;
	}
	and.b32  	%r65, %r48, 2147483647;
	setp.eq.s32 	%p394, %r65, 2146435072;
	setp.eq.s32 	%p395, %r671, 0;
	and.pred  	%p396, %p394, %p395;
	@%p396 bra 	$L__BB35_252;
	bra.uni 	$L__BB35_250;

$L__BB35_252:
	setp.gt.f64 	%p403, %fd1983, 0d3FF0000000000000;
	selp.b32 	%r678, 2146435072, 0, %p403;
	mov.u32 	%r679, 0;
	xor.b32  	%r680, %r678, 2146435072;
	setp.lt.s32 	%p404, %r48, 0;
	selp.b32 	%r681, %r680, %r678, %p404;
	setp.eq.f64 	%p405, %fd5129, 0dBFF0000000000000;
	selp.b32 	%r682, 1072693248, %r681, %p405;
	mov.b64 	%fd5736, {%r679, %r682};
	bra.uni 	$L__BB35_254;

$L__BB35_250:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r672, %temp}, %fd5129;
	}
	and.b32  	%r673, %r64, 2147483647;
	setp.ne.s32 	%p397, %r673, 2146435072;
	setp.ne.s32 	%p398, %r672, 0;
	or.pred  	%p399, %p397, %p398;
	@%p399 bra 	$L__BB35_254;

	setp.gt.s32 	%p400, %r48, -1;
	selp.b32 	%r674, 2146435072, 0, %p400;
	mov.u32 	%r675, 0;
	setp.ne.s32 	%p401, %r65, 1071644672;
	and.pred  	%p402, %p401, %p16;
	or.b32  	%r676, %r674, -2147483648;
	selp.b32 	%r677, %r676, %r674, %p402;
	mov.b64 	%fd5736, {%r675, %r677};

$L__BB35_254:
	mul.f64 	%fd3946, %fd5736, 0d4008000000000000;
	setp.eq.f64 	%p406, %fd5129, 0d3FF0000000000000;
	selp.f64 	%fd3947, 0d4008000000000000, %fd3946, %p406;
	div.rn.f64 	%fd3948, %fd1981, 0d4008000000000000;
	add.f64 	%fd3949, %fd3948, 0d0000000000000000;
	mov.f64 	%fd3950, 0d0000000000000000;
	sub.f64 	%fd3951, %fd3950, %fd3949;
	fma.rn.f64 	%fd3952, %fd3951, %fd3947, 0d0000000000000000;
	sub.f64 	%fd3953, %fd3950, %fd3952;
	add.f64 	%fd3954, %fd1977, %fd3953;
	add.f64 	%fd3955, %fd1978, %fd3953;
	add.f64 	%fd3956, %fd1979, %fd3953;
	fma.rn.f64 	%fd3957, %fd1980, 0dBFE2000000000000, 0d0000000000000000;
	add.f64 	%fd3958, %fd3956, 0d0000000000000000;
	add.f64 	%fd3959, %fd3955, 0d0000000000000000;
	add.f64 	%fd3960, %fd3954, 0d0000000000000000;
	div.rn.f64 	%fd3961, %fd1878, 0d4014000000000000;
	add.f64 	%fd3962, %fd3961, 0d0000000000000000;
	sub.f64 	%fd3963, %fd3950, %fd3962;
	add.f64 	%fd3964, %fd5754, %fd3963;
	add.f64 	%fd3965, %fd5745, %fd3962;
	fma.rn.f64 	%fd3966, %fd3962, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd3967, %fd3950, %fd3966;
	add.f64 	%fd3968, %fd3964, %fd3967;
	add.f64 	%fd3969, %fd5748, %fd3966;
	add.f64 	%fd3970, %fd3963, %fd3968;
	add.f64 	%fd3971, %fd5751, %fd3962;
	div.rn.f64 	%fd3972, %fd1917, 0d4014000000000000;
	add.f64 	%fd3973, %fd3972, 0d0000000000000000;
	fma.rn.f64 	%fd3974, %fd3973, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd3975, %fd3950, %fd3974;
	add.f64 	%fd3976, %fd3975, %fd3970;
	add.f64 	%fd3977, %fd3965, %fd3974;
	sub.f64 	%fd3978, %fd3950, %fd3973;
	add.f64 	%fd3979, %fd3978, %fd3976;
	add.f64 	%fd3980, %fd3973, %fd3969;
	add.f64 	%fd3981, %fd3978, %fd3979;
	add.f64 	%fd3982, %fd3971, %fd3973;
	div.rn.f64 	%fd3983, %fd1956, 0d4014000000000000;
	add.f64 	%fd3984, %fd3983, 0d0000000000000000;
	sub.f64 	%fd3985, %fd3950, %fd3984;
	add.f64 	%fd3986, %fd3985, %fd3981;
	add.f64 	%fd3987, %fd3984, %fd3977;
	add.f64 	%fd3988, %fd3985, %fd3986;
	add.f64 	%fd3989, %fd3984, %fd3980;
	fma.rn.f64 	%fd3990, %fd3984, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd3991, %fd3950, %fd3990;
	add.f64 	%fd3992, %fd3991, %fd3988;
	add.f64 	%fd3993, %fd3982, %fd3990;
	div.rn.f64 	%fd3994, %fd3960, 0d4008000000000000;
	add.f64 	%fd3995, %fd3994, 0d0000000000000000;
	sub.f64 	%fd3996, %fd3950, %fd3995;
	add.f64 	%fd3997, %fd3996, %fd3992;
	add.f64 	%fd3998, %fd3996, %fd3997;
	div.rn.f64 	%fd3999, %fd1877, 0d4014000000000000;
	add.f64 	%fd4000, %fd3999, 0d0000000000000000;
	sub.f64 	%fd4001, %fd3950, %fd4000;
	add.f64 	%fd4002, %fd5753, %fd4001;
	add.f64 	%fd4003, %fd5744, %fd4000;
	fma.rn.f64 	%fd4004, %fd4000, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd4005, %fd3950, %fd4004;
	add.f64 	%fd4006, %fd4002, %fd4005;
	add.f64 	%fd4007, %fd5747, %fd4004;
	add.f64 	%fd4008, %fd4001, %fd4006;
	add.f64 	%fd4009, %fd5750, %fd4000;
	div.rn.f64 	%fd4010, %fd1916, 0d4014000000000000;
	add.f64 	%fd4011, %fd4010, 0d0000000000000000;
	fma.rn.f64 	%fd4012, %fd4011, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd4013, %fd3950, %fd4012;
	add.f64 	%fd4014, %fd4013, %fd4008;
	add.f64 	%fd4015, %fd4003, %fd4012;
	sub.f64 	%fd4016, %fd3950, %fd4011;
	add.f64 	%fd4017, %fd4016, %fd4014;
	add.f64 	%fd4018, %fd4011, %fd4007;
	add.f64 	%fd4019, %fd4016, %fd4017;
	add.f64 	%fd4020, %fd4009, %fd4011;
	div.rn.f64 	%fd4021, %fd1955, 0d4014000000000000;
	add.f64 	%fd4022, %fd4021, 0d0000000000000000;
	sub.f64 	%fd4023, %fd3950, %fd4022;
	add.f64 	%fd4024, %fd4023, %fd4019;
	add.f64 	%fd4025, %fd4022, %fd4015;
	add.f64 	%fd4026, %fd4023, %fd4024;
	add.f64 	%fd4027, %fd4022, %fd4018;
	fma.rn.f64 	%fd4028, %fd4022, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd4029, %fd3950, %fd4028;
	add.f64 	%fd4030, %fd4029, %fd4026;
	add.f64 	%fd4031, %fd4020, %fd4028;
	div.rn.f64 	%fd4032, %fd3959, 0d4008000000000000;
	add.f64 	%fd4033, %fd4032, 0d0000000000000000;
	sub.f64 	%fd4034, %fd3950, %fd4033;
	add.f64 	%fd4035, %fd4034, %fd4030;
	add.f64 	%fd4036, %fd4034, %fd4035;
	div.rn.f64 	%fd4037, %fd1876, 0d4014000000000000;
	add.f64 	%fd4038, %fd4037, 0d0000000000000000;
	sub.f64 	%fd4039, %fd3950, %fd4038;
	add.f64 	%fd4040, %fd5752, %fd4039;
	add.f64 	%fd4041, %fd5743, %fd4038;
	fma.rn.f64 	%fd4042, %fd4038, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd4043, %fd3950, %fd4042;
	add.f64 	%fd4044, %fd4040, %fd4043;
	add.f64 	%fd4045, %fd5746, %fd4042;
	add.f64 	%fd4046, %fd4039, %fd4044;
	add.f64 	%fd4047, %fd5749, %fd4038;
	div.rn.f64 	%fd4048, %fd1915, 0d4014000000000000;
	add.f64 	%fd4049, %fd4048, 0d0000000000000000;
	fma.rn.f64 	%fd4050, %fd4049, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd4051, %fd3950, %fd4050;
	add.f64 	%fd4052, %fd4051, %fd4046;
	add.f64 	%fd4053, %fd4041, %fd4050;
	sub.f64 	%fd4054, %fd3950, %fd4049;
	add.f64 	%fd4055, %fd4054, %fd4052;
	add.f64 	%fd4056, %fd4049, %fd4045;
	add.f64 	%fd4057, %fd4054, %fd4055;
	add.f64 	%fd4058, %fd4047, %fd4049;
	div.rn.f64 	%fd4059, %fd1954, 0d4014000000000000;
	add.f64 	%fd4060, %fd4059, 0d0000000000000000;
	sub.f64 	%fd4061, %fd3950, %fd4060;
	add.f64 	%fd4062, %fd4061, %fd4057;
	add.f64 	%fd4063, %fd4060, %fd4053;
	add.f64 	%fd4064, %fd4061, %fd4062;
	add.f64 	%fd4065, %fd4060, %fd4056;
	fma.rn.f64 	%fd4066, %fd4060, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd4067, %fd3950, %fd4066;
	add.f64 	%fd4068, %fd4067, %fd4064;
	add.f64 	%fd4069, %fd4058, %fd4066;
	div.rn.f64 	%fd4070, %fd3958, 0d4008000000000000;
	add.f64 	%fd4071, %fd4070, 0d0000000000000000;
	sub.f64 	%fd4072, %fd3950, %fd4071;
	add.f64 	%fd4073, %fd4072, %fd4068;
	add.f64 	%fd4074, %fd4072, %fd4073;
	mul.f64 	%fd4075, %fd5100, %fd1976;
	fma.rn.f64 	%fd4076, %fd5099, %fd1982, %fd4075;
	fma.rn.f64 	%fd4077, %fd5101, %fd1975, %fd4076;
	mul.f64 	%fd4078, %fd5127, %fd5127;
	div.rn.f64 	%fd4079, %fd4077, %fd4078;
	div.rn.f64 	%fd4080, %fd1982, %fd5127;
	add.f64 	%fd5737, %fd4080, 0d0000000000000000;
	div.rn.f64 	%fd4081, %fd1976, %fd5127;
	add.f64 	%fd5738, %fd4081, 0d0000000000000000;
	div.rn.f64 	%fd4082, %fd1975, %fd5127;
	add.f64 	%fd5739, %fd4082, 0d0000000000000000;
	fma.rn.f64 	%fd4083, %fd5124, %fd3957, %fd1957;
	fma.rn.f64 	%fd5759, %fd5126, %fd3957, %fd1958;
	add.f64 	%fd1997, %fd3995, %fd3987;
	add.f64 	%fd1998, %fd3995, %fd3989;
	add.f64 	%fd5754, %fd3996, %fd3998;
	add.f64 	%fd2000, %fd3995, %fd3993;
	add.f64 	%fd2001, %fd4033, %fd4025;
	add.f64 	%fd2002, %fd4033, %fd4027;
	add.f64 	%fd5753, %fd4034, %fd4036;
	add.f64 	%fd2004, %fd4033, %fd4031;
	add.f64 	%fd2005, %fd4071, %fd4063;
	add.f64 	%fd2006, %fd4071, %fd4065;
	add.f64 	%fd5752, %fd4072, %fd4074;
	add.f64 	%fd2008, %fd4071, %fd4069;
	sub.f64 	%fd4084, %fd3950, %fd4079;
	fma.rn.f64 	%fd5755, %fd4084, 0d4000000000000000, %fd4083;
	setp.ne.s32 	%p407, %r697, 0;
	@%p407 bra 	$L__BB35_256;

	add.f64 	%fd5756, %fd5756, %fd5737;
	add.f64 	%fd5757, %fd5757, %fd5738;
	add.f64 	%fd5758, %fd5758, %fd5739;
	mov.f64 	%fd5737, %fd3950;
	mov.f64 	%fd5738, %fd3950;
	mov.f64 	%fd5739, %fd3950;

$L__BB35_256:
	sub.f64 	%fd4088, %fd5756, %fd5737;
	setp.eq.s32 	%p408, %r697, 0;
	selp.f64 	%fd5756, %fd5756, %fd4088, %p408;
	sub.f64 	%fd4089, %fd5757, %fd5738;
	selp.f64 	%fd5757, %fd5757, %fd4089, %p408;
	sub.f64 	%fd4090, %fd5758, %fd5739;
	selp.f64 	%fd5758, %fd5758, %fd4090, %p408;
	fma.rn.f64 	%fd4091, %fd5102, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd4092, %fd5103, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd4093, %fd5104, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd5743, %fd5111, 0d0000000000000000, %fd2005;
	fma.rn.f64 	%fd5744, %fd5112, 0d0000000000000000, %fd2001;
	fma.rn.f64 	%fd5745, %fd5113, 0d0000000000000000, %fd1997;
	mul.f64 	%fd4094, %fd5106, %fd4093;
	mul.f64 	%fd4095, %fd4092, %fd5107;
	sub.f64 	%fd4096, %fd4094, %fd4095;
	mul.f64 	%fd4097, %fd4091, %fd5107;
	mul.f64 	%fd4098, %fd5105, %fd4093;
	sub.f64 	%fd4099, %fd4097, %fd4098;
	mul.f64 	%fd4100, %fd5105, %fd4092;
	mul.f64 	%fd4101, %fd4091, %fd5106;
	sub.f64 	%fd4102, %fd4100, %fd4101;
	add.f64 	%fd5749, %fd4096, %fd2008;
	add.f64 	%fd5750, %fd4099, %fd2004;
	add.f64 	%fd5751, %fd4102, %fd2000;
	mul.f64 	%fd4103, %fd4093, %fd5109;
	mul.f64 	%fd4104, %fd4092, %fd5110;
	sub.f64 	%fd4105, %fd4103, %fd4104;
	mul.f64 	%fd4106, %fd4091, %fd5110;
	mul.f64 	%fd4107, %fd4093, %fd5108;
	sub.f64 	%fd4108, %fd4106, %fd4107;
	mul.f64 	%fd4109, %fd4092, %fd5108;
	mul.f64 	%fd4110, %fd4091, %fd5109;
	sub.f64 	%fd4111, %fd4109, %fd4110;
	sub.f64 	%fd5746, %fd2006, %fd4105;
	sub.f64 	%fd5747, %fd2002, %fd4108;
	sub.f64 	%fd5748, %fd1998, %fd4111;

$L__BB35_257:
	fma.rn.f64 	%fd2048, %fd5755, 0d3FE0000000000000, 0d0000000000000000;
	setp.leu.f64 	%p409, %fd5125, 0d0000000000000000;
	@%p409 bra 	$L__BB35_259;

	div.rn.f64 	%fd4112, %fd5114, %fd5125;
	div.rn.f64 	%fd4113, %fd5115, %fd5125;
	div.rn.f64 	%fd4114, %fd5116, %fd5125;
	fma.rn.f64 	%fd5756, %fd4112, %fd2048, %fd5756;
	fma.rn.f64 	%fd5757, %fd4113, %fd2048, %fd5757;
	fma.rn.f64 	%fd5758, %fd4114, %fd2048, %fd5758;

$L__BB35_259:
	mul.f64 	%fd4115, %fd5119, %fd5757;
	mul.f64 	%fd4116, %fd5118, %fd5758;
	sub.f64 	%fd4117, %fd4116, %fd4115;
	mul.f64 	%fd4118, %fd5117, %fd5758;
	mul.f64 	%fd4119, %fd5119, %fd5756;
	sub.f64 	%fd4120, %fd4119, %fd4118;
	mul.f64 	%fd4121, %fd5118, %fd5756;
	mul.f64 	%fd4122, %fd5117, %fd5757;
	sub.f64 	%fd4123, %fd4122, %fd4121;
	add.f64 	%fd4124, %fd4117, 0d0000000000000000;
	add.f64 	%fd4125, %fd4120, 0d0000000000000000;
	add.f64 	%fd4126, %fd4123, 0d0000000000000000;
	mul.f64 	%fd4127, %fd5122, %fd5757;
	mul.f64 	%fd4128, %fd5121, %fd5758;
	sub.f64 	%fd4129, %fd4127, %fd4128;
	add.f64 	%fd4130, %fd4129, 0d0000000000000000;
	mul.f64 	%fd4131, %fd5120, %fd5758;
	mul.f64 	%fd4132, %fd5122, %fd5756;
	sub.f64 	%fd4133, %fd4131, %fd4132;
	add.f64 	%fd4134, %fd4133, 0d0000000000000000;
	mul.f64 	%fd4135, %fd5121, %fd5756;
	mul.f64 	%fd4136, %fd5120, %fd5757;
	sub.f64 	%fd4137, %fd4135, %fd4136;
	add.f64 	%fd4138, %fd4137, 0d0000000000000000;
	add.f64 	%fd2055, %fd5743, %fd4130;
	add.f64 	%fd2056, %fd5744, %fd4134;
	add.f64 	%fd2057, %fd5745, %fd4138;
	sub.f64 	%fd4139, %fd5749, %fd4130;
	sub.f64 	%fd4140, %fd5750, %fd4134;
	sub.f64 	%fd4141, %fd5751, %fd4138;
	add.f64 	%fd2058, %fd5746, %fd4124;
	add.f64 	%fd2059, %fd5747, %fd4125;
	add.f64 	%fd2060, %fd5748, %fd4126;
	sub.f64 	%fd2061, %fd4139, %fd4124;
	sub.f64 	%fd2062, %fd4140, %fd4125;
	sub.f64 	%fd2063, %fd4141, %fd4126;
	setp.eq.s64 	%p410, %rd74, 0;
	@%p410 bra 	$L__BB35_261;

	cvt.s64.s32 	%rd128, %r704;
	mul.lo.s64 	%rd129, %rd128, %rd36;
	add.s64 	%rd125, %rd74, %rd129;
	// begin inline asm
	{ atom.add.f64 %fd4142,[%rd125],%fd2055; }

	// end inline asm
	add.s64 	%rd126, %rd125, 8;
	// begin inline asm
	{ atom.add.f64 %fd4144,[%rd126],%fd2056; }

	// end inline asm
	add.s64 	%rd127, %rd125, 16;
	// begin inline asm
	{ atom.add.f64 %fd4146,[%rd127],%fd2057; }

	// end inline asm
	bra.uni 	$L__BB35_263;

$L__BB35_261:
	setp.eq.s64 	%p411, %rd55, 0;
	@%p411 bra 	$L__BB35_263;

	cvt.s64.s32 	%rd133, %r704;
	mul.lo.s64 	%rd134, %rd133, %rd33;
	add.s64 	%rd130, %rd55, %rd134;
	// begin inline asm
	{ atom.add.f64 %fd4148,[%rd130],%fd2055; }

	// end inline asm
	add.s64 	%rd131, %rd130, 8;
	// begin inline asm
	{ atom.add.f64 %fd4150,[%rd131],%fd2056; }

	// end inline asm
	add.s64 	%rd132, %rd130, 16;
	// begin inline asm
	{ atom.add.f64 %fd4152,[%rd132],%fd2057; }

	// end inline asm

$L__BB35_263:
	@%p410 bra 	$L__BB35_265;

	cvt.s64.s32 	%rd138, %r703;
	mul.lo.s64 	%rd139, %rd138, %rd36;
	add.s64 	%rd135, %rd74, %rd139;
	// begin inline asm
	{ atom.add.f64 %fd4154,[%rd135],%fd2058; }

	// end inline asm
	add.s64 	%rd136, %rd135, 8;
	// begin inline asm
	{ atom.add.f64 %fd4156,[%rd136],%fd2059; }

	// end inline asm
	add.s64 	%rd137, %rd135, 16;
	// begin inline asm
	{ atom.add.f64 %fd4158,[%rd137],%fd2060; }

	// end inline asm
	bra.uni 	$L__BB35_267;

$L__BB35_265:
	setp.eq.s64 	%p413, %rd55, 0;
	@%p413 bra 	$L__BB35_267;

	cvt.s64.s32 	%rd143, %r703;
	mul.lo.s64 	%rd144, %rd143, %rd33;
	add.s64 	%rd140, %rd55, %rd144;
	// begin inline asm
	{ atom.add.f64 %fd4160,[%rd140],%fd2058; }

	// end inline asm
	add.s64 	%rd141, %rd140, 8;
	// begin inline asm
	{ atom.add.f64 %fd4162,[%rd141],%fd2059; }

	// end inline asm
	add.s64 	%rd142, %rd140, 16;
	// begin inline asm
	{ atom.add.f64 %fd4164,[%rd142],%fd2060; }

	// end inline asm

$L__BB35_267:
	@%p410 bra 	$L__BB35_269;

	cvt.s64.s32 	%rd148, %r702;
	mul.lo.s64 	%rd149, %rd148, %rd36;
	add.s64 	%rd145, %rd74, %rd149;
	// begin inline asm
	{ atom.add.f64 %fd4166,[%rd145],%fd2061; }

	// end inline asm
	add.s64 	%rd146, %rd145, 8;
	// begin inline asm
	{ atom.add.f64 %fd4168,[%rd146],%fd2062; }

	// end inline asm
	add.s64 	%rd147, %rd145, 16;
	// begin inline asm
	{ atom.add.f64 %fd4170,[%rd147],%fd2063; }

	// end inline asm
	bra.uni 	$L__BB35_271;

$L__BB35_269:
	setp.eq.s64 	%p415, %rd55, 0;
	@%p415 bra 	$L__BB35_271;

	cvt.s64.s32 	%rd153, %r702;
	mul.lo.s64 	%rd154, %rd153, %rd33;
	add.s64 	%rd150, %rd55, %rd154;
	// begin inline asm
	{ atom.add.f64 %fd4172,[%rd150],%fd2061; }

	// end inline asm
	add.s64 	%rd151, %rd150, 8;
	// begin inline asm
	{ atom.add.f64 %fd4174,[%rd151],%fd2062; }

	// end inline asm
	add.s64 	%rd152, %rd150, 16;
	// begin inline asm
	{ atom.add.f64 %fd4176,[%rd152],%fd2063; }

	// end inline asm

$L__BB35_271:
	setp.eq.s64 	%p416, %rd78, 0;
	add.f64 	%fd2064, %fd5759, 0d0000000000000000;
	@%p416 bra 	$L__BB35_273;

	cvt.s64.s32 	%rd156, %r701;
	mul.lo.s64 	%rd157, %rd156, %rd37;
	add.s64 	%rd155, %rd78, %rd157;
	// begin inline asm
	{ atom.add.f64 %fd4178,[%rd155],%fd2064; }

	// end inline asm
	bra.uni 	$L__BB35_275;

$L__BB35_273:
	setp.eq.s64 	%p417, %rd69, 0;
	@%p417 bra 	$L__BB35_275;

	cvt.s64.s32 	%rd159, %r701;
	mul.lo.s64 	%rd160, %rd159, %rd31;
	add.s64 	%rd158, %rd69, %rd160;
	// begin inline asm
	{ atom.add.f64 %fd4180,[%rd158],%fd2064; }

	// end inline asm

$L__BB35_275:
	setp.eq.s64 	%p418, %rd76, 0;
	add.f64 	%fd2065, %fd5712, 0d0000000000000000;
	@%p418 bra 	$L__BB35_277;

	cvt.s64.s32 	%rd162, %r700;
	mul.lo.s64 	%rd163, %rd162, %rd38;
	add.s64 	%rd161, %rd76, %rd163;
	// begin inline asm
	{ atom.add.f64 %fd4182,[%rd161],%fd2065; }

	// end inline asm
	bra.uni 	$L__BB35_279;

$L__BB35_277:
	setp.eq.s64 	%p419, %rd67, 0;
	@%p419 bra 	$L__BB35_279;

	cvt.s64.s32 	%rd165, %r700;
	mul.lo.s64 	%rd166, %rd165, %rd30;
	add.s64 	%rd164, %rd67, %rd166;
	// begin inline asm
	{ atom.add.f64 %fd4184,[%rd164],%fd2065; }

	// end inline asm

$L__BB35_279:
	setp.eq.s64 	%p420, %rd80, 0;
	add.f64 	%fd2066, %fd5752, 0d0000000000000000;
	add.f64 	%fd2067, %fd5753, 0d0000000000000000;
	add.f64 	%fd2068, %fd5754, 0d0000000000000000;
	@%p420 bra 	$L__BB35_281;

	cvt.s64.s32 	%rd170, %r699;
	mul.lo.s64 	%rd171, %rd170, %rd39;
	add.s64 	%rd167, %rd80, %rd171;
	// begin inline asm
	{ atom.add.f64 %fd4186,[%rd167],%fd2066; }

	// end inline asm
	add.s64 	%rd168, %rd167, 8;
	// begin inline asm
	{ atom.add.f64 %fd4188,[%rd168],%fd2067; }

	// end inline asm
	add.s64 	%rd169, %rd167, 16;
	// begin inline asm
	{ atom.add.f64 %fd4190,[%rd169],%fd2068; }

	// end inline asm
	bra.uni 	$L__BB35_283;

$L__BB35_281:
	setp.eq.s64 	%p421, %rd71, 0;
	@%p421 bra 	$L__BB35_283;

	cvt.s64.s32 	%rd175, %r699;
	mul.lo.s64 	%rd176, %rd175, %rd29;
	add.s64 	%rd172, %rd71, %rd176;
	// begin inline asm
	{ atom.add.f64 %fd4192,[%rd172],%fd2066; }

	// end inline asm
	add.s64 	%rd173, %rd172, 8;
	// begin inline asm
	{ atom.add.f64 %fd4194,[%rd173],%fd2067; }

	// end inline asm
	add.s64 	%rd174, %rd172, 16;
	// begin inline asm
	{ atom.add.f64 %fd4196,[%rd174],%fd2068; }

	// end inline asm

$L__BB35_283:
	ld.param.u64 	%rd178, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_0+24];
	mov.u32 	%r685, %ntid.x;
	mov.u32 	%r684, %nctaid.x;
	mul.wide.u32 	%rd177, %r685, %r684;
	add.s64 	%rd179, %rd179, %rd177;
	setp.lt.u64 	%p422, %rd179, %rd178;
	@%p422 bra 	$L__BB35_2;

$L__BB35_284:
	ret;

}
	// .globl	initialize_tilde_y_cuda_kernel_forward
.visible .entry initialize_tilde_y_cuda_kernel_forward(
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_forward_param_3[56],
	.param .f64 initialize_tilde_y_cuda_kernel_forward_param_4,
	.param .u32 initialize_tilde_y_cuda_kernel_forward_param_5
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<25>;
	.reg .b32 	%r<79>;
	.reg .f64 	%fd<50>;
	.reg .b64 	%rd<51>;


	ld.param.v2.u32 	{%r36, %r37}, [initialize_tilde_y_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r38, %r39}, [initialize_tilde_y_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r44, %r45}, [initialize_tilde_y_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r52, %r53}, [initialize_tilde_y_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r60, %r61}, [initialize_tilde_y_cuda_kernel_forward_param_3+32];
	ld.param.f64 	%fd1, [initialize_tilde_y_cuda_kernel_forward_param_4];
	ld.param.u32 	%r35, [initialize_tilde_y_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd30, [initialize_tilde_y_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd28, [initialize_tilde_y_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd26, [initialize_tilde_y_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd25, [initialize_tilde_y_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r7, [initialize_tilde_y_cuda_kernel_forward_param_0+16];
	mov.u32 	%r64, %ntid.x;
	mov.u32 	%r65, %ctaid.x;
	mul.wide.u32 	%rd32, %r64, %r65;
	mov.u32 	%r66, %tid.x;
	cvt.u64.u32 	%rd33, %r66;
	add.s64 	%rd47, %rd32, %rd33;
	setp.ge.u64 	%p1, %rd47, %rd25;
	@%p1 bra 	$L__BB36_19;

	cvta.to.global.u64 	%rd3, %rd28;
	cvta.to.global.u64 	%rd4, %rd26;
	cvta.to.global.u64 	%rd5, %rd30;
	cvt.s64.s32 	%rd6, %r39;
	cvt.s64.s32 	%rd7, %r38;
	cvt.s64.s32 	%rd8, %r37;
	cvt.s64.s32 	%rd9, %r52;
	cvt.s64.s32 	%rd10, %r44;
	cvt.s64.s32 	%rd11, %r60;

$L__BB36_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd48, %rd47;
	@%p2 bra 	$L__BB36_6;

	or.b64  	%rd34, %rd47, %rd6;
	and.b64  	%rd35, %rd34, -4294967296;
	setp.eq.s64 	%p3, %rd35, 0;
	@%p3 bra 	$L__BB36_5;

	div.u64 	%rd48, %rd47, %rd6;
	bra.uni 	$L__BB36_6;

$L__BB36_5:
	cvt.u32.u64 	%r67, %rd6;
	cvt.u32.u64 	%r68, %rd47;
	div.u32 	%r69, %r68, %r67;
	cvt.u64.u32 	%rd48, %r69;

$L__BB36_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB36_10;

	or.b64  	%rd36, %rd48, %rd7;
	and.b64  	%rd37, %rd36, -4294967296;
	setp.eq.s64 	%p5, %rd37, 0;
	@%p5 bra 	$L__BB36_9;

	div.u64 	%rd48, %rd48, %rd7;
	bra.uni 	$L__BB36_10;

$L__BB36_9:
	cvt.u32.u64 	%r70, %rd7;
	cvt.u32.u64 	%r71, %rd48;
	div.u32 	%r72, %r71, %r70;
	cvt.u64.u32 	%rd48, %r72;

$L__BB36_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB36_14;

	or.b64  	%rd38, %rd48, %rd8;
	and.b64  	%rd39, %rd38, -4294967296;
	setp.eq.s64 	%p7, %rd39, 0;
	@%p7 bra 	$L__BB36_13;

	div.u64 	%rd48, %rd48, %rd8;
	bra.uni 	$L__BB36_14;

$L__BB36_13:
	cvt.u32.u64 	%r73, %rd8;
	cvt.u32.u64 	%r74, %rd48;
	div.u32 	%r75, %r74, %r73;
	cvt.u64.u32 	%rd48, %r75;

$L__BB36_14:
	cvt.u32.u64 	%r76, %rd48;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r76, 0, %p8;
	cvt.s64.s32 	%rd40, %r2;
	mul.lo.s64 	%rd41, %rd40, %rd9;
	add.s64 	%rd22, %rd3, %rd41;
	mul.lo.s64 	%rd42, %rd40, %rd10;
	add.s64 	%rd23, %rd4, %rd42;
	setp.eq.s32 	%p9, %r35, 0;
	@%p9 bra 	$L__BB36_17;

	setp.ne.s32 	%p10, %r35, 1;
	@%p10 bra 	$L__BB36_18;

	mul.lo.s64 	%rd44, %rd40, %rd11;
	add.s64 	%rd45, %rd5, %rd44;
	ld.global.f64 	%fd2, [%rd45];
	ld.global.f64 	%fd3, [%rd45+8];
	ld.global.f64 	%fd4, [%rd45+16];
	ld.global.f64 	%fd5, [%rd45+24];
	ld.global.f64 	%fd6, [%rd45+32];
	ld.global.f64 	%fd7, [%rd45+40];
	ld.global.f64 	%fd8, [%rd45+48];
	ld.global.f64 	%fd9, [%rd45+56];
	ld.global.f64 	%fd10, [%rd45+64];
	ld.global.f64 	%fd11, [%rd45+72];
	ld.global.f64 	%fd12, [%rd45+80];
	ld.global.f64 	%fd13, [%rd45+88];
	ld.global.f64 	%fd14, [%rd22];
	fma.rn.f64 	%fd15, %fd2, %fd1, %fd14;
	ld.global.f64 	%fd16, [%rd22+8];
	fma.rn.f64 	%fd17, %fd3, %fd1, %fd16;
	ld.global.f64 	%fd18, [%rd22+16];
	fma.rn.f64 	%fd19, %fd4, %fd1, %fd18;
	ld.global.f64 	%fd20, [%rd22+24];
	fma.rn.f64 	%fd21, %fd5, %fd1, %fd20;
	ld.global.f64 	%fd22, [%rd22+32];
	fma.rn.f64 	%fd23, %fd6, %fd1, %fd22;
	ld.global.f64 	%fd24, [%rd22+40];
	fma.rn.f64 	%fd25, %fd7, %fd1, %fd24;
	ld.global.f64 	%fd26, [%rd22+48];
	fma.rn.f64 	%fd27, %fd8, %fd1, %fd26;
	ld.global.f64 	%fd28, [%rd22+56];
	fma.rn.f64 	%fd29, %fd9, %fd1, %fd28;
	ld.global.f64 	%fd30, [%rd22+64];
	fma.rn.f64 	%fd31, %fd10, %fd1, %fd30;
	ld.global.f64 	%fd32, [%rd22+72];
	fma.rn.f64 	%fd33, %fd11, %fd1, %fd32;
	ld.global.f64 	%fd34, [%rd22+80];
	fma.rn.f64 	%fd35, %fd12, %fd1, %fd34;
	ld.global.f64 	%fd36, [%rd22+88];
	fma.rn.f64 	%fd37, %fd13, %fd1, %fd36;
	st.global.f64 	[%rd23], %fd15;
	st.global.f64 	[%rd23+8], %fd17;
	st.global.f64 	[%rd23+16], %fd19;
	st.global.f64 	[%rd23+24], %fd21;
	st.global.f64 	[%rd23+32], %fd23;
	st.global.f64 	[%rd23+40], %fd25;
	st.global.f64 	[%rd23+48], %fd27;
	st.global.f64 	[%rd23+56], %fd29;
	st.global.f64 	[%rd23+64], %fd31;
	st.global.f64 	[%rd23+72], %fd33;
	st.global.f64 	[%rd23+80], %fd35;
	st.global.f64 	[%rd23+88], %fd37;
	bra.uni 	$L__BB36_18;

$L__BB36_17:
	ld.global.f64 	%fd38, [%rd22];
	ld.global.f64 	%fd39, [%rd22+8];
	ld.global.f64 	%fd40, [%rd22+16];
	ld.global.f64 	%fd41, [%rd22+24];
	ld.global.f64 	%fd42, [%rd22+32];
	ld.global.f64 	%fd43, [%rd22+40];
	ld.global.f64 	%fd44, [%rd22+48];
	ld.global.f64 	%fd45, [%rd22+56];
	ld.global.f64 	%fd46, [%rd22+64];
	ld.global.f64 	%fd47, [%rd22+72];
	ld.global.f64 	%fd48, [%rd22+80];
	ld.global.f64 	%fd49, [%rd22+88];
	st.global.f64 	[%rd23], %fd38;
	st.global.f64 	[%rd23+8], %fd39;
	st.global.f64 	[%rd23+16], %fd40;
	st.global.f64 	[%rd23+24], %fd41;
	st.global.f64 	[%rd23+32], %fd42;
	st.global.f64 	[%rd23+40], %fd43;
	st.global.f64 	[%rd23+48], %fd44;
	st.global.f64 	[%rd23+56], %fd45;
	st.global.f64 	[%rd23+64], %fd46;
	st.global.f64 	[%rd23+72], %fd47;
	st.global.f64 	[%rd23+80], %fd48;
	st.global.f64 	[%rd23+88], %fd49;

$L__BB36_18:
	mov.u32 	%r78, %nctaid.x;
	mul.wide.u32 	%rd46, %r64, %r78;
	add.s64 	%rd47, %rd47, %rd46;
	setp.lt.u64 	%p11, %rd47, %rd25;
	@%p11 bra 	$L__BB36_2;

$L__BB36_19:
	ret;

}
	// .globl	initialize_tilde_y_cuda_kernel_backward
.visible .entry initialize_tilde_y_cuda_kernel_backward(
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_backward_param_3[56],
	.param .f64 initialize_tilde_y_cuda_kernel_backward_param_4,
	.param .u32 initialize_tilde_y_cuda_kernel_backward_param_5,
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_backward_param_8[56],
	.param .f64 initialize_tilde_y_cuda_kernel_backward_param_9,
	.param .u32 initialize_tilde_y_cuda_kernel_backward_param_10
)
{
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<55>;
	.reg .b32 	%r<130>;
	.reg .f64 	%fd<363>;
	.reg .b64 	%rd<151>;


	ld.param.v2.u32 	{%r63, %r64}, [initialize_tilde_y_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r65, %r66}, [initialize_tilde_y_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r71, %r72}, [initialize_tilde_y_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r79, %r80}, [initialize_tilde_y_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r87, %r88}, [initialize_tilde_y_cuda_kernel_backward_param_3+32];
	ld.param.u32 	%r35, [initialize_tilde_y_cuda_kernel_backward_param_5];
	ld.param.v2.u32 	{%r95, %r96}, [initialize_tilde_y_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r103, %r104}, [initialize_tilde_y_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r111, %r112}, [initialize_tilde_y_cuda_kernel_backward_param_8+32];
	ld.param.u64 	%rd46, [initialize_tilde_y_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd44, [initialize_tilde_y_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd42, [initialize_tilde_y_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd41, [initialize_tilde_y_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd39, [initialize_tilde_y_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd37, [initialize_tilde_y_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd35, [initialize_tilde_y_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [initialize_tilde_y_cuda_kernel_backward_param_0+16];
	mov.u32 	%r115, %ntid.x;
	cvt.u64.u32 	%rd1, %r115;
	mov.u32 	%r116, %ctaid.x;
	mul.wide.u32 	%rd48, %r115, %r116;
	mov.u32 	%r117, %tid.x;
	cvt.u64.u32 	%rd49, %r117;
	add.s64 	%rd147, %rd48, %rd49;
	setp.ge.u64 	%p2, %rd147, %rd35;
	@%p2 bra 	$L__BB37_37;

	cvt.s64.s32 	%rd12, %r66;
	cvt.s64.s32 	%rd13, %r65;
	cvt.s64.s32 	%rd14, %r64;
	setp.ne.s32 	%p1, %r35, 0;
	cvt.s64.s32 	%rd15, %r79;
	cvt.s64.s32 	%rd16, %r87;
	cvt.s64.s32 	%rd17, %r95;
	mov.u32 	%r118, %nctaid.x;
	cvt.u64.u32 	%rd50, %r118;
	mul.lo.s64 	%rd18, %rd1, %rd50;
	cvt.s64.s32 	%rd19, %r71;
	cvt.s64.s32 	%rd20, %r103;
	cvt.s64.s32 	%rd21, %r111;
	not.pred 	%p13, %p1;

$L__BB37_2:
	setp.lt.s32 	%p3, %r7, 4;
	mov.u64 	%rd148, %rd147;
	@%p3 bra 	$L__BB37_6;

	or.b64  	%rd51, %rd147, %rd12;
	and.b64  	%rd52, %rd51, -4294967296;
	setp.eq.s64 	%p4, %rd52, 0;
	@%p4 bra 	$L__BB37_5;

	div.u64 	%rd148, %rd147, %rd12;
	bra.uni 	$L__BB37_6;

$L__BB37_5:
	cvt.u32.u64 	%r119, %rd12;
	cvt.u32.u64 	%r120, %rd147;
	div.u32 	%r121, %r120, %r119;
	cvt.u64.u32 	%rd148, %r121;

$L__BB37_6:
	setp.lt.s32 	%p5, %r7, 3;
	@%p5 bra 	$L__BB37_10;

	or.b64  	%rd53, %rd148, %rd13;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p6, %rd54, 0;
	@%p6 bra 	$L__BB37_9;

	div.u64 	%rd148, %rd148, %rd13;
	bra.uni 	$L__BB37_10;

$L__BB37_9:
	cvt.u32.u64 	%r122, %rd13;
	cvt.u32.u64 	%r123, %rd148;
	div.u32 	%r124, %r123, %r122;
	cvt.u64.u32 	%rd148, %r124;

$L__BB37_10:
	setp.lt.s32 	%p7, %r7, 2;
	@%p7 bra 	$L__BB37_14;

	or.b64  	%rd55, %rd148, %rd14;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p8, %rd56, 0;
	@%p8 bra 	$L__BB37_13;

	div.u64 	%rd148, %rd148, %rd14;
	bra.uni 	$L__BB37_14;

$L__BB37_13:
	cvt.u32.u64 	%r125, %rd14;
	cvt.u32.u64 	%r126, %rd148;
	div.u32 	%r127, %r126, %r125;
	cvt.u64.u32 	%rd148, %r127;

$L__BB37_14:
	cvta.to.global.u64 	%rd146, %rd37;
	cvta.to.global.u64 	%rd145, %rd42;
	ld.param.u32 	%r129, [initialize_tilde_y_cuda_kernel_backward_param_5];
	cvt.u32.u64 	%r128, %rd148;
	setp.gt.s32 	%p9, %r7, 0;
	selp.b32 	%r2, %r128, 0, %p9;
	setp.eq.s32 	%p10, %r129, 1;
	selp.u16 	%rs52, 1, 0, %p10;
	setp.eq.s32 	%p11, %r129, 0;
	selp.b16 	%rs54, %rs54, %rs52, %p11;
	and.b16  	%rs53, %rs54, 255;
	setp.eq.s16 	%p12, %rs53, 0;
	cvt.s64.s32 	%rd57, %r2;
	mul.lo.s64 	%rd58, %rd57, %rd17;
	add.s64 	%rd32, %rd145, %rd58;
	mul.lo.s64 	%rd59, %rd57, %rd19;
	add.s64 	%rd33, %rd146, %rd59;
	or.pred  	%p14, %p12, %p13;
	@%p14 bra 	$L__BB37_27;

	setp.eq.s64 	%p15, %rd42, 0;
	@%p15 bra 	$L__BB37_17;

	ld.global.f64 	%fd98, [%rd32];
	add.f64 	%fd350, %fd98, 0d0000000000000000;
	ld.global.f64 	%fd99, [%rd32+8];
	add.f64 	%fd349, %fd99, 0d0000000000000000;
	ld.global.f64 	%fd100, [%rd32+16];
	add.f64 	%fd348, %fd100, 0d0000000000000000;
	ld.global.f64 	%fd101, [%rd32+24];
	add.f64 	%fd347, %fd101, 0d0000000000000000;
	ld.global.f64 	%fd102, [%rd32+32];
	add.f64 	%fd346, %fd102, 0d0000000000000000;
	ld.global.f64 	%fd103, [%rd32+40];
	add.f64 	%fd345, %fd103, 0d0000000000000000;
	ld.global.f64 	%fd104, [%rd32+48];
	add.f64 	%fd344, %fd104, 0d0000000000000000;
	ld.global.f64 	%fd105, [%rd32+56];
	add.f64 	%fd343, %fd105, 0d0000000000000000;
	ld.global.f64 	%fd106, [%rd32+64];
	add.f64 	%fd342, %fd106, 0d0000000000000000;
	ld.global.f64 	%fd107, [%rd32+72];
	add.f64 	%fd341, %fd107, 0d0000000000000000;
	ld.global.f64 	%fd108, [%rd32+80];
	add.f64 	%fd340, %fd108, 0d0000000000000000;
	ld.global.f64 	%fd109, [%rd32+88];
	add.f64 	%fd339, %fd109, 0d0000000000000000;
	bra.uni 	$L__BB37_19;

$L__BB37_17:
	setp.eq.s64 	%p16, %rd37, 0;
	mov.f64 	%fd339, 0d0000000000000000;
	mov.f64 	%fd340, %fd339;
	mov.f64 	%fd341, %fd339;
	mov.f64 	%fd342, %fd339;
	mov.f64 	%fd343, %fd339;
	mov.f64 	%fd344, %fd339;
	mov.f64 	%fd345, %fd339;
	mov.f64 	%fd346, %fd339;
	mov.f64 	%fd347, %fd339;
	mov.f64 	%fd348, %fd339;
	mov.f64 	%fd349, %fd339;
	mov.f64 	%fd350, %fd339;
	@%p16 bra 	$L__BB37_19;

	ld.global.f64 	%fd122, [%rd33];
	add.f64 	%fd350, %fd122, 0d0000000000000000;
	ld.global.f64 	%fd123, [%rd33+8];
	add.f64 	%fd349, %fd123, 0d0000000000000000;
	ld.global.f64 	%fd124, [%rd33+16];
	add.f64 	%fd348, %fd124, 0d0000000000000000;
	ld.global.f64 	%fd125, [%rd33+24];
	add.f64 	%fd347, %fd125, 0d0000000000000000;
	ld.global.f64 	%fd126, [%rd33+32];
	add.f64 	%fd346, %fd126, 0d0000000000000000;
	ld.global.f64 	%fd127, [%rd33+40];
	add.f64 	%fd345, %fd127, 0d0000000000000000;
	ld.global.f64 	%fd128, [%rd33+48];
	add.f64 	%fd344, %fd128, 0d0000000000000000;
	ld.global.f64 	%fd129, [%rd33+56];
	add.f64 	%fd343, %fd129, 0d0000000000000000;
	ld.global.f64 	%fd130, [%rd33+64];
	add.f64 	%fd342, %fd130, 0d0000000000000000;
	ld.global.f64 	%fd131, [%rd33+72];
	add.f64 	%fd341, %fd131, 0d0000000000000000;
	ld.global.f64 	%fd132, [%rd33+80];
	add.f64 	%fd340, %fd132, 0d0000000000000000;
	ld.global.f64 	%fd133, [%rd33+88];
	add.f64 	%fd339, %fd133, 0d0000000000000000;

$L__BB37_19:
	ld.param.f64 	%fd326, [initialize_tilde_y_cuda_kernel_backward_param_4];
	add.f64 	%fd37, %fd350, 0d0000000000000000;
	fma.rn.f64 	%fd38, %fd37, %fd326, 0d0000000000000000;
	add.f64 	%fd39, %fd349, 0d0000000000000000;
	fma.rn.f64 	%fd40, %fd39, %fd326, 0d0000000000000000;
	add.f64 	%fd41, %fd348, 0d0000000000000000;
	fma.rn.f64 	%fd42, %fd41, %fd326, 0d0000000000000000;
	add.f64 	%fd43, %fd347, 0d0000000000000000;
	fma.rn.f64 	%fd44, %fd43, %fd326, 0d0000000000000000;
	add.f64 	%fd45, %fd346, 0d0000000000000000;
	fma.rn.f64 	%fd46, %fd45, %fd326, 0d0000000000000000;
	add.f64 	%fd47, %fd345, 0d0000000000000000;
	fma.rn.f64 	%fd48, %fd47, %fd326, 0d0000000000000000;
	add.f64 	%fd49, %fd344, 0d0000000000000000;
	fma.rn.f64 	%fd50, %fd49, %fd326, 0d0000000000000000;
	add.f64 	%fd51, %fd343, 0d0000000000000000;
	fma.rn.f64 	%fd52, %fd51, %fd326, 0d0000000000000000;
	add.f64 	%fd53, %fd342, 0d0000000000000000;
	fma.rn.f64 	%fd54, %fd53, %fd326, 0d0000000000000000;
	add.f64 	%fd55, %fd341, 0d0000000000000000;
	fma.rn.f64 	%fd56, %fd55, %fd326, 0d0000000000000000;
	add.f64 	%fd57, %fd340, 0d0000000000000000;
	fma.rn.f64 	%fd58, %fd57, %fd326, 0d0000000000000000;
	add.f64 	%fd59, %fd339, 0d0000000000000000;
	fma.rn.f64 	%fd60, %fd59, %fd326, 0d0000000000000000;
	setp.eq.s64 	%p17, %rd46, 0;
	@%p17 bra 	$L__BB37_21;

	mul.lo.s64 	%rd73, %rd57, %rd21;
	add.s64 	%rd60, %rd46, %rd73;
	// begin inline asm
	{ atom.add.f64 %fd134,[%rd60],%fd38; }

	// end inline asm
	add.s64 	%rd61, %rd60, 8;
	// begin inline asm
	{ atom.add.f64 %fd136,[%rd61],%fd40; }

	// end inline asm
	add.s64 	%rd62, %rd60, 16;
	// begin inline asm
	{ atom.add.f64 %fd138,[%rd62],%fd42; }

	// end inline asm
	add.s64 	%rd63, %rd60, 24;
	// begin inline asm
	{ atom.add.f64 %fd140,[%rd63],%fd44; }

	// end inline asm
	add.s64 	%rd64, %rd60, 32;
	// begin inline asm
	{ atom.add.f64 %fd142,[%rd64],%fd46; }

	// end inline asm
	add.s64 	%rd65, %rd60, 40;
	// begin inline asm
	{ atom.add.f64 %fd144,[%rd65],%fd48; }

	// end inline asm
	add.s64 	%rd66, %rd60, 48;
	// begin inline asm
	{ atom.add.f64 %fd146,[%rd66],%fd50; }

	// end inline asm
	add.s64 	%rd67, %rd60, 56;
	// begin inline asm
	{ atom.add.f64 %fd148,[%rd67],%fd52; }

	// end inline asm
	add.s64 	%rd68, %rd60, 64;
	// begin inline asm
	{ atom.add.f64 %fd150,[%rd68],%fd54; }

	// end inline asm
	add.s64 	%rd69, %rd60, 72;
	// begin inline asm
	{ atom.add.f64 %fd152,[%rd69],%fd56; }

	// end inline asm
	add.s64 	%rd70, %rd60, 80;
	// begin inline asm
	{ atom.add.f64 %fd154,[%rd70],%fd58; }

	// end inline asm
	add.s64 	%rd71, %rd60, 88;
	// begin inline asm
	{ atom.add.f64 %fd156,[%rd71],%fd60; }

	// end inline asm
	bra.uni 	$L__BB37_23;

$L__BB37_21:
	setp.eq.s64 	%p18, %rd41, 0;
	@%p18 bra 	$L__BB37_23;

	mul.lo.s64 	%rd87, %rd57, %rd16;
	add.s64 	%rd74, %rd41, %rd87;
	// begin inline asm
	{ atom.add.f64 %fd158,[%rd74],%fd38; }

	// end inline asm
	add.s64 	%rd75, %rd74, 8;
	// begin inline asm
	{ atom.add.f64 %fd160,[%rd75],%fd40; }

	// end inline asm
	add.s64 	%rd76, %rd74, 16;
	// begin inline asm
	{ atom.add.f64 %fd162,[%rd76],%fd42; }

	// end inline asm
	add.s64 	%rd77, %rd74, 24;
	// begin inline asm
	{ atom.add.f64 %fd164,[%rd77],%fd44; }

	// end inline asm
	add.s64 	%rd78, %rd74, 32;
	// begin inline asm
	{ atom.add.f64 %fd166,[%rd78],%fd46; }

	// end inline asm
	add.s64 	%rd79, %rd74, 40;
	// begin inline asm
	{ atom.add.f64 %fd168,[%rd79],%fd48; }

	// end inline asm
	add.s64 	%rd80, %rd74, 48;
	// begin inline asm
	{ atom.add.f64 %fd170,[%rd80],%fd50; }

	// end inline asm
	add.s64 	%rd81, %rd74, 56;
	// begin inline asm
	{ atom.add.f64 %fd172,[%rd81],%fd52; }

	// end inline asm
	add.s64 	%rd82, %rd74, 64;
	// begin inline asm
	{ atom.add.f64 %fd174,[%rd82],%fd54; }

	// end inline asm
	add.s64 	%rd83, %rd74, 72;
	// begin inline asm
	{ atom.add.f64 %fd176,[%rd83],%fd56; }

	// end inline asm
	add.s64 	%rd84, %rd74, 80;
	// begin inline asm
	{ atom.add.f64 %fd178,[%rd84],%fd58; }

	// end inline asm
	add.s64 	%rd85, %rd74, 88;
	// begin inline asm
	{ atom.add.f64 %fd180,[%rd85],%fd60; }

	// end inline asm

$L__BB37_23:
	setp.eq.s64 	%p19, %rd44, 0;
	@%p19 bra 	$L__BB37_25;

	add.f64 	%fd325, %fd339, 0d0000000000000000;
	add.f64 	%fd324, %fd340, 0d0000000000000000;
	add.f64 	%fd323, %fd341, 0d0000000000000000;
	add.f64 	%fd322, %fd342, 0d0000000000000000;
	add.f64 	%fd321, %fd343, 0d0000000000000000;
	add.f64 	%fd320, %fd344, 0d0000000000000000;
	add.f64 	%fd319, %fd345, 0d0000000000000000;
	add.f64 	%fd318, %fd346, 0d0000000000000000;
	add.f64 	%fd317, %fd347, 0d0000000000000000;
	add.f64 	%fd316, %fd348, 0d0000000000000000;
	add.f64 	%fd315, %fd349, 0d0000000000000000;
	add.f64 	%fd314, %fd350, 0d0000000000000000;
	mul.lo.s64 	%rd101, %rd57, %rd20;
	add.s64 	%rd88, %rd44, %rd101;
	// begin inline asm
	{ atom.add.f64 %fd182,[%rd88],%fd314; }

	// end inline asm
	add.s64 	%rd89, %rd88, 8;
	// begin inline asm
	{ atom.add.f64 %fd184,[%rd89],%fd315; }

	// end inline asm
	add.s64 	%rd90, %rd88, 16;
	// begin inline asm
	{ atom.add.f64 %fd186,[%rd90],%fd316; }

	// end inline asm
	add.s64 	%rd91, %rd88, 24;
	// begin inline asm
	{ atom.add.f64 %fd188,[%rd91],%fd317; }

	// end inline asm
	add.s64 	%rd92, %rd88, 32;
	// begin inline asm
	{ atom.add.f64 %fd190,[%rd92],%fd318; }

	// end inline asm
	add.s64 	%rd93, %rd88, 40;
	// begin inline asm
	{ atom.add.f64 %fd192,[%rd93],%fd319; }

	// end inline asm
	add.s64 	%rd94, %rd88, 48;
	// begin inline asm
	{ atom.add.f64 %fd194,[%rd94],%fd320; }

	// end inline asm
	add.s64 	%rd95, %rd88, 56;
	// begin inline asm
	{ atom.add.f64 %fd196,[%rd95],%fd321; }

	// end inline asm
	add.s64 	%rd96, %rd88, 64;
	// begin inline asm
	{ atom.add.f64 %fd198,[%rd96],%fd322; }

	// end inline asm
	add.s64 	%rd97, %rd88, 72;
	// begin inline asm
	{ atom.add.f64 %fd200,[%rd97],%fd323; }

	// end inline asm
	add.s64 	%rd98, %rd88, 80;
	// begin inline asm
	{ atom.add.f64 %fd202,[%rd98],%fd324; }

	// end inline asm
	add.s64 	%rd99, %rd88, 88;
	// begin inline asm
	{ atom.add.f64 %fd204,[%rd99],%fd325; }

	// end inline asm
	bra.uni 	$L__BB37_27;

$L__BB37_25:
	setp.eq.s64 	%p20, %rd39, 0;
	@%p20 bra 	$L__BB37_27;

	add.f64 	%fd338, %fd339, 0d0000000000000000;
	add.f64 	%fd337, %fd340, 0d0000000000000000;
	add.f64 	%fd336, %fd341, 0d0000000000000000;
	add.f64 	%fd335, %fd342, 0d0000000000000000;
	add.f64 	%fd334, %fd343, 0d0000000000000000;
	add.f64 	%fd333, %fd344, 0d0000000000000000;
	add.f64 	%fd332, %fd345, 0d0000000000000000;
	add.f64 	%fd331, %fd346, 0d0000000000000000;
	add.f64 	%fd330, %fd347, 0d0000000000000000;
	add.f64 	%fd329, %fd348, 0d0000000000000000;
	add.f64 	%fd328, %fd349, 0d0000000000000000;
	add.f64 	%fd327, %fd350, 0d0000000000000000;
	mul.lo.s64 	%rd115, %rd57, %rd15;
	add.s64 	%rd102, %rd39, %rd115;
	// begin inline asm
	{ atom.add.f64 %fd206,[%rd102],%fd327; }

	// end inline asm
	add.s64 	%rd103, %rd102, 8;
	// begin inline asm
	{ atom.add.f64 %fd208,[%rd103],%fd328; }

	// end inline asm
	add.s64 	%rd104, %rd102, 16;
	// begin inline asm
	{ atom.add.f64 %fd210,[%rd104],%fd329; }

	// end inline asm
	add.s64 	%rd105, %rd102, 24;
	// begin inline asm
	{ atom.add.f64 %fd212,[%rd105],%fd330; }

	// end inline asm
	add.s64 	%rd106, %rd102, 32;
	// begin inline asm
	{ atom.add.f64 %fd214,[%rd106],%fd331; }

	// end inline asm
	add.s64 	%rd107, %rd102, 40;
	// begin inline asm
	{ atom.add.f64 %fd216,[%rd107],%fd332; }

	// end inline asm
	add.s64 	%rd108, %rd102, 48;
	// begin inline asm
	{ atom.add.f64 %fd218,[%rd108],%fd333; }

	// end inline asm
	add.s64 	%rd109, %rd102, 56;
	// begin inline asm
	{ atom.add.f64 %fd220,[%rd109],%fd334; }

	// end inline asm
	add.s64 	%rd110, %rd102, 64;
	// begin inline asm
	{ atom.add.f64 %fd222,[%rd110],%fd335; }

	// end inline asm
	add.s64 	%rd111, %rd102, 72;
	// begin inline asm
	{ atom.add.f64 %fd224,[%rd111],%fd336; }

	// end inline asm
	add.s64 	%rd112, %rd102, 80;
	// begin inline asm
	{ atom.add.f64 %fd226,[%rd112],%fd337; }

	// end inline asm
	add.s64 	%rd113, %rd102, 88;
	// begin inline asm
	{ atom.add.f64 %fd228,[%rd113],%fd338; }

	// end inline asm

$L__BB37_27:
	@%p1 bra 	$L__BB37_36;

	setp.eq.s64 	%p22, %rd42, 0;
	@%p22 bra 	$L__BB37_30;

	ld.global.f64 	%fd230, [%rd32];
	add.f64 	%fd362, %fd230, 0d0000000000000000;
	ld.global.f64 	%fd231, [%rd32+8];
	add.f64 	%fd361, %fd231, 0d0000000000000000;
	ld.global.f64 	%fd232, [%rd32+16];
	add.f64 	%fd360, %fd232, 0d0000000000000000;
	ld.global.f64 	%fd233, [%rd32+24];
	add.f64 	%fd359, %fd233, 0d0000000000000000;
	ld.global.f64 	%fd234, [%rd32+32];
	add.f64 	%fd358, %fd234, 0d0000000000000000;
	ld.global.f64 	%fd235, [%rd32+40];
	add.f64 	%fd357, %fd235, 0d0000000000000000;
	ld.global.f64 	%fd236, [%rd32+48];
	add.f64 	%fd356, %fd236, 0d0000000000000000;
	ld.global.f64 	%fd237, [%rd32+56];
	add.f64 	%fd355, %fd237, 0d0000000000000000;
	ld.global.f64 	%fd238, [%rd32+64];
	add.f64 	%fd354, %fd238, 0d0000000000000000;
	ld.global.f64 	%fd239, [%rd32+72];
	add.f64 	%fd353, %fd239, 0d0000000000000000;
	ld.global.f64 	%fd240, [%rd32+80];
	add.f64 	%fd352, %fd240, 0d0000000000000000;
	ld.global.f64 	%fd241, [%rd32+88];
	add.f64 	%fd351, %fd241, 0d0000000000000000;
	bra.uni 	$L__BB37_32;

$L__BB37_30:
	setp.eq.s64 	%p23, %rd37, 0;
	mov.f64 	%fd351, 0d0000000000000000;
	mov.f64 	%fd352, %fd351;
	mov.f64 	%fd353, %fd351;
	mov.f64 	%fd354, %fd351;
	mov.f64 	%fd355, %fd351;
	mov.f64 	%fd356, %fd351;
	mov.f64 	%fd357, %fd351;
	mov.f64 	%fd358, %fd351;
	mov.f64 	%fd359, %fd351;
	mov.f64 	%fd360, %fd351;
	mov.f64 	%fd361, %fd351;
	mov.f64 	%fd362, %fd351;
	@%p23 bra 	$L__BB37_32;

	ld.global.f64 	%fd254, [%rd33];
	add.f64 	%fd362, %fd254, 0d0000000000000000;
	ld.global.f64 	%fd255, [%rd33+8];
	add.f64 	%fd361, %fd255, 0d0000000000000000;
	ld.global.f64 	%fd256, [%rd33+16];
	add.f64 	%fd360, %fd256, 0d0000000000000000;
	ld.global.f64 	%fd257, [%rd33+24];
	add.f64 	%fd359, %fd257, 0d0000000000000000;
	ld.global.f64 	%fd258, [%rd33+32];
	add.f64 	%fd358, %fd258, 0d0000000000000000;
	ld.global.f64 	%fd259, [%rd33+40];
	add.f64 	%fd357, %fd259, 0d0000000000000000;
	ld.global.f64 	%fd260, [%rd33+48];
	add.f64 	%fd356, %fd260, 0d0000000000000000;
	ld.global.f64 	%fd261, [%rd33+56];
	add.f64 	%fd355, %fd261, 0d0000000000000000;
	ld.global.f64 	%fd262, [%rd33+64];
	add.f64 	%fd354, %fd262, 0d0000000000000000;
	ld.global.f64 	%fd263, [%rd33+72];
	add.f64 	%fd353, %fd263, 0d0000000000000000;
	ld.global.f64 	%fd264, [%rd33+80];
	add.f64 	%fd352, %fd264, 0d0000000000000000;
	ld.global.f64 	%fd265, [%rd33+88];
	add.f64 	%fd351, %fd265, 0d0000000000000000;

$L__BB37_32:
	setp.eq.s64 	%p24, %rd44, 0;
	@%p24 bra 	$L__BB37_34;

	mul.lo.s64 	%rd129, %rd57, %rd20;
	add.s64 	%rd116, %rd44, %rd129;
	// begin inline asm
	{ atom.add.f64 %fd266,[%rd116],%fd362; }

	// end inline asm
	add.s64 	%rd117, %rd116, 8;
	// begin inline asm
	{ atom.add.f64 %fd268,[%rd117],%fd361; }

	// end inline asm
	add.s64 	%rd118, %rd116, 16;
	// begin inline asm
	{ atom.add.f64 %fd270,[%rd118],%fd360; }

	// end inline asm
	add.s64 	%rd119, %rd116, 24;
	// begin inline asm
	{ atom.add.f64 %fd272,[%rd119],%fd359; }

	// end inline asm
	add.s64 	%rd120, %rd116, 32;
	// begin inline asm
	{ atom.add.f64 %fd274,[%rd120],%fd358; }

	// end inline asm
	add.s64 	%rd121, %rd116, 40;
	// begin inline asm
	{ atom.add.f64 %fd276,[%rd121],%fd357; }

	// end inline asm
	add.s64 	%rd122, %rd116, 48;
	// begin inline asm
	{ atom.add.f64 %fd278,[%rd122],%fd356; }

	// end inline asm
	add.s64 	%rd123, %rd116, 56;
	// begin inline asm
	{ atom.add.f64 %fd280,[%rd123],%fd355; }

	// end inline asm
	add.s64 	%rd124, %rd116, 64;
	// begin inline asm
	{ atom.add.f64 %fd282,[%rd124],%fd354; }

	// end inline asm
	add.s64 	%rd125, %rd116, 72;
	// begin inline asm
	{ atom.add.f64 %fd284,[%rd125],%fd353; }

	// end inline asm
	add.s64 	%rd126, %rd116, 80;
	// begin inline asm
	{ atom.add.f64 %fd286,[%rd126],%fd352; }

	// end inline asm
	add.s64 	%rd127, %rd116, 88;
	// begin inline asm
	{ atom.add.f64 %fd288,[%rd127],%fd351; }

	// end inline asm
	bra.uni 	$L__BB37_36;

$L__BB37_34:
	setp.eq.s64 	%p25, %rd39, 0;
	@%p25 bra 	$L__BB37_36;

	mul.lo.s64 	%rd143, %rd57, %rd15;
	add.s64 	%rd130, %rd39, %rd143;
	// begin inline asm
	{ atom.add.f64 %fd290,[%rd130],%fd362; }

	// end inline asm
	add.s64 	%rd131, %rd130, 8;
	// begin inline asm
	{ atom.add.f64 %fd292,[%rd131],%fd361; }

	// end inline asm
	add.s64 	%rd132, %rd130, 16;
	// begin inline asm
	{ atom.add.f64 %fd294,[%rd132],%fd360; }

	// end inline asm
	add.s64 	%rd133, %rd130, 24;
	// begin inline asm
	{ atom.add.f64 %fd296,[%rd133],%fd359; }

	// end inline asm
	add.s64 	%rd134, %rd130, 32;
	// begin inline asm
	{ atom.add.f64 %fd298,[%rd134],%fd358; }

	// end inline asm
	add.s64 	%rd135, %rd130, 40;
	// begin inline asm
	{ atom.add.f64 %fd300,[%rd135],%fd357; }

	// end inline asm
	add.s64 	%rd136, %rd130, 48;
	// begin inline asm
	{ atom.add.f64 %fd302,[%rd136],%fd356; }

	// end inline asm
	add.s64 	%rd137, %rd130, 56;
	// begin inline asm
	{ atom.add.f64 %fd304,[%rd137],%fd355; }

	// end inline asm
	add.s64 	%rd138, %rd130, 64;
	// begin inline asm
	{ atom.add.f64 %fd306,[%rd138],%fd354; }

	// end inline asm
	add.s64 	%rd139, %rd130, 72;
	// begin inline asm
	{ atom.add.f64 %fd308,[%rd139],%fd353; }

	// end inline asm
	add.s64 	%rd140, %rd130, 80;
	// begin inline asm
	{ atom.add.f64 %fd310,[%rd140],%fd352; }

	// end inline asm
	add.s64 	%rd141, %rd130, 88;
	// begin inline asm
	{ atom.add.f64 %fd312,[%rd141],%fd351; }

	// end inline asm

$L__BB37_36:
	ld.param.u64 	%rd144, [initialize_tilde_y_cuda_kernel_backward_param_0+24];
	add.s64 	%rd147, %rd147, %rd18;
	setp.lt.u64 	%p26, %rd147, %rd144;
	@%p26 bra 	$L__BB37_2;

$L__BB37_37:
	ret;

}
	// .globl	update_x_kernel_cuda_kernel_forward
.visible .entry update_x_kernel_cuda_kernel_forward(
	.param .align 8 .b8 update_x_kernel_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 update_x_kernel_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 update_x_kernel_cuda_kernel_forward_param_2[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<67>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<83>;


	ld.param.v2.u32 	{%r25, %r26}, [update_x_kernel_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r27, %r28}, [update_x_kernel_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r33, %r34}, [update_x_kernel_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r41, %r42}, [update_x_kernel_cuda_kernel_forward_param_2+32];
	ld.param.u64 	%rd39, [update_x_kernel_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd37, [update_x_kernel_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd36, [update_x_kernel_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [update_x_kernel_cuda_kernel_forward_param_0+16];
	mov.u32 	%r45, %ntid.x;
	cvt.u64.u32 	%rd1, %r45;
	mov.u32 	%r46, %ctaid.x;
	mul.wide.u32 	%rd41, %r45, %r46;
	mov.u32 	%r47, %tid.x;
	cvt.u64.u32 	%rd42, %r47;
	add.s64 	%rd74, %rd41, %rd42;
	setp.ge.u64 	%p1, %rd74, %rd36;
	@%p1 bra 	$L__BB38_31;

	cvta.to.global.u64 	%rd4, %rd39;
	cvta.to.global.u64 	%rd5, %rd37;
	cvt.s64.s32 	%rd6, %r28;
	cvt.s64.s32 	%rd7, %r27;
	cvt.s64.s32 	%rd8, %r26;
	cvt.s64.s32 	%rd9, %r41;
	cvt.s64.s32 	%rd10, %r33;
	mov.u32 	%r48, %nctaid.x;
	cvt.u64.u32 	%rd43, %r48;
	mul.lo.s64 	%rd11, %rd1, %rd43;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB38_18;
	bra.uni 	$L__BB38_2;

$L__BB38_18:
	cvt.u32.u64 	%r58, %rd6;
	cvt.u32.u64 	%r61, %rd7;
	cvt.u32.u64 	%r64, %rd8;

$L__BB38_19:
	or.b64  	%rd62, %rd74, %rd6;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p13, %rd63, 0;
	@%p13 bra 	$L__BB38_21;

	div.u64 	%rd81, %rd74, %rd6;
	bra.uni 	$L__BB38_22;

$L__BB38_21:
	cvt.u32.u64 	%r59, %rd74;
	div.u32 	%r60, %r59, %r58;
	cvt.u64.u32 	%rd81, %r60;

$L__BB38_22:
	setp.lt.s32 	%p14, %r6, 3;
	@%p14 bra 	$L__BB38_26;

	or.b64  	%rd64, %rd81, %rd7;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p15, %rd65, 0;
	@%p15 bra 	$L__BB38_25;

	div.u64 	%rd81, %rd81, %rd7;
	bra.uni 	$L__BB38_26;

$L__BB38_25:
	cvt.u32.u64 	%r62, %rd81;
	div.u32 	%r63, %r62, %r61;
	cvt.u64.u32 	%rd81, %r63;

$L__BB38_26:
	setp.lt.s32 	%p16, %r6, 2;
	@%p16 bra 	$L__BB38_30;

	or.b64  	%rd66, %rd81, %rd8;
	and.b64  	%rd67, %rd66, -4294967296;
	setp.eq.s64 	%p17, %rd67, 0;
	@%p17 bra 	$L__BB38_29;

	div.u64 	%rd81, %rd81, %rd8;
	bra.uni 	$L__BB38_30;

$L__BB38_29:
	cvt.u32.u64 	%r65, %rd81;
	div.u32 	%r66, %r65, %r64;
	cvt.u64.u32 	%rd81, %r66;

$L__BB38_30:
	cvt.s64.s32 	%rd68, %rd81;
	setp.gt.s32 	%p18, %r6, 0;
	selp.b64 	%rd69, %rd68, 0, %p18;
	mul.lo.s64 	%rd70, %rd69, %rd9;
	add.s64 	%rd71, %rd4, %rd70;
	ld.global.f64 	%fd7, [%rd71];
	ld.global.f64 	%fd8, [%rd71+8];
	ld.global.f64 	%fd9, [%rd71+16];
	mul.lo.s64 	%rd72, %rd69, %rd10;
	add.s64 	%rd73, %rd5, %rd72;
	st.global.f64 	[%rd73], %fd7;
	st.global.f64 	[%rd73+8], %fd8;
	st.global.f64 	[%rd73+16], %fd9;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p19, %rd74, %rd36;
	@%p19 bra 	$L__BB38_19;
	bra.uni 	$L__BB38_31;

$L__BB38_2:
	setp.gt.s32 	%p3, %r6, 2;
	@%p3 bra 	$L__BB38_9;
	bra.uni 	$L__BB38_3;

$L__BB38_9:
	cvt.u32.u64 	%r52, %rd7;
	cvt.u32.u64 	%r55, %rd8;

$L__BB38_10:
	or.b64  	%rd52, %rd74, %rd7;
	and.b64  	%rd53, %rd52, -4294967296;
	setp.eq.s64 	%p8, %rd53, 0;
	@%p8 bra 	$L__BB38_12;

	div.u64 	%rd78, %rd74, %rd7;
	bra.uni 	$L__BB38_13;

$L__BB38_12:
	cvt.u32.u64 	%r53, %rd74;
	div.u32 	%r54, %r53, %r52;
	cvt.u64.u32 	%rd78, %r54;

$L__BB38_13:
	setp.lt.s32 	%p9, %r6, 2;
	@%p9 bra 	$L__BB38_17;

	or.b64  	%rd54, %rd78, %rd8;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p10, %rd55, 0;
	@%p10 bra 	$L__BB38_16;

	div.u64 	%rd78, %rd78, %rd8;
	bra.uni 	$L__BB38_17;

$L__BB38_16:
	cvt.u32.u64 	%r56, %rd78;
	div.u32 	%r57, %r56, %r55;
	cvt.u64.u32 	%rd78, %r57;

$L__BB38_17:
	cvt.s64.s32 	%rd56, %rd78;
	setp.gt.s32 	%p11, %r6, 0;
	selp.b64 	%rd57, %rd56, 0, %p11;
	mul.lo.s64 	%rd58, %rd57, %rd9;
	add.s64 	%rd59, %rd4, %rd58;
	ld.global.f64 	%fd4, [%rd59];
	ld.global.f64 	%fd5, [%rd59+8];
	ld.global.f64 	%fd6, [%rd59+16];
	mul.lo.s64 	%rd60, %rd57, %rd10;
	add.s64 	%rd61, %rd5, %rd60;
	st.global.f64 	[%rd61], %fd4;
	st.global.f64 	[%rd61+8], %fd5;
	st.global.f64 	[%rd61+16], %fd6;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p12, %rd74, %rd36;
	@%p12 bra 	$L__BB38_10;
	bra.uni 	$L__BB38_31;

$L__BB38_3:
	cvt.u32.u64 	%r49, %rd8;

$L__BB38_4:
	setp.lt.s32 	%p4, %r6, 2;
	mov.u64 	%rd75, %rd74;
	@%p4 bra 	$L__BB38_8;

	or.b64  	%rd44, %rd74, %rd8;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p5, %rd45, 0;
	@%p5 bra 	$L__BB38_7;

	div.u64 	%rd75, %rd74, %rd8;
	bra.uni 	$L__BB38_8;

$L__BB38_7:
	cvt.u32.u64 	%r50, %rd74;
	div.u32 	%r51, %r50, %r49;
	cvt.u64.u32 	%rd75, %r51;

$L__BB38_8:
	cvt.s64.s32 	%rd46, %rd75;
	setp.gt.s32 	%p6, %r6, 0;
	selp.b64 	%rd47, %rd46, 0, %p6;
	mul.lo.s64 	%rd48, %rd47, %rd9;
	add.s64 	%rd49, %rd4, %rd48;
	ld.global.f64 	%fd1, [%rd49];
	ld.global.f64 	%fd2, [%rd49+8];
	ld.global.f64 	%fd3, [%rd49+16];
	mul.lo.s64 	%rd50, %rd47, %rd10;
	add.s64 	%rd51, %rd5, %rd50;
	st.global.f64 	[%rd51], %fd1;
	st.global.f64 	[%rd51+8], %fd2;
	st.global.f64 	[%rd51+16], %fd3;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p7, %rd74, %rd36;
	@%p7 bra 	$L__BB38_4;

$L__BB38_31:
	ret;

}
	// .globl	update_x_kernel_cuda_kernel_backward
.visible .entry update_x_kernel_cuda_kernel_backward(
	.param .align 8 .b8 update_x_kernel_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 update_x_kernel_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 update_x_kernel_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 update_x_kernel_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 update_x_kernel_cuda_kernel_backward_param_4[56]
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<94>;
	.reg .f64 	%fd<34>;
	.reg .b64 	%rd<67>;


	ld.param.v2.u32 	{%r44, %r45}, [update_x_kernel_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r46, %r47}, [update_x_kernel_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r52, %r53}, [update_x_kernel_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r60, %r61}, [update_x_kernel_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r68, %r69}, [update_x_kernel_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r76, %r77}, [update_x_kernel_cuda_kernel_backward_param_4+32];
	ld.param.u64 	%rd36, [update_x_kernel_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd34, [update_x_kernel_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd33, [update_x_kernel_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd31, [update_x_kernel_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd29, [update_x_kernel_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [update_x_kernel_cuda_kernel_backward_param_0+16];
	mov.u32 	%r80, %ntid.x;
	cvt.u64.u32 	%rd1, %r80;
	mov.u32 	%r81, %ctaid.x;
	mul.wide.u32 	%rd38, %r80, %r81;
	mov.u32 	%r82, %tid.x;
	cvt.u64.u32 	%rd39, %r82;
	add.s64 	%rd63, %rd38, %rd39;
	setp.ge.u64 	%p1, %rd63, %rd29;
	@%p1 bra 	$L__BB39_23;

	cvta.to.global.u64 	%rd8, %rd34;
	cvta.to.global.u64 	%rd9, %rd31;
	cvt.s64.s32 	%rd10, %r47;
	cvt.s64.s32 	%rd11, %r46;
	cvt.s64.s32 	%rd12, %r45;
	cvt.s64.s32 	%rd13, %r68;
	cvt.s64.s32 	%rd14, %r52;
	mov.u32 	%r83, %nctaid.x;
	cvt.u64.u32 	%rd40, %r83;
	mul.lo.s64 	%rd15, %rd1, %rd40;
	cvt.s64.s32 	%rd16, %r76;
	cvt.s64.s32 	%rd17, %r60;

$L__BB39_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd64, %rd63;
	@%p2 bra 	$L__BB39_6;

	or.b64  	%rd41, %rd63, %rd10;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p3, %rd42, 0;
	@%p3 bra 	$L__BB39_5;

	div.u64 	%rd64, %rd63, %rd10;
	bra.uni 	$L__BB39_6;

$L__BB39_5:
	cvt.u32.u64 	%r84, %rd10;
	cvt.u32.u64 	%r85, %rd63;
	div.u32 	%r86, %r85, %r84;
	cvt.u64.u32 	%rd64, %r86;

$L__BB39_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB39_10;

	or.b64  	%rd43, %rd64, %rd11;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p5, %rd44, 0;
	@%p5 bra 	$L__BB39_9;

	div.u64 	%rd64, %rd64, %rd11;
	bra.uni 	$L__BB39_10;

$L__BB39_9:
	cvt.u32.u64 	%r87, %rd11;
	cvt.u32.u64 	%r88, %rd64;
	div.u32 	%r89, %r88, %r87;
	cvt.u64.u32 	%rd64, %r89;

$L__BB39_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB39_14;

	or.b64  	%rd45, %rd64, %rd12;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p7, %rd46, 0;
	@%p7 bra 	$L__BB39_13;

	div.u64 	%rd64, %rd64, %rd12;
	bra.uni 	$L__BB39_14;

$L__BB39_13:
	cvt.u32.u64 	%r90, %rd12;
	cvt.u32.u64 	%r91, %rd64;
	div.u32 	%r92, %r91, %r90;
	cvt.u64.u32 	%rd64, %r92;

$L__BB39_14:
	cvt.u32.u64 	%r93, %rd64;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r93, 0, %p8;
	setp.eq.s64 	%p9, %rd34, 0;
	@%p9 bra 	$L__BB39_16;

	cvt.s64.s32 	%rd47, %r2;
	mul.lo.s64 	%rd48, %rd47, %rd13;
	add.s64 	%rd49, %rd8, %rd48;
	ld.global.f64 	%fd10, [%rd49];
	add.f64 	%fd33, %fd10, 0d0000000000000000;
	ld.global.f64 	%fd11, [%rd49+8];
	add.f64 	%fd32, %fd11, 0d0000000000000000;
	ld.global.f64 	%fd12, [%rd49+16];
	add.f64 	%fd31, %fd12, 0d0000000000000000;
	bra.uni 	$L__BB39_18;

$L__BB39_16:
	setp.eq.s64 	%p10, %rd31, 0;
	mov.f64 	%fd31, 0d0000000000000000;
	mov.f64 	%fd32, %fd31;
	mov.f64 	%fd33, %fd31;
	@%p10 bra 	$L__BB39_18;

	cvt.s64.s32 	%rd50, %r2;
	mul.lo.s64 	%rd51, %rd50, %rd14;
	add.s64 	%rd52, %rd9, %rd51;
	ld.global.f64 	%fd16, [%rd52];
	add.f64 	%fd33, %fd16, 0d0000000000000000;
	ld.global.f64 	%fd17, [%rd52+8];
	add.f64 	%fd32, %fd17, 0d0000000000000000;
	ld.global.f64 	%fd18, [%rd52+16];
	add.f64 	%fd31, %fd18, 0d0000000000000000;

$L__BB39_18:
	setp.eq.s64 	%p11, %rd36, 0;
	@%p11 bra 	$L__BB39_20;

	cvt.s64.s32 	%rd56, %r2;
	mul.lo.s64 	%rd57, %rd56, %rd16;
	add.s64 	%rd53, %rd36, %rd57;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd53],%fd33; }

	// end inline asm
	add.s64 	%rd54, %rd53, 8;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd54],%fd32; }

	// end inline asm
	add.s64 	%rd55, %rd53, 16;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd55],%fd31; }

	// end inline asm
	bra.uni 	$L__BB39_22;

$L__BB39_20:
	setp.eq.s64 	%p12, %rd33, 0;
	@%p12 bra 	$L__BB39_22;

	cvt.s64.s32 	%rd61, %r2;
	mul.lo.s64 	%rd62, %rd61, %rd17;
	add.s64 	%rd58, %rd33, %rd62;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd58],%fd33; }

	// end inline asm
	add.s64 	%rd59, %rd58, 8;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd59],%fd32; }

	// end inline asm
	add.s64 	%rd60, %rd58, 16;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd60],%fd31; }

	// end inline asm

$L__BB39_22:
	add.s64 	%rd63, %rd63, %rd15;
	setp.lt.u64 	%p13, %rd63, %rd29;
	@%p13 bra 	$L__BB39_2;

$L__BB39_23:
	ret;

}
	// .globl	multiply_arr_vec3d_mul_scalar_cuda_kernel_forward
.visible .entry multiply_arr_vec3d_mul_scalar_cuda_kernel_forward(
	.param .align 8 .b8 multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_1[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<50>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<73>;


	ld.param.v2.u32 	{%r16, %r17}, [multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r18, %r19}, [multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r24, %r25}, [multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_1+32];
	ld.param.u64 	%rd35, [multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd34, [multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_0+16];
	mov.u32 	%r28, %ntid.x;
	cvt.u64.u32 	%rd1, %r28;
	mov.u32 	%r29, %ctaid.x;
	mul.wide.u32 	%rd37, %r28, %r29;
	mov.u32 	%r30, %tid.x;
	cvt.u64.u32 	%rd38, %r30;
	add.s64 	%rd64, %rd37, %rd38;
	setp.ge.u64 	%p1, %rd64, %rd34;
	@%p1 bra 	$L__BB40_31;

	cvta.to.global.u64 	%rd4, %rd35;
	cvt.s64.s32 	%rd5, %r19;
	cvt.s64.s32 	%rd6, %r18;
	cvt.s64.s32 	%rd7, %r17;
	cvt.s64.s32 	%rd8, %r24;
	mov.u32 	%r31, %nctaid.x;
	cvt.u64.u32 	%rd39, %r31;
	mul.lo.s64 	%rd9, %rd1, %rd39;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB40_18;
	bra.uni 	$L__BB40_2;

$L__BB40_18:
	cvt.u32.u64 	%r41, %rd5;
	cvt.u32.u64 	%r44, %rd6;
	cvt.u32.u64 	%r47, %rd7;

$L__BB40_19:
	or.b64  	%rd54, %rd64, %rd5;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p13, %rd55, 0;
	@%p13 bra 	$L__BB40_21;

	div.u64 	%rd71, %rd64, %rd5;
	bra.uni 	$L__BB40_22;

$L__BB40_21:
	cvt.u32.u64 	%r42, %rd64;
	div.u32 	%r43, %r42, %r41;
	cvt.u64.u32 	%rd71, %r43;

$L__BB40_22:
	setp.lt.s32 	%p14, %r6, 3;
	@%p14 bra 	$L__BB40_26;

	or.b64  	%rd56, %rd71, %rd6;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p15, %rd57, 0;
	@%p15 bra 	$L__BB40_25;

	div.u64 	%rd71, %rd71, %rd6;
	bra.uni 	$L__BB40_26;

$L__BB40_25:
	cvt.u32.u64 	%r45, %rd71;
	div.u32 	%r46, %r45, %r44;
	cvt.u64.u32 	%rd71, %r46;

$L__BB40_26:
	setp.lt.s32 	%p16, %r6, 2;
	@%p16 bra 	$L__BB40_30;

	or.b64  	%rd58, %rd71, %rd7;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p17, %rd59, 0;
	@%p17 bra 	$L__BB40_29;

	div.u64 	%rd71, %rd71, %rd7;
	bra.uni 	$L__BB40_30;

$L__BB40_29:
	cvt.u32.u64 	%r48, %rd71;
	div.u32 	%r49, %r48, %r47;
	cvt.u64.u32 	%rd71, %r49;

$L__BB40_30:
	cvt.s64.s32 	%rd60, %rd71;
	setp.gt.s32 	%p18, %r6, 0;
	selp.b64 	%rd61, %rd60, 0, %p18;
	mul.lo.s64 	%rd62, %rd61, %rd8;
	add.s64 	%rd63, %rd4, %rd62;
	ld.global.f64 	%fd13, [%rd63];
	neg.f64 	%fd14, %fd13;
	ld.global.f64 	%fd15, [%rd63+8];
	neg.f64 	%fd16, %fd15;
	ld.global.f64 	%fd17, [%rd63+16];
	neg.f64 	%fd18, %fd17;
	st.global.f64 	[%rd63], %fd14;
	st.global.f64 	[%rd63+8], %fd16;
	st.global.f64 	[%rd63+16], %fd18;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p19, %rd64, %rd34;
	@%p19 bra 	$L__BB40_19;
	bra.uni 	$L__BB40_31;

$L__BB40_2:
	setp.gt.s32 	%p3, %r6, 2;
	@%p3 bra 	$L__BB40_9;
	bra.uni 	$L__BB40_3;

$L__BB40_9:
	cvt.u32.u64 	%r35, %rd6;
	cvt.u32.u64 	%r38, %rd7;

$L__BB40_10:
	or.b64  	%rd46, %rd64, %rd6;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p8, %rd47, 0;
	@%p8 bra 	$L__BB40_12;

	div.u64 	%rd68, %rd64, %rd6;
	bra.uni 	$L__BB40_13;

$L__BB40_12:
	cvt.u32.u64 	%r36, %rd64;
	div.u32 	%r37, %r36, %r35;
	cvt.u64.u32 	%rd68, %r37;

$L__BB40_13:
	setp.lt.s32 	%p9, %r6, 2;
	@%p9 bra 	$L__BB40_17;

	or.b64  	%rd48, %rd68, %rd7;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p10, %rd49, 0;
	@%p10 bra 	$L__BB40_16;

	div.u64 	%rd68, %rd68, %rd7;
	bra.uni 	$L__BB40_17;

$L__BB40_16:
	cvt.u32.u64 	%r39, %rd68;
	div.u32 	%r40, %r39, %r38;
	cvt.u64.u32 	%rd68, %r40;

$L__BB40_17:
	cvt.s64.s32 	%rd50, %rd68;
	setp.gt.s32 	%p11, %r6, 0;
	selp.b64 	%rd51, %rd50, 0, %p11;
	mul.lo.s64 	%rd52, %rd51, %rd8;
	add.s64 	%rd53, %rd4, %rd52;
	ld.global.f64 	%fd7, [%rd53];
	neg.f64 	%fd8, %fd7;
	ld.global.f64 	%fd9, [%rd53+8];
	neg.f64 	%fd10, %fd9;
	ld.global.f64 	%fd11, [%rd53+16];
	neg.f64 	%fd12, %fd11;
	st.global.f64 	[%rd53], %fd8;
	st.global.f64 	[%rd53+8], %fd10;
	st.global.f64 	[%rd53+16], %fd12;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p12, %rd64, %rd34;
	@%p12 bra 	$L__BB40_10;
	bra.uni 	$L__BB40_31;

$L__BB40_3:
	cvt.u32.u64 	%r32, %rd7;

$L__BB40_4:
	setp.lt.s32 	%p4, %r6, 2;
	mov.u64 	%rd65, %rd64;
	@%p4 bra 	$L__BB40_8;

	or.b64  	%rd40, %rd64, %rd7;
	and.b64  	%rd41, %rd40, -4294967296;
	setp.eq.s64 	%p5, %rd41, 0;
	@%p5 bra 	$L__BB40_7;

	div.u64 	%rd65, %rd64, %rd7;
	bra.uni 	$L__BB40_8;

$L__BB40_7:
	cvt.u32.u64 	%r33, %rd64;
	div.u32 	%r34, %r33, %r32;
	cvt.u64.u32 	%rd65, %r34;

$L__BB40_8:
	cvt.s64.s32 	%rd42, %rd65;
	setp.gt.s32 	%p6, %r6, 0;
	selp.b64 	%rd43, %rd42, 0, %p6;
	mul.lo.s64 	%rd44, %rd43, %rd8;
	add.s64 	%rd45, %rd4, %rd44;
	ld.global.f64 	%fd1, [%rd45];
	neg.f64 	%fd2, %fd1;
	ld.global.f64 	%fd3, [%rd45+8];
	neg.f64 	%fd4, %fd3;
	ld.global.f64 	%fd5, [%rd45+16];
	neg.f64 	%fd6, %fd5;
	st.global.f64 	[%rd45], %fd2;
	st.global.f64 	[%rd45+8], %fd4;
	st.global.f64 	[%rd45+16], %fd6;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p7, %rd64, %rd34;
	@%p7 bra 	$L__BB40_4;

$L__BB40_31:
	ret;

}
	// .globl	multiply_arr_vec3d_mul_scalar_cuda_kernel_backward
.visible .entry multiply_arr_vec3d_mul_scalar_cuda_kernel_backward(
	.param .align 8 .b8 multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_2[56]
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<60>;
	.reg .f64 	%fd<38>;
	.reg .b64 	%rd<59>;


	ld.param.v2.u32 	{%r26, %r27}, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r28, %r29}, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r34, %r35}, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r42, %r43}, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_2+32];
	ld.param.u64 	%rd26, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd25, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd23, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_0+16];
	mov.u32 	%r46, %ntid.x;
	cvt.u64.u32 	%rd1, %r46;
	mov.u32 	%r47, %ctaid.x;
	mul.wide.u32 	%rd28, %r46, %r47;
	mov.u32 	%r48, %tid.x;
	cvt.u64.u32 	%rd29, %r48;
	add.s64 	%rd55, %rd28, %rd29;
	setp.ge.u64 	%p1, %rd55, %rd23;
	@%p1 bra 	$L__BB41_23;

	cvt.s64.s32 	%rd6, %r29;
	cvt.s64.s32 	%rd7, %r28;
	cvt.s64.s32 	%rd8, %r27;
	cvt.s64.s32 	%rd9, %r42;
	cvt.s64.s32 	%rd10, %r34;
	mov.u32 	%r49, %nctaid.x;
	cvt.u64.u32 	%rd30, %r49;
	mul.lo.s64 	%rd11, %rd1, %rd30;

$L__BB41_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd56, %rd55;
	@%p2 bra 	$L__BB41_6;

	or.b64  	%rd31, %rd55, %rd6;
	and.b64  	%rd32, %rd31, -4294967296;
	setp.eq.s64 	%p3, %rd32, 0;
	@%p3 bra 	$L__BB41_5;

	div.u64 	%rd56, %rd55, %rd6;
	bra.uni 	$L__BB41_6;

$L__BB41_5:
	cvt.u32.u64 	%r50, %rd6;
	cvt.u32.u64 	%r51, %rd55;
	div.u32 	%r52, %r51, %r50;
	cvt.u64.u32 	%rd56, %r52;

$L__BB41_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB41_10;

	or.b64  	%rd33, %rd56, %rd7;
	and.b64  	%rd34, %rd33, -4294967296;
	setp.eq.s64 	%p5, %rd34, 0;
	@%p5 bra 	$L__BB41_9;

	div.u64 	%rd56, %rd56, %rd7;
	bra.uni 	$L__BB41_10;

$L__BB41_9:
	cvt.u32.u64 	%r53, %rd7;
	cvt.u32.u64 	%r54, %rd56;
	div.u32 	%r55, %r54, %r53;
	cvt.u64.u32 	%rd56, %r55;

$L__BB41_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB41_14;

	or.b64  	%rd35, %rd56, %rd8;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p7, %rd36, 0;
	@%p7 bra 	$L__BB41_13;

	div.u64 	%rd56, %rd56, %rd8;
	bra.uni 	$L__BB41_14;

$L__BB41_13:
	cvt.u32.u64 	%r56, %rd8;
	cvt.u32.u64 	%r57, %rd56;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd56, %r58;

$L__BB41_14:
	cvt.u32.u64 	%r59, %rd56;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r59, 0, %p8;
	setp.eq.s64 	%p9, %rd26, 0;
	@%p9 bra 	$L__BB41_16;

	cvta.to.global.u64 	%rd37, %rd26;
	cvt.s64.s32 	%rd38, %r2;
	mul.lo.s64 	%rd39, %rd38, %rd9;
	add.s64 	%rd40, %rd37, %rd39;
	ld.global.f64 	%fd13, [%rd40];
	add.f64 	%fd37, %fd13, 0d0000000000000000;
	ld.global.f64 	%fd14, [%rd40+8];
	add.f64 	%fd36, %fd14, 0d0000000000000000;
	ld.global.f64 	%fd15, [%rd40+16];
	add.f64 	%fd35, %fd15, 0d0000000000000000;
	bra.uni 	$L__BB41_18;

$L__BB41_16:
	setp.eq.s64 	%p10, %rd25, 0;
	mov.f64 	%fd35, 0d0000000000000000;
	mov.f64 	%fd36, %fd35;
	mov.f64 	%fd37, %fd35;
	@%p10 bra 	$L__BB41_18;

	cvta.to.global.u64 	%rd41, %rd25;
	cvt.s64.s32 	%rd42, %r2;
	mul.lo.s64 	%rd43, %rd42, %rd10;
	add.s64 	%rd44, %rd41, %rd43;
	ld.global.f64 	%fd19, [%rd44];
	add.f64 	%fd37, %fd19, 0d0000000000000000;
	ld.global.f64 	%fd20, [%rd44+8];
	add.f64 	%fd36, %fd20, 0d0000000000000000;
	ld.global.f64 	%fd21, [%rd44+16];
	add.f64 	%fd35, %fd21, 0d0000000000000000;

$L__BB41_18:
	mov.f64 	%fd22, 0d0000000000000000;
	sub.f64 	%fd10, %fd22, %fd37;
	sub.f64 	%fd11, %fd22, %fd36;
	sub.f64 	%fd12, %fd22, %fd35;
	@%p9 bra 	$L__BB41_20;

	cvt.s64.s32 	%rd48, %r2;
	mul.lo.s64 	%rd49, %rd48, %rd9;
	add.s64 	%rd45, %rd26, %rd49;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd45],%fd10; }

	// end inline asm
	add.s64 	%rd46, %rd45, 8;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd46],%fd11; }

	// end inline asm
	add.s64 	%rd47, %rd45, 16;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd47],%fd12; }

	// end inline asm
	bra.uni 	$L__BB41_22;

$L__BB41_20:
	setp.eq.s64 	%p12, %rd25, 0;
	@%p12 bra 	$L__BB41_22;

	cvt.s64.s32 	%rd53, %r2;
	mul.lo.s64 	%rd54, %rd53, %rd10;
	add.s64 	%rd50, %rd25, %rd54;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd50],%fd10; }

	// end inline asm
	add.s64 	%rd51, %rd50, 8;
	// begin inline asm
	{ atom.add.f64 %fd31,[%rd51],%fd11; }

	// end inline asm
	add.s64 	%rd52, %rd50, 16;
	// begin inline asm
	{ atom.add.f64 %fd33,[%rd52],%fd12; }

	// end inline asm

$L__BB41_22:
	add.s64 	%rd55, %rd55, %rd11;
	setp.lt.u64 	%p13, %rd55, %rd23;
	@%p13 bra 	$L__BB41_2;

$L__BB41_23:
	ret;

}
	// .globl	y_to_x_cuda_kernel_forward
.visible .entry y_to_x_cuda_kernel_forward(
	.param .align 8 .b8 y_to_x_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 y_to_x_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_forward_param_4[56]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<98>;
	.reg .f64 	%fd<63>;
	.reg .b64 	%rd<86>;


	ld.param.v2.u32 	{%r43, %r44}, [y_to_x_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r45, %r46}, [y_to_x_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r51, %r52}, [y_to_x_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r59, %r60}, [y_to_x_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r67, %r68}, [y_to_x_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r75, %r76}, [y_to_x_cuda_kernel_forward_param_4+32];
	ld.param.u64 	%rd42, [y_to_x_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd40, [y_to_x_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd38, [y_to_x_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd36, [y_to_x_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd35, [y_to_x_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [y_to_x_cuda_kernel_forward_param_0+16];
	mov.u32 	%r79, %ntid.x;
	cvt.u64.u32 	%rd1, %r79;
	mov.u32 	%r80, %ctaid.x;
	mul.wide.u32 	%rd44, %r79, %r80;
	mov.u32 	%r81, %tid.x;
	cvt.u64.u32 	%rd45, %r81;
	add.s64 	%rd79, %rd44, %rd45;
	setp.ge.u64 	%p1, %rd79, %rd35;
	@%p1 bra 	$L__BB42_25;

	cvta.to.global.u64 	%rd4, %rd42;
	cvta.to.global.u64 	%rd5, %rd40;
	cvta.to.global.u64 	%rd6, %rd38;
	cvta.to.global.u64 	%rd7, %rd36;
	cvt.s64.s32 	%rd8, %r46;
	cvt.s64.s32 	%rd9, %r45;
	cvt.s64.s32 	%rd10, %r44;
	cvt.s64.s32 	%rd11, %r67;
	cvt.s64.s32 	%rd12, %r75;
	cvt.s64.s32 	%rd13, %r59;
	cvt.s64.s32 	%rd14, %r51;
	mov.u32 	%r82, %nctaid.x;
	cvt.u64.u32 	%rd46, %r82;
	mul.lo.s64 	%rd15, %rd1, %rd46;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB42_12;
	bra.uni 	$L__BB42_2;

$L__BB42_12:
	cvt.u32.u64 	%r89, %rd8;
	cvt.u32.u64 	%r92, %rd9;
	cvt.u32.u64 	%r95, %rd10;

$L__BB42_13:
	or.b64  	%rd62, %rd79, %rd8;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p9, %rd63, 0;
	@%p9 bra 	$L__BB42_15;

	div.u64 	%rd84, %rd79, %rd8;
	bra.uni 	$L__BB42_16;

$L__BB42_15:
	cvt.u32.u64 	%r90, %rd79;
	div.u32 	%r91, %r90, %r89;
	cvt.u64.u32 	%rd84, %r91;

$L__BB42_16:
	setp.lt.s32 	%p10, %r6, 3;
	@%p10 bra 	$L__BB42_20;

	or.b64  	%rd64, %rd84, %rd9;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p11, %rd65, 0;
	@%p11 bra 	$L__BB42_19;

	div.u64 	%rd84, %rd84, %rd9;
	bra.uni 	$L__BB42_20;

$L__BB42_19:
	cvt.u32.u64 	%r93, %rd84;
	div.u32 	%r94, %r93, %r92;
	cvt.u64.u32 	%rd84, %r94;

$L__BB42_20:
	setp.lt.s32 	%p12, %r6, 2;
	@%p12 bra 	$L__BB42_24;

	or.b64  	%rd66, %rd84, %rd10;
	and.b64  	%rd67, %rd66, -4294967296;
	setp.eq.s64 	%p13, %rd67, 0;
	@%p13 bra 	$L__BB42_23;

	div.u64 	%rd84, %rd84, %rd10;
	bra.uni 	$L__BB42_24;

$L__BB42_23:
	cvt.u32.u64 	%r96, %rd84;
	div.u32 	%r97, %r96, %r95;
	cvt.u64.u32 	%rd84, %r97;

$L__BB42_24:
	cvt.s64.s32 	%rd68, %rd84;
	setp.gt.s32 	%p14, %r6, 0;
	selp.b64 	%rd69, %rd68, 0, %p14;
	mul.lo.s64 	%rd70, %rd69, %rd11;
	add.s64 	%rd71, %rd5, %rd70;
	ld.global.s32 	%rd72, [%rd71];
	mul.lo.s64 	%rd73, %rd72, %rd12;
	add.s64 	%rd74, %rd4, %rd73;
	mul.lo.s64 	%rd75, %rd69, %rd13;
	add.s64 	%rd76, %rd6, %rd75;
	ld.global.f64 	%fd32, [%rd76];
	mov.f64 	%fd33, 0d3FF0000000000000;
	sub.f64 	%fd34, %fd33, %fd32;
	ld.global.f64 	%fd35, [%rd76+8];
	sub.f64 	%fd36, %fd34, %fd35;
	ld.global.f64 	%fd37, [%rd76+16];
	sub.f64 	%fd38, %fd36, %fd37;
	ld.global.f64 	%fd39, [%rd74];
	mul.f64 	%fd40, %fd39, %fd38;
	ld.global.f64 	%fd41, [%rd74+8];
	mul.f64 	%fd42, %fd41, %fd38;
	ld.global.f64 	%fd43, [%rd74+16];
	mul.f64 	%fd44, %fd43, %fd38;
	ld.global.f64 	%fd45, [%rd74+24];
	ld.global.f64 	%fd46, [%rd74+32];
	ld.global.f64 	%fd47, [%rd74+40];
	fma.rn.f64 	%fd48, %fd45, %fd32, %fd40;
	fma.rn.f64 	%fd49, %fd46, %fd32, %fd42;
	fma.rn.f64 	%fd50, %fd47, %fd32, %fd44;
	ld.global.f64 	%fd51, [%rd74+48];
	ld.global.f64 	%fd52, [%rd74+56];
	ld.global.f64 	%fd53, [%rd74+64];
	fma.rn.f64 	%fd54, %fd51, %fd35, %fd48;
	fma.rn.f64 	%fd55, %fd52, %fd35, %fd49;
	fma.rn.f64 	%fd56, %fd53, %fd35, %fd50;
	ld.global.f64 	%fd57, [%rd74+72];
	ld.global.f64 	%fd58, [%rd74+80];
	ld.global.f64 	%fd59, [%rd74+88];
	fma.rn.f64 	%fd60, %fd57, %fd37, %fd54;
	fma.rn.f64 	%fd61, %fd58, %fd37, %fd55;
	fma.rn.f64 	%fd62, %fd59, %fd37, %fd56;
	mul.lo.s64 	%rd77, %rd69, %rd14;
	add.s64 	%rd78, %rd7, %rd77;
	st.global.f64 	[%rd78], %fd60;
	st.global.f64 	[%rd78+8], %fd61;
	st.global.f64 	[%rd78+16], %fd62;
	add.s64 	%rd79, %rd79, %rd15;
	setp.lt.u64 	%p15, %rd79, %rd35;
	@%p15 bra 	$L__BB42_13;
	bra.uni 	$L__BB42_25;

$L__BB42_2:
	cvt.u32.u64 	%r83, %rd9;
	cvt.u32.u64 	%r86, %rd10;

$L__BB42_3:
	setp.lt.s32 	%p3, %r6, 3;
	mov.u64 	%rd80, %rd79;
	@%p3 bra 	$L__BB42_7;

	or.b64  	%rd47, %rd79, %rd9;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p4, %rd48, 0;
	@%p4 bra 	$L__BB42_6;

	div.u64 	%rd80, %rd79, %rd9;
	bra.uni 	$L__BB42_7;

$L__BB42_6:
	cvt.u32.u64 	%r84, %rd79;
	div.u32 	%r85, %r84, %r83;
	cvt.u64.u32 	%rd80, %r85;

$L__BB42_7:
	setp.lt.s32 	%p5, %r6, 2;
	@%p5 bra 	$L__BB42_11;

	or.b64  	%rd49, %rd80, %rd10;
	and.b64  	%rd50, %rd49, -4294967296;
	setp.eq.s64 	%p6, %rd50, 0;
	@%p6 bra 	$L__BB42_10;

	div.u64 	%rd80, %rd80, %rd10;
	bra.uni 	$L__BB42_11;

$L__BB42_10:
	cvt.u32.u64 	%r87, %rd80;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd80, %r88;

$L__BB42_11:
	cvt.s64.s32 	%rd51, %rd80;
	setp.gt.s32 	%p7, %r6, 0;
	selp.b64 	%rd52, %rd51, 0, %p7;
	mul.lo.s64 	%rd53, %rd52, %rd11;
	add.s64 	%rd54, %rd5, %rd53;
	ld.global.s32 	%rd55, [%rd54];
	mul.lo.s64 	%rd56, %rd55, %rd12;
	add.s64 	%rd57, %rd4, %rd56;
	mul.lo.s64 	%rd58, %rd52, %rd13;
	add.s64 	%rd59, %rd6, %rd58;
	ld.global.f64 	%fd1, [%rd59];
	mov.f64 	%fd2, 0d3FF0000000000000;
	sub.f64 	%fd3, %fd2, %fd1;
	ld.global.f64 	%fd4, [%rd59+8];
	sub.f64 	%fd5, %fd3, %fd4;
	ld.global.f64 	%fd6, [%rd59+16];
	sub.f64 	%fd7, %fd5, %fd6;
	ld.global.f64 	%fd8, [%rd57];
	mul.f64 	%fd9, %fd8, %fd7;
	ld.global.f64 	%fd10, [%rd57+8];
	mul.f64 	%fd11, %fd10, %fd7;
	ld.global.f64 	%fd12, [%rd57+16];
	mul.f64 	%fd13, %fd12, %fd7;
	ld.global.f64 	%fd14, [%rd57+24];
	ld.global.f64 	%fd15, [%rd57+32];
	ld.global.f64 	%fd16, [%rd57+40];
	fma.rn.f64 	%fd17, %fd14, %fd1, %fd9;
	fma.rn.f64 	%fd18, %fd15, %fd1, %fd11;
	fma.rn.f64 	%fd19, %fd16, %fd1, %fd13;
	ld.global.f64 	%fd20, [%rd57+48];
	ld.global.f64 	%fd21, [%rd57+56];
	ld.global.f64 	%fd22, [%rd57+64];
	fma.rn.f64 	%fd23, %fd20, %fd4, %fd17;
	fma.rn.f64 	%fd24, %fd21, %fd4, %fd18;
	fma.rn.f64 	%fd25, %fd22, %fd4, %fd19;
	ld.global.f64 	%fd26, [%rd57+72];
	ld.global.f64 	%fd27, [%rd57+80];
	ld.global.f64 	%fd28, [%rd57+88];
	fma.rn.f64 	%fd29, %fd26, %fd6, %fd23;
	fma.rn.f64 	%fd30, %fd27, %fd6, %fd24;
	fma.rn.f64 	%fd31, %fd28, %fd6, %fd25;
	mul.lo.s64 	%rd60, %rd52, %rd14;
	add.s64 	%rd61, %rd7, %rd60;
	st.global.f64 	[%rd61], %fd29;
	st.global.f64 	[%rd61+8], %fd30;
	st.global.f64 	[%rd61+16], %fd31;
	add.s64 	%rd79, %rd79, %rd15;
	setp.lt.u64 	%p8, %rd79, %rd35;
	@%p8 bra 	$L__BB42_3;

$L__BB42_25:
	ret;

}
	// .globl	y_to_x_cuda_kernel_backward
.visible .entry y_to_x_cuda_kernel_backward(
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_8[56]
)
{
	.reg .pred 	%p<38>;
	.reg .b16 	%rs<57>;
	.reg .b32 	%r<143>;
	.reg .f64 	%fd<668>;
	.reg .b64 	%rd<388>;


	ld.param.v2.u32 	{%r70, %r71}, [y_to_x_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r72, %r73}, [y_to_x_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r78, %r79}, [y_to_x_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r86, %r87}, [y_to_x_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r94, %r95}, [y_to_x_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r102, %r103}, [y_to_x_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r110, %r111}, [y_to_x_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r118, %r119}, [y_to_x_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r126, %r127}, [y_to_x_cuda_kernel_backward_param_8+32];
	ld.param.u64 	%rd54, [y_to_x_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd52, [y_to_x_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd50, [y_to_x_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd49, [y_to_x_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd48, [y_to_x_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd46, [y_to_x_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd45, [y_to_x_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd44, [y_to_x_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd43, [y_to_x_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd41, [y_to_x_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [y_to_x_cuda_kernel_backward_param_0+16];
	mov.u32 	%r130, %ntid.x;
	cvt.u64.u32 	%rd1, %r130;
	mov.u32 	%r131, %ctaid.x;
	mul.wide.u32 	%rd56, %r130, %r131;
	mov.u32 	%r132, %tid.x;
	cvt.u64.u32 	%rd57, %r132;
	add.s64 	%rd384, %rd56, %rd57;
	setp.ge.u64 	%p1, %rd384, %rd41;
	@%p1 bra 	$L__BB43_71;

	cvta.to.global.u64 	%rd10, %rd50;
	cvta.to.global.u64 	%rd11, %rd48;
	cvta.to.global.u64 	%rd12, %rd46;
	cvta.to.global.u64 	%rd13, %rd44;
	cvta.to.global.u64 	%rd14, %rd43;
	cvt.s64.s32 	%rd15, %r73;
	cvt.s64.s32 	%rd16, %r72;
	cvt.s64.s32 	%rd17, %r71;
	cvt.s64.s32 	%rd18, %r94;
	cvt.s64.s32 	%rd19, %r102;
	cvt.s64.s32 	%rd20, %r86;
	cvt.s64.s32 	%rd21, %r110;
	cvt.s64.s32 	%rd22, %r78;
	cvt.s64.s32 	%rd23, %r118;
	cvt.s64.s32 	%rd24, %r126;
	mov.u32 	%r133, %nctaid.x;
	cvt.u64.u32 	%rd58, %r133;
	mul.lo.s64 	%rd25, %rd1, %rd58;

$L__BB43_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd385, %rd384;
	@%p2 bra 	$L__BB43_6;

	or.b64  	%rd59, %rd384, %rd15;
	and.b64  	%rd60, %rd59, -4294967296;
	setp.eq.s64 	%p3, %rd60, 0;
	@%p3 bra 	$L__BB43_5;

	div.u64 	%rd385, %rd384, %rd15;
	bra.uni 	$L__BB43_6;

$L__BB43_5:
	cvt.u32.u64 	%r134, %rd15;
	cvt.u32.u64 	%r135, %rd384;
	div.u32 	%r136, %r135, %r134;
	cvt.u64.u32 	%rd385, %r136;

$L__BB43_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB43_10;

	or.b64  	%rd61, %rd385, %rd16;
	and.b64  	%rd62, %rd61, -4294967296;
	setp.eq.s64 	%p5, %rd62, 0;
	@%p5 bra 	$L__BB43_9;

	div.u64 	%rd385, %rd385, %rd16;
	bra.uni 	$L__BB43_10;

$L__BB43_9:
	cvt.u32.u64 	%r137, %rd16;
	cvt.u32.u64 	%r138, %rd385;
	div.u32 	%r139, %r138, %r137;
	cvt.u64.u32 	%rd385, %r139;

$L__BB43_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB43_14;

	or.b64  	%rd63, %rd385, %rd17;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.eq.s64 	%p7, %rd64, 0;
	@%p7 bra 	$L__BB43_13;

	div.u64 	%rd385, %rd385, %rd17;
	bra.uni 	$L__BB43_14;

$L__BB43_13:
	cvt.u32.u64 	%r140, %rd17;
	cvt.u32.u64 	%r141, %rd385;
	div.u32 	%r142, %r141, %r140;
	cvt.u64.u32 	%rd385, %r142;

$L__BB43_14:
	ld.param.u64 	%rd382, [y_to_x_cuda_kernel_backward_param_5];
	cvt.s64.s32 	%rd65, %rd385;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd36, %rd65, 0, %p8;
	mul.lo.s64 	%rd66, %rd36, %rd18;
	add.s64 	%rd67, %rd12, %rd66;
	ld.global.s32 	%rd37, [%rd67];
	mul.lo.s64 	%rd38, %rd37, %rd19;
	add.s64 	%rd68, %rd11, %rd38;
	ld.global.f64 	%fd1, [%rd68];
	ld.global.f64 	%fd2, [%rd68+8];
	ld.global.f64 	%fd3, [%rd68+16];
	ld.global.f64 	%fd4, [%rd68+24];
	ld.global.f64 	%fd5, [%rd68+32];
	ld.global.f64 	%fd6, [%rd68+40];
	ld.global.f64 	%fd7, [%rd68+48];
	ld.global.f64 	%fd8, [%rd68+56];
	ld.global.f64 	%fd9, [%rd68+64];
	ld.global.f64 	%fd10, [%rd68+72];
	ld.global.f64 	%fd11, [%rd68+80];
	ld.global.f64 	%fd12, [%rd68+88];
	mul.lo.s64 	%rd39, %rd36, %rd20;
	add.s64 	%rd69, %rd13, %rd39;
	ld.global.f64 	%fd13, [%rd69];
	ld.global.f64 	%fd14, [%rd69+8];
	ld.global.f64 	%fd15, [%rd69+16];
	setp.eq.s64 	%p9, %rd382, 0;
	@%p9 bra 	$L__BB43_16;

	mul.lo.s64 	%rd70, %rd36, %rd21;
	add.s64 	%rd71, %rd10, %rd70;
	ld.global.f64 	%fd40, [%rd71];
	add.f64 	%fd667, %fd40, 0d0000000000000000;
	ld.global.f64 	%fd41, [%rd71+8];
	add.f64 	%fd666, %fd41, 0d0000000000000000;
	ld.global.f64 	%fd42, [%rd71+16];
	add.f64 	%fd665, %fd42, 0d0000000000000000;
	bra.uni 	$L__BB43_18;

$L__BB43_16:
	ld.param.u64 	%rd383, [y_to_x_cuda_kernel_backward_param_1+8];
	setp.eq.s64 	%p10, %rd383, 0;
	mov.f64 	%fd665, 0d0000000000000000;
	mov.f64 	%fd666, %fd665;
	mov.f64 	%fd667, %fd665;
	@%p10 bra 	$L__BB43_18;

	mul.lo.s64 	%rd72, %rd36, %rd22;
	add.s64 	%rd73, %rd14, %rd72;
	ld.global.f64 	%fd46, [%rd73];
	add.f64 	%fd667, %fd46, 0d0000000000000000;
	ld.global.f64 	%fd47, [%rd73+8];
	add.f64 	%fd666, %fd47, 0d0000000000000000;
	ld.global.f64 	%fd48, [%rd73+16];
	add.f64 	%fd665, %fd48, 0d0000000000000000;

$L__BB43_18:
	mov.f64 	%fd49, 0d3FF0000000000000;
	sub.f64 	%fd50, %fd49, %fd13;
	sub.f64 	%fd51, %fd50, %fd14;
	sub.f64 	%fd52, %fd51, %fd15;
	add.f64 	%fd53, %fd667, 0d0000000000000000;
	mov.f64 	%fd54, 0d0000000000000000;
	fma.rn.f64 	%fd25, %fd15, %fd53, 0d0000000000000000;
	add.f64 	%fd55, %fd666, 0d0000000000000000;
	fma.rn.f64 	%fd26, %fd15, %fd55, 0d0000000000000000;
	add.f64 	%fd56, %fd665, 0d0000000000000000;
	fma.rn.f64 	%fd27, %fd15, %fd56, 0d0000000000000000;
	mul.f64 	%fd57, %fd11, %fd55;
	fma.rn.f64 	%fd58, %fd10, %fd53, %fd57;
	fma.rn.f64 	%fd59, %fd12, %fd56, %fd58;
	fma.rn.f64 	%fd28, %fd14, %fd53, 0d0000000000000000;
	fma.rn.f64 	%fd29, %fd14, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd30, %fd14, %fd56, 0d0000000000000000;
	mul.f64 	%fd60, %fd8, %fd55;
	fma.rn.f64 	%fd61, %fd7, %fd53, %fd60;
	fma.rn.f64 	%fd62, %fd9, %fd56, %fd61;
	fma.rn.f64 	%fd31, %fd13, %fd53, 0d0000000000000000;
	fma.rn.f64 	%fd32, %fd13, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd33, %fd13, %fd56, 0d0000000000000000;
	mul.f64 	%fd63, %fd5, %fd55;
	fma.rn.f64 	%fd64, %fd4, %fd53, %fd63;
	fma.rn.f64 	%fd65, %fd6, %fd56, %fd64;
	add.f64 	%fd66, %fd59, 0d0000000000000000;
	fma.rn.f64 	%fd34, %fd52, %fd53, 0d0000000000000000;
	fma.rn.f64 	%fd35, %fd52, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd36, %fd52, %fd56, 0d0000000000000000;
	add.f64 	%fd67, %fd62, 0d0000000000000000;
	add.f64 	%fd68, %fd65, 0d0000000000000000;
	mul.f64 	%fd69, %fd2, %fd55;
	fma.rn.f64 	%fd70, %fd1, %fd53, %fd69;
	fma.rn.f64 	%fd71, %fd3, %fd56, %fd70;
	add.f64 	%fd72, %fd71, 0d0000000000000000;
	sub.f64 	%fd73, %fd54, %fd72;
	add.f64 	%fd74, %fd66, %fd73;
	add.f64 	%fd75, %fd67, %fd73;
	add.f64 	%fd76, %fd68, %fd73;
	add.f64 	%fd37, %fd76, 0d0000000000000000;
	add.f64 	%fd38, %fd75, 0d0000000000000000;
	add.f64 	%fd39, %fd74, 0d0000000000000000;
	setp.eq.s64 	%p11, %rd52, 0;
	@%p11 bra 	$L__BB43_20;

	mul.lo.s64 	%rd77, %rd36, %rd23;
	add.s64 	%rd74, %rd52, %rd77;
	// begin inline asm
	{ atom.add.f64 %fd77,[%rd74],%fd37; }

	// end inline asm
	add.s64 	%rd75, %rd74, 8;
	// begin inline asm
	{ atom.add.f64 %fd79,[%rd75],%fd38; }

	// end inline asm
	add.s64 	%rd76, %rd74, 16;
	// begin inline asm
	{ atom.add.f64 %fd81,[%rd76],%fd39; }

	// end inline asm
	bra.uni 	$L__BB43_22;

$L__BB43_20:
	setp.eq.s64 	%p12, %rd45, 0;
	@%p12 bra 	$L__BB43_22;

	add.s64 	%rd78, %rd45, %rd39;
	// begin inline asm
	{ atom.add.f64 %fd83,[%rd78],%fd37; }

	// end inline asm
	add.s64 	%rd79, %rd78, 8;
	// begin inline asm
	{ atom.add.f64 %fd85,[%rd79],%fd38; }

	// end inline asm
	add.s64 	%rd80, %rd78, 16;
	// begin inline asm
	{ atom.add.f64 %fd87,[%rd80],%fd39; }

	// end inline asm

$L__BB43_22:
	setp.eq.s64 	%p13, %rd54, 0;
	@%p13 bra 	$L__BB43_24;

	mul.lo.s64 	%rd93, %rd37, %rd24;
	add.s64 	%rd81, %rd54, %rd93;
	mov.f64 	%fd110, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd89,[%rd81],%fd110; }

	// end inline asm
	add.s64 	%rd82, %rd81, 8;
	// begin inline asm
	{ atom.add.f64 %fd91,[%rd82],%fd110; }

	// end inline asm
	add.s64 	%rd83, %rd81, 16;
	// begin inline asm
	{ atom.add.f64 %fd93,[%rd83],%fd110; }

	// end inline asm
	add.s64 	%rd84, %rd81, 24;
	// begin inline asm
	{ atom.add.f64 %fd95,[%rd84],%fd110; }

	// end inline asm
	add.s64 	%rd85, %rd81, 32;
	// begin inline asm
	{ atom.add.f64 %fd97,[%rd85],%fd110; }

	// end inline asm
	add.s64 	%rd86, %rd81, 40;
	// begin inline asm
	{ atom.add.f64 %fd99,[%rd86],%fd110; }

	// end inline asm
	add.s64 	%rd87, %rd81, 48;
	// begin inline asm
	{ atom.add.f64 %fd101,[%rd87],%fd110; }

	// end inline asm
	add.s64 	%rd88, %rd81, 56;
	// begin inline asm
	{ atom.add.f64 %fd103,[%rd88],%fd110; }

	// end inline asm
	add.s64 	%rd89, %rd81, 64;
	// begin inline asm
	{ atom.add.f64 %fd105,[%rd89],%fd110; }

	// end inline asm
	add.s64 	%rd90, %rd81, 72;
	// begin inline asm
	{ atom.add.f64 %fd107,[%rd90],%fd110; }

	// end inline asm
	add.s64 	%rd91, %rd81, 80;
	// begin inline asm
	{ atom.add.f64 %fd109,[%rd91],%fd110; }

	// end inline asm
	add.s64 	%rd92, %rd81, 88;
	// begin inline asm
	{ atom.add.f64 %fd111,[%rd92],%fd27; }

	// end inline asm
	bra.uni 	$L__BB43_26;

$L__BB43_24:
	setp.eq.s64 	%p14, %rd49, 0;
	@%p14 bra 	$L__BB43_26;

	add.s64 	%rd94, %rd49, %rd38;
	mov.f64 	%fd134, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd113,[%rd94],%fd134; }

	// end inline asm
	add.s64 	%rd95, %rd94, 8;
	// begin inline asm
	{ atom.add.f64 %fd115,[%rd95],%fd134; }

	// end inline asm
	add.s64 	%rd96, %rd94, 16;
	// begin inline asm
	{ atom.add.f64 %fd117,[%rd96],%fd134; }

	// end inline asm
	add.s64 	%rd97, %rd94, 24;
	// begin inline asm
	{ atom.add.f64 %fd119,[%rd97],%fd134; }

	// end inline asm
	add.s64 	%rd98, %rd94, 32;
	// begin inline asm
	{ atom.add.f64 %fd121,[%rd98],%fd134; }

	// end inline asm
	add.s64 	%rd99, %rd94, 40;
	// begin inline asm
	{ atom.add.f64 %fd123,[%rd99],%fd134; }

	// end inline asm
	add.s64 	%rd100, %rd94, 48;
	// begin inline asm
	{ atom.add.f64 %fd125,[%rd100],%fd134; }

	// end inline asm
	add.s64 	%rd101, %rd94, 56;
	// begin inline asm
	{ atom.add.f64 %fd127,[%rd101],%fd134; }

	// end inline asm
	add.s64 	%rd102, %rd94, 64;
	// begin inline asm
	{ atom.add.f64 %fd129,[%rd102],%fd134; }

	// end inline asm
	add.s64 	%rd103, %rd94, 72;
	// begin inline asm
	{ atom.add.f64 %fd131,[%rd103],%fd134; }

	// end inline asm
	add.s64 	%rd104, %rd94, 80;
	// begin inline asm
	{ atom.add.f64 %fd133,[%rd104],%fd134; }

	// end inline asm
	add.s64 	%rd105, %rd94, 88;
	// begin inline asm
	{ atom.add.f64 %fd135,[%rd105],%fd27; }

	// end inline asm

$L__BB43_26:
	@%p13 bra 	$L__BB43_28;

	mul.lo.s64 	%rd118, %rd37, %rd24;
	add.s64 	%rd106, %rd54, %rd118;
	mov.f64 	%fd160, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd137,[%rd106],%fd160; }

	// end inline asm
	add.s64 	%rd107, %rd106, 8;
	// begin inline asm
	{ atom.add.f64 %fd139,[%rd107],%fd160; }

	// end inline asm
	add.s64 	%rd108, %rd106, 16;
	// begin inline asm
	{ atom.add.f64 %fd141,[%rd108],%fd160; }

	// end inline asm
	add.s64 	%rd109, %rd106, 24;
	// begin inline asm
	{ atom.add.f64 %fd143,[%rd109],%fd160; }

	// end inline asm
	add.s64 	%rd110, %rd106, 32;
	// begin inline asm
	{ atom.add.f64 %fd145,[%rd110],%fd160; }

	// end inline asm
	add.s64 	%rd111, %rd106, 40;
	// begin inline asm
	{ atom.add.f64 %fd147,[%rd111],%fd160; }

	// end inline asm
	add.s64 	%rd112, %rd106, 48;
	// begin inline asm
	{ atom.add.f64 %fd149,[%rd112],%fd160; }

	// end inline asm
	add.s64 	%rd113, %rd106, 56;
	// begin inline asm
	{ atom.add.f64 %fd151,[%rd113],%fd160; }

	// end inline asm
	add.s64 	%rd114, %rd106, 64;
	// begin inline asm
	{ atom.add.f64 %fd153,[%rd114],%fd160; }

	// end inline asm
	add.s64 	%rd115, %rd106, 72;
	// begin inline asm
	{ atom.add.f64 %fd155,[%rd115],%fd160; }

	// end inline asm
	add.s64 	%rd116, %rd106, 80;
	// begin inline asm
	{ atom.add.f64 %fd157,[%rd116],%fd26; }

	// end inline asm
	add.s64 	%rd117, %rd106, 88;
	// begin inline asm
	{ atom.add.f64 %fd159,[%rd117],%fd160; }

	// end inline asm
	bra.uni 	$L__BB43_30;

$L__BB43_28:
	setp.eq.s64 	%p16, %rd49, 0;
	@%p16 bra 	$L__BB43_30;

	add.s64 	%rd119, %rd49, %rd38;
	mov.f64 	%fd184, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd161,[%rd119],%fd184; }

	// end inline asm
	add.s64 	%rd120, %rd119, 8;
	// begin inline asm
	{ atom.add.f64 %fd163,[%rd120],%fd184; }

	// end inline asm
	add.s64 	%rd121, %rd119, 16;
	// begin inline asm
	{ atom.add.f64 %fd165,[%rd121],%fd184; }

	// end inline asm
	add.s64 	%rd122, %rd119, 24;
	// begin inline asm
	{ atom.add.f64 %fd167,[%rd122],%fd184; }

	// end inline asm
	add.s64 	%rd123, %rd119, 32;
	// begin inline asm
	{ atom.add.f64 %fd169,[%rd123],%fd184; }

	// end inline asm
	add.s64 	%rd124, %rd119, 40;
	// begin inline asm
	{ atom.add.f64 %fd171,[%rd124],%fd184; }

	// end inline asm
	add.s64 	%rd125, %rd119, 48;
	// begin inline asm
	{ atom.add.f64 %fd173,[%rd125],%fd184; }

	// end inline asm
	add.s64 	%rd126, %rd119, 56;
	// begin inline asm
	{ atom.add.f64 %fd175,[%rd126],%fd184; }

	// end inline asm
	add.s64 	%rd127, %rd119, 64;
	// begin inline asm
	{ atom.add.f64 %fd177,[%rd127],%fd184; }

	// end inline asm
	add.s64 	%rd128, %rd119, 72;
	// begin inline asm
	{ atom.add.f64 %fd179,[%rd128],%fd184; }

	// end inline asm
	add.s64 	%rd129, %rd119, 80;
	// begin inline asm
	{ atom.add.f64 %fd181,[%rd129],%fd26; }

	// end inline asm
	add.s64 	%rd130, %rd119, 88;
	// begin inline asm
	{ atom.add.f64 %fd183,[%rd130],%fd184; }

	// end inline asm

$L__BB43_30:
	@%p13 bra 	$L__BB43_32;

	mul.lo.s64 	%rd143, %rd37, %rd24;
	add.s64 	%rd131, %rd54, %rd143;
	mov.f64 	%fd208, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd185,[%rd131],%fd208; }

	// end inline asm
	add.s64 	%rd132, %rd131, 8;
	// begin inline asm
	{ atom.add.f64 %fd187,[%rd132],%fd208; }

	// end inline asm
	add.s64 	%rd133, %rd131, 16;
	// begin inline asm
	{ atom.add.f64 %fd189,[%rd133],%fd208; }

	// end inline asm
	add.s64 	%rd134, %rd131, 24;
	// begin inline asm
	{ atom.add.f64 %fd191,[%rd134],%fd208; }

	// end inline asm
	add.s64 	%rd135, %rd131, 32;
	// begin inline asm
	{ atom.add.f64 %fd193,[%rd135],%fd208; }

	// end inline asm
	add.s64 	%rd136, %rd131, 40;
	// begin inline asm
	{ atom.add.f64 %fd195,[%rd136],%fd208; }

	// end inline asm
	add.s64 	%rd137, %rd131, 48;
	// begin inline asm
	{ atom.add.f64 %fd197,[%rd137],%fd208; }

	// end inline asm
	add.s64 	%rd138, %rd131, 56;
	// begin inline asm
	{ atom.add.f64 %fd199,[%rd138],%fd208; }

	// end inline asm
	add.s64 	%rd139, %rd131, 64;
	// begin inline asm
	{ atom.add.f64 %fd201,[%rd139],%fd208; }

	// end inline asm
	add.s64 	%rd140, %rd131, 72;
	// begin inline asm
	{ atom.add.f64 %fd203,[%rd140],%fd25; }

	// end inline asm
	add.s64 	%rd141, %rd131, 80;
	// begin inline asm
	{ atom.add.f64 %fd205,[%rd141],%fd208; }

	// end inline asm
	add.s64 	%rd142, %rd131, 88;
	// begin inline asm
	{ atom.add.f64 %fd207,[%rd142],%fd208; }

	// end inline asm
	bra.uni 	$L__BB43_34;

$L__BB43_32:
	setp.eq.s64 	%p18, %rd49, 0;
	@%p18 bra 	$L__BB43_34;

	add.s64 	%rd144, %rd49, %rd38;
	mov.f64 	%fd232, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd209,[%rd144],%fd232; }

	// end inline asm
	add.s64 	%rd145, %rd144, 8;
	// begin inline asm
	{ atom.add.f64 %fd211,[%rd145],%fd232; }

	// end inline asm
	add.s64 	%rd146, %rd144, 16;
	// begin inline asm
	{ atom.add.f64 %fd213,[%rd146],%fd232; }

	// end inline asm
	add.s64 	%rd147, %rd144, 24;
	// begin inline asm
	{ atom.add.f64 %fd215,[%rd147],%fd232; }

	// end inline asm
	add.s64 	%rd148, %rd144, 32;
	// begin inline asm
	{ atom.add.f64 %fd217,[%rd148],%fd232; }

	// end inline asm
	add.s64 	%rd149, %rd144, 40;
	// begin inline asm
	{ atom.add.f64 %fd219,[%rd149],%fd232; }

	// end inline asm
	add.s64 	%rd150, %rd144, 48;
	// begin inline asm
	{ atom.add.f64 %fd221,[%rd150],%fd232; }

	// end inline asm
	add.s64 	%rd151, %rd144, 56;
	// begin inline asm
	{ atom.add.f64 %fd223,[%rd151],%fd232; }

	// end inline asm
	add.s64 	%rd152, %rd144, 64;
	// begin inline asm
	{ atom.add.f64 %fd225,[%rd152],%fd232; }

	// end inline asm
	add.s64 	%rd153, %rd144, 72;
	// begin inline asm
	{ atom.add.f64 %fd227,[%rd153],%fd25; }

	// end inline asm
	add.s64 	%rd154, %rd144, 80;
	// begin inline asm
	{ atom.add.f64 %fd229,[%rd154],%fd232; }

	// end inline asm
	add.s64 	%rd155, %rd144, 88;
	// begin inline asm
	{ atom.add.f64 %fd231,[%rd155],%fd232; }

	// end inline asm

$L__BB43_34:
	@%p13 bra 	$L__BB43_36;

	mul.lo.s64 	%rd168, %rd37, %rd24;
	add.s64 	%rd156, %rd54, %rd168;
	mov.f64 	%fd256, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd233,[%rd156],%fd256; }

	// end inline asm
	add.s64 	%rd157, %rd156, 8;
	// begin inline asm
	{ atom.add.f64 %fd235,[%rd157],%fd256; }

	// end inline asm
	add.s64 	%rd158, %rd156, 16;
	// begin inline asm
	{ atom.add.f64 %fd237,[%rd158],%fd256; }

	// end inline asm
	add.s64 	%rd159, %rd156, 24;
	// begin inline asm
	{ atom.add.f64 %fd239,[%rd159],%fd256; }

	// end inline asm
	add.s64 	%rd160, %rd156, 32;
	// begin inline asm
	{ atom.add.f64 %fd241,[%rd160],%fd256; }

	// end inline asm
	add.s64 	%rd161, %rd156, 40;
	// begin inline asm
	{ atom.add.f64 %fd243,[%rd161],%fd256; }

	// end inline asm
	add.s64 	%rd162, %rd156, 48;
	// begin inline asm
	{ atom.add.f64 %fd245,[%rd162],%fd256; }

	// end inline asm
	add.s64 	%rd163, %rd156, 56;
	// begin inline asm
	{ atom.add.f64 %fd247,[%rd163],%fd256; }

	// end inline asm
	add.s64 	%rd164, %rd156, 64;
	// begin inline asm
	{ atom.add.f64 %fd249,[%rd164],%fd30; }

	// end inline asm
	add.s64 	%rd165, %rd156, 72;
	// begin inline asm
	{ atom.add.f64 %fd251,[%rd165],%fd256; }

	// end inline asm
	add.s64 	%rd166, %rd156, 80;
	// begin inline asm
	{ atom.add.f64 %fd253,[%rd166],%fd256; }

	// end inline asm
	add.s64 	%rd167, %rd156, 88;
	// begin inline asm
	{ atom.add.f64 %fd255,[%rd167],%fd256; }

	// end inline asm
	bra.uni 	$L__BB43_38;

$L__BB43_36:
	setp.eq.s64 	%p20, %rd49, 0;
	@%p20 bra 	$L__BB43_38;

	add.s64 	%rd169, %rd49, %rd38;
	mov.f64 	%fd280, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd257,[%rd169],%fd280; }

	// end inline asm
	add.s64 	%rd170, %rd169, 8;
	// begin inline asm
	{ atom.add.f64 %fd259,[%rd170],%fd280; }

	// end inline asm
	add.s64 	%rd171, %rd169, 16;
	// begin inline asm
	{ atom.add.f64 %fd261,[%rd171],%fd280; }

	// end inline asm
	add.s64 	%rd172, %rd169, 24;
	// begin inline asm
	{ atom.add.f64 %fd263,[%rd172],%fd280; }

	// end inline asm
	add.s64 	%rd173, %rd169, 32;
	// begin inline asm
	{ atom.add.f64 %fd265,[%rd173],%fd280; }

	// end inline asm
	add.s64 	%rd174, %rd169, 40;
	// begin inline asm
	{ atom.add.f64 %fd267,[%rd174],%fd280; }

	// end inline asm
	add.s64 	%rd175, %rd169, 48;
	// begin inline asm
	{ atom.add.f64 %fd269,[%rd175],%fd280; }

	// end inline asm
	add.s64 	%rd176, %rd169, 56;
	// begin inline asm
	{ atom.add.f64 %fd271,[%rd176],%fd280; }

	// end inline asm
	add.s64 	%rd177, %rd169, 64;
	// begin inline asm
	{ atom.add.f64 %fd273,[%rd177],%fd30; }

	// end inline asm
	add.s64 	%rd178, %rd169, 72;
	// begin inline asm
	{ atom.add.f64 %fd275,[%rd178],%fd280; }

	// end inline asm
	add.s64 	%rd179, %rd169, 80;
	// begin inline asm
	{ atom.add.f64 %fd277,[%rd179],%fd280; }

	// end inline asm
	add.s64 	%rd180, %rd169, 88;
	// begin inline asm
	{ atom.add.f64 %fd279,[%rd180],%fd280; }

	// end inline asm

$L__BB43_38:
	@%p13 bra 	$L__BB43_40;

	mul.lo.s64 	%rd193, %rd37, %rd24;
	add.s64 	%rd181, %rd54, %rd193;
	mov.f64 	%fd304, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd281,[%rd181],%fd304; }

	// end inline asm
	add.s64 	%rd182, %rd181, 8;
	// begin inline asm
	{ atom.add.f64 %fd283,[%rd182],%fd304; }

	// end inline asm
	add.s64 	%rd183, %rd181, 16;
	// begin inline asm
	{ atom.add.f64 %fd285,[%rd183],%fd304; }

	// end inline asm
	add.s64 	%rd184, %rd181, 24;
	// begin inline asm
	{ atom.add.f64 %fd287,[%rd184],%fd304; }

	// end inline asm
	add.s64 	%rd185, %rd181, 32;
	// begin inline asm
	{ atom.add.f64 %fd289,[%rd185],%fd304; }

	// end inline asm
	add.s64 	%rd186, %rd181, 40;
	// begin inline asm
	{ atom.add.f64 %fd291,[%rd186],%fd304; }

	// end inline asm
	add.s64 	%rd187, %rd181, 48;
	// begin inline asm
	{ atom.add.f64 %fd293,[%rd187],%fd304; }

	// end inline asm
	add.s64 	%rd188, %rd181, 56;
	// begin inline asm
	{ atom.add.f64 %fd295,[%rd188],%fd29; }

	// end inline asm
	add.s64 	%rd189, %rd181, 64;
	// begin inline asm
	{ atom.add.f64 %fd297,[%rd189],%fd304; }

	// end inline asm
	add.s64 	%rd190, %rd181, 72;
	// begin inline asm
	{ atom.add.f64 %fd299,[%rd190],%fd304; }

	// end inline asm
	add.s64 	%rd191, %rd181, 80;
	// begin inline asm
	{ atom.add.f64 %fd301,[%rd191],%fd304; }

	// end inline asm
	add.s64 	%rd192, %rd181, 88;
	// begin inline asm
	{ atom.add.f64 %fd303,[%rd192],%fd304; }

	// end inline asm
	bra.uni 	$L__BB43_42;

$L__BB43_40:
	setp.eq.s64 	%p22, %rd49, 0;
	@%p22 bra 	$L__BB43_42;

	add.s64 	%rd194, %rd49, %rd38;
	mov.f64 	%fd328, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd305,[%rd194],%fd328; }

	// end inline asm
	add.s64 	%rd195, %rd194, 8;
	// begin inline asm
	{ atom.add.f64 %fd307,[%rd195],%fd328; }

	// end inline asm
	add.s64 	%rd196, %rd194, 16;
	// begin inline asm
	{ atom.add.f64 %fd309,[%rd196],%fd328; }

	// end inline asm
	add.s64 	%rd197, %rd194, 24;
	// begin inline asm
	{ atom.add.f64 %fd311,[%rd197],%fd328; }

	// end inline asm
	add.s64 	%rd198, %rd194, 32;
	// begin inline asm
	{ atom.add.f64 %fd313,[%rd198],%fd328; }

	// end inline asm
	add.s64 	%rd199, %rd194, 40;
	// begin inline asm
	{ atom.add.f64 %fd315,[%rd199],%fd328; }

	// end inline asm
	add.s64 	%rd200, %rd194, 48;
	// begin inline asm
	{ atom.add.f64 %fd317,[%rd200],%fd328; }

	// end inline asm
	add.s64 	%rd201, %rd194, 56;
	// begin inline asm
	{ atom.add.f64 %fd319,[%rd201],%fd29; }

	// end inline asm
	add.s64 	%rd202, %rd194, 64;
	// begin inline asm
	{ atom.add.f64 %fd321,[%rd202],%fd328; }

	// end inline asm
	add.s64 	%rd203, %rd194, 72;
	// begin inline asm
	{ atom.add.f64 %fd323,[%rd203],%fd328; }

	// end inline asm
	add.s64 	%rd204, %rd194, 80;
	// begin inline asm
	{ atom.add.f64 %fd325,[%rd204],%fd328; }

	// end inline asm
	add.s64 	%rd205, %rd194, 88;
	// begin inline asm
	{ atom.add.f64 %fd327,[%rd205],%fd328; }

	// end inline asm

$L__BB43_42:
	@%p13 bra 	$L__BB43_44;

	mul.lo.s64 	%rd218, %rd37, %rd24;
	add.s64 	%rd206, %rd54, %rd218;
	mov.f64 	%fd352, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd329,[%rd206],%fd352; }

	// end inline asm
	add.s64 	%rd207, %rd206, 8;
	// begin inline asm
	{ atom.add.f64 %fd331,[%rd207],%fd352; }

	// end inline asm
	add.s64 	%rd208, %rd206, 16;
	// begin inline asm
	{ atom.add.f64 %fd333,[%rd208],%fd352; }

	// end inline asm
	add.s64 	%rd209, %rd206, 24;
	// begin inline asm
	{ atom.add.f64 %fd335,[%rd209],%fd352; }

	// end inline asm
	add.s64 	%rd210, %rd206, 32;
	// begin inline asm
	{ atom.add.f64 %fd337,[%rd210],%fd352; }

	// end inline asm
	add.s64 	%rd211, %rd206, 40;
	// begin inline asm
	{ atom.add.f64 %fd339,[%rd211],%fd352; }

	// end inline asm
	add.s64 	%rd212, %rd206, 48;
	// begin inline asm
	{ atom.add.f64 %fd341,[%rd212],%fd28; }

	// end inline asm
	add.s64 	%rd213, %rd206, 56;
	// begin inline asm
	{ atom.add.f64 %fd343,[%rd213],%fd352; }

	// end inline asm
	add.s64 	%rd214, %rd206, 64;
	// begin inline asm
	{ atom.add.f64 %fd345,[%rd214],%fd352; }

	// end inline asm
	add.s64 	%rd215, %rd206, 72;
	// begin inline asm
	{ atom.add.f64 %fd347,[%rd215],%fd352; }

	// end inline asm
	add.s64 	%rd216, %rd206, 80;
	// begin inline asm
	{ atom.add.f64 %fd349,[%rd216],%fd352; }

	// end inline asm
	add.s64 	%rd217, %rd206, 88;
	// begin inline asm
	{ atom.add.f64 %fd351,[%rd217],%fd352; }

	// end inline asm
	bra.uni 	$L__BB43_46;

$L__BB43_44:
	setp.eq.s64 	%p24, %rd49, 0;
	@%p24 bra 	$L__BB43_46;

	add.s64 	%rd219, %rd49, %rd38;
	mov.f64 	%fd376, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd353,[%rd219],%fd376; }

	// end inline asm
	add.s64 	%rd220, %rd219, 8;
	// begin inline asm
	{ atom.add.f64 %fd355,[%rd220],%fd376; }

	// end inline asm
	add.s64 	%rd221, %rd219, 16;
	// begin inline asm
	{ atom.add.f64 %fd357,[%rd221],%fd376; }

	// end inline asm
	add.s64 	%rd222, %rd219, 24;
	// begin inline asm
	{ atom.add.f64 %fd359,[%rd222],%fd376; }

	// end inline asm
	add.s64 	%rd223, %rd219, 32;
	// begin inline asm
	{ atom.add.f64 %fd361,[%rd223],%fd376; }

	// end inline asm
	add.s64 	%rd224, %rd219, 40;
	// begin inline asm
	{ atom.add.f64 %fd363,[%rd224],%fd376; }

	// end inline asm
	add.s64 	%rd225, %rd219, 48;
	// begin inline asm
	{ atom.add.f64 %fd365,[%rd225],%fd28; }

	// end inline asm
	add.s64 	%rd226, %rd219, 56;
	// begin inline asm
	{ atom.add.f64 %fd367,[%rd226],%fd376; }

	// end inline asm
	add.s64 	%rd227, %rd219, 64;
	// begin inline asm
	{ atom.add.f64 %fd369,[%rd227],%fd376; }

	// end inline asm
	add.s64 	%rd228, %rd219, 72;
	// begin inline asm
	{ atom.add.f64 %fd371,[%rd228],%fd376; }

	// end inline asm
	add.s64 	%rd229, %rd219, 80;
	// begin inline asm
	{ atom.add.f64 %fd373,[%rd229],%fd376; }

	// end inline asm
	add.s64 	%rd230, %rd219, 88;
	// begin inline asm
	{ atom.add.f64 %fd375,[%rd230],%fd376; }

	// end inline asm

$L__BB43_46:
	@%p13 bra 	$L__BB43_48;

	mul.lo.s64 	%rd243, %rd37, %rd24;
	add.s64 	%rd231, %rd54, %rd243;
	mov.f64 	%fd400, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd377,[%rd231],%fd400; }

	// end inline asm
	add.s64 	%rd232, %rd231, 8;
	// begin inline asm
	{ atom.add.f64 %fd379,[%rd232],%fd400; }

	// end inline asm
	add.s64 	%rd233, %rd231, 16;
	// begin inline asm
	{ atom.add.f64 %fd381,[%rd233],%fd400; }

	// end inline asm
	add.s64 	%rd234, %rd231, 24;
	// begin inline asm
	{ atom.add.f64 %fd383,[%rd234],%fd400; }

	// end inline asm
	add.s64 	%rd235, %rd231, 32;
	// begin inline asm
	{ atom.add.f64 %fd385,[%rd235],%fd400; }

	// end inline asm
	add.s64 	%rd236, %rd231, 40;
	// begin inline asm
	{ atom.add.f64 %fd387,[%rd236],%fd33; }

	// end inline asm
	add.s64 	%rd237, %rd231, 48;
	// begin inline asm
	{ atom.add.f64 %fd389,[%rd237],%fd400; }

	// end inline asm
	add.s64 	%rd238, %rd231, 56;
	// begin inline asm
	{ atom.add.f64 %fd391,[%rd238],%fd400; }

	// end inline asm
	add.s64 	%rd239, %rd231, 64;
	// begin inline asm
	{ atom.add.f64 %fd393,[%rd239],%fd400; }

	// end inline asm
	add.s64 	%rd240, %rd231, 72;
	// begin inline asm
	{ atom.add.f64 %fd395,[%rd240],%fd400; }

	// end inline asm
	add.s64 	%rd241, %rd231, 80;
	// begin inline asm
	{ atom.add.f64 %fd397,[%rd241],%fd400; }

	// end inline asm
	add.s64 	%rd242, %rd231, 88;
	// begin inline asm
	{ atom.add.f64 %fd399,[%rd242],%fd400; }

	// end inline asm
	bra.uni 	$L__BB43_50;

$L__BB43_48:
	setp.eq.s64 	%p26, %rd49, 0;
	@%p26 bra 	$L__BB43_50;

	add.s64 	%rd244, %rd49, %rd38;
	mov.f64 	%fd424, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd401,[%rd244],%fd424; }

	// end inline asm
	add.s64 	%rd245, %rd244, 8;
	// begin inline asm
	{ atom.add.f64 %fd403,[%rd245],%fd424; }

	// end inline asm
	add.s64 	%rd246, %rd244, 16;
	// begin inline asm
	{ atom.add.f64 %fd405,[%rd246],%fd424; }

	// end inline asm
	add.s64 	%rd247, %rd244, 24;
	// begin inline asm
	{ atom.add.f64 %fd407,[%rd247],%fd424; }

	// end inline asm
	add.s64 	%rd248, %rd244, 32;
	// begin inline asm
	{ atom.add.f64 %fd409,[%rd248],%fd424; }

	// end inline asm
	add.s64 	%rd249, %rd244, 40;
	// begin inline asm
	{ atom.add.f64 %fd411,[%rd249],%fd33; }

	// end inline asm
	add.s64 	%rd250, %rd244, 48;
	// begin inline asm
	{ atom.add.f64 %fd413,[%rd250],%fd424; }

	// end inline asm
	add.s64 	%rd251, %rd244, 56;
	// begin inline asm
	{ atom.add.f64 %fd415,[%rd251],%fd424; }

	// end inline asm
	add.s64 	%rd252, %rd244, 64;
	// begin inline asm
	{ atom.add.f64 %fd417,[%rd252],%fd424; }

	// end inline asm
	add.s64 	%rd253, %rd244, 72;
	// begin inline asm
	{ atom.add.f64 %fd419,[%rd253],%fd424; }

	// end inline asm
	add.s64 	%rd254, %rd244, 80;
	// begin inline asm
	{ atom.add.f64 %fd421,[%rd254],%fd424; }

	// end inline asm
	add.s64 	%rd255, %rd244, 88;
	// begin inline asm
	{ atom.add.f64 %fd423,[%rd255],%fd424; }

	// end inline asm

$L__BB43_50:
	@%p13 bra 	$L__BB43_52;

	mul.lo.s64 	%rd268, %rd37, %rd24;
	add.s64 	%rd256, %rd54, %rd268;
	mov.f64 	%fd448, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd425,[%rd256],%fd448; }

	// end inline asm
	add.s64 	%rd257, %rd256, 8;
	// begin inline asm
	{ atom.add.f64 %fd427,[%rd257],%fd448; }

	// end inline asm
	add.s64 	%rd258, %rd256, 16;
	// begin inline asm
	{ atom.add.f64 %fd429,[%rd258],%fd448; }

	// end inline asm
	add.s64 	%rd259, %rd256, 24;
	// begin inline asm
	{ atom.add.f64 %fd431,[%rd259],%fd448; }

	// end inline asm
	add.s64 	%rd260, %rd256, 32;
	// begin inline asm
	{ atom.add.f64 %fd433,[%rd260],%fd32; }

	// end inline asm
	add.s64 	%rd261, %rd256, 40;
	// begin inline asm
	{ atom.add.f64 %fd435,[%rd261],%fd448; }

	// end inline asm
	add.s64 	%rd262, %rd256, 48;
	// begin inline asm
	{ atom.add.f64 %fd437,[%rd262],%fd448; }

	// end inline asm
	add.s64 	%rd263, %rd256, 56;
	// begin inline asm
	{ atom.add.f64 %fd439,[%rd263],%fd448; }

	// end inline asm
	add.s64 	%rd264, %rd256, 64;
	// begin inline asm
	{ atom.add.f64 %fd441,[%rd264],%fd448; }

	// end inline asm
	add.s64 	%rd265, %rd256, 72;
	// begin inline asm
	{ atom.add.f64 %fd443,[%rd265],%fd448; }

	// end inline asm
	add.s64 	%rd266, %rd256, 80;
	// begin inline asm
	{ atom.add.f64 %fd445,[%rd266],%fd448; }

	// end inline asm
	add.s64 	%rd267, %rd256, 88;
	// begin inline asm
	{ atom.add.f64 %fd447,[%rd267],%fd448; }

	// end inline asm
	bra.uni 	$L__BB43_54;

$L__BB43_52:
	setp.eq.s64 	%p28, %rd49, 0;
	@%p28 bra 	$L__BB43_54;

	add.s64 	%rd269, %rd49, %rd38;
	mov.f64 	%fd472, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd449,[%rd269],%fd472; }

	// end inline asm
	add.s64 	%rd270, %rd269, 8;
	// begin inline asm
	{ atom.add.f64 %fd451,[%rd270],%fd472; }

	// end inline asm
	add.s64 	%rd271, %rd269, 16;
	// begin inline asm
	{ atom.add.f64 %fd453,[%rd271],%fd472; }

	// end inline asm
	add.s64 	%rd272, %rd269, 24;
	// begin inline asm
	{ atom.add.f64 %fd455,[%rd272],%fd472; }

	// end inline asm
	add.s64 	%rd273, %rd269, 32;
	// begin inline asm
	{ atom.add.f64 %fd457,[%rd273],%fd32; }

	// end inline asm
	add.s64 	%rd274, %rd269, 40;
	// begin inline asm
	{ atom.add.f64 %fd459,[%rd274],%fd472; }

	// end inline asm
	add.s64 	%rd275, %rd269, 48;
	// begin inline asm
	{ atom.add.f64 %fd461,[%rd275],%fd472; }

	// end inline asm
	add.s64 	%rd276, %rd269, 56;
	// begin inline asm
	{ atom.add.f64 %fd463,[%rd276],%fd472; }

	// end inline asm
	add.s64 	%rd277, %rd269, 64;
	// begin inline asm
	{ atom.add.f64 %fd465,[%rd277],%fd472; }

	// end inline asm
	add.s64 	%rd278, %rd269, 72;
	// begin inline asm
	{ atom.add.f64 %fd467,[%rd278],%fd472; }

	// end inline asm
	add.s64 	%rd279, %rd269, 80;
	// begin inline asm
	{ atom.add.f64 %fd469,[%rd279],%fd472; }

	// end inline asm
	add.s64 	%rd280, %rd269, 88;
	// begin inline asm
	{ atom.add.f64 %fd471,[%rd280],%fd472; }

	// end inline asm

$L__BB43_54:
	@%p13 bra 	$L__BB43_56;

	mul.lo.s64 	%rd293, %rd37, %rd24;
	add.s64 	%rd281, %rd54, %rd293;
	mov.f64 	%fd496, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd473,[%rd281],%fd496; }

	// end inline asm
	add.s64 	%rd282, %rd281, 8;
	// begin inline asm
	{ atom.add.f64 %fd475,[%rd282],%fd496; }

	// end inline asm
	add.s64 	%rd283, %rd281, 16;
	// begin inline asm
	{ atom.add.f64 %fd477,[%rd283],%fd496; }

	// end inline asm
	add.s64 	%rd284, %rd281, 24;
	// begin inline asm
	{ atom.add.f64 %fd479,[%rd284],%fd31; }

	// end inline asm
	add.s64 	%rd285, %rd281, 32;
	// begin inline asm
	{ atom.add.f64 %fd481,[%rd285],%fd496; }

	// end inline asm
	add.s64 	%rd286, %rd281, 40;
	// begin inline asm
	{ atom.add.f64 %fd483,[%rd286],%fd496; }

	// end inline asm
	add.s64 	%rd287, %rd281, 48;
	// begin inline asm
	{ atom.add.f64 %fd485,[%rd287],%fd496; }

	// end inline asm
	add.s64 	%rd288, %rd281, 56;
	// begin inline asm
	{ atom.add.f64 %fd487,[%rd288],%fd496; }

	// end inline asm
	add.s64 	%rd289, %rd281, 64;
	// begin inline asm
	{ atom.add.f64 %fd489,[%rd289],%fd496; }

	// end inline asm
	add.s64 	%rd290, %rd281, 72;
	// begin inline asm
	{ atom.add.f64 %fd491,[%rd290],%fd496; }

	// end inline asm
	add.s64 	%rd291, %rd281, 80;
	// begin inline asm
	{ atom.add.f64 %fd493,[%rd291],%fd496; }

	// end inline asm
	add.s64 	%rd292, %rd281, 88;
	// begin inline asm
	{ atom.add.f64 %fd495,[%rd292],%fd496; }

	// end inline asm
	bra.uni 	$L__BB43_58;

$L__BB43_56:
	setp.eq.s64 	%p30, %rd49, 0;
	@%p30 bra 	$L__BB43_58;

	add.s64 	%rd294, %rd49, %rd38;
	mov.f64 	%fd520, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd497,[%rd294],%fd520; }

	// end inline asm
	add.s64 	%rd295, %rd294, 8;
	// begin inline asm
	{ atom.add.f64 %fd499,[%rd295],%fd520; }

	// end inline asm
	add.s64 	%rd296, %rd294, 16;
	// begin inline asm
	{ atom.add.f64 %fd501,[%rd296],%fd520; }

	// end inline asm
	add.s64 	%rd297, %rd294, 24;
	// begin inline asm
	{ atom.add.f64 %fd503,[%rd297],%fd31; }

	// end inline asm
	add.s64 	%rd298, %rd294, 32;
	// begin inline asm
	{ atom.add.f64 %fd505,[%rd298],%fd520; }

	// end inline asm
	add.s64 	%rd299, %rd294, 40;
	// begin inline asm
	{ atom.add.f64 %fd507,[%rd299],%fd520; }

	// end inline asm
	add.s64 	%rd300, %rd294, 48;
	// begin inline asm
	{ atom.add.f64 %fd509,[%rd300],%fd520; }

	// end inline asm
	add.s64 	%rd301, %rd294, 56;
	// begin inline asm
	{ atom.add.f64 %fd511,[%rd301],%fd520; }

	// end inline asm
	add.s64 	%rd302, %rd294, 64;
	// begin inline asm
	{ atom.add.f64 %fd513,[%rd302],%fd520; }

	// end inline asm
	add.s64 	%rd303, %rd294, 72;
	// begin inline asm
	{ atom.add.f64 %fd515,[%rd303],%fd520; }

	// end inline asm
	add.s64 	%rd304, %rd294, 80;
	// begin inline asm
	{ atom.add.f64 %fd517,[%rd304],%fd520; }

	// end inline asm
	add.s64 	%rd305, %rd294, 88;
	// begin inline asm
	{ atom.add.f64 %fd519,[%rd305],%fd520; }

	// end inline asm

$L__BB43_58:
	@%p13 bra 	$L__BB43_60;

	mul.lo.s64 	%rd318, %rd37, %rd24;
	add.s64 	%rd306, %rd54, %rd318;
	mov.f64 	%fd544, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd521,[%rd306],%fd544; }

	// end inline asm
	add.s64 	%rd307, %rd306, 8;
	// begin inline asm
	{ atom.add.f64 %fd523,[%rd307],%fd544; }

	// end inline asm
	add.s64 	%rd308, %rd306, 16;
	// begin inline asm
	{ atom.add.f64 %fd525,[%rd308],%fd36; }

	// end inline asm
	add.s64 	%rd309, %rd306, 24;
	// begin inline asm
	{ atom.add.f64 %fd527,[%rd309],%fd544; }

	// end inline asm
	add.s64 	%rd310, %rd306, 32;
	// begin inline asm
	{ atom.add.f64 %fd529,[%rd310],%fd544; }

	// end inline asm
	add.s64 	%rd311, %rd306, 40;
	// begin inline asm
	{ atom.add.f64 %fd531,[%rd311],%fd544; }

	// end inline asm
	add.s64 	%rd312, %rd306, 48;
	// begin inline asm
	{ atom.add.f64 %fd533,[%rd312],%fd544; }

	// end inline asm
	add.s64 	%rd313, %rd306, 56;
	// begin inline asm
	{ atom.add.f64 %fd535,[%rd313],%fd544; }

	// end inline asm
	add.s64 	%rd314, %rd306, 64;
	// begin inline asm
	{ atom.add.f64 %fd537,[%rd314],%fd544; }

	// end inline asm
	add.s64 	%rd315, %rd306, 72;
	// begin inline asm
	{ atom.add.f64 %fd539,[%rd315],%fd544; }

	// end inline asm
	add.s64 	%rd316, %rd306, 80;
	// begin inline asm
	{ atom.add.f64 %fd541,[%rd316],%fd544; }

	// end inline asm
	add.s64 	%rd317, %rd306, 88;
	// begin inline asm
	{ atom.add.f64 %fd543,[%rd317],%fd544; }

	// end inline asm
	bra.uni 	$L__BB43_62;

$L__BB43_60:
	setp.eq.s64 	%p32, %rd49, 0;
	@%p32 bra 	$L__BB43_62;

	add.s64 	%rd319, %rd49, %rd38;
	mov.f64 	%fd568, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd545,[%rd319],%fd568; }

	// end inline asm
	add.s64 	%rd320, %rd319, 8;
	// begin inline asm
	{ atom.add.f64 %fd547,[%rd320],%fd568; }

	// end inline asm
	add.s64 	%rd321, %rd319, 16;
	// begin inline asm
	{ atom.add.f64 %fd549,[%rd321],%fd36; }

	// end inline asm
	add.s64 	%rd322, %rd319, 24;
	// begin inline asm
	{ atom.add.f64 %fd551,[%rd322],%fd568; }

	// end inline asm
	add.s64 	%rd323, %rd319, 32;
	// begin inline asm
	{ atom.add.f64 %fd553,[%rd323],%fd568; }

	// end inline asm
	add.s64 	%rd324, %rd319, 40;
	// begin inline asm
	{ atom.add.f64 %fd555,[%rd324],%fd568; }

	// end inline asm
	add.s64 	%rd325, %rd319, 48;
	// begin inline asm
	{ atom.add.f64 %fd557,[%rd325],%fd568; }

	// end inline asm
	add.s64 	%rd326, %rd319, 56;
	// begin inline asm
	{ atom.add.f64 %fd559,[%rd326],%fd568; }

	// end inline asm
	add.s64 	%rd327, %rd319, 64;
	// begin inline asm
	{ atom.add.f64 %fd561,[%rd327],%fd568; }

	// end inline asm
	add.s64 	%rd328, %rd319, 72;
	// begin inline asm
	{ atom.add.f64 %fd563,[%rd328],%fd568; }

	// end inline asm
	add.s64 	%rd329, %rd319, 80;
	// begin inline asm
	{ atom.add.f64 %fd565,[%rd329],%fd568; }

	// end inline asm
	add.s64 	%rd330, %rd319, 88;
	// begin inline asm
	{ atom.add.f64 %fd567,[%rd330],%fd568; }

	// end inline asm

$L__BB43_62:
	@%p13 bra 	$L__BB43_64;

	mul.lo.s64 	%rd343, %rd37, %rd24;
	add.s64 	%rd331, %rd54, %rd343;
	mov.f64 	%fd592, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd569,[%rd331],%fd592; }

	// end inline asm
	add.s64 	%rd332, %rd331, 8;
	// begin inline asm
	{ atom.add.f64 %fd571,[%rd332],%fd35; }

	// end inline asm
	add.s64 	%rd333, %rd331, 16;
	// begin inline asm
	{ atom.add.f64 %fd573,[%rd333],%fd592; }

	// end inline asm
	add.s64 	%rd334, %rd331, 24;
	// begin inline asm
	{ atom.add.f64 %fd575,[%rd334],%fd592; }

	// end inline asm
	add.s64 	%rd335, %rd331, 32;
	// begin inline asm
	{ atom.add.f64 %fd577,[%rd335],%fd592; }

	// end inline asm
	add.s64 	%rd336, %rd331, 40;
	// begin inline asm
	{ atom.add.f64 %fd579,[%rd336],%fd592; }

	// end inline asm
	add.s64 	%rd337, %rd331, 48;
	// begin inline asm
	{ atom.add.f64 %fd581,[%rd337],%fd592; }

	// end inline asm
	add.s64 	%rd338, %rd331, 56;
	// begin inline asm
	{ atom.add.f64 %fd583,[%rd338],%fd592; }

	// end inline asm
	add.s64 	%rd339, %rd331, 64;
	// begin inline asm
	{ atom.add.f64 %fd585,[%rd339],%fd592; }

	// end inline asm
	add.s64 	%rd340, %rd331, 72;
	// begin inline asm
	{ atom.add.f64 %fd587,[%rd340],%fd592; }

	// end inline asm
	add.s64 	%rd341, %rd331, 80;
	// begin inline asm
	{ atom.add.f64 %fd589,[%rd341],%fd592; }

	// end inline asm
	add.s64 	%rd342, %rd331, 88;
	// begin inline asm
	{ atom.add.f64 %fd591,[%rd342],%fd592; }

	// end inline asm
	bra.uni 	$L__BB43_66;

$L__BB43_64:
	setp.eq.s64 	%p34, %rd49, 0;
	@%p34 bra 	$L__BB43_66;

	add.s64 	%rd344, %rd49, %rd38;
	mov.f64 	%fd616, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd593,[%rd344],%fd616; }

	// end inline asm
	add.s64 	%rd345, %rd344, 8;
	// begin inline asm
	{ atom.add.f64 %fd595,[%rd345],%fd35; }

	// end inline asm
	add.s64 	%rd346, %rd344, 16;
	// begin inline asm
	{ atom.add.f64 %fd597,[%rd346],%fd616; }

	// end inline asm
	add.s64 	%rd347, %rd344, 24;
	// begin inline asm
	{ atom.add.f64 %fd599,[%rd347],%fd616; }

	// end inline asm
	add.s64 	%rd348, %rd344, 32;
	// begin inline asm
	{ atom.add.f64 %fd601,[%rd348],%fd616; }

	// end inline asm
	add.s64 	%rd349, %rd344, 40;
	// begin inline asm
	{ atom.add.f64 %fd603,[%rd349],%fd616; }

	// end inline asm
	add.s64 	%rd350, %rd344, 48;
	// begin inline asm
	{ atom.add.f64 %fd605,[%rd350],%fd616; }

	// end inline asm
	add.s64 	%rd351, %rd344, 56;
	// begin inline asm
	{ atom.add.f64 %fd607,[%rd351],%fd616; }

	// end inline asm
	add.s64 	%rd352, %rd344, 64;
	// begin inline asm
	{ atom.add.f64 %fd609,[%rd352],%fd616; }

	// end inline asm
	add.s64 	%rd353, %rd344, 72;
	// begin inline asm
	{ atom.add.f64 %fd611,[%rd353],%fd616; }

	// end inline asm
	add.s64 	%rd354, %rd344, 80;
	// begin inline asm
	{ atom.add.f64 %fd613,[%rd354],%fd616; }

	// end inline asm
	add.s64 	%rd355, %rd344, 88;
	// begin inline asm
	{ atom.add.f64 %fd615,[%rd355],%fd616; }

	// end inline asm

$L__BB43_66:
	@%p13 bra 	$L__BB43_68;

	mul.lo.s64 	%rd368, %rd37, %rd24;
	add.s64 	%rd356, %rd54, %rd368;
	// begin inline asm
	{ atom.add.f64 %fd617,[%rd356],%fd34; }

	// end inline asm
	add.s64 	%rd357, %rd356, 8;
	mov.f64 	%fd640, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd619,[%rd357],%fd640; }

	// end inline asm
	add.s64 	%rd358, %rd356, 16;
	// begin inline asm
	{ atom.add.f64 %fd621,[%rd358],%fd640; }

	// end inline asm
	add.s64 	%rd359, %rd356, 24;
	// begin inline asm
	{ atom.add.f64 %fd623,[%rd359],%fd640; }

	// end inline asm
	add.s64 	%rd360, %rd356, 32;
	// begin inline asm
	{ atom.add.f64 %fd625,[%rd360],%fd640; }

	// end inline asm
	add.s64 	%rd361, %rd356, 40;
	// begin inline asm
	{ atom.add.f64 %fd627,[%rd361],%fd640; }

	// end inline asm
	add.s64 	%rd362, %rd356, 48;
	// begin inline asm
	{ atom.add.f64 %fd629,[%rd362],%fd640; }

	// end inline asm
	add.s64 	%rd363, %rd356, 56;
	// begin inline asm
	{ atom.add.f64 %fd631,[%rd363],%fd640; }

	// end inline asm
	add.s64 	%rd364, %rd356, 64;
	// begin inline asm
	{ atom.add.f64 %fd633,[%rd364],%fd640; }

	// end inline asm
	add.s64 	%rd365, %rd356, 72;
	// begin inline asm
	{ atom.add.f64 %fd635,[%rd365],%fd640; }

	// end inline asm
	add.s64 	%rd366, %rd356, 80;
	// begin inline asm
	{ atom.add.f64 %fd637,[%rd366],%fd640; }

	// end inline asm
	add.s64 	%rd367, %rd356, 88;
	// begin inline asm
	{ atom.add.f64 %fd639,[%rd367],%fd640; }

	// end inline asm
	bra.uni 	$L__BB43_70;

$L__BB43_68:
	setp.eq.s64 	%p36, %rd49, 0;
	@%p36 bra 	$L__BB43_70;

	add.s64 	%rd369, %rd49, %rd38;
	// begin inline asm
	{ atom.add.f64 %fd641,[%rd369],%fd34; }

	// end inline asm
	add.s64 	%rd370, %rd369, 8;
	mov.f64 	%fd664, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd643,[%rd370],%fd664; }

	// end inline asm
	add.s64 	%rd371, %rd369, 16;
	// begin inline asm
	{ atom.add.f64 %fd645,[%rd371],%fd664; }

	// end inline asm
	add.s64 	%rd372, %rd369, 24;
	// begin inline asm
	{ atom.add.f64 %fd647,[%rd372],%fd664; }

	// end inline asm
	add.s64 	%rd373, %rd369, 32;
	// begin inline asm
	{ atom.add.f64 %fd649,[%rd373],%fd664; }

	// end inline asm
	add.s64 	%rd374, %rd369, 40;
	// begin inline asm
	{ atom.add.f64 %fd651,[%rd374],%fd664; }

	// end inline asm
	add.s64 	%rd375, %rd369, 48;
	// begin inline asm
	{ atom.add.f64 %fd653,[%rd375],%fd664; }

	// end inline asm
	add.s64 	%rd376, %rd369, 56;
	// begin inline asm
	{ atom.add.f64 %fd655,[%rd376],%fd664; }

	// end inline asm
	add.s64 	%rd377, %rd369, 64;
	// begin inline asm
	{ atom.add.f64 %fd657,[%rd377],%fd664; }

	// end inline asm
	add.s64 	%rd378, %rd369, 72;
	// begin inline asm
	{ atom.add.f64 %fd659,[%rd378],%fd664; }

	// end inline asm
	add.s64 	%rd379, %rd369, 80;
	// begin inline asm
	{ atom.add.f64 %fd661,[%rd379],%fd664; }

	// end inline asm
	add.s64 	%rd380, %rd369, 88;
	// begin inline asm
	{ atom.add.f64 %fd663,[%rd380],%fd664; }

	// end inline asm

$L__BB43_70:
	ld.param.u64 	%rd381, [y_to_x_cuda_kernel_backward_param_0+24];
	add.s64 	%rd384, %rd384, %rd25;
	setp.lt.u64 	%p37, %rd384, %rd381;
	@%p37 bra 	$L__BB43_2;

$L__BB43_71:
	ret;

}
.func  (.param .b64 func_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<53>;
	.reg .f64 	%fd<138>;


	ld.param.f64 	%fd12, [__internal_accurate_pow_param_0];
	ld.param.f64 	%fd13, [__internal_accurate_pow_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd12;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd12;
	}
	shr.u32 	%r51, %r50, 20;
	setp.ne.s32 	%p1, %r51, 0;
	@%p1 bra 	$L__BB44_2;

	mul.f64 	%fd14, %fd12, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd14;
	}
	shr.u32 	%r16, %r50, 20;
	add.s32 	%r51, %r16, -54;

$L__BB44_2:
	add.s32 	%r52, %r51, -1023;
	and.b32  	%r17, %r50, -2146435073;
	or.b32  	%r18, %r17, 1072693248;
	mov.b64 	%fd135, {%r49, %r18};
	setp.lt.u32 	%p2, %r18, 1073127583;
	@%p2 bra 	$L__BB44_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd135;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd135;
	}
	add.s32 	%r21, %r20, -1048576;
	mov.b64 	%fd135, {%r19, %r21};
	add.s32 	%r52, %r51, -1022;

$L__BB44_4:
	add.f64 	%fd15, %fd135, 0d3FF0000000000000;
	mov.f64 	%fd16, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd17, %fd15;
	neg.f64 	%fd18, %fd15;
	fma.rn.f64 	%fd19, %fd18, %fd17, %fd16;
	fma.rn.f64 	%fd20, %fd19, %fd19, %fd19;
	fma.rn.f64 	%fd21, %fd20, %fd17, %fd17;
	add.f64 	%fd22, %fd135, 0dBFF0000000000000;
	mul.f64 	%fd23, %fd22, %fd21;
	fma.rn.f64 	%fd24, %fd22, %fd21, %fd23;
	mul.f64 	%fd25, %fd24, %fd24;
	mov.f64 	%fd26, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd27, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd28, %fd27, %fd25, %fd26;
	mov.f64 	%fd29, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd30, %fd28, %fd25, %fd29;
	mov.f64 	%fd31, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd32, %fd30, %fd25, %fd31;
	mov.f64 	%fd33, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd34, %fd32, %fd25, %fd33;
	mov.f64 	%fd35, 0d3F6249249242B910;
	fma.rn.f64 	%fd36, %fd34, %fd25, %fd35;
	mov.f64 	%fd37, 0d3F89999999999DFB;
	fma.rn.f64 	%fd38, %fd36, %fd25, %fd37;
	sub.f64 	%fd39, %fd22, %fd24;
	add.f64 	%fd40, %fd39, %fd39;
	neg.f64 	%fd41, %fd24;
	fma.rn.f64 	%fd42, %fd41, %fd22, %fd40;
	mul.f64 	%fd43, %fd21, %fd42;
	fma.rn.f64 	%fd44, %fd25, %fd38, 0d3FB5555555555555;
	mov.f64 	%fd45, 0d3FB5555555555555;
	sub.f64 	%fd46, %fd45, %fd44;
	fma.rn.f64 	%fd47, %fd25, %fd38, %fd46;
	add.f64 	%fd48, %fd47, 0d0000000000000000;
	add.f64 	%fd49, %fd48, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd50, %fd44, %fd49;
	sub.f64 	%fd51, %fd44, %fd50;
	add.f64 	%fd52, %fd49, %fd51;
	mul.rn.f64 	%fd53, %fd24, %fd24;
	neg.f64 	%fd54, %fd53;
	fma.rn.f64 	%fd55, %fd24, %fd24, %fd54;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd43;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd43;
	}
	add.s32 	%r24, %r23, 1048576;
	mov.b64 	%fd56, {%r22, %r24};
	fma.rn.f64 	%fd57, %fd24, %fd56, %fd55;
	mul.rn.f64 	%fd58, %fd53, %fd24;
	neg.f64 	%fd59, %fd58;
	fma.rn.f64 	%fd60, %fd53, %fd24, %fd59;
	fma.rn.f64 	%fd61, %fd53, %fd43, %fd60;
	fma.rn.f64 	%fd62, %fd57, %fd24, %fd61;
	mul.rn.f64 	%fd63, %fd50, %fd58;
	neg.f64 	%fd64, %fd63;
	fma.rn.f64 	%fd65, %fd50, %fd58, %fd64;
	fma.rn.f64 	%fd66, %fd50, %fd62, %fd65;
	fma.rn.f64 	%fd67, %fd52, %fd58, %fd66;
	add.f64 	%fd68, %fd63, %fd67;
	sub.f64 	%fd69, %fd63, %fd68;
	add.f64 	%fd70, %fd67, %fd69;
	add.f64 	%fd71, %fd24, %fd68;
	sub.f64 	%fd72, %fd24, %fd71;
	add.f64 	%fd73, %fd68, %fd72;
	add.f64 	%fd74, %fd70, %fd73;
	add.f64 	%fd75, %fd43, %fd74;
	add.f64 	%fd76, %fd71, %fd75;
	sub.f64 	%fd77, %fd71, %fd76;
	add.f64 	%fd78, %fd75, %fd77;
	xor.b32  	%r25, %r52, -2147483648;
	mov.u32 	%r26, -2147483648;
	mov.u32 	%r27, 1127219200;
	mov.b64 	%fd79, {%r25, %r27};
	mov.b64 	%fd80, {%r26, %r27};
	sub.f64 	%fd81, %fd79, %fd80;
	mov.f64 	%fd82, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd83, %fd81, %fd82, %fd76;
	neg.f64 	%fd84, %fd81;
	fma.rn.f64 	%fd85, %fd84, %fd82, %fd83;
	sub.f64 	%fd86, %fd85, %fd76;
	sub.f64 	%fd87, %fd78, %fd86;
	mov.f64 	%fd88, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd89, %fd81, %fd88, %fd87;
	add.f64 	%fd90, %fd83, %fd89;
	sub.f64 	%fd91, %fd83, %fd90;
	add.f64 	%fd92, %fd89, %fd91;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd13;
	}
	shl.b32 	%r29, %r28, 1;
	setp.gt.u32 	%p3, %r29, -33554433;
	and.b32  	%r30, %r28, -15728641;
	selp.b32 	%r31, %r30, %r28, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd13;
	}
	mov.b64 	%fd93, {%r32, %r31};
	mul.rn.f64 	%fd94, %fd90, %fd93;
	neg.f64 	%fd95, %fd94;
	fma.rn.f64 	%fd96, %fd90, %fd93, %fd95;
	fma.rn.f64 	%fd97, %fd92, %fd93, %fd96;
	add.f64 	%fd4, %fd94, %fd97;
	sub.f64 	%fd98, %fd94, %fd4;
	add.f64 	%fd5, %fd97, %fd98;
	mov.f64 	%fd99, 0d4338000000000000;
	mov.f64 	%fd100, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd101, %fd4, %fd100, %fd99;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd101;
	}
	mov.f64 	%fd102, 0dC338000000000000;
	add.rn.f64 	%fd103, %fd101, %fd102;
	mov.f64 	%fd104, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd105, %fd103, %fd104, %fd4;
	mov.f64 	%fd106, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd107, %fd103, %fd106, %fd105;
	mov.f64 	%fd108, 0d3E928AF3FCA213EA;
	mov.f64 	%fd109, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd110, %fd109, %fd107, %fd108;
	mov.f64 	%fd111, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd112, %fd110, %fd107, %fd111;
	mov.f64 	%fd113, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd114, %fd112, %fd107, %fd113;
	mov.f64 	%fd115, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd116, %fd114, %fd107, %fd115;
	mov.f64 	%fd117, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd118, %fd116, %fd107, %fd117;
	mov.f64 	%fd119, 0d3F81111111122322;
	fma.rn.f64 	%fd120, %fd118, %fd107, %fd119;
	mov.f64 	%fd121, 0d3FA55555555502A1;
	fma.rn.f64 	%fd122, %fd120, %fd107, %fd121;
	mov.f64 	%fd123, 0d3FC5555555555511;
	fma.rn.f64 	%fd124, %fd122, %fd107, %fd123;
	mov.f64 	%fd125, 0d3FE000000000000B;
	fma.rn.f64 	%fd126, %fd124, %fd107, %fd125;
	fma.rn.f64 	%fd127, %fd126, %fd107, %fd16;
	fma.rn.f64 	%fd128, %fd127, %fd107, %fd16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd128;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd128;
	}
	shl.b32 	%r33, %r13, 20;
	add.s32 	%r34, %r15, %r33;
	mov.b64 	%fd136, {%r14, %r34};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd4;
	}
	mov.b32 	%f2, %r35;
	abs.f32 	%f1, %f2;
	setp.lt.f32 	%p4, %f1, 0f4086232B;
	@%p4 bra 	$L__BB44_7;

	setp.lt.f64 	%p5, %fd4, 0d0000000000000000;
	add.f64 	%fd129, %fd4, 0d7FF0000000000000;
	selp.f64 	%fd136, 0d0000000000000000, %fd129, %p5;
	setp.geu.f32 	%p6, %f1, 0f40874800;
	@%p6 bra 	$L__BB44_7;

	mov.f64 	%fd134, 0d4338000000000000;
	mov.f64 	%fd133, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd132, %fd4, %fd133, %fd134;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd132;
	}
	shr.u32 	%r36, %r48, 31;
	add.s32 	%r37, %r48, %r36;
	shr.s32 	%r38, %r37, 1;
	shl.b32 	%r39, %r38, 20;
	add.s32 	%r40, %r15, %r39;
	mov.b64 	%fd130, {%r14, %r40};
	sub.s32 	%r41, %r48, %r38;
	shl.b32 	%r42, %r41, 20;
	add.s32 	%r43, %r42, 1072693248;
	mov.u32 	%r44, 0;
	mov.b64 	%fd131, {%r44, %r43};
	mul.f64 	%fd136, %fd130, %fd131;

$L__BB44_7:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd136;
	}
	and.b32  	%r46, %r45, 2147483647;
	setp.eq.s32 	%p7, %r46, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd136;
	}
	setp.eq.s32 	%p8, %r47, 0;
	and.pred  	%p9, %p8, %p7;
	@%p9 bra 	$L__BB44_9;

	fma.rn.f64 	%fd136, %fd136, %fd5, %fd136;

$L__BB44_9:
	st.param.f64 	[func_retval0+0], %fd136;
	ret;

}

 