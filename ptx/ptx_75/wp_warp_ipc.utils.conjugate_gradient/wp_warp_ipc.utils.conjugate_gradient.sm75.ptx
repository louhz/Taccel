//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32345990
// Cuda compilation tools, release 12.1, V12.1.55
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_75
.address_size 64

	// .globl	axpy_cuda_kernel_forward
.const .align 4 .b8 pnanovdb_grid_type_value_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 1, 0, 0, 0, 32, 0, 0, 0, 4, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_table_strides_bits[108] = {64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_minmax_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_minmax_aligns_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_stat_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_leaf_type[108] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_constants[3024] = {28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 4, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 40, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 68, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 44, 32, 0, 0, 56, 32, 0, 0, 60, 32, 0, 0, 64, 32, 0, 0, 64, 32, 8, 0, 32, 4, 0, 0, 44, 4, 0, 0, 56, 4, 0, 0, 60, 4, 0, 0, 64, 4, 0, 0, 64, 4, 1, 0, 80, 0, 0, 0, 92, 0, 0, 0, 104, 0, 0, 0, 108, 0, 0, 0, 128, 0, 0, 0, 128, 24, 0, 0, 32, 0, 0, 0, 56, 0, 0, 0, 80, 0, 0, 0, 104, 0, 0, 0, 112, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 24, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 56, 32, 0, 0, 80, 32, 0, 0, 88, 32, 0, 0, 96, 32, 0, 0, 96, 32, 12, 0, 32, 4, 0, 0, 56, 4, 0, 0, 80, 4, 0, 0, 88, 4, 0, 0, 96, 4, 0, 0, 96, 132, 1, 0, 80, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 160, 48, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 1, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 160, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 1, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 2, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 44, 0, 0, 0, 60, 0, 0, 0, 76, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 68, 32, 0, 0, 96, 32, 0, 0, 96, 32, 8, 0, 32, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 68, 4, 0, 0, 96, 4, 0, 0, 96, 4, 1, 0, 80, 0, 0, 0, 96, 0, 0, 0, 112, 0, 0, 0, 116, 0, 0, 0, 128, 0, 0, 0, 128, 32, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 0, 1, 0, 0, 32, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 64, 32, 0, 0, 96, 32, 0, 0, 104, 32, 0, 0, 128, 32, 0, 0, 128, 32, 16, 0, 32, 4, 0, 0, 64, 4, 0, 0, 96, 4, 0, 0, 104, 4, 0, 0, 128, 4, 0, 0, 128, 4, 2, 0, 80, 0, 0, 0, 112, 0, 0, 0, 144, 0, 0, 0, 152, 0, 0, 0, 160, 0, 0, 0, 160, 64, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 31, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 35, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 35, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 83, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 6, 0, 0, 28, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 38, 32, 0, 0, 44, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 38, 4, 0, 0, 44, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 86, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 128, 12, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 81, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 2, 0, 0};
.const .align 4 .b8 pnanovdb_dither_lut[2048] = {70, 182, 19, 62, 172, 173, 36, 63, 175, 149, 84, 63, 42, 171, 169, 62, 33, 148, 215, 61, 175, 178, 26, 63, 21, 170, 43, 62, 176, 170, 42, 63, 193, 141, 100, 63, 44, 155, 201, 62, 36, 172, 167, 61, 170, 181, 20, 63, 180, 146, 90, 63, 51, 165, 181, 62, 181, 138, 106, 63, 54, 149, 213, 62, 171, 177, 28, 63, 0, 140, 231, 61, 175, 153, 76, 63, 41, 179, 153, 62, 157, 190, 2, 63, 41, 160, 63, 60, 182, 134, 114, 63, 55, 141, 229, 62, 43, 163, 185, 62, 176, 145, 92, 63, 62, 152, 79, 61, 170, 185, 12, 63, 48, 189, 133, 62, 178, 158, 66, 63, 23, 154, 75, 62, 177, 166, 50, 63, 44, 184, 15, 62, 37, 174, 35, 63, 36, 182, 19, 63, 39, 176, 159, 61, 31, 189, 5, 63, 41, 160, 191, 60, 59, 138, 235, 62, 56, 133, 117, 63, 59, 142, 99, 63, 65, 156, 199, 62, 29, 172, 167, 62, 58, 150, 83, 63, 19, 186, 139, 62, 52, 157, 69, 63, 51, 165, 53, 63, 33, 148, 87, 62, 69, 132, 247, 62, 61, 130, 123, 63, 28, 180, 151, 62, 57, 154, 75, 63, 51, 136, 239, 61, 33, 177, 29, 63, 57, 144, 95, 61, 31, 185, 13, 63, 39, 162, 59, 63, 51, 136, 111, 62, 35, 186, 11, 63, 41, 160, 63, 61, 54, 145, 93, 63, 56, 162, 187, 62, 53, 153, 77, 63, 53, 178, 155, 62, 169, 189, 4, 63, 52, 176, 159, 60, 47, 139, 233, 62, 194, 133, 116, 63, 177, 162, 58, 63, 26, 138, 107, 62, 157, 186, 10, 63, 47, 168, 47, 61, 40, 187, 137, 62, 174, 157, 68, 63, 173, 165, 52, 63, 74, 150, 83, 62, 56, 133, 245, 62, 199, 130, 122, 63, 49, 181, 149, 62, 179, 154, 74, 63, 77, 134, 115, 62, 173, 161, 60, 63, 45, 147, 217, 62, 194, 137, 108, 63, 19, 186, 11, 62, 176, 174, 34, 63, 50, 173, 165, 62, 179, 150, 82, 63, 195, 129, 124, 63, 48, 131, 249, 62, 172, 169, 44, 63, 72, 166, 51, 62, 180, 142, 98, 63, 52, 157, 197, 62, 158, 182, 18, 63, 41, 180, 151, 61, 35, 190, 3, 63, 19, 128, 127, 60, 49, 152, 79, 62, 38, 166, 51, 63, 28, 180, 23, 62, 50, 173, 37, 63, 54, 149, 85, 63, 55, 170, 171, 62, 27, 188, 135, 62, 56, 158, 67, 63, 60, 134, 115, 63, 67, 140, 231, 62, 55, 141, 101, 63, 57, 154, 203, 62, 47, 168, 175, 61, 32, 181, 21, 63, 58, 146, 91, 63, 30, 164, 183, 62, 59, 138, 107, 63, 66, 148, 215, 62, 52, 161, 61, 63, 35, 132, 119, 62, 51, 169, 45, 63, 30, 164, 55, 62, 84, 144, 223, 61, 37, 178, 27, 63, 47, 168, 47, 62, 38, 170, 43, 63, 61, 130, 251, 62, 56, 129, 125, 63, 58, 146, 219, 62, 55, 137, 109, 63, 34, 188, 135, 61, 162, 183, 16, 63, 163, 175, 32, 63, 35, 190, 3, 62, 56, 162, 59, 62, 168, 168, 46, 63, 169, 160, 62, 63, 61, 130, 123, 62, 183, 151, 80, 63, 25, 175, 161, 62, 60, 159, 193, 62, 184, 143, 96, 63, 190, 136, 110, 63, 71, 145, 221, 62, 73, 129, 253, 62, 191, 128, 126, 63, 31, 184, 15, 61, 161, 187, 8, 63, 186, 131, 120, 63, 64, 135, 241, 62, 58, 146, 91, 62, 169, 164, 54, 63, 165, 188, 6, 63, 30, 144, 223, 60, 183, 155, 72, 63, 23, 183, 145, 62, 42, 142, 99, 62, 181, 163, 56, 63, 190, 132, 118, 63, 72, 137, 237, 62, 31, 185, 141, 62, 170, 156, 70, 63, 69, 132, 119, 63, 51, 136, 239, 62, 49, 152, 207, 62, 51, 140, 103, 63, 31, 184, 143, 61, 40, 183, 17, 63, 63, 143, 97, 63, 40, 158, 195, 62, 84, 144, 95, 62, 47, 164, 55, 63, 46, 172, 39, 63, 12, 176, 31, 62, 45, 151, 81, 63, 37, 174, 163, 62, 60, 188, 7, 62, 41, 175, 33, 63, 73, 128, 127, 61, 44, 184, 15, 63, 48, 160, 63, 63, 86, 128, 127, 62, 63, 139, 105, 63, 41, 150, 211, 62, 65, 131, 121, 63, 77, 134, 243, 62, 49, 152, 79, 63, 45, 176, 159, 62, 52, 128, 255, 62, 69, 128, 127, 63, 63, 172, 39, 62, 42, 171, 41, 63, 67, 140, 103, 62, 43, 163, 57, 63, 164, 167, 48, 63, 40, 158, 67, 62, 83, 128, 127, 59, 161, 191, 0, 63, 166, 184, 14, 63, 51, 136, 111, 61, 102, 132, 247, 61, 167, 176, 30, 63, 63, 143, 225, 62, 186, 135, 112, 63, 182, 159, 64, 63, 22, 191, 129, 62, 33, 177, 157, 62, 187, 152, 78, 63, 188, 144, 94, 63, 35, 161, 189, 62, 164, 171, 40, 63, 37, 174, 35, 62, 59, 167, 177, 62, 184, 147, 88, 63, 166, 180, 22, 63, 44, 164, 183, 61, 53, 178, 27, 62, 168, 172, 38, 63, 62, 151, 209, 62, 185, 139, 104, 63, 162, 179, 24, 63, 52, 156, 199, 61, 34, 169, 173, 62, 188, 148, 86, 63, 189, 140, 102, 63, 36, 153, 205, 62, 47, 168, 175, 62, 49, 148, 87, 63, 48, 156, 71, 63, 44, 184, 143, 62, 42, 167, 49, 63, 65, 156, 71, 62, 35, 190, 131, 62, 44, 159, 65, 63, 45, 180, 23, 63, 54, 160, 191, 61, 73, 128, 255, 60, 27, 188, 7, 63, 42, 142, 227, 62, 64, 135, 113, 63, 39, 191, 1, 63, 62, 128, 255, 59, 47, 168, 47, 63, 81, 160, 63, 62, 19, 128, 255, 61, 45, 176, 31, 63, 36, 182, 147, 62, 44, 155, 73, 63, 38, 166, 179, 62, 62, 147, 89, 63, 50, 144, 223, 62, 68, 136, 111, 63, 50, 144, 95, 63, 48, 160, 191, 62, 40, 187, 9, 63, 52, 176, 31, 61, 41, 179, 25, 63, 116, 152, 207, 61, 227, 54, 18, 63, 43, 182, 147, 61, 247, 30, 66, 63, 152, 189, 132, 62, 234, 35, 56, 63, 63, 143, 97, 62, 230, 59, 8, 63, 34, 188, 7, 61, 155, 173, 164, 62, 248, 22, 82, 63, 41, 176, 31, 60, 226, 62, 2, 63, 202, 135, 240, 62, 255, 3, 120, 63, 162, 183, 144, 62, 235, 27, 72, 63, 226, 58, 10, 63, 49, 172, 39, 61, 247, 34, 58, 63, 47, 139, 105, 62, 230, 63, 0, 63, 83, 128, 255, 58, 231, 55, 16, 63, 35, 190, 131, 61, 154, 181, 148, 62, 248, 26, 74, 63, 194, 133, 244, 62, 251, 2, 122, 63, 161, 191, 128, 62, 235, 31, 64, 63, 163, 175, 160, 62, 236, 23, 80, 63, 116, 7, 113, 63, 180, 142, 226, 62, 115, 15, 97, 63, 178, 158, 194, 62, 186, 160, 190, 62, 119, 16, 95, 63, 184, 176, 158, 62, 118, 24, 79, 63, 19, 157, 69, 62, 112, 39, 49, 63, 14, 189, 5, 62, 111, 47, 33, 63, 98, 48, 31, 63, 61, 130, 251, 61, 97, 56, 15, 63, 75, 132, 119, 61, 111, 43, 41, 63, 16, 173, 37, 62, 112, 35, 57, 63, 88, 141, 101, 62, 187, 152, 206, 62, 120, 12, 103, 63, 119, 20, 87, 63, 185, 168, 174, 62, 179, 150, 210, 62, 116, 11, 105, 63, 182, 134, 242, 62, 134, 3, 121, 63, 98, 44, 39, 63, 33, 177, 29, 62, 42, 162, 187, 61, 97, 52, 23, 63, 44, 155, 73, 62, 229, 38, 50, 63, 191, 157, 196, 62, 250, 14, 98, 63, 53, 158, 195, 61, 232, 51, 24, 63, 58, 175, 33, 62, 233, 43, 40, 63, 251, 6, 114, 63, 193, 141, 228, 62, 228, 46, 34, 63, 40, 187, 9, 62, 253, 19, 88, 63, 164, 167, 176, 62, 254, 11, 104, 63, 166, 151, 208, 62, 42, 171, 41, 62, 229, 42, 42, 63, 74, 150, 211, 61, 228, 50, 26, 63, 56, 191, 1, 62, 232, 47, 32, 63, 60, 159, 65, 62, 233, 39, 48, 63, 250, 10, 106, 63, 192, 149, 212, 62, 249, 18, 90, 63, 190, 165, 180, 62, 254, 15, 96, 63, 165, 159, 192, 62, 255, 7, 112, 63, 168, 143, 224, 62, 176, 174, 162, 62, 114, 23, 81, 63, 173, 190, 130, 62, 113, 31, 65, 63, 122, 0, 127, 63, 191, 128, 254, 62, 120, 8, 111, 63, 188, 144, 222, 62, 93, 55, 17, 63, 32, 186, 139, 61, 91, 63, 1, 63, 41, 160, 191, 59, 40, 129, 125, 62, 117, 32, 63, 63, 35, 161, 61, 62, 116, 40, 47, 63, 28, 180, 23, 61, 92, 59, 9, 63, 50, 154, 203, 61, 110, 51, 25, 63, 118, 28, 71, 63, 149, 184, 142, 62, 190, 136, 238, 62, 121, 4, 119, 63, 113, 27, 73, 63, 174, 182, 146, 62, 115, 19, 89, 63, 177, 166, 178, 62, 78, 136, 239, 60, 96, 60, 7, 63, 116, 36, 55, 63, 37, 145, 93, 62, 175, 153, 204, 62, 2, 13, 102, 63, 3, 5, 118, 63, 177, 137, 236, 62, 38, 156, 71, 61, 222, 57, 12, 63, 42, 142, 227, 61, 223, 49, 28, 63, 237, 44, 38, 63, 7, 179, 25, 62, 79, 147, 89, 62, 238, 36, 54, 63, 244, 25, 76, 63, 179, 179, 152, 62, 245, 17, 92, 63, 181, 163, 184, 62, 10, 134, 243, 61, 236, 48, 30, 63, 54, 140, 103, 61, 235, 56, 14, 63, 180, 171, 168, 62, 244, 21, 84, 63, 222, 61, 4, 63, 58, 184, 143, 60, 241, 16, 94, 63, 173, 161, 188, 62, 240, 24, 78, 63, 171, 177, 156, 62, 223, 53, 20, 63, 37, 174, 163, 61, 178, 187, 136, 62, 243, 29, 68, 63, 157, 186, 138, 62, 105, 29, 69, 63, 104, 37, 53, 63, 54, 149, 85, 62, 107, 42, 43, 63, 67, 169, 45, 62, 106, 50, 27, 63, 247, 145, 219, 61, 100, 61, 5, 63, 47, 168, 175, 60, 198, 138, 234, 62, 125, 5, 117, 63, 171, 148, 214, 62, 129, 10, 107, 63, 169, 164, 182, 62, 127, 18, 91, 63, 126, 1, 125, 63, 199, 130, 250, 62, 197, 146, 218, 62, 125, 9, 109, 63, 172, 140, 230, 62, 129, 6, 115, 63, 104, 62, 3, 63, 30, 144, 95, 60, 56, 133, 117, 62, 104, 33, 61, 63, 103, 41, 45, 63, 51, 165, 53, 62, 108, 38, 51, 63, 70, 153, 77, 62, 165, 188, 134, 62, 109, 30, 67, 63, 239, 28, 70, 63, 170, 185, 140, 62, 172, 169, 172, 62, 240, 20, 86, 63, 241, 41, 44, 63, 26, 167, 49, 62, 243, 33, 60, 63, 30, 135, 113, 62, 35, 152, 207, 60, 218, 60, 6, 63, 236, 52, 22, 63, 45, 166, 179, 61, 184, 147, 216, 62, 246, 9, 108, 63, 186, 131, 248, 62, 247, 1, 124, 63, 239, 32, 62, 63, 81, 131, 121, 62, 237, 40, 46, 63, 77, 163, 57, 62, 247, 5, 116, 63, 185, 139, 232, 62, 23, 183, 17, 62, 241, 45, 36, 63, 178, 129, 252, 62, 4, 1, 126, 63, 176, 145, 220, 62, 3, 9, 110, 63, 28, 151, 81, 62, 242, 37, 52, 63, 246, 13, 100, 63, 183, 155, 200, 62, 124, 13, 101, 63, 195, 154, 202, 62, 48, 170, 171, 61, 101, 53, 21, 63, 44, 164, 55, 61, 105, 58, 11, 63, 72, 137, 109, 62, 108, 34, 59, 63, 49, 181, 21, 62, 102, 45, 37, 63, 123, 21, 85, 63, 159, 170, 170, 62, 109, 26, 75, 63, 166, 180, 150, 62, 130, 2, 123, 63, 173, 132, 246, 62, 161, 162, 186, 62, 123, 17, 93, 63, 122, 25, 77, 63, 158, 178, 154, 62, 110, 22, 83, 63, 168, 172, 166, 62, 65, 185, 13, 62, 106, 46, 35, 63, 102, 49, 29, 63, 93, 138, 235, 61, 60, 148, 87, 61, 101, 57, 13, 63, 40, 178, 155, 61, 105, 54, 19, 63, 128, 14, 99, 63, 170, 156, 198, 62};
.global .align 8 .f64 _ZN2wp11_svd_configIdE17QR_GIVENS_EPSILONE = 0d3D719799812DEA11;
.global .align 4 .u32 _ZN2wp11_svd_configIdE17JACOBI_ITERATIONSE = 8;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIiE9GRID_TYPEE = 4;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIxE9GRID_TYPEE = 5;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIjE9GRID_TYPEE = 10;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIfE9GRID_TYPEE = 1;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIdE9GRID_TYPEE = 2;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EfEEE9GRID_TYPEE = 6;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EdEEE9GRID_TYPEE = 7;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EfEEE9GRID_TYPEE = 17;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EdEEE9GRID_TYPEE = 18;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9all_hostsE = 1;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_35_bitE = 2;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_37_bitE = 4;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_50_bitE = 8;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_52_bitE = 16;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_53_bitE = 32;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_60_bitE = 64;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_61_bitE = 128;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_62_bitE = 256;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_70_bitE = 512;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_72_bitE = 1024;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_75_bitE = 2048;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_80_bitE = 4096;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_86_bitE = 8192;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_87_bitE = 16384;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_89_bitE = 32768;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_90_bitE = 65536;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail11all_devicesE = 131070;
.global .align 8 .b8 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target7is_hostE[8] = {1, 0, 0, 0, 0, 0, 0, 0};
.global .align 8 .b8 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target9is_deviceE[8] = {254, 255, 1, 0, 0, 0, 0, 0};
.global .align 8 .b8 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target10any_targetE[8] = {255, 255, 1, 0, 0, 0, 0, 0};
.global .align 8 .b8 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target9no_targetE[8];
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_35E = 35;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_37E = 37;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_50E = 50;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_52E = 52;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_53E = 53;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_60E = 60;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_61E = 61;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_62E = 62;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_70E = 70;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_72E = 72;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_75E = 75;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_80E = 80;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_86E = 86;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_87E = 87;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_89E = 89;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_90E = 90;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp6volume7CLOSESTE;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp6volume6LINEARE = 1;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp15LAUNCH_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp14ARRAY_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp18ARRAY_TYPE_REGULARE;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp18ARRAY_TYPE_INDEXEDE = 1;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp17ARRAY_TYPE_FABRICE = 2;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp25ARRAY_TYPE_FABRIC_INDEXEDE = 3;

.visible .entry axpy_cuda_kernel_forward(
	.param .align 8 .b8 axpy_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 axpy_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 axpy_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 axpy_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 axpy_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 axpy_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 axpy_cuda_kernel_forward_param_6[56],
	.param .u32 axpy_cuda_kernel_forward_param_7,
	.param .u32 axpy_cuda_kernel_forward_param_8,
	.param .u32 axpy_cuda_kernel_forward_param_9
)
{
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<49>;
	.reg .b32 	%r<150>;
	.reg .f64 	%fd<51>;
	.reg .b64 	%rd<110>;


	ld.param.v2.u32 	{%r69, %r70}, [axpy_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r71, %r72}, [axpy_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r77, %r78}, [axpy_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r85, %r86}, [axpy_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r93, %r94}, [axpy_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r101, %r102}, [axpy_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r109, %r110}, [axpy_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r117, %r118}, [axpy_cuda_kernel_forward_param_6+32];
	ld.param.u32 	%r66, [axpy_cuda_kernel_forward_param_7];
	ld.param.u32 	%r67, [axpy_cuda_kernel_forward_param_8];
	ld.param.u32 	%r68, [axpy_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd50, [axpy_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd48, [axpy_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd46, [axpy_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd44, [axpy_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd42, [axpy_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd40, [axpy_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd39, [axpy_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r11, [axpy_cuda_kernel_forward_param_0+16];
	mov.u32 	%r121, %ntid.x;
	cvt.u64.u32 	%rd1, %r121;
	mov.u32 	%r122, %ctaid.x;
	mul.wide.u32 	%rd52, %r121, %r122;
	mov.u32 	%r123, %tid.x;
	cvt.u64.u32 	%rd53, %r123;
	add.s64 	%rd103, %rd52, %rd53;
	setp.ge.u64 	%p1, %rd103, %rd39;
	@%p1 bra 	$L__BB0_37;

	cvta.to.global.u64 	%rd4, %rd44;
	cvta.to.global.u64 	%rd5, %rd50;
	cvta.to.global.u64 	%rd6, %rd48;
	cvta.to.global.u64 	%rd7, %rd46;
	cvta.to.global.u64 	%rd8, %rd42;
	cvta.to.global.u64 	%rd9, %rd40;
	cvt.s64.s32 	%rd10, %r72;
	cvt.s64.s32 	%rd11, %r71;
	cvt.s64.s32 	%rd12, %r70;
	mul.hi.s32 	%r124, %r67, 1431655766;
	shr.u32 	%r125, %r124, 31;
	add.s32 	%r2, %r124, %r125;
	mov.u32 	%r126, %nctaid.x;
	cvt.u64.u32 	%rd54, %r126;
	mul.lo.s64 	%rd13, %rd1, %rd54;
	cvt.s64.s32 	%rd14, %r109;
	cvt.s64.s32 	%rd15, %r93;
	cvt.s64.s32 	%rd16, %r77;
	cvt.s64.s32 	%rd17, %r85;
	cvt.s64.s32 	%rd18, %r101;
	mul.hi.s32 	%r127, %r66, 1431655766;
	shr.u32 	%r128, %r127, 31;
	add.s32 	%r3, %r127, %r128;
	sub.s32 	%r4, %r68, %r3;
	cvt.s64.s32 	%rd19, %r117;
	setp.gt.s32 	%p2, %r11, 3;
	@%p2 bra 	$L__BB0_18;
	bra.uni 	$L__BB0_2;

$L__BB0_18:

$L__BB0_19:
	or.b64  	%rd78, %rd103, %rd10;
	and.b64  	%rd79, %rd78, -4294967296;
	setp.eq.s64 	%p13, %rd79, 0;
	@%p13 bra 	$L__BB0_21;

	div.u64 	%rd108, %rd103, %rd10;
	bra.uni 	$L__BB0_22;

$L__BB0_21:
	cvt.u32.u64 	%r138, %rd10;
	cvt.u32.u64 	%r139, %rd103;
	div.u32 	%r140, %r139, %r138;
	cvt.u64.u32 	%rd108, %r140;

$L__BB0_22:
	setp.lt.s32 	%p14, %r11, 3;
	@%p14 bra 	$L__BB0_26;

	or.b64  	%rd80, %rd108, %rd11;
	and.b64  	%rd81, %rd80, -4294967296;
	setp.eq.s64 	%p15, %rd81, 0;
	@%p15 bra 	$L__BB0_25;

	div.u64 	%rd108, %rd108, %rd11;
	bra.uni 	$L__BB0_26;

$L__BB0_25:
	cvt.u32.u64 	%r141, %rd11;
	cvt.u32.u64 	%r142, %rd108;
	div.u32 	%r143, %r142, %r141;
	cvt.u64.u32 	%rd108, %r143;

$L__BB0_26:
	setp.lt.s32 	%p16, %r11, 2;
	@%p16 bra 	$L__BB0_30;

	or.b64  	%rd82, %rd108, %rd12;
	and.b64  	%rd83, %rd82, -4294967296;
	setp.eq.s64 	%p17, %rd83, 0;
	@%p17 bra 	$L__BB0_29;

	div.u64 	%rd108, %rd108, %rd12;
	bra.uni 	$L__BB0_30;

$L__BB0_29:
	cvt.u32.u64 	%r144, %rd12;
	cvt.u32.u64 	%r145, %rd108;
	div.u32 	%r146, %r145, %r144;
	cvt.u64.u32 	%rd108, %r146;

$L__BB0_30:
	cvt.u32.u64 	%r147, %rd108;
	setp.gt.s32 	%p18, %r11, 0;
	selp.b32 	%r6, %r147, 0, %p18;
	setp.lt.s32 	%p19, %r6, %r2;
	@%p19 bra 	$L__BB0_36;

	setp.ge.s32 	%p20, %r6, %r3;
	@%p20 bra 	$L__BB0_33;

	sub.s32 	%r148, %r6, %r2;
	cvt.s64.s32 	%rd84, %r148;
	mul.lo.s64 	%rd85, %rd84, %rd14;
	add.s64 	%rd86, %rd6, %rd85;
	ld.global.s32 	%rd87, [%rd86];
	mul.lo.s64 	%rd88, %rd87, %rd15;
	add.s64 	%rd89, %rd4, %rd88;
	ld.global.f64 	%fd49, [%rd89];

$L__BB0_33:
	setp.lt.s32 	%p21, %r6, %r3;
	@%p21 bra 	$L__BB0_35;

	add.s32 	%r149, %r4, %r6;
	cvt.s64.s32 	%rd90, %r149;
	mul.lo.s64 	%rd91, %rd90, %rd19;
	add.s64 	%rd92, %rd5, %rd91;
	ld.global.s32 	%rd93, [%rd92];
	mul.lo.s64 	%rd94, %rd93, %rd15;
	add.s64 	%rd95, %rd4, %rd94;
	ld.global.f64 	%fd50, [%rd95];

$L__BB0_35:
	selp.f64 	%fd29, %fd49, %fd50, %p21;
	cvt.s64.s32 	%rd96, %r6;
	mul.lo.s64 	%rd97, %rd96, %rd16;
	add.s64 	%rd98, %rd9, %rd97;
	mul.lo.s64 	%rd99, %rd96, %rd17;
	add.s64 	%rd100, %rd8, %rd99;
	ld.global.f64 	%fd30, [%rd100];
	ld.global.f64 	%fd31, [%rd100+8];
	ld.global.f64 	%fd32, [%rd100+16];
	ld.global.f64 	%fd33, [%rd98];
	fma.rn.f64 	%fd34, %fd30, %fd29, %fd33;
	ld.global.f64 	%fd35, [%rd98+8];
	fma.rn.f64 	%fd36, %fd29, %fd31, %fd35;
	ld.global.f64 	%fd37, [%rd98+16];
	fma.rn.f64 	%fd38, %fd29, %fd32, %fd37;
	mul.lo.s64 	%rd101, %rd96, %rd18;
	add.s64 	%rd102, %rd7, %rd101;
	st.global.f64 	[%rd102], %fd34;
	st.global.f64 	[%rd102+8], %fd36;
	st.global.f64 	[%rd102+16], %fd38;

$L__BB0_36:
	add.s64 	%rd103, %rd103, %rd13;
	setp.lt.u64 	%p23, %rd103, %rd39;
	@%p23 bra 	$L__BB0_19;
	bra.uni 	$L__BB0_37;

$L__BB0_2:

$L__BB0_3:
	setp.lt.s32 	%p3, %r11, 3;
	mov.u64 	%rd104, %rd103;
	@%p3 bra 	$L__BB0_7;

	or.b64  	%rd55, %rd103, %rd11;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p4, %rd56, 0;
	@%p4 bra 	$L__BB0_6;

	div.u64 	%rd104, %rd103, %rd11;
	bra.uni 	$L__BB0_7;

$L__BB0_6:
	cvt.u32.u64 	%r129, %rd11;
	cvt.u32.u64 	%r130, %rd103;
	div.u32 	%r131, %r130, %r129;
	cvt.u64.u32 	%rd104, %r131;

$L__BB0_7:
	setp.lt.s32 	%p5, %r11, 2;
	@%p5 bra 	$L__BB0_11;

	or.b64  	%rd57, %rd104, %rd12;
	and.b64  	%rd58, %rd57, -4294967296;
	setp.eq.s64 	%p6, %rd58, 0;
	@%p6 bra 	$L__BB0_10;

	div.u64 	%rd104, %rd104, %rd12;
	bra.uni 	$L__BB0_11;

$L__BB0_10:
	cvt.u32.u64 	%r132, %rd12;
	cvt.u32.u64 	%r133, %rd104;
	div.u32 	%r134, %r133, %r132;
	cvt.u64.u32 	%rd104, %r134;

$L__BB0_11:
	cvt.u32.u64 	%r135, %rd104;
	setp.gt.s32 	%p7, %r11, 0;
	selp.b32 	%r5, %r135, 0, %p7;
	setp.lt.s32 	%p8, %r5, %r2;
	@%p8 bra 	$L__BB0_17;

	setp.ge.s32 	%p9, %r5, %r3;
	@%p9 bra 	$L__BB0_14;

	sub.s32 	%r136, %r5, %r2;
	cvt.s64.s32 	%rd59, %r136;
	mul.lo.s64 	%rd60, %rd59, %rd14;
	add.s64 	%rd61, %rd6, %rd60;
	ld.global.s32 	%rd62, [%rd61];
	mul.lo.s64 	%rd63, %rd62, %rd15;
	add.s64 	%rd64, %rd4, %rd63;
	ld.global.f64 	%fd43, [%rd64];

$L__BB0_14:
	setp.lt.s32 	%p10, %r5, %r3;
	@%p10 bra 	$L__BB0_16;

	add.s32 	%r137, %r4, %r5;
	cvt.s64.s32 	%rd65, %r137;
	mul.lo.s64 	%rd66, %rd65, %rd19;
	add.s64 	%rd67, %rd5, %rd66;
	ld.global.s32 	%rd68, [%rd67];
	mul.lo.s64 	%rd69, %rd68, %rd15;
	add.s64 	%rd70, %rd4, %rd69;
	ld.global.f64 	%fd44, [%rd70];

$L__BB0_16:
	selp.f64 	%fd18, %fd43, %fd44, %p10;
	cvt.s64.s32 	%rd71, %r5;
	mul.lo.s64 	%rd72, %rd71, %rd16;
	add.s64 	%rd73, %rd9, %rd72;
	mul.lo.s64 	%rd74, %rd71, %rd17;
	add.s64 	%rd75, %rd8, %rd74;
	ld.global.f64 	%fd19, [%rd75];
	ld.global.f64 	%fd20, [%rd75+8];
	ld.global.f64 	%fd21, [%rd75+16];
	ld.global.f64 	%fd22, [%rd73];
	fma.rn.f64 	%fd23, %fd19, %fd18, %fd22;
	ld.global.f64 	%fd24, [%rd73+8];
	fma.rn.f64 	%fd25, %fd18, %fd20, %fd24;
	ld.global.f64 	%fd26, [%rd73+16];
	fma.rn.f64 	%fd27, %fd18, %fd21, %fd26;
	mul.lo.s64 	%rd76, %rd71, %rd18;
	add.s64 	%rd77, %rd7, %rd76;
	st.global.f64 	[%rd77], %fd23;
	st.global.f64 	[%rd77+8], %fd25;
	st.global.f64 	[%rd77+16], %fd27;

$L__BB0_17:
	add.s64 	%rd103, %rd103, %rd13;
	setp.lt.u64 	%p12, %rd103, %rd39;
	@%p12 bra 	$L__BB0_3;

$L__BB0_37:
	ret;

}
	// .globl	axpy_cuda_kernel_backward
.visible .entry axpy_cuda_kernel_backward(
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_6[56],
	.param .u32 axpy_cuda_kernel_backward_param_7,
	.param .u32 axpy_cuda_kernel_backward_param_8,
	.param .u32 axpy_cuda_kernel_backward_param_9,
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_12[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_14[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_15[56],
	.param .u32 axpy_cuda_kernel_backward_param_16,
	.param .u32 axpy_cuda_kernel_backward_param_17,
	.param .u32 axpy_cuda_kernel_backward_param_18
)
{
	.reg .pred 	%p<29>;
	.reg .b16 	%rs<81>;
	.reg .b32 	%r<221>;
	.reg .f64 	%fd<83>;
	.reg .b64 	%rd<134>;


	ld.param.v2.u32 	{%r107, %r108}, [axpy_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r109, %r110}, [axpy_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r115, %r116}, [axpy_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r123, %r124}, [axpy_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r131, %r132}, [axpy_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r139, %r140}, [axpy_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r147, %r148}, [axpy_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r155, %r156}, [axpy_cuda_kernel_backward_param_6+32];
	ld.param.u32 	%r68, [axpy_cuda_kernel_backward_param_7];
	ld.param.u32 	%r69, [axpy_cuda_kernel_backward_param_8];
	ld.param.v2.u32 	{%r163, %r164}, [axpy_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r171, %r172}, [axpy_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r179, %r180}, [axpy_cuda_kernel_backward_param_12+32];
	ld.param.v2.u32 	{%r187, %r188}, [axpy_cuda_kernel_backward_param_13+32];
	ld.param.u64 	%rd68, [axpy_cuda_kernel_backward_param_13];
	ld.param.u64 	%rd66, [axpy_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd64, [axpy_cuda_kernel_backward_param_11];
	ld.param.u64 	%rd62, [axpy_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd60, [axpy_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd58, [axpy_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd57, [axpy_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd55, [axpy_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd54, [axpy_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd53, [axpy_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd52, [axpy_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd51, [axpy_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd49, [axpy_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r13, [axpy_cuda_kernel_backward_param_0+16];
	mov.u32 	%r191, %ntid.x;
	cvt.u64.u32 	%rd1, %r191;
	mov.u32 	%r192, %ctaid.x;
	mul.wide.u32 	%rd70, %r191, %r192;
	mov.u32 	%r193, %tid.x;
	cvt.u64.u32 	%rd71, %r193;
	add.s64 	%rd127, %rd70, %rd71;
	setp.ge.u64 	%p1, %rd127, %rd49;
	@%p1 bra 	$L__BB1_42;

	cvta.to.global.u64 	%rd12, %rd54;
	cvta.to.global.u64 	%rd13, %rd68;
	cvta.to.global.u64 	%rd14, %rd60;
	cvta.to.global.u64 	%rd15, %rd58;
	cvta.to.global.u64 	%rd16, %rd57;
	cvta.to.global.u64 	%rd17, %rd52;
	cvt.s64.s32 	%rd18, %r110;
	cvt.s64.s32 	%rd19, %r109;
	cvt.s64.s32 	%rd20, %r108;
	mul.hi.s32 	%r195, %r69, 1431655766;
	shr.u32 	%r196, %r195, 31;
	add.s32 	%r2, %r195, %r196;
	mov.u32 	%r197, %nctaid.x;
	cvt.u64.u32 	%rd73, %r197;
	mul.lo.s64 	%rd21, %rd1, %rd73;
	mul.hi.s32 	%r198, %r68, 1431655766;
	shr.u32 	%r199, %r198, 31;
	add.s32 	%r3, %r198, %r199;
	cvt.s64.s32 	%rd22, %r147;
	cvt.s64.s32 	%rd23, %r131;
	cvt.s64.s32 	%rd24, %r123;
	cvt.s64.s32 	%rd25, %r155;
	cvt.s64.s32 	%rd26, %r187;
	cvt.s64.s32 	%rd27, %r139;
	cvt.s64.s32 	%rd28, %r171;
	cvt.s64.s32 	%rd29, %r163;
	cvt.s64.s32 	%rd30, %r115;
	cvt.s64.s32 	%rd31, %r179;

$L__BB1_2:
	setp.lt.s32 	%p2, %r13, 4;
	mov.u64 	%rd129, %rd127;
	@%p2 bra 	$L__BB1_6;

	or.b64  	%rd74, %rd127, %rd18;
	and.b64  	%rd75, %rd74, -4294967296;
	setp.eq.s64 	%p3, %rd75, 0;
	@%p3 bra 	$L__BB1_5;

	div.u64 	%rd129, %rd127, %rd18;
	bra.uni 	$L__BB1_6;

$L__BB1_5:
	cvt.u32.u64 	%r200, %rd18;
	cvt.u32.u64 	%r201, %rd127;
	div.u32 	%r202, %r201, %r200;
	cvt.u64.u32 	%rd129, %r202;

$L__BB1_6:
	setp.lt.s32 	%p4, %r13, 3;
	@%p4 bra 	$L__BB1_10;

	or.b64  	%rd76, %rd129, %rd19;
	and.b64  	%rd77, %rd76, -4294967296;
	setp.eq.s64 	%p5, %rd77, 0;
	@%p5 bra 	$L__BB1_9;

	div.u64 	%rd129, %rd129, %rd19;
	bra.uni 	$L__BB1_10;

$L__BB1_9:
	cvt.u32.u64 	%r203, %rd19;
	cvt.u32.u64 	%r204, %rd129;
	div.u32 	%r205, %r204, %r203;
	cvt.u64.u32 	%rd129, %r205;

$L__BB1_10:
	setp.lt.s32 	%p6, %r13, 2;
	@%p6 bra 	$L__BB1_14;

	or.b64  	%rd78, %rd129, %rd20;
	and.b64  	%rd79, %rd78, -4294967296;
	setp.eq.s64 	%p7, %rd79, 0;
	@%p7 bra 	$L__BB1_13;

	div.u64 	%rd129, %rd129, %rd20;
	bra.uni 	$L__BB1_14;

$L__BB1_13:
	cvt.u32.u64 	%r206, %rd20;
	cvt.u32.u64 	%r207, %rd129;
	div.u32 	%r208, %r207, %r206;
	cvt.u64.u32 	%rd129, %r208;

$L__BB1_14:
	cvt.u32.u64 	%r209, %rd129;
	setp.gt.s32 	%p8, %r13, 0;
	selp.b32 	%r5, %r209, 0, %p8;
	setp.lt.s32 	%p9, %r5, %r2;
	@%p9 bra 	$L__BB1_41;

	setp.gt.s32 	%p28, %r13, 0;
	cvt.u32.u64 	%r216, %rd129;
	selp.b32 	%r215, %r216, 0, %p28;
	setp.ge.s32 	%p10, %r215, %r3;
	@%p10 bra 	$L__BB1_17;

	sub.s32 	%r210, %r5, %r2;
	cvt.s64.s32 	%rd80, %r210;
	mul.lo.s64 	%rd81, %rd80, %rd22;
	add.s64 	%rd82, %rd15, %rd81;
	ld.global.u64 	%rd133, [%rd82];
	cvt.s64.s32 	%rd83, %rd133;
	mul.lo.s64 	%rd84, %rd83, %rd23;
	add.s64 	%rd85, %rd12, %rd84;
	ld.global.f64 	%fd82, [%rd85];

$L__BB1_17:
	setp.lt.s32 	%p11, %r5, %r3;
	@%p11 bra 	$L__BB1_19;

	ld.param.u32 	%r217, [axpy_cuda_kernel_backward_param_9];
	sub.s32 	%r211, %r5, %r3;
	add.s32 	%r212, %r211, %r217;
	cvt.s64.s32 	%rd86, %r212;
	mul.lo.s64 	%rd87, %rd86, %rd25;
	add.s64 	%rd88, %rd14, %rd87;
	ld.global.u32 	%r220, [%rd88];
	cvt.s64.s32 	%rd89, %r220;
	mul.lo.s64 	%rd90, %rd89, %rd23;
	add.s64 	%rd91, %rd12, %rd90;
	ld.global.f64 	%fd81, [%rd91];

$L__BB1_19:
	ld.param.u64 	%rd125, [axpy_cuda_kernel_backward_param_13];
	cvt.s64.s32 	%rd45, %r5;
	mul.lo.s64 	%rd46, %rd45, %rd24;
	add.s64 	%rd92, %rd17, %rd46;
	ld.global.f64 	%fd7, [%rd92];
	ld.global.f64 	%fd8, [%rd92+8];
	ld.global.f64 	%fd9, [%rd92+16];
	setp.eq.s64 	%p12, %rd125, 0;
	@%p12 bra 	$L__BB1_21;

	mul.lo.s64 	%rd93, %rd45, %rd26;
	add.s64 	%rd94, %rd13, %rd93;
	ld.global.f64 	%fd29, [%rd94];
	add.f64 	%fd80, %fd29, 0d0000000000000000;
	ld.global.f64 	%fd30, [%rd94+8];
	add.f64 	%fd79, %fd30, 0d0000000000000000;
	ld.global.f64 	%fd31, [%rd94+16];
	add.f64 	%fd78, %fd31, 0d0000000000000000;
	bra.uni 	$L__BB1_23;

$L__BB1_21:
	ld.param.u64 	%rd126, [axpy_cuda_kernel_backward_param_4+8];
	setp.eq.s64 	%p13, %rd126, 0;
	mov.f64 	%fd78, 0d0000000000000000;
	mov.f64 	%fd79, %fd78;
	mov.f64 	%fd80, %fd78;
	@%p13 bra 	$L__BB1_23;

	mul.lo.s64 	%rd95, %rd45, %rd27;
	add.s64 	%rd96, %rd16, %rd95;
	ld.global.f64 	%fd35, [%rd96];
	add.f64 	%fd80, %fd35, 0d0000000000000000;
	ld.global.f64 	%fd36, [%rd96+8];
	add.f64 	%fd79, %fd36, 0d0000000000000000;
	ld.global.f64 	%fd37, [%rd96+16];
	add.f64 	%fd78, %fd37, 0d0000000000000000;

$L__BB1_23:
	selp.f64 	%fd38, %fd82, %fd81, %p11;
	add.f64 	%fd19, %fd80, 0d0000000000000000;
	fma.rn.f64 	%fd20, %fd38, %fd19, 0d0000000000000000;
	add.f64 	%fd21, %fd79, 0d0000000000000000;
	fma.rn.f64 	%fd22, %fd38, %fd21, 0d0000000000000000;
	add.f64 	%fd23, %fd78, 0d0000000000000000;
	fma.rn.f64 	%fd24, %fd38, %fd23, 0d0000000000000000;
	mul.f64 	%fd39, %fd8, %fd21;
	fma.rn.f64 	%fd40, %fd7, %fd19, %fd39;
	fma.rn.f64 	%fd41, %fd9, %fd23, %fd40;
	add.f64 	%fd25, %fd41, 0d0000000000000000;
	setp.eq.s64 	%p15, %rd64, 0;
	@%p15 bra 	$L__BB1_25;

	mul.lo.s64 	%rd100, %rd45, %rd28;
	add.s64 	%rd97, %rd64, %rd100;
	// begin inline asm
	{ atom.add.f64 %fd42,[%rd97],%fd20; }

	// end inline asm
	add.s64 	%rd98, %rd97, 8;
	// begin inline asm
	{ atom.add.f64 %fd44,[%rd98],%fd22; }

	// end inline asm
	add.s64 	%rd99, %rd97, 16;
	// begin inline asm
	{ atom.add.f64 %fd46,[%rd99],%fd24; }

	// end inline asm
	bra.uni 	$L__BB1_27;

$L__BB1_25:
	setp.eq.s64 	%p16, %rd53, 0;
	@%p16 bra 	$L__BB1_27;

	add.s64 	%rd101, %rd53, %rd46;
	// begin inline asm
	{ atom.add.f64 %fd48,[%rd101],%fd20; }

	// end inline asm
	add.s64 	%rd102, %rd101, 8;
	// begin inline asm
	{ atom.add.f64 %fd50,[%rd102],%fd22; }

	// end inline asm
	add.s64 	%rd103, %rd101, 16;
	// begin inline asm
	{ atom.add.f64 %fd52,[%rd103],%fd24; }

	// end inline asm

$L__BB1_27:
	setp.eq.s64 	%p17, %rd62, 0;
	@%p17 bra 	$L__BB1_29;

	mul.lo.s64 	%rd107, %rd45, %rd29;
	add.s64 	%rd104, %rd62, %rd107;
	// begin inline asm
	{ atom.add.f64 %fd54,[%rd104],%fd19; }

	// end inline asm
	add.s64 	%rd105, %rd104, 8;
	// begin inline asm
	{ atom.add.f64 %fd56,[%rd105],%fd21; }

	// end inline asm
	add.s64 	%rd106, %rd104, 16;
	// begin inline asm
	{ atom.add.f64 %fd58,[%rd106],%fd23; }

	// end inline asm
	bra.uni 	$L__BB1_31;

$L__BB1_29:
	setp.eq.s64 	%p18, %rd51, 0;
	@%p18 bra 	$L__BB1_31;

	mul.lo.s64 	%rd111, %rd45, %rd30;
	add.s64 	%rd108, %rd51, %rd111;
	// begin inline asm
	{ atom.add.f64 %fd60,[%rd108],%fd19; }

	// end inline asm
	add.s64 	%rd109, %rd108, 8;
	// begin inline asm
	{ atom.add.f64 %fd62,[%rd109],%fd21; }

	// end inline asm
	add.s64 	%rd110, %rd108, 16;
	// begin inline asm
	{ atom.add.f64 %fd64,[%rd110],%fd23; }

	// end inline asm

$L__BB1_31:
	@%p11 bra 	$L__BB1_36;

	setp.eq.s64 	%p20, %rd66, 0;
	@%p20 bra 	$L__BB1_34;

	cvt.s64.s32 	%rd113, %r220;
	mul.lo.s64 	%rd114, %rd113, %rd31;
	add.s64 	%rd112, %rd66, %rd114;
	// begin inline asm
	{ atom.add.f64 %fd66,[%rd112],%fd25; }

	// end inline asm
	bra.uni 	$L__BB1_36;

$L__BB1_34:
	setp.eq.s64 	%p21, %rd55, 0;
	@%p21 bra 	$L__BB1_36;

	cvt.s64.s32 	%rd116, %r220;
	mul.lo.s64 	%rd117, %rd116, %rd23;
	add.s64 	%rd115, %rd55, %rd117;
	// begin inline asm
	{ atom.add.f64 %fd68,[%rd115],%fd25; }

	// end inline asm

$L__BB1_36:
	setp.gt.s32 	%p27, %r13, 0;
	cvt.u32.u64 	%r214, %rd129;
	selp.b32 	%r213, %r214, 0, %p27;
	setp.ge.s32 	%p26, %r213, %r3;
	@%p26 bra 	$L__BB1_41;

	setp.eq.s64 	%p23, %rd66, 0;
	@%p23 bra 	$L__BB1_39;

	cvt.s64.s32 	%rd119, %rd133;
	mul.lo.s64 	%rd120, %rd119, %rd31;
	add.s64 	%rd118, %rd66, %rd120;
	// begin inline asm
	{ atom.add.f64 %fd70,[%rd118],%fd25; }

	// end inline asm
	bra.uni 	$L__BB1_41;

$L__BB1_39:
	setp.eq.s64 	%p24, %rd55, 0;
	@%p24 bra 	$L__BB1_41;

	cvt.s64.s32 	%rd122, %rd133;
	mul.lo.s64 	%rd123, %rd122, %rd23;
	add.s64 	%rd121, %rd55, %rd123;
	// begin inline asm
	{ atom.add.f64 %fd72,[%rd121],%fd25; }

	// end inline asm

$L__BB1_41:
	ld.param.u64 	%rd124, [axpy_cuda_kernel_backward_param_0+24];
	add.s64 	%rd127, %rd127, %rd21;
	setp.lt.u64 	%p25, %rd127, %rd124;
	@%p25 bra 	$L__BB1_2;

$L__BB1_42:
	ret;

}
	// .globl	array_inv_cuda_kernel_forward
.visible .entry array_inv_cuda_kernel_forward(
	.param .align 8 .b8 array_inv_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 array_inv_cuda_kernel_forward_param_1[56]
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<77>;
	.reg .b64 	%rd<42>;


	ld.param.v2.u32 	{%r16, %r17}, [array_inv_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r18, %r19}, [array_inv_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r24, %r25}, [array_inv_cuda_kernel_forward_param_1+32];
	ld.param.u64 	%rd23, [array_inv_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd22, [array_inv_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [array_inv_cuda_kernel_forward_param_0+16];
	mov.u32 	%r28, %ntid.x;
	cvt.u64.u32 	%rd1, %r28;
	mov.u32 	%r29, %ctaid.x;
	mul.wide.u32 	%rd25, %r28, %r29;
	mov.u32 	%r30, %tid.x;
	cvt.u64.u32 	%rd26, %r30;
	add.s64 	%rd38, %rd25, %rd26;
	setp.ge.u64 	%p1, %rd38, %rd22;
	@%p1 bra 	$L__BB2_17;

	cvta.to.global.u64 	%rd4, %rd23;
	cvt.s64.s32 	%rd5, %r19;
	cvt.s64.s32 	%rd6, %r18;
	cvt.s64.s32 	%rd7, %r17;
	cvt.s64.s32 	%rd8, %r24;
	mov.u32 	%r31, %nctaid.x;
	cvt.u64.u32 	%rd27, %r31;
	mul.lo.s64 	%rd9, %rd1, %rd27;

$L__BB2_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd39, %rd38;
	@%p2 bra 	$L__BB2_6;

	or.b64  	%rd28, %rd38, %rd5;
	and.b64  	%rd29, %rd28, -4294967296;
	setp.eq.s64 	%p3, %rd29, 0;
	@%p3 bra 	$L__BB2_5;

	div.u64 	%rd39, %rd38, %rd5;
	bra.uni 	$L__BB2_6;

$L__BB2_5:
	cvt.u32.u64 	%r32, %rd5;
	cvt.u32.u64 	%r33, %rd38;
	div.u32 	%r34, %r33, %r32;
	cvt.u64.u32 	%rd39, %r34;

$L__BB2_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB2_10;

	or.b64  	%rd30, %rd39, %rd6;
	and.b64  	%rd31, %rd30, -4294967296;
	setp.eq.s64 	%p5, %rd31, 0;
	@%p5 bra 	$L__BB2_9;

	div.u64 	%rd39, %rd39, %rd6;
	bra.uni 	$L__BB2_10;

$L__BB2_9:
	cvt.u32.u64 	%r35, %rd6;
	cvt.u32.u64 	%r36, %rd39;
	div.u32 	%r37, %r36, %r35;
	cvt.u64.u32 	%rd39, %r37;

$L__BB2_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB2_14;

	or.b64  	%rd32, %rd39, %rd7;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.eq.s64 	%p7, %rd33, 0;
	@%p7 bra 	$L__BB2_13;

	div.u64 	%rd39, %rd39, %rd7;
	bra.uni 	$L__BB2_14;

$L__BB2_13:
	cvt.u32.u64 	%r38, %rd7;
	cvt.u32.u64 	%r39, %rd39;
	div.u32 	%r40, %r39, %r38;
	cvt.u64.u32 	%rd39, %r40;

$L__BB2_14:
	cvt.s64.s32 	%rd34, %rd39;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd35, %rd34, 0, %p8;
	mul.lo.s64 	%rd36, %rd35, %rd8;
	add.s64 	%rd20, %rd4, %rd36;
	ld.global.f64 	%fd1, [%rd20+64];
	ld.global.f64 	%fd2, [%rd20+32];
	mul.f64 	%fd41, %fd2, %fd1;
	ld.global.f64 	%fd3, [%rd20+56];
	ld.global.f64 	%fd4, [%rd20+40];
	mul.f64 	%fd42, %fd4, %fd3;
	sub.f64 	%fd5, %fd41, %fd42;
	ld.global.f64 	%fd6, [%rd20+48];
	mul.f64 	%fd43, %fd4, %fd6;
	ld.global.f64 	%fd7, [%rd20+24];
	mul.f64 	%fd44, %fd7, %fd1;
	sub.f64 	%fd8, %fd43, %fd44;
	mul.f64 	%fd45, %fd7, %fd3;
	mul.f64 	%fd46, %fd2, %fd6;
	sub.f64 	%fd9, %fd45, %fd46;
	ld.global.f64 	%fd10, [%rd20];
	ld.global.f64 	%fd11, [%rd20+8];
	mul.f64 	%fd47, %fd11, %fd8;
	fma.rn.f64 	%fd48, %fd10, %fd5, %fd47;
	ld.global.f64 	%fd12, [%rd20+16];
	fma.rn.f64 	%fd13, %fd12, %fd9, %fd48;
	setp.eq.f64 	%p9, %fd13, 0d0000000000000000;
	mov.f64 	%fd68, 0d0000000000000000;
	mov.f64 	%fd69, %fd68;
	mov.f64 	%fd70, %fd68;
	mov.f64 	%fd71, %fd68;
	mov.f64 	%fd72, %fd68;
	mov.f64 	%fd73, %fd68;
	mov.f64 	%fd74, %fd68;
	mov.f64 	%fd75, %fd68;
	mov.f64 	%fd76, %fd68;
	@%p9 bra 	$L__BB2_16;

	mul.f64 	%fd49, %fd12, %fd3;
	mul.f64 	%fd50, %fd11, %fd1;
	sub.f64 	%fd51, %fd49, %fd50;
	mul.f64 	%fd52, %fd12, %fd6;
	mul.f64 	%fd53, %fd10, %fd1;
	sub.f64 	%fd54, %fd53, %fd52;
	mul.f64 	%fd55, %fd10, %fd3;
	mul.f64 	%fd56, %fd11, %fd6;
	sub.f64 	%fd57, %fd56, %fd55;
	mul.f64 	%fd58, %fd12, %fd2;
	mul.f64 	%fd59, %fd11, %fd4;
	sub.f64 	%fd60, %fd59, %fd58;
	mul.f64 	%fd61, %fd10, %fd4;
	mul.f64 	%fd62, %fd12, %fd7;
	sub.f64 	%fd63, %fd62, %fd61;
	mul.f64 	%fd64, %fd11, %fd7;
	mul.f64 	%fd65, %fd10, %fd2;
	sub.f64 	%fd66, %fd65, %fd64;
	rcp.rn.f64 	%fd67, %fd13;
	mul.f64 	%fd76, %fd67, %fd66;
	mul.f64 	%fd75, %fd67, %fd57;
	mul.f64 	%fd73, %fd67, %fd63;
	mul.f64 	%fd72, %fd67, %fd54;
	mul.f64 	%fd70, %fd67, %fd60;
	mul.f64 	%fd69, %fd67, %fd51;
	mul.f64 	%fd74, %fd67, %fd9;
	mul.f64 	%fd71, %fd67, %fd8;
	mul.f64 	%fd68, %fd67, %fd5;

$L__BB2_16:
	ld.param.u64 	%rd37, [array_inv_cuda_kernel_forward_param_0+24];
	st.global.f64 	[%rd20], %fd68;
	st.global.f64 	[%rd20+8], %fd69;
	st.global.f64 	[%rd20+16], %fd70;
	st.global.f64 	[%rd20+24], %fd71;
	st.global.f64 	[%rd20+32], %fd72;
	st.global.f64 	[%rd20+40], %fd73;
	st.global.f64 	[%rd20+48], %fd74;
	st.global.f64 	[%rd20+56], %fd75;
	st.global.f64 	[%rd20+64], %fd76;
	add.s64 	%rd38, %rd38, %rd9;
	setp.lt.u64 	%p10, %rd38, %rd37;
	@%p10 bra 	$L__BB2_2;

$L__BB2_17:
	ret;

}
	// .globl	array_inv_cuda_kernel_backward
.visible .entry array_inv_cuda_kernel_backward(
	.param .align 8 .b8 array_inv_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 array_inv_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 array_inv_cuda_kernel_backward_param_2[56]
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<58>;
	.reg .f64 	%fd<240>;
	.reg .b64 	%rd<73>;


	ld.param.v2.u32 	{%r25, %r26}, [array_inv_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r27, %r28}, [array_inv_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r33, %r34}, [array_inv_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r41, %r42}, [array_inv_cuda_kernel_backward_param_2+32];
	ld.param.u64 	%rd31, [array_inv_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd30, [array_inv_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd29, [array_inv_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd28, [array_inv_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [array_inv_cuda_kernel_backward_param_0+16];
	mov.u32 	%r45, %ntid.x;
	cvt.u64.u32 	%rd1, %r45;
	mov.u32 	%r46, %ctaid.x;
	mul.wide.u32 	%rd33, %r45, %r46;
	mov.u32 	%r47, %tid.x;
	cvt.u64.u32 	%rd34, %r47;
	add.s64 	%rd69, %rd33, %rd34;
	setp.ge.u64 	%p1, %rd69, %rd28;
	@%p1 bra 	$L__BB3_25;

	cvta.to.global.u64 	%rd8, %rd29;
	cvt.s64.s32 	%rd9, %r28;
	cvt.s64.s32 	%rd10, %r27;
	cvt.s64.s32 	%rd11, %r26;
	cvt.s64.s32 	%rd12, %r33;
	cvt.s64.s32 	%rd13, %r41;
	mov.u32 	%r48, %nctaid.x;
	cvt.u64.u32 	%rd35, %r48;
	mul.lo.s64 	%rd14, %rd1, %rd35;

$L__BB3_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd70, %rd69;
	@%p2 bra 	$L__BB3_6;

	or.b64  	%rd36, %rd69, %rd9;
	and.b64  	%rd37, %rd36, -4294967296;
	setp.eq.s64 	%p3, %rd37, 0;
	@%p3 bra 	$L__BB3_5;

	div.u64 	%rd70, %rd69, %rd9;
	bra.uni 	$L__BB3_6;

$L__BB3_5:
	cvt.u32.u64 	%r49, %rd9;
	cvt.u32.u64 	%r50, %rd69;
	div.u32 	%r51, %r50, %r49;
	cvt.u64.u32 	%rd70, %r51;

$L__BB3_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB3_10;

	or.b64  	%rd38, %rd70, %rd10;
	and.b64  	%rd39, %rd38, -4294967296;
	setp.eq.s64 	%p5, %rd39, 0;
	@%p5 bra 	$L__BB3_9;

	div.u64 	%rd70, %rd70, %rd10;
	bra.uni 	$L__BB3_10;

$L__BB3_9:
	cvt.u32.u64 	%r52, %rd10;
	cvt.u32.u64 	%r53, %rd70;
	div.u32 	%r54, %r53, %r52;
	cvt.u64.u32 	%rd70, %r54;

$L__BB3_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB3_14;

	or.b64  	%rd40, %rd70, %rd11;
	and.b64  	%rd41, %rd40, -4294967296;
	setp.eq.s64 	%p7, %rd41, 0;
	@%p7 bra 	$L__BB3_13;

	div.u64 	%rd70, %rd70, %rd11;
	bra.uni 	$L__BB3_14;

$L__BB3_13:
	cvt.u32.u64 	%r55, %rd11;
	cvt.u32.u64 	%r56, %rd70;
	div.u32 	%r57, %r56, %r55;
	cvt.u64.u32 	%rd70, %r57;

$L__BB3_14:
	cvt.s64.s32 	%rd42, %rd70;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd25, %rd42, 0, %p8;
	mul.lo.s64 	%rd26, %rd25, %rd12;
	add.s64 	%rd43, %rd8, %rd26;
	ld.global.f64 	%fd1, [%rd43+64];
	ld.global.f64 	%fd2, [%rd43+32];
	mul.f64 	%fd77, %fd2, %fd1;
	ld.global.f64 	%fd3, [%rd43+56];
	ld.global.f64 	%fd4, [%rd43+40];
	mul.f64 	%fd78, %fd4, %fd3;
	sub.f64 	%fd5, %fd77, %fd78;
	ld.global.f64 	%fd6, [%rd43+48];
	mul.f64 	%fd79, %fd4, %fd6;
	ld.global.f64 	%fd7, [%rd43+24];
	mul.f64 	%fd80, %fd7, %fd1;
	sub.f64 	%fd8, %fd79, %fd80;
	mul.f64 	%fd81, %fd7, %fd3;
	mul.f64 	%fd82, %fd2, %fd6;
	sub.f64 	%fd9, %fd81, %fd82;
	ld.global.f64 	%fd10, [%rd43];
	ld.global.f64 	%fd11, [%rd43+8];
	mul.f64 	%fd83, %fd11, %fd8;
	fma.rn.f64 	%fd84, %fd10, %fd5, %fd83;
	ld.global.f64 	%fd12, [%rd43+16];
	fma.rn.f64 	%fd13, %fd12, %fd9, %fd84;
	setp.eq.f64 	%p9, %fd13, 0d0000000000000000;
	mov.f64 	%fd222, 0d0000000000000000;
	mov.f64 	%fd223, %fd222;
	mov.f64 	%fd224, %fd222;
	mov.f64 	%fd225, %fd222;
	mov.f64 	%fd226, %fd222;
	mov.f64 	%fd227, %fd222;
	mov.f64 	%fd228, %fd222;
	mov.f64 	%fd229, %fd222;
	mov.f64 	%fd230, %fd222;
	@%p9 bra 	$L__BB3_16;

	mul.f64 	%fd85, %fd12, %fd3;
	mul.f64 	%fd86, %fd11, %fd1;
	sub.f64 	%fd87, %fd85, %fd86;
	mul.f64 	%fd88, %fd12, %fd6;
	mul.f64 	%fd89, %fd10, %fd1;
	sub.f64 	%fd90, %fd89, %fd88;
	mul.f64 	%fd91, %fd10, %fd3;
	mul.f64 	%fd92, %fd11, %fd6;
	sub.f64 	%fd93, %fd92, %fd91;
	mul.f64 	%fd94, %fd12, %fd2;
	mul.f64 	%fd95, %fd11, %fd4;
	sub.f64 	%fd96, %fd95, %fd94;
	mul.f64 	%fd97, %fd10, %fd4;
	mul.f64 	%fd98, %fd12, %fd7;
	sub.f64 	%fd99, %fd98, %fd97;
	mul.f64 	%fd100, %fd11, %fd7;
	mul.f64 	%fd101, %fd10, %fd2;
	sub.f64 	%fd102, %fd101, %fd100;
	rcp.rn.f64 	%fd103, %fd13;
	mul.f64 	%fd230, %fd103, %fd102;
	mul.f64 	%fd229, %fd103, %fd93;
	mul.f64 	%fd227, %fd103, %fd99;
	mul.f64 	%fd226, %fd103, %fd90;
	mul.f64 	%fd224, %fd103, %fd96;
	mul.f64 	%fd223, %fd103, %fd87;
	mul.f64 	%fd228, %fd103, %fd9;
	mul.f64 	%fd225, %fd103, %fd8;
	mul.f64 	%fd222, %fd103, %fd5;

$L__BB3_16:
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB3_18;

	cvta.to.global.u64 	%rd66, %rd31;
	mul.lo.s64 	%rd44, %rd25, %rd13;
	add.s64 	%rd45, %rd66, %rd44;
	ld.global.f64 	%fd104, [%rd45];
	add.f64 	%fd239, %fd104, 0d0000000000000000;
	ld.global.f64 	%fd105, [%rd45+8];
	add.f64 	%fd238, %fd105, 0d0000000000000000;
	ld.global.f64 	%fd106, [%rd45+16];
	add.f64 	%fd237, %fd106, 0d0000000000000000;
	ld.global.f64 	%fd107, [%rd45+24];
	add.f64 	%fd236, %fd107, 0d0000000000000000;
	ld.global.f64 	%fd108, [%rd45+32];
	add.f64 	%fd235, %fd108, 0d0000000000000000;
	ld.global.f64 	%fd109, [%rd45+40];
	add.f64 	%fd234, %fd109, 0d0000000000000000;
	ld.global.f64 	%fd110, [%rd45+48];
	add.f64 	%fd233, %fd110, 0d0000000000000000;
	ld.global.f64 	%fd111, [%rd45+56];
	add.f64 	%fd232, %fd111, 0d0000000000000000;
	ld.global.f64 	%fd112, [%rd45+64];
	add.f64 	%fd231, %fd112, 0d0000000000000000;
	bra.uni 	$L__BB3_20;

$L__BB3_18:
	setp.eq.s64 	%p11, %rd30, 0;
	mov.f64 	%fd231, 0d0000000000000000;
	mov.f64 	%fd232, %fd231;
	mov.f64 	%fd233, %fd231;
	mov.f64 	%fd234, %fd231;
	mov.f64 	%fd235, %fd231;
	mov.f64 	%fd236, %fd231;
	mov.f64 	%fd237, %fd231;
	mov.f64 	%fd238, %fd231;
	mov.f64 	%fd239, %fd231;
	@%p11 bra 	$L__BB3_20;

	cvta.to.global.u64 	%rd68, %rd30;
	add.s64 	%rd46, %rd68, %rd26;
	ld.global.f64 	%fd122, [%rd46];
	add.f64 	%fd239, %fd122, 0d0000000000000000;
	ld.global.f64 	%fd123, [%rd46+8];
	add.f64 	%fd238, %fd123, 0d0000000000000000;
	ld.global.f64 	%fd124, [%rd46+16];
	add.f64 	%fd237, %fd124, 0d0000000000000000;
	ld.global.f64 	%fd125, [%rd46+24];
	add.f64 	%fd236, %fd125, 0d0000000000000000;
	ld.global.f64 	%fd126, [%rd46+32];
	add.f64 	%fd235, %fd126, 0d0000000000000000;
	ld.global.f64 	%fd127, [%rd46+40];
	add.f64 	%fd234, %fd127, 0d0000000000000000;
	ld.global.f64 	%fd128, [%rd46+48];
	add.f64 	%fd233, %fd128, 0d0000000000000000;
	ld.global.f64 	%fd129, [%rd46+56];
	add.f64 	%fd232, %fd129, 0d0000000000000000;
	ld.global.f64 	%fd130, [%rd46+64];
	add.f64 	%fd231, %fd130, 0d0000000000000000;

$L__BB3_20:
	mov.f64 	%fd131, 0d0000000000000000;
	fma.rn.f64 	%fd132, %fd222, %fd239, %fd131;
	fma.rn.f64 	%fd133, %fd225, %fd236, %fd132;
	fma.rn.f64 	%fd134, %fd228, %fd233, %fd133;
	fma.rn.f64 	%fd135, %fd222, %fd238, %fd131;
	fma.rn.f64 	%fd136, %fd225, %fd235, %fd135;
	fma.rn.f64 	%fd137, %fd228, %fd232, %fd136;
	fma.rn.f64 	%fd138, %fd222, %fd237, %fd131;
	fma.rn.f64 	%fd139, %fd225, %fd234, %fd138;
	fma.rn.f64 	%fd140, %fd228, %fd231, %fd139;
	fma.rn.f64 	%fd141, %fd223, %fd239, %fd131;
	fma.rn.f64 	%fd142, %fd226, %fd236, %fd141;
	fma.rn.f64 	%fd143, %fd229, %fd233, %fd142;
	fma.rn.f64 	%fd144, %fd223, %fd238, %fd131;
	fma.rn.f64 	%fd145, %fd226, %fd235, %fd144;
	fma.rn.f64 	%fd146, %fd229, %fd232, %fd145;
	fma.rn.f64 	%fd147, %fd223, %fd237, %fd131;
	fma.rn.f64 	%fd148, %fd226, %fd234, %fd147;
	fma.rn.f64 	%fd149, %fd229, %fd231, %fd148;
	fma.rn.f64 	%fd150, %fd224, %fd239, %fd131;
	fma.rn.f64 	%fd151, %fd227, %fd236, %fd150;
	fma.rn.f64 	%fd152, %fd230, %fd233, %fd151;
	fma.rn.f64 	%fd153, %fd224, %fd238, %fd131;
	fma.rn.f64 	%fd154, %fd227, %fd235, %fd153;
	fma.rn.f64 	%fd155, %fd230, %fd232, %fd154;
	fma.rn.f64 	%fd156, %fd224, %fd237, %fd131;
	fma.rn.f64 	%fd157, %fd227, %fd234, %fd156;
	fma.rn.f64 	%fd158, %fd230, %fd231, %fd157;
	fma.rn.f64 	%fd159, %fd134, %fd222, %fd131;
	fma.rn.f64 	%fd160, %fd137, %fd223, %fd159;
	fma.rn.f64 	%fd161, %fd140, %fd224, %fd160;
	fma.rn.f64 	%fd162, %fd134, %fd225, %fd131;
	fma.rn.f64 	%fd163, %fd137, %fd226, %fd162;
	fma.rn.f64 	%fd164, %fd140, %fd227, %fd163;
	fma.rn.f64 	%fd165, %fd134, %fd228, %fd131;
	fma.rn.f64 	%fd166, %fd137, %fd229, %fd165;
	fma.rn.f64 	%fd167, %fd140, %fd230, %fd166;
	fma.rn.f64 	%fd168, %fd143, %fd222, %fd131;
	fma.rn.f64 	%fd169, %fd146, %fd223, %fd168;
	fma.rn.f64 	%fd170, %fd149, %fd224, %fd169;
	fma.rn.f64 	%fd171, %fd143, %fd225, %fd131;
	fma.rn.f64 	%fd172, %fd146, %fd226, %fd171;
	fma.rn.f64 	%fd173, %fd149, %fd227, %fd172;
	fma.rn.f64 	%fd174, %fd143, %fd228, %fd131;
	fma.rn.f64 	%fd175, %fd146, %fd229, %fd174;
	fma.rn.f64 	%fd176, %fd149, %fd230, %fd175;
	fma.rn.f64 	%fd177, %fd152, %fd222, %fd131;
	fma.rn.f64 	%fd178, %fd155, %fd223, %fd177;
	fma.rn.f64 	%fd179, %fd158, %fd224, %fd178;
	fma.rn.f64 	%fd180, %fd152, %fd225, %fd131;
	fma.rn.f64 	%fd181, %fd155, %fd226, %fd180;
	fma.rn.f64 	%fd182, %fd158, %fd227, %fd181;
	fma.rn.f64 	%fd183, %fd152, %fd228, %fd131;
	fma.rn.f64 	%fd184, %fd155, %fd229, %fd183;
	fma.rn.f64 	%fd185, %fd158, %fd230, %fd184;
	sub.f64 	%fd59, %fd131, %fd161;
	sub.f64 	%fd60, %fd131, %fd164;
	sub.f64 	%fd61, %fd131, %fd167;
	sub.f64 	%fd62, %fd131, %fd170;
	sub.f64 	%fd63, %fd131, %fd173;
	sub.f64 	%fd64, %fd131, %fd176;
	sub.f64 	%fd65, %fd131, %fd179;
	sub.f64 	%fd66, %fd131, %fd182;
	sub.f64 	%fd67, %fd131, %fd185;
	@%p10 bra 	$L__BB3_22;

	mul.lo.s64 	%rd56, %rd25, %rd13;
	add.s64 	%rd47, %rd31, %rd56;
	// begin inline asm
	{ atom.add.f64 %fd186,[%rd47],%fd59; }

	// end inline asm
	add.s64 	%rd48, %rd47, 8;
	// begin inline asm
	{ atom.add.f64 %fd188,[%rd48],%fd60; }

	// end inline asm
	add.s64 	%rd49, %rd47, 16;
	// begin inline asm
	{ atom.add.f64 %fd190,[%rd49],%fd61; }

	// end inline asm
	add.s64 	%rd50, %rd47, 24;
	// begin inline asm
	{ atom.add.f64 %fd192,[%rd50],%fd62; }

	// end inline asm
	add.s64 	%rd51, %rd47, 32;
	// begin inline asm
	{ atom.add.f64 %fd194,[%rd51],%fd63; }

	// end inline asm
	add.s64 	%rd52, %rd47, 40;
	// begin inline asm
	{ atom.add.f64 %fd196,[%rd52],%fd64; }

	// end inline asm
	add.s64 	%rd53, %rd47, 48;
	// begin inline asm
	{ atom.add.f64 %fd198,[%rd53],%fd65; }

	// end inline asm
	add.s64 	%rd54, %rd47, 56;
	// begin inline asm
	{ atom.add.f64 %fd200,[%rd54],%fd66; }

	// end inline asm
	add.s64 	%rd55, %rd47, 64;
	// begin inline asm
	{ atom.add.f64 %fd202,[%rd55],%fd67; }

	// end inline asm
	bra.uni 	$L__BB3_24;

$L__BB3_22:
	setp.eq.s64 	%p13, %rd30, 0;
	@%p13 bra 	$L__BB3_24;

	add.s64 	%rd57, %rd30, %rd26;
	// begin inline asm
	{ atom.add.f64 %fd204,[%rd57],%fd59; }

	// end inline asm
	add.s64 	%rd58, %rd57, 8;
	// begin inline asm
	{ atom.add.f64 %fd206,[%rd58],%fd60; }

	// end inline asm
	add.s64 	%rd59, %rd57, 16;
	// begin inline asm
	{ atom.add.f64 %fd208,[%rd59],%fd61; }

	// end inline asm
	add.s64 	%rd60, %rd57, 24;
	// begin inline asm
	{ atom.add.f64 %fd210,[%rd60],%fd62; }

	// end inline asm
	add.s64 	%rd61, %rd57, 32;
	// begin inline asm
	{ atom.add.f64 %fd212,[%rd61],%fd63; }

	// end inline asm
	add.s64 	%rd62, %rd57, 40;
	// begin inline asm
	{ atom.add.f64 %fd214,[%rd62],%fd64; }

	// end inline asm
	add.s64 	%rd63, %rd57, 48;
	// begin inline asm
	{ atom.add.f64 %fd216,[%rd63],%fd65; }

	// end inline asm
	add.s64 	%rd64, %rd57, 56;
	// begin inline asm
	{ atom.add.f64 %fd218,[%rd64],%fd66; }

	// end inline asm
	add.s64 	%rd65, %rd57, 64;
	// begin inline asm
	{ atom.add.f64 %fd220,[%rd65],%fd67; }

	// end inline asm

$L__BB3_24:
	ld.param.u64 	%rd67, [array_inv_cuda_kernel_backward_param_0+24];
	add.s64 	%rd69, %rd69, %rd14;
	setp.lt.u64 	%p14, %rd69, %rd67;
	@%p14 bra 	$L__BB3_2;

$L__BB3_25:
	ret;

}
	// .globl	array_matmul_cuda_kernel_forward
.visible .entry array_matmul_cuda_kernel_forward(
	.param .align 8 .b8 array_matmul_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 array_matmul_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 array_matmul_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 array_matmul_cuda_kernel_forward_param_3[56]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<25>;
	.reg .b32 	%r<81>;
	.reg .f64 	%fd<43>;
	.reg .b64 	%rd<76>;


	ld.param.v2.u32 	{%r34, %r35}, [array_matmul_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r36, %r37}, [array_matmul_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r42, %r43}, [array_matmul_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r50, %r51}, [array_matmul_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r58, %r59}, [array_matmul_cuda_kernel_forward_param_3+32];
	ld.param.u64 	%rd38, [array_matmul_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd36, [array_matmul_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd34, [array_matmul_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd33, [array_matmul_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [array_matmul_cuda_kernel_forward_param_0+16];
	mov.u32 	%r62, %ntid.x;
	cvt.u64.u32 	%rd1, %r62;
	mov.u32 	%r63, %ctaid.x;
	mul.wide.u32 	%rd40, %r62, %r63;
	mov.u32 	%r64, %tid.x;
	cvt.u64.u32 	%rd41, %r64;
	add.s64 	%rd69, %rd40, %rd41;
	setp.ge.u64 	%p1, %rd69, %rd33;
	@%p1 bra 	$L__BB4_25;

	cvta.to.global.u64 	%rd4, %rd38;
	cvta.to.global.u64 	%rd5, %rd36;
	cvta.to.global.u64 	%rd6, %rd34;
	cvt.s64.s32 	%rd7, %r58;
	cvt.s64.s32 	%rd8, %r37;
	cvt.s64.s32 	%rd9, %r36;
	cvt.s64.s32 	%rd10, %r35;
	cvt.s64.s32 	%rd11, %r42;
	cvt.s64.s32 	%rd12, %r50;
	mov.u32 	%r65, %nctaid.x;
	cvt.u64.u32 	%rd42, %r65;
	mul.lo.s64 	%rd13, %rd1, %rd42;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB4_12;
	bra.uni 	$L__BB4_2;

$L__BB4_12:
	cvt.u32.u64 	%r72, %rd8;
	cvt.u32.u64 	%r75, %rd9;
	cvt.u32.u64 	%r78, %rd10;

$L__BB4_13:
	or.b64  	%rd55, %rd69, %rd8;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p9, %rd56, 0;
	@%p9 bra 	$L__BB4_15;

	div.u64 	%rd74, %rd69, %rd8;
	bra.uni 	$L__BB4_16;

$L__BB4_15:
	cvt.u32.u64 	%r73, %rd69;
	div.u32 	%r74, %r73, %r72;
	cvt.u64.u32 	%rd74, %r74;

$L__BB4_16:
	setp.lt.s32 	%p10, %r6, 3;
	@%p10 bra 	$L__BB4_20;

	or.b64  	%rd57, %rd74, %rd9;
	and.b64  	%rd58, %rd57, -4294967296;
	setp.eq.s64 	%p11, %rd58, 0;
	@%p11 bra 	$L__BB4_19;

	div.u64 	%rd74, %rd74, %rd9;
	bra.uni 	$L__BB4_20;

$L__BB4_19:
	cvt.u32.u64 	%r76, %rd74;
	div.u32 	%r77, %r76, %r75;
	cvt.u64.u32 	%rd74, %r77;

$L__BB4_20:
	setp.lt.s32 	%p12, %r6, 2;
	@%p12 bra 	$L__BB4_24;

	or.b64  	%rd59, %rd74, %rd10;
	and.b64  	%rd60, %rd59, -4294967296;
	setp.eq.s64 	%p13, %rd60, 0;
	@%p13 bra 	$L__BB4_23;

	div.u64 	%rd74, %rd74, %rd10;
	bra.uni 	$L__BB4_24;

$L__BB4_23:
	cvt.u32.u64 	%r79, %rd74;
	div.u32 	%r80, %r79, %r78;
	cvt.u64.u32 	%rd74, %r80;

$L__BB4_24:
	cvt.s64.s32 	%rd61, %rd74;
	setp.gt.s32 	%p14, %r6, 0;
	selp.b64 	%rd62, %rd61, 0, %p14;
	mul.lo.s64 	%rd63, %rd62, %rd11;
	add.s64 	%rd64, %rd6, %rd63;
	mul.lo.s64 	%rd65, %rd62, %rd12;
	add.s64 	%rd66, %rd5, %rd65;
	ld.global.f64 	%fd22, [%rd66];
	ld.global.f64 	%fd23, [%rd64];
	ld.global.f64 	%fd24, [%rd64+24];
	ld.global.f64 	%fd25, [%rd64+48];
	ld.global.f64 	%fd26, [%rd66+8];
	ld.global.f64 	%fd27, [%rd64+8];
	mul.f64 	%fd28, %fd27, %fd26;
	ld.global.f64 	%fd29, [%rd64+32];
	mul.f64 	%fd30, %fd29, %fd26;
	ld.global.f64 	%fd31, [%rd64+56];
	mul.f64 	%fd32, %fd31, %fd26;
	fma.rn.f64 	%fd33, %fd23, %fd22, %fd28;
	fma.rn.f64 	%fd34, %fd24, %fd22, %fd30;
	fma.rn.f64 	%fd35, %fd25, %fd22, %fd32;
	ld.global.f64 	%fd36, [%rd66+16];
	ld.global.f64 	%fd37, [%rd64+16];
	ld.global.f64 	%fd38, [%rd64+40];
	ld.global.f64 	%fd39, [%rd64+64];
	fma.rn.f64 	%fd40, %fd37, %fd36, %fd33;
	fma.rn.f64 	%fd41, %fd38, %fd36, %fd34;
	fma.rn.f64 	%fd42, %fd39, %fd36, %fd35;
	mul.lo.s64 	%rd67, %rd62, %rd7;
	add.s64 	%rd68, %rd4, %rd67;
	st.global.f64 	[%rd68], %fd40;
	st.global.f64 	[%rd68+8], %fd41;
	st.global.f64 	[%rd68+16], %fd42;
	add.s64 	%rd69, %rd69, %rd13;
	setp.lt.u64 	%p15, %rd69, %rd33;
	@%p15 bra 	$L__BB4_13;
	bra.uni 	$L__BB4_25;

$L__BB4_2:
	cvt.u32.u64 	%r66, %rd9;
	cvt.u32.u64 	%r69, %rd10;

$L__BB4_3:
	setp.lt.s32 	%p3, %r6, 3;
	mov.u64 	%rd70, %rd69;
	@%p3 bra 	$L__BB4_7;

	or.b64  	%rd43, %rd69, %rd9;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p4, %rd44, 0;
	@%p4 bra 	$L__BB4_6;

	div.u64 	%rd70, %rd69, %rd9;
	bra.uni 	$L__BB4_7;

$L__BB4_6:
	cvt.u32.u64 	%r67, %rd69;
	div.u32 	%r68, %r67, %r66;
	cvt.u64.u32 	%rd70, %r68;

$L__BB4_7:
	setp.lt.s32 	%p5, %r6, 2;
	@%p5 bra 	$L__BB4_11;

	or.b64  	%rd45, %rd70, %rd10;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p6, %rd46, 0;
	@%p6 bra 	$L__BB4_10;

	div.u64 	%rd70, %rd70, %rd10;
	bra.uni 	$L__BB4_11;

$L__BB4_10:
	cvt.u32.u64 	%r70, %rd70;
	div.u32 	%r71, %r70, %r69;
	cvt.u64.u32 	%rd70, %r71;

$L__BB4_11:
	cvt.s64.s32 	%rd47, %rd70;
	setp.gt.s32 	%p7, %r6, 0;
	selp.b64 	%rd48, %rd47, 0, %p7;
	mul.lo.s64 	%rd49, %rd48, %rd11;
	add.s64 	%rd50, %rd6, %rd49;
	mul.lo.s64 	%rd51, %rd48, %rd12;
	add.s64 	%rd52, %rd5, %rd51;
	ld.global.f64 	%fd1, [%rd52];
	ld.global.f64 	%fd2, [%rd50];
	ld.global.f64 	%fd3, [%rd50+24];
	ld.global.f64 	%fd4, [%rd50+48];
	ld.global.f64 	%fd5, [%rd52+8];
	ld.global.f64 	%fd6, [%rd50+8];
	mul.f64 	%fd7, %fd6, %fd5;
	ld.global.f64 	%fd8, [%rd50+32];
	mul.f64 	%fd9, %fd8, %fd5;
	ld.global.f64 	%fd10, [%rd50+56];
	mul.f64 	%fd11, %fd10, %fd5;
	fma.rn.f64 	%fd12, %fd2, %fd1, %fd7;
	fma.rn.f64 	%fd13, %fd3, %fd1, %fd9;
	fma.rn.f64 	%fd14, %fd4, %fd1, %fd11;
	ld.global.f64 	%fd15, [%rd52+16];
	ld.global.f64 	%fd16, [%rd50+16];
	ld.global.f64 	%fd17, [%rd50+40];
	ld.global.f64 	%fd18, [%rd50+64];
	fma.rn.f64 	%fd19, %fd16, %fd15, %fd12;
	fma.rn.f64 	%fd20, %fd17, %fd15, %fd13;
	fma.rn.f64 	%fd21, %fd18, %fd15, %fd14;
	mul.lo.s64 	%rd53, %rd48, %rd7;
	add.s64 	%rd54, %rd4, %rd53;
	st.global.f64 	[%rd54], %fd19;
	st.global.f64 	[%rd54+8], %fd20;
	st.global.f64 	[%rd54+16], %fd21;
	add.s64 	%rd69, %rd69, %rd13;
	setp.lt.u64 	%p8, %rd69, %rd33;
	@%p8 bra 	$L__BB4_3;

$L__BB4_25:
	ret;

}
	// .globl	array_matmul_cuda_kernel_backward
.visible .entry array_matmul_cuda_kernel_backward(
	.param .align 8 .b8 array_matmul_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 array_matmul_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 array_matmul_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 array_matmul_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 array_matmul_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 array_matmul_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 array_matmul_cuda_kernel_backward_param_6[56]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<49>;
	.reg .b32 	%r<126>;
	.reg .f64 	%fd<103>;
	.reg .b64 	%rd<100>;


	ld.param.v2.u32 	{%r61, %r62}, [array_matmul_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r63, %r64}, [array_matmul_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r69, %r70}, [array_matmul_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r77, %r78}, [array_matmul_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r85, %r86}, [array_matmul_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r93, %r94}, [array_matmul_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r101, %r102}, [array_matmul_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r109, %r110}, [array_matmul_cuda_kernel_backward_param_6+32];
	ld.param.u64 	%rd49, [array_matmul_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd47, [array_matmul_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd45, [array_matmul_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd44, [array_matmul_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd42, [array_matmul_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd41, [array_matmul_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd40, [array_matmul_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd39, [array_matmul_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd38, [array_matmul_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [array_matmul_cuda_kernel_backward_param_0+16];
	mov.u32 	%r113, %ntid.x;
	cvt.u64.u32 	%rd1, %r113;
	mov.u32 	%r114, %ctaid.x;
	mul.wide.u32 	%rd51, %r113, %r114;
	mov.u32 	%r115, %tid.x;
	cvt.u64.u32 	%rd52, %r115;
	add.s64 	%rd96, %rd51, %rd52;
	setp.ge.u64 	%p1, %rd96, %rd38;
	@%p1 bra 	$L__BB5_27;

	cvta.to.global.u64 	%rd10, %rd49;
	cvta.to.global.u64 	%rd11, %rd44;
	cvta.to.global.u64 	%rd12, %rd41;
	cvta.to.global.u64 	%rd13, %rd39;
	cvt.s64.s32 	%rd14, %r64;
	cvt.s64.s32 	%rd15, %r63;
	cvt.s64.s32 	%rd16, %r62;
	cvt.s64.s32 	%rd17, %r69;
	cvt.s64.s32 	%rd18, %r77;
	cvt.s64.s32 	%rd19, %r109;
	cvt.s64.s32 	%rd20, %r85;
	cvt.s64.s32 	%rd21, %r101;
	mov.u32 	%r116, %nctaid.x;
	cvt.u64.u32 	%rd53, %r116;
	mul.lo.s64 	%rd22, %rd1, %rd53;
	cvt.s64.s32 	%rd23, %r93;

$L__BB5_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd97, %rd96;
	@%p2 bra 	$L__BB5_6;

	or.b64  	%rd54, %rd96, %rd14;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p3, %rd55, 0;
	@%p3 bra 	$L__BB5_5;

	div.u64 	%rd97, %rd96, %rd14;
	bra.uni 	$L__BB5_6;

$L__BB5_5:
	cvt.u32.u64 	%r117, %rd14;
	cvt.u32.u64 	%r118, %rd96;
	div.u32 	%r119, %r118, %r117;
	cvt.u64.u32 	%rd97, %r119;

$L__BB5_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB5_10;

	or.b64  	%rd56, %rd97, %rd15;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p5, %rd57, 0;
	@%p5 bra 	$L__BB5_9;

	div.u64 	%rd97, %rd97, %rd15;
	bra.uni 	$L__BB5_10;

$L__BB5_9:
	cvt.u32.u64 	%r120, %rd15;
	cvt.u32.u64 	%r121, %rd97;
	div.u32 	%r122, %r121, %r120;
	cvt.u64.u32 	%rd97, %r122;

$L__BB5_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB5_14;

	or.b64  	%rd58, %rd97, %rd16;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p7, %rd59, 0;
	@%p7 bra 	$L__BB5_13;

	div.u64 	%rd97, %rd97, %rd16;
	bra.uni 	$L__BB5_14;

$L__BB5_13:
	cvt.u32.u64 	%r123, %rd16;
	cvt.u32.u64 	%r124, %rd97;
	div.u32 	%r125, %r124, %r123;
	cvt.u64.u32 	%rd97, %r125;

$L__BB5_14:
	ld.param.u64 	%rd94, [array_matmul_cuda_kernel_backward_param_6];
	cvt.s64.s32 	%rd60, %rd97;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd34, %rd60, 0, %p8;
	mul.lo.s64 	%rd35, %rd34, %rd17;
	add.s64 	%rd61, %rd13, %rd35;
	mul.lo.s64 	%rd36, %rd34, %rd18;
	add.s64 	%rd62, %rd12, %rd36;
	ld.global.f64 	%fd1, [%rd61];
	ld.global.f64 	%fd2, [%rd61+8];
	ld.global.f64 	%fd3, [%rd61+16];
	ld.global.f64 	%fd4, [%rd61+24];
	ld.global.f64 	%fd5, [%rd61+32];
	ld.global.f64 	%fd6, [%rd61+40];
	ld.global.f64 	%fd7, [%rd61+48];
	ld.global.f64 	%fd8, [%rd61+56];
	ld.global.f64 	%fd9, [%rd61+64];
	ld.global.f64 	%fd10, [%rd62];
	ld.global.f64 	%fd11, [%rd62+8];
	ld.global.f64 	%fd12, [%rd62+16];
	setp.eq.s64 	%p9, %rd94, 0;
	@%p9 bra 	$L__BB5_16;

	mul.lo.s64 	%rd63, %rd34, %rd19;
	add.s64 	%rd64, %rd10, %rd63;
	ld.global.f64 	%fd34, [%rd64];
	add.f64 	%fd102, %fd34, 0d0000000000000000;
	ld.global.f64 	%fd35, [%rd64+8];
	add.f64 	%fd101, %fd35, 0d0000000000000000;
	ld.global.f64 	%fd36, [%rd64+16];
	add.f64 	%fd100, %fd36, 0d0000000000000000;
	bra.uni 	$L__BB5_18;

$L__BB5_16:
	ld.param.u64 	%rd95, [array_matmul_cuda_kernel_backward_param_3+8];
	setp.eq.s64 	%p10, %rd95, 0;
	mov.f64 	%fd100, 0d0000000000000000;
	mov.f64 	%fd101, %fd100;
	mov.f64 	%fd102, %fd100;
	@%p10 bra 	$L__BB5_18;

	mul.lo.s64 	%rd65, %rd34, %rd20;
	add.s64 	%rd66, %rd11, %rd65;
	ld.global.f64 	%fd40, [%rd66];
	add.f64 	%fd102, %fd40, 0d0000000000000000;
	ld.global.f64 	%fd41, [%rd66+8];
	add.f64 	%fd101, %fd41, 0d0000000000000000;
	ld.global.f64 	%fd42, [%rd66+16];
	add.f64 	%fd100, %fd42, 0d0000000000000000;

$L__BB5_18:
	fma.rn.f64 	%fd22, %fd102, %fd10, 0d0000000000000000;
	fma.rn.f64 	%fd23, %fd102, %fd11, 0d0000000000000000;
	fma.rn.f64 	%fd24, %fd102, %fd12, 0d0000000000000000;
	fma.rn.f64 	%fd25, %fd101, %fd10, 0d0000000000000000;
	fma.rn.f64 	%fd26, %fd101, %fd11, 0d0000000000000000;
	fma.rn.f64 	%fd27, %fd101, %fd12, 0d0000000000000000;
	fma.rn.f64 	%fd28, %fd100, %fd10, 0d0000000000000000;
	fma.rn.f64 	%fd29, %fd100, %fd11, 0d0000000000000000;
	fma.rn.f64 	%fd30, %fd100, %fd12, 0d0000000000000000;
	mul.f64 	%fd43, %fd4, %fd101;
	fma.rn.f64 	%fd44, %fd1, %fd102, %fd43;
	mul.f64 	%fd45, %fd5, %fd101;
	fma.rn.f64 	%fd46, %fd2, %fd102, %fd45;
	mul.f64 	%fd47, %fd6, %fd101;
	fma.rn.f64 	%fd48, %fd3, %fd102, %fd47;
	fma.rn.f64 	%fd49, %fd7, %fd100, %fd44;
	fma.rn.f64 	%fd50, %fd8, %fd100, %fd46;
	fma.rn.f64 	%fd51, %fd9, %fd100, %fd48;
	add.f64 	%fd31, %fd49, 0d0000000000000000;
	add.f64 	%fd32, %fd50, 0d0000000000000000;
	add.f64 	%fd33, %fd51, 0d0000000000000000;
	setp.eq.s64 	%p11, %rd47, 0;
	@%p11 bra 	$L__BB5_20;

	mul.lo.s64 	%rd70, %rd34, %rd21;
	add.s64 	%rd67, %rd47, %rd70;
	// begin inline asm
	{ atom.add.f64 %fd52,[%rd67],%fd31; }

	// end inline asm
	add.s64 	%rd68, %rd67, 8;
	// begin inline asm
	{ atom.add.f64 %fd54,[%rd68],%fd32; }

	// end inline asm
	add.s64 	%rd69, %rd67, 16;
	// begin inline asm
	{ atom.add.f64 %fd56,[%rd69],%fd33; }

	// end inline asm
	bra.uni 	$L__BB5_22;

$L__BB5_20:
	setp.eq.s64 	%p12, %rd42, 0;
	@%p12 bra 	$L__BB5_22;

	add.s64 	%rd71, %rd42, %rd36;
	// begin inline asm
	{ atom.add.f64 %fd58,[%rd71],%fd31; }

	// end inline asm
	add.s64 	%rd72, %rd71, 8;
	// begin inline asm
	{ atom.add.f64 %fd60,[%rd72],%fd32; }

	// end inline asm
	add.s64 	%rd73, %rd71, 16;
	// begin inline asm
	{ atom.add.f64 %fd62,[%rd73],%fd33; }

	// end inline asm

$L__BB5_22:
	setp.eq.s64 	%p13, %rd45, 0;
	@%p13 bra 	$L__BB5_24;

	mul.lo.s64 	%rd83, %rd34, %rd23;
	add.s64 	%rd74, %rd45, %rd83;
	// begin inline asm
	{ atom.add.f64 %fd64,[%rd74],%fd22; }

	// end inline asm
	add.s64 	%rd75, %rd74, 8;
	// begin inline asm
	{ atom.add.f64 %fd66,[%rd75],%fd23; }

	// end inline asm
	add.s64 	%rd76, %rd74, 16;
	// begin inline asm
	{ atom.add.f64 %fd68,[%rd76],%fd24; }

	// end inline asm
	add.s64 	%rd77, %rd74, 24;
	// begin inline asm
	{ atom.add.f64 %fd70,[%rd77],%fd25; }

	// end inline asm
	add.s64 	%rd78, %rd74, 32;
	// begin inline asm
	{ atom.add.f64 %fd72,[%rd78],%fd26; }

	// end inline asm
	add.s64 	%rd79, %rd74, 40;
	// begin inline asm
	{ atom.add.f64 %fd74,[%rd79],%fd27; }

	// end inline asm
	add.s64 	%rd80, %rd74, 48;
	// begin inline asm
	{ atom.add.f64 %fd76,[%rd80],%fd28; }

	// end inline asm
	add.s64 	%rd81, %rd74, 56;
	// begin inline asm
	{ atom.add.f64 %fd78,[%rd81],%fd29; }

	// end inline asm
	add.s64 	%rd82, %rd74, 64;
	// begin inline asm
	{ atom.add.f64 %fd80,[%rd82],%fd30; }

	// end inline asm
	bra.uni 	$L__BB5_26;

$L__BB5_24:
	setp.eq.s64 	%p14, %rd40, 0;
	@%p14 bra 	$L__BB5_26;

	add.s64 	%rd84, %rd40, %rd35;
	// begin inline asm
	{ atom.add.f64 %fd82,[%rd84],%fd22; }

	// end inline asm
	add.s64 	%rd85, %rd84, 8;
	// begin inline asm
	{ atom.add.f64 %fd84,[%rd85],%fd23; }

	// end inline asm
	add.s64 	%rd86, %rd84, 16;
	// begin inline asm
	{ atom.add.f64 %fd86,[%rd86],%fd24; }

	// end inline asm
	add.s64 	%rd87, %rd84, 24;
	// begin inline asm
	{ atom.add.f64 %fd88,[%rd87],%fd25; }

	// end inline asm
	add.s64 	%rd88, %rd84, 32;
	// begin inline asm
	{ atom.add.f64 %fd90,[%rd88],%fd26; }

	// end inline asm
	add.s64 	%rd89, %rd84, 40;
	// begin inline asm
	{ atom.add.f64 %fd92,[%rd89],%fd27; }

	// end inline asm
	add.s64 	%rd90, %rd84, 48;
	// begin inline asm
	{ atom.add.f64 %fd94,[%rd90],%fd28; }

	// end inline asm
	add.s64 	%rd91, %rd84, 56;
	// begin inline asm
	{ atom.add.f64 %fd96,[%rd91],%fd29; }

	// end inline asm
	add.s64 	%rd92, %rd84, 64;
	// begin inline asm
	{ atom.add.f64 %fd98,[%rd92],%fd30; }

	// end inline asm

$L__BB5_26:
	ld.param.u64 	%rd93, [array_matmul_cuda_kernel_backward_param_0+24];
	add.s64 	%rd96, %rd96, %rd22;
	setp.lt.u64 	%p15, %rd96, %rd93;
	@%p15 bra 	$L__BB5_2;

$L__BB5_27:
	ret;

}
	// .globl	cg_one_iter_cuda_kernel_forward
.visible .entry cg_one_iter_cuda_kernel_forward(
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_8[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_9[56],
	.param .u32 cg_one_iter_cuda_kernel_forward_param_10,
	.param .u32 cg_one_iter_cuda_kernel_forward_param_11,
	.param .u32 cg_one_iter_cuda_kernel_forward_param_12
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<73>;
	.reg .b32 	%r<193>;
	.reg .f64 	%fd<59>;
	.reg .b64 	%rd<104>;


	ld.param.v2.u32 	{%r95, %r96}, [cg_one_iter_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r97, %r98}, [cg_one_iter_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r103, %r104}, [cg_one_iter_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r111, %r112}, [cg_one_iter_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r119, %r120}, [cg_one_iter_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r127, %r128}, [cg_one_iter_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r135, %r136}, [cg_one_iter_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r143, %r144}, [cg_one_iter_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r151, %r152}, [cg_one_iter_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r159, %r160}, [cg_one_iter_cuda_kernel_forward_param_8+32];
	ld.param.v2.u32 	{%r167, %r168}, [cg_one_iter_cuda_kernel_forward_param_9+32];
	ld.param.u32 	%r92, [cg_one_iter_cuda_kernel_forward_param_10];
	ld.param.u32 	%r93, [cg_one_iter_cuda_kernel_forward_param_11];
	ld.param.u64 	%rd55, [cg_one_iter_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd53, [cg_one_iter_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd51, [cg_one_iter_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd49, [cg_one_iter_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd47, [cg_one_iter_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd45, [cg_one_iter_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd43, [cg_one_iter_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd38, [cg_one_iter_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r10, [cg_one_iter_cuda_kernel_forward_param_0+16];
	mov.u32 	%r171, %ntid.x;
	cvt.u64.u32 	%rd1, %r171;
	mov.u32 	%r172, %ctaid.x;
	mul.wide.u32 	%rd57, %r171, %r172;
	mov.u32 	%r173, %tid.x;
	cvt.u64.u32 	%rd58, %r173;
	add.s64 	%rd100, %rd57, %rd58;
	setp.ge.u64 	%p1, %rd100, %rd38;
	@%p1 bra 	$L__BB6_21;

	cvta.to.global.u64 	%rd5, %rd45;
	cvta.to.global.u64 	%rd6, %rd55;
	cvta.to.global.u64 	%rd7, %rd53;
	cvta.to.global.u64 	%rd8, %rd51;
	cvta.to.global.u64 	%rd9, %rd49;
	cvta.to.global.u64 	%rd10, %rd47;
	cvta.to.global.u64 	%rd11, %rd43;
	cvt.s64.s32 	%rd14, %r98;
	cvt.s64.s32 	%rd15, %r97;
	cvt.s64.s32 	%rd16, %r96;
	mul.hi.s32 	%r174, %r93, 1431655766;
	shr.u32 	%r175, %r174, 31;
	add.s32 	%r2, %r174, %r175;
	mov.u32 	%r176, %nctaid.x;
	cvt.u64.u32 	%rd59, %r176;
	mul.lo.s64 	%rd17, %rd1, %rd59;
	cvt.s64.s32 	%rd18, %r159;
	cvt.s64.s32 	%rd19, %r127;
	cvt.s64.s32 	%rd20, %r135;
	cvt.s64.s32 	%rd21, %r103;
	mul.hi.s32 	%r177, %r92, 1431655766;
	shr.u32 	%r178, %r177, 31;
	add.s32 	%r3, %r177, %r178;
	cvt.s64.s32 	%rd22, %r167;
	cvt.s64.s32 	%rd23, %r143;
	cvt.s64.s32 	%rd24, %r111;
	cvt.s64.s32 	%rd25, %r151;
	cvt.s64.s32 	%rd26, %r119;

$L__BB6_2:
	setp.lt.s32 	%p2, %r10, 4;
	mov.u64 	%rd101, %rd100;
	@%p2 bra 	$L__BB6_6;

	or.b64  	%rd60, %rd100, %rd14;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.eq.s64 	%p3, %rd61, 0;
	@%p3 bra 	$L__BB6_5;

	div.u64 	%rd101, %rd100, %rd14;
	bra.uni 	$L__BB6_6;

$L__BB6_5:
	cvt.u32.u64 	%r179, %rd14;
	cvt.u32.u64 	%r180, %rd100;
	div.u32 	%r181, %r180, %r179;
	cvt.u64.u32 	%rd101, %r181;

$L__BB6_6:
	setp.lt.s32 	%p4, %r10, 3;
	@%p4 bra 	$L__BB6_10;

	or.b64  	%rd62, %rd101, %rd15;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p5, %rd63, 0;
	@%p5 bra 	$L__BB6_9;

	div.u64 	%rd101, %rd101, %rd15;
	bra.uni 	$L__BB6_10;

$L__BB6_9:
	cvt.u32.u64 	%r182, %rd15;
	cvt.u32.u64 	%r183, %rd101;
	div.u32 	%r184, %r183, %r182;
	cvt.u64.u32 	%rd101, %r184;

$L__BB6_10:
	setp.lt.s32 	%p6, %r10, 2;
	@%p6 bra 	$L__BB6_14;

	or.b64  	%rd64, %rd101, %rd16;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p7, %rd65, 0;
	@%p7 bra 	$L__BB6_13;

	div.u64 	%rd101, %rd101, %rd16;
	bra.uni 	$L__BB6_14;

$L__BB6_13:
	cvt.u32.u64 	%r185, %rd16;
	cvt.u32.u64 	%r186, %rd101;
	div.u32 	%r187, %r186, %r185;
	cvt.u64.u32 	%rd101, %r187;

$L__BB6_14:
	cvt.u32.u64 	%r188, %rd101;
	setp.gt.s32 	%p8, %r10, 0;
	selp.b32 	%r5, %r188, 0, %p8;
	setp.lt.s32 	%p9, %r5, %r2;
	@%p9 bra 	$L__BB6_20;

	setp.ge.s32 	%p10, %r5, %r3;
	@%p10 bra 	$L__BB6_17;

	sub.s32 	%r189, %r5, %r2;
	cvt.s64.s32 	%rd66, %r189;
	mul.lo.s64 	%rd67, %rd66, %rd18;
	add.s64 	%rd68, %rd7, %rd67;
	ld.global.s32 	%rd69, [%rd68];
	mul.lo.s64 	%rd70, %rd69, %rd19;
	add.s64 	%rd71, %rd5, %rd70;
	ld.global.f64 	%fd57, [%rd71];

$L__BB6_17:
	setp.lt.s32 	%p11, %r5, %r3;
	@%p11 bra 	$L__BB6_19;

	ld.param.u32 	%r192, [cg_one_iter_cuda_kernel_forward_param_12];
	sub.s32 	%r191, %r192, %r3;
	add.s32 	%r190, %r191, %r5;
	cvt.s64.s32 	%rd72, %r190;
	mul.lo.s64 	%rd73, %rd72, %rd22;
	add.s64 	%rd74, %rd6, %rd73;
	ld.global.s32 	%rd75, [%rd74];
	mul.lo.s64 	%rd76, %rd75, %rd19;
	add.s64 	%rd77, %rd5, %rd76;
	ld.global.f64 	%fd58, [%rd77];

$L__BB6_19:
	ld.param.u64 	%rd99, [cg_one_iter_cuda_kernel_forward_param_2];
	cvta.to.global.u64 	%rd98, %rd99;
	ld.param.u64 	%rd97, [cg_one_iter_cuda_kernel_forward_param_1];
	selp.f64 	%fd22, %fd57, %fd58, %p11;
	cvt.s64.s32 	%rd84, %r5;
	mul.lo.s64 	%rd85, %rd84, %rd20;
	add.s64 	%rd86, %rd10, %rd85;
	ld.global.f64 	%fd23, [%rd86];
	mul.f64 	%fd11, %fd23, %fd22;
	ld.global.f64 	%fd24, [%rd86+8];
	mul.f64 	%fd13, %fd22, %fd24;
	ld.global.f64 	%fd25, [%rd86+16];
	mul.f64 	%fd15, %fd22, %fd25;
	mul.lo.s64 	%rd87, %rd84, %rd21;
	add.s64 	%rd78, %rd97, %rd87;
	// begin inline asm
	{ atom.add.f64 %fd10,[%rd78],%fd11; }

	// end inline asm
	add.s64 	%rd79, %rd78, 8;
	// begin inline asm
	{ atom.add.f64 %fd12,[%rd79],%fd13; }

	// end inline asm
	add.s64 	%rd80, %rd78, 16;
	// begin inline asm
	{ atom.add.f64 %fd14,[%rd80],%fd15; }

	// end inline asm
	mul.lo.s64 	%rd88, %rd84, %rd23;
	add.s64 	%rd89, %rd9, %rd88;
	ld.global.f64 	%fd26, [%rd89];
	mul.f64 	%fd27, %fd22, %fd26;
	ld.global.f64 	%fd28, [%rd89+8];
	mul.f64 	%fd29, %fd22, %fd28;
	ld.global.f64 	%fd30, [%rd89+16];
	mul.f64 	%fd31, %fd22, %fd30;
	mul.lo.s64 	%rd90, %rd84, %rd24;
	add.s64 	%rd81, %rd99, %rd90;
	neg.f64 	%fd17, %fd27;
	neg.f64 	%fd19, %fd29;
	neg.f64 	%fd21, %fd31;
	// begin inline asm
	{ atom.add.f64 %fd16,[%rd81],%fd17; }

	// end inline asm
	add.s64 	%rd82, %rd81, 8;
	// begin inline asm
	{ atom.add.f64 %fd18,[%rd82],%fd19; }

	// end inline asm
	add.s64 	%rd83, %rd81, 16;
	// begin inline asm
	{ atom.add.f64 %fd20,[%rd83],%fd21; }

	// end inline asm
	mul.lo.s64 	%rd91, %rd84, %rd25;
	add.s64 	%rd92, %rd8, %rd91;
	add.s64 	%rd93, %rd98, %rd90;
	ld.global.f64 	%fd32, [%rd93];
	ld.global.f64 	%fd33, [%rd92];
	ld.global.f64 	%fd34, [%rd92+24];
	ld.global.f64 	%fd35, [%rd92+48];
	ld.global.f64 	%fd36, [%rd93+8];
	ld.global.f64 	%fd37, [%rd92+8];
	mul.f64 	%fd38, %fd37, %fd36;
	ld.global.f64 	%fd39, [%rd92+32];
	mul.f64 	%fd40, %fd39, %fd36;
	ld.global.f64 	%fd41, [%rd92+56];
	mul.f64 	%fd42, %fd41, %fd36;
	fma.rn.f64 	%fd43, %fd33, %fd32, %fd38;
	fma.rn.f64 	%fd44, %fd34, %fd32, %fd40;
	fma.rn.f64 	%fd45, %fd35, %fd32, %fd42;
	ld.global.f64 	%fd46, [%rd93+16];
	ld.global.f64 	%fd47, [%rd92+16];
	ld.global.f64 	%fd48, [%rd92+40];
	ld.global.f64 	%fd49, [%rd92+64];
	fma.rn.f64 	%fd50, %fd47, %fd46, %fd43;
	fma.rn.f64 	%fd51, %fd48, %fd46, %fd44;
	fma.rn.f64 	%fd52, %fd49, %fd46, %fd45;
	mul.lo.s64 	%rd94, %rd84, %rd26;
	add.s64 	%rd95, %rd11, %rd94;
	st.global.f64 	[%rd95], %fd50;
	st.global.f64 	[%rd95+8], %fd51;
	st.global.f64 	[%rd95+16], %fd52;

$L__BB6_20:
	ld.param.u64 	%rd96, [cg_one_iter_cuda_kernel_forward_param_0+24];
	add.s64 	%rd100, %rd100, %rd17;
	setp.lt.u64 	%p13, %rd100, %rd96;
	@%p13 bra 	$L__BB6_2;

$L__BB6_21:
	ret;

}
	// .globl	cg_one_iter_cuda_kernel_backward
.visible .entry cg_one_iter_cuda_kernel_backward(
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_9[56],
	.param .u32 cg_one_iter_cuda_kernel_backward_param_10,
	.param .u32 cg_one_iter_cuda_kernel_backward_param_11,
	.param .u32 cg_one_iter_cuda_kernel_backward_param_12,
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_14[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_15[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_16[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_17[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_18[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_19[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_20[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_21[56],
	.param .u32 cg_one_iter_cuda_kernel_backward_param_22,
	.param .u32 cg_one_iter_cuda_kernel_backward_param_23,
	.param .u32 cg_one_iter_cuda_kernel_backward_param_24
)
{
	.reg .pred 	%p<38>;
	.reg .b16 	%rs<129>;
	.reg .b32 	%r<323>;
	.reg .f64 	%fd<216>;
	.reg .b64 	%rd<207>;


	ld.param.v2.u32 	{%r161, %r162}, [cg_one_iter_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r163, %r164}, [cg_one_iter_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r169, %r170}, [cg_one_iter_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r177, %r178}, [cg_one_iter_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r185, %r186}, [cg_one_iter_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r193, %r194}, [cg_one_iter_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r201, %r202}, [cg_one_iter_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r209, %r210}, [cg_one_iter_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r217, %r218}, [cg_one_iter_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r225, %r226}, [cg_one_iter_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r233, %r234}, [cg_one_iter_cuda_kernel_backward_param_9+32];
	ld.param.u32 	%r95, [cg_one_iter_cuda_kernel_backward_param_10];
	ld.param.u32 	%r96, [cg_one_iter_cuda_kernel_backward_param_11];
	ld.param.v2.u32 	{%r241, %r242}, [cg_one_iter_cuda_kernel_backward_param_13+32];
	ld.param.v2.u32 	{%r249, %r250}, [cg_one_iter_cuda_kernel_backward_param_14+32];
	ld.param.v2.u32 	{%r257, %r258}, [cg_one_iter_cuda_kernel_backward_param_15+32];
	ld.param.v2.u32 	{%r265, %r266}, [cg_one_iter_cuda_kernel_backward_param_16+32];
	ld.param.v2.u32 	{%r273, %r274}, [cg_one_iter_cuda_kernel_backward_param_17+32];
	ld.param.v2.u32 	{%r281, %r282}, [cg_one_iter_cuda_kernel_backward_param_18+32];
	ld.param.v2.u32 	{%r289, %r290}, [cg_one_iter_cuda_kernel_backward_param_19+32];
	ld.param.u64 	%rd102, [cg_one_iter_cuda_kernel_backward_param_19];
	ld.param.u64 	%rd100, [cg_one_iter_cuda_kernel_backward_param_18];
	ld.param.u64 	%rd98, [cg_one_iter_cuda_kernel_backward_param_17];
	ld.param.u64 	%rd96, [cg_one_iter_cuda_kernel_backward_param_16];
	ld.param.u64 	%rd94, [cg_one_iter_cuda_kernel_backward_param_15];
	ld.param.u64 	%rd92, [cg_one_iter_cuda_kernel_backward_param_14];
	ld.param.u64 	%rd90, [cg_one_iter_cuda_kernel_backward_param_13];
	ld.param.u64 	%rd88, [cg_one_iter_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd86, [cg_one_iter_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd85, [cg_one_iter_cuda_kernel_backward_param_7+8];
	ld.param.u64 	%rd84, [cg_one_iter_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd83, [cg_one_iter_cuda_kernel_backward_param_6+8];
	ld.param.u64 	%rd82, [cg_one_iter_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd81, [cg_one_iter_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd80, [cg_one_iter_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd79, [cg_one_iter_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd78, [cg_one_iter_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd77, [cg_one_iter_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd75, [cg_one_iter_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd74, [cg_one_iter_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd73, [cg_one_iter_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd71, [cg_one_iter_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r13, [cg_one_iter_cuda_kernel_backward_param_0+16];
	mov.u32 	%r293, %ntid.x;
	cvt.u64.u32 	%rd1, %r293;
	mov.u32 	%r294, %ctaid.x;
	mul.wide.u32 	%rd104, %r293, %r294;
	mov.u32 	%r295, %tid.x;
	cvt.u64.u32 	%rd105, %r295;
	add.s64 	%rd200, %rd104, %rd105;
	setp.ge.u64 	%p1, %rd200, %rd71;
	@%p1 bra 	$L__BB7_58;

	cvta.to.global.u64 	%rd18, %rd78;
	cvta.to.global.u64 	%rd19, %rd94;
	cvta.to.global.u64 	%rd21, %rd90;
	cvta.to.global.u64 	%rd22, %rd88;
	cvta.to.global.u64 	%rd23, %rd86;
	cvta.to.global.u64 	%rd24, %rd84;
	cvta.to.global.u64 	%rd25, %rd82;
	cvta.to.global.u64 	%rd26, %rd80;
	cvta.to.global.u64 	%rd27, %rd77;
	cvta.to.global.u64 	%rd29, %rd74;
	cvta.to.global.u64 	%rd30, %rd73;
	cvt.s64.s32 	%rd31, %r164;
	cvt.s64.s32 	%rd32, %r163;
	cvt.s64.s32 	%rd33, %r162;
	mul.hi.s32 	%r297, %r96, 1431655766;
	shr.u32 	%r298, %r297, 31;
	add.s32 	%r2, %r297, %r298;
	mov.u32 	%r299, %nctaid.x;
	cvt.u64.u32 	%rd107, %r299;
	mul.lo.s64 	%rd34, %rd1, %rd107;
	mul.hi.s32 	%r300, %r95, 1431655766;
	shr.u32 	%r301, %r300, 31;
	add.s32 	%r3, %r300, %r301;
	cvt.s64.s32 	%rd35, %r225;
	cvt.s64.s32 	%rd36, %r193;
	cvt.s64.s32 	%rd37, %r201;
	cvt.s64.s32 	%rd38, %r209;
	cvt.s64.s32 	%rd39, %r217;
	cvt.s64.s32 	%rd40, %r177;
	cvt.s64.s32 	%rd41, %r233;
	cvt.s64.s32 	%rd42, %r257;
	cvt.s64.s32 	%rd43, %r185;
	cvt.s64.s32 	%rd44, %r249;
	cvt.s64.s32 	%rd45, %r289;
	cvt.s64.s32 	%rd46, %r281;
	cvt.s64.s32 	%rd47, %r241;
	cvt.s64.s32 	%rd48, %r169;
	cvt.s64.s32 	%rd49, %r273;
	cvt.s64.s32 	%rd50, %r265;

$L__BB7_2:
	setp.lt.s32 	%p2, %r13, 4;
	mov.u64 	%rd202, %rd200;
	@%p2 bra 	$L__BB7_6;

	or.b64  	%rd108, %rd200, %rd31;
	and.b64  	%rd109, %rd108, -4294967296;
	setp.eq.s64 	%p3, %rd109, 0;
	@%p3 bra 	$L__BB7_5;

	div.u64 	%rd202, %rd200, %rd31;
	bra.uni 	$L__BB7_6;

$L__BB7_5:
	cvt.u32.u64 	%r302, %rd31;
	cvt.u32.u64 	%r303, %rd200;
	div.u32 	%r304, %r303, %r302;
	cvt.u64.u32 	%rd202, %r304;

$L__BB7_6:
	setp.lt.s32 	%p4, %r13, 3;
	@%p4 bra 	$L__BB7_10;

	or.b64  	%rd110, %rd202, %rd32;
	and.b64  	%rd111, %rd110, -4294967296;
	setp.eq.s64 	%p5, %rd111, 0;
	@%p5 bra 	$L__BB7_9;

	div.u64 	%rd202, %rd202, %rd32;
	bra.uni 	$L__BB7_10;

$L__BB7_9:
	cvt.u32.u64 	%r305, %rd32;
	cvt.u32.u64 	%r306, %rd202;
	div.u32 	%r307, %r306, %r305;
	cvt.u64.u32 	%rd202, %r307;

$L__BB7_10:
	setp.lt.s32 	%p6, %r13, 2;
	@%p6 bra 	$L__BB7_14;

	or.b64  	%rd112, %rd202, %rd33;
	and.b64  	%rd113, %rd112, -4294967296;
	setp.eq.s64 	%p7, %rd113, 0;
	@%p7 bra 	$L__BB7_13;

	div.u64 	%rd202, %rd202, %rd33;
	bra.uni 	$L__BB7_14;

$L__BB7_13:
	cvt.u32.u64 	%r308, %rd33;
	cvt.u32.u64 	%r309, %rd202;
	div.u32 	%r310, %r309, %r308;
	cvt.u64.u32 	%rd202, %r310;

$L__BB7_14:
	cvt.u32.u64 	%r311, %rd202;
	setp.gt.s32 	%p8, %r13, 0;
	selp.b32 	%r5, %r311, 0, %p8;
	setp.lt.s32 	%p9, %r5, %r2;
	@%p9 bra 	$L__BB7_57;

	setp.gt.s32 	%p37, %r13, 0;
	cvt.u32.u64 	%r318, %rd202;
	selp.b32 	%r317, %r318, 0, %p37;
	setp.ge.s32 	%p10, %r317, %r3;
	@%p10 bra 	$L__BB7_17;

	sub.s32 	%r312, %r5, %r2;
	cvt.s64.s32 	%rd114, %r312;
	mul.lo.s64 	%rd115, %rd114, %rd35;
	add.s64 	%rd116, %rd23, %rd115;
	ld.global.u64 	%rd206, [%rd116];
	cvt.s64.s32 	%rd117, %rd206;
	mul.lo.s64 	%rd118, %rd117, %rd36;
	add.s64 	%rd119, %rd18, %rd118;
	ld.global.f64 	%fd215, [%rd119];

$L__BB7_17:
	setp.lt.s32 	%p11, %r5, %r3;
	@%p11 bra 	$L__BB7_19;

	ld.param.u32 	%r319, [cg_one_iter_cuda_kernel_backward_param_12];
	sub.s32 	%r313, %r5, %r3;
	add.s32 	%r314, %r313, %r319;
	cvt.s64.s32 	%rd120, %r314;
	mul.lo.s64 	%rd121, %rd120, %rd41;
	add.s64 	%rd122, %rd22, %rd121;
	ld.global.u32 	%r322, [%rd122];
	cvt.s64.s32 	%rd123, %r322;
	mul.lo.s64 	%rd124, %rd123, %rd36;
	add.s64 	%rd125, %rd18, %rd124;
	ld.global.f64 	%fd214, [%rd125];

$L__BB7_19:
	ld.param.u64 	%rd196, [cg_one_iter_cuda_kernel_backward_param_15];
	cvt.s64.s32 	%rd64, %r5;
	mul.lo.s64 	%rd65, %rd64, %rd37;
	add.s64 	%rd126, %rd26, %rd65;
	ld.global.f64 	%fd7, [%rd126];
	ld.global.f64 	%fd8, [%rd126+8];
	ld.global.f64 	%fd9, [%rd126+16];
	mul.lo.s64 	%rd66, %rd64, %rd38;
	add.s64 	%rd127, %rd25, %rd66;
	ld.global.f64 	%fd10, [%rd127];
	ld.global.f64 	%fd11, [%rd127+8];
	ld.global.f64 	%fd12, [%rd127+16];
	mul.lo.s64 	%rd67, %rd64, %rd39;
	add.s64 	%rd128, %rd24, %rd67;
	mul.lo.s64 	%rd68, %rd64, %rd40;
	add.s64 	%rd129, %rd29, %rd68;
	ld.global.f64 	%fd13, [%rd128];
	ld.global.f64 	%fd14, [%rd128+8];
	ld.global.f64 	%fd15, [%rd128+16];
	ld.global.f64 	%fd16, [%rd128+24];
	ld.global.f64 	%fd17, [%rd128+32];
	ld.global.f64 	%fd18, [%rd128+40];
	ld.global.f64 	%fd19, [%rd128+48];
	ld.global.f64 	%fd20, [%rd128+56];
	ld.global.f64 	%fd21, [%rd128+64];
	ld.global.f64 	%fd22, [%rd129];
	ld.global.f64 	%fd23, [%rd129+8];
	ld.global.f64 	%fd24, [%rd129+16];
	selp.f64 	%fd25, %fd215, %fd214, %p11;
	setp.eq.s64 	%p13, %rd196, 0;
	@%p13 bra 	$L__BB7_21;

	mul.lo.s64 	%rd130, %rd64, %rd42;
	add.s64 	%rd131, %rd19, %rd130;
	ld.global.f64 	%fd77, [%rd131];
	add.f64 	%fd207, %fd77, 0d0000000000000000;
	ld.global.f64 	%fd78, [%rd131+8];
	add.f64 	%fd206, %fd78, 0d0000000000000000;
	ld.global.f64 	%fd79, [%rd131+16];
	add.f64 	%fd205, %fd79, 0d0000000000000000;
	bra.uni 	$L__BB7_23;

$L__BB7_21:
	ld.param.u64 	%rd197, [cg_one_iter_cuda_kernel_backward_param_3+8];
	setp.eq.s64 	%p14, %rd197, 0;
	mov.f64 	%fd205, 0d0000000000000000;
	mov.f64 	%fd206, %fd205;
	mov.f64 	%fd207, %fd205;
	@%p14 bra 	$L__BB7_23;

	mul.lo.s64 	%rd132, %rd64, %rd43;
	add.s64 	%rd133, %rd27, %rd132;
	ld.global.f64 	%fd83, [%rd133];
	add.f64 	%fd207, %fd83, 0d0000000000000000;
	ld.global.f64 	%fd84, [%rd133+8];
	add.f64 	%fd206, %fd84, 0d0000000000000000;
	ld.global.f64 	%fd85, [%rd133+16];
	add.f64 	%fd205, %fd85, 0d0000000000000000;

$L__BB7_23:
	fma.rn.f64 	%fd35, %fd207, %fd22, 0d0000000000000000;
	fma.rn.f64 	%fd36, %fd207, %fd23, 0d0000000000000000;
	fma.rn.f64 	%fd37, %fd207, %fd24, 0d0000000000000000;
	fma.rn.f64 	%fd38, %fd206, %fd22, 0d0000000000000000;
	fma.rn.f64 	%fd39, %fd206, %fd23, 0d0000000000000000;
	fma.rn.f64 	%fd40, %fd206, %fd24, 0d0000000000000000;
	fma.rn.f64 	%fd41, %fd205, %fd22, 0d0000000000000000;
	fma.rn.f64 	%fd42, %fd205, %fd23, 0d0000000000000000;
	fma.rn.f64 	%fd43, %fd205, %fd24, 0d0000000000000000;
	mul.f64 	%fd86, %fd16, %fd206;
	fma.rn.f64 	%fd87, %fd13, %fd207, %fd86;
	mul.f64 	%fd88, %fd17, %fd206;
	fma.rn.f64 	%fd89, %fd14, %fd207, %fd88;
	mul.f64 	%fd90, %fd18, %fd206;
	fma.rn.f64 	%fd91, %fd15, %fd207, %fd90;
	fma.rn.f64 	%fd92, %fd19, %fd205, %fd87;
	fma.rn.f64 	%fd93, %fd20, %fd205, %fd89;
	fma.rn.f64 	%fd94, %fd21, %fd205, %fd91;
	add.f64 	%fd44, %fd92, 0d0000000000000000;
	add.f64 	%fd45, %fd93, 0d0000000000000000;
	add.f64 	%fd46, %fd94, 0d0000000000000000;
	setp.eq.s64 	%p15, %rd92, 0;
	@%p15 bra 	$L__BB7_25;

	mul.lo.s64 	%rd137, %rd64, %rd44;
	add.s64 	%rd134, %rd92, %rd137;
	// begin inline asm
	{ atom.add.f64 %fd95,[%rd134],%fd44; }

	// end inline asm
	add.s64 	%rd135, %rd134, 8;
	// begin inline asm
	{ atom.add.f64 %fd97,[%rd135],%fd45; }

	// end inline asm
	add.s64 	%rd136, %rd134, 16;
	// begin inline asm
	{ atom.add.f64 %fd99,[%rd136],%fd46; }

	// end inline asm
	bra.uni 	$L__BB7_27;

$L__BB7_25:
	setp.eq.s64 	%p16, %rd75, 0;
	@%p16 bra 	$L__BB7_27;

	add.s64 	%rd138, %rd75, %rd68;
	// begin inline asm
	{ atom.add.f64 %fd101,[%rd138],%fd44; }

	// end inline asm
	add.s64 	%rd139, %rd138, 8;
	// begin inline asm
	{ atom.add.f64 %fd103,[%rd139],%fd45; }

	// end inline asm
	add.s64 	%rd140, %rd138, 16;
	// begin inline asm
	{ atom.add.f64 %fd105,[%rd140],%fd46; }

	// end inline asm

$L__BB7_27:
	setp.eq.s64 	%p17, %rd102, 0;
	@%p17 bra 	$L__BB7_29;

	mul.lo.s64 	%rd150, %rd64, %rd45;
	add.s64 	%rd141, %rd102, %rd150;
	// begin inline asm
	{ atom.add.f64 %fd107,[%rd141],%fd35; }

	// end inline asm
	add.s64 	%rd142, %rd141, 8;
	// begin inline asm
	{ atom.add.f64 %fd109,[%rd142],%fd36; }

	// end inline asm
	add.s64 	%rd143, %rd141, 16;
	// begin inline asm
	{ atom.add.f64 %fd111,[%rd143],%fd37; }

	// end inline asm
	add.s64 	%rd144, %rd141, 24;
	// begin inline asm
	{ atom.add.f64 %fd113,[%rd144],%fd38; }

	// end inline asm
	add.s64 	%rd145, %rd141, 32;
	// begin inline asm
	{ atom.add.f64 %fd115,[%rd145],%fd39; }

	// end inline asm
	add.s64 	%rd146, %rd141, 40;
	// begin inline asm
	{ atom.add.f64 %fd117,[%rd146],%fd40; }

	// end inline asm
	add.s64 	%rd147, %rd141, 48;
	// begin inline asm
	{ atom.add.f64 %fd119,[%rd147],%fd41; }

	// end inline asm
	add.s64 	%rd148, %rd141, 56;
	// begin inline asm
	{ atom.add.f64 %fd121,[%rd148],%fd42; }

	// end inline asm
	add.s64 	%rd149, %rd141, 64;
	// begin inline asm
	{ atom.add.f64 %fd123,[%rd149],%fd43; }

	// end inline asm
	bra.uni 	$L__BB7_31;

$L__BB7_29:
	setp.eq.s64 	%p18, %rd85, 0;
	@%p18 bra 	$L__BB7_31;

	add.s64 	%rd151, %rd85, %rd67;
	// begin inline asm
	{ atom.add.f64 %fd125,[%rd151],%fd35; }

	// end inline asm
	add.s64 	%rd152, %rd151, 8;
	// begin inline asm
	{ atom.add.f64 %fd127,[%rd152],%fd36; }

	// end inline asm
	add.s64 	%rd153, %rd151, 16;
	// begin inline asm
	{ atom.add.f64 %fd129,[%rd153],%fd37; }

	// end inline asm
	add.s64 	%rd154, %rd151, 24;
	// begin inline asm
	{ atom.add.f64 %fd131,[%rd154],%fd38; }

	// end inline asm
	add.s64 	%rd155, %rd151, 32;
	// begin inline asm
	{ atom.add.f64 %fd133,[%rd155],%fd39; }

	// end inline asm
	add.s64 	%rd156, %rd151, 40;
	// begin inline asm
	{ atom.add.f64 %fd135,[%rd156],%fd40; }

	// end inline asm
	add.s64 	%rd157, %rd151, 48;
	// begin inline asm
	{ atom.add.f64 %fd137,[%rd157],%fd41; }

	// end inline asm
	add.s64 	%rd158, %rd151, 56;
	// begin inline asm
	{ atom.add.f64 %fd139,[%rd158],%fd42; }

	// end inline asm
	add.s64 	%rd159, %rd151, 64;
	// begin inline asm
	{ atom.add.f64 %fd141,[%rd159],%fd43; }

	// end inline asm

$L__BB7_31:
	setp.eq.s64 	%p34, %rd92, 0;
	@%p34 bra 	$L__BB7_33;

	cvta.to.global.u64 	%rd193, %rd92;
	mul.lo.s64 	%rd160, %rd64, %rd44;
	add.s64 	%rd161, %rd193, %rd160;
	ld.global.f64 	%fd143, [%rd161];
	mov.f64 	%fd144, 0d0000000000000000;
	sub.f64 	%fd210, %fd144, %fd143;
	ld.global.f64 	%fd145, [%rd161+8];
	sub.f64 	%fd209, %fd144, %fd145;
	ld.global.f64 	%fd146, [%rd161+16];
	sub.f64 	%fd208, %fd144, %fd146;
	bra.uni 	$L__BB7_35;

$L__BB7_33:
	setp.eq.s64 	%p20, %rd75, 0;
	mov.f64 	%fd208, 0d0000000000000000;
	mov.f64 	%fd209, %fd208;
	mov.f64 	%fd210, %fd208;
	@%p20 bra 	$L__BB7_35;

	cvta.to.global.u64 	%rd199, %rd75;
	add.s64 	%rd162, %rd199, %rd68;
	ld.global.f64 	%fd150, [%rd162];
	mov.f64 	%fd151, 0d0000000000000000;
	sub.f64 	%fd210, %fd151, %fd150;
	ld.global.f64 	%fd152, [%rd162+8];
	sub.f64 	%fd209, %fd151, %fd152;
	ld.global.f64 	%fd153, [%rd162+16];
	sub.f64 	%fd208, %fd151, %fd153;

$L__BB7_35:
	fma.rn.f64 	%fd56, %fd25, %fd210, 0d0000000000000000;
	fma.rn.f64 	%fd57, %fd25, %fd209, 0d0000000000000000;
	fma.rn.f64 	%fd58, %fd25, %fd208, 0d0000000000000000;
	mul.f64 	%fd154, %fd11, %fd209;
	fma.rn.f64 	%fd155, %fd10, %fd210, %fd154;
	fma.rn.f64 	%fd59, %fd12, %fd208, %fd155;
	setp.eq.s64 	%p21, %rd100, 0;
	@%p21 bra 	$L__BB7_37;

	mul.lo.s64 	%rd166, %rd64, %rd46;
	add.s64 	%rd163, %rd100, %rd166;
	// begin inline asm
	{ atom.add.f64 %fd156,[%rd163],%fd56; }

	// end inline asm
	add.s64 	%rd164, %rd163, 8;
	// begin inline asm
	{ atom.add.f64 %fd158,[%rd164],%fd57; }

	// end inline asm
	add.s64 	%rd165, %rd163, 16;
	// begin inline asm
	{ atom.add.f64 %fd160,[%rd165],%fd58; }

	// end inline asm
	bra.uni 	$L__BB7_39;

$L__BB7_37:
	setp.eq.s64 	%p22, %rd83, 0;
	@%p22 bra 	$L__BB7_39;

	add.s64 	%rd167, %rd83, %rd66;
	// begin inline asm
	{ atom.add.f64 %fd162,[%rd167],%fd56; }

	// end inline asm
	add.s64 	%rd168, %rd167, 8;
	// begin inline asm
	{ atom.add.f64 %fd164,[%rd168],%fd57; }

	// end inline asm
	add.s64 	%rd169, %rd167, 16;
	// begin inline asm
	{ atom.add.f64 %fd166,[%rd169],%fd58; }

	// end inline asm

$L__BB7_39:
	ld.param.u64 	%rd194, [cg_one_iter_cuda_kernel_backward_param_13];
	setp.eq.s64 	%p23, %rd194, 0;
	@%p23 bra 	$L__BB7_41;

	mul.lo.s64 	%rd170, %rd64, %rd47;
	add.s64 	%rd171, %rd21, %rd170;
	ld.global.f64 	%fd168, [%rd171];
	add.f64 	%fd213, %fd168, 0d0000000000000000;
	ld.global.f64 	%fd169, [%rd171+8];
	add.f64 	%fd212, %fd169, 0d0000000000000000;
	ld.global.f64 	%fd170, [%rd171+16];
	add.f64 	%fd211, %fd170, 0d0000000000000000;
	bra.uni 	$L__BB7_43;

$L__BB7_41:
	ld.param.u64 	%rd198, [cg_one_iter_cuda_kernel_backward_param_1+8];
	setp.eq.s64 	%p24, %rd198, 0;
	mov.f64 	%fd211, 0d0000000000000000;
	mov.f64 	%fd212, %fd211;
	mov.f64 	%fd213, %fd211;
	@%p24 bra 	$L__BB7_43;

	mul.lo.s64 	%rd172, %rd64, %rd48;
	add.s64 	%rd173, %rd30, %rd172;
	ld.global.f64 	%fd174, [%rd173];
	add.f64 	%fd213, %fd174, 0d0000000000000000;
	ld.global.f64 	%fd175, [%rd173+8];
	add.f64 	%fd212, %fd175, 0d0000000000000000;
	ld.global.f64 	%fd176, [%rd173+16];
	add.f64 	%fd211, %fd176, 0d0000000000000000;

$L__BB7_43:
	fma.rn.f64 	%fd69, %fd25, %fd213, 0d0000000000000000;
	fma.rn.f64 	%fd70, %fd25, %fd212, 0d0000000000000000;
	fma.rn.f64 	%fd71, %fd25, %fd211, 0d0000000000000000;
	mul.f64 	%fd177, %fd8, %fd212;
	fma.rn.f64 	%fd178, %fd7, %fd213, %fd177;
	fma.rn.f64 	%fd179, %fd9, %fd211, %fd178;
	add.f64 	%fd180, %fd59, 0d0000000000000000;
	add.f64 	%fd72, %fd180, %fd179;
	setp.eq.s64 	%p25, %rd98, 0;
	@%p25 bra 	$L__BB7_45;

	mul.lo.s64 	%rd177, %rd64, %rd49;
	add.s64 	%rd174, %rd98, %rd177;
	// begin inline asm
	{ atom.add.f64 %fd181,[%rd174],%fd69; }

	// end inline asm
	add.s64 	%rd175, %rd174, 8;
	// begin inline asm
	{ atom.add.f64 %fd183,[%rd175],%fd70; }

	// end inline asm
	add.s64 	%rd176, %rd174, 16;
	// begin inline asm
	{ atom.add.f64 %fd185,[%rd176],%fd71; }

	// end inline asm
	bra.uni 	$L__BB7_47;

$L__BB7_45:
	setp.eq.s64 	%p26, %rd81, 0;
	@%p26 bra 	$L__BB7_47;

	add.s64 	%rd178, %rd81, %rd65;
	// begin inline asm
	{ atom.add.f64 %fd187,[%rd178],%fd69; }

	// end inline asm
	add.s64 	%rd179, %rd178, 8;
	// begin inline asm
	{ atom.add.f64 %fd189,[%rd179],%fd70; }

	// end inline asm
	add.s64 	%rd180, %rd178, 16;
	// begin inline asm
	{ atom.add.f64 %fd191,[%rd180],%fd71; }

	// end inline asm

$L__BB7_47:
	add.f64 	%fd73, %fd72, 0d0000000000000000;
	@%p11 bra 	$L__BB7_52;

	setp.eq.s64 	%p28, %rd96, 0;
	@%p28 bra 	$L__BB7_50;

	cvt.s64.s32 	%rd182, %r322;
	mul.lo.s64 	%rd183, %rd182, %rd50;
	add.s64 	%rd181, %rd96, %rd183;
	// begin inline asm
	{ atom.add.f64 %fd193,[%rd181],%fd73; }

	// end inline asm
	bra.uni 	$L__BB7_52;

$L__BB7_50:
	setp.eq.s64 	%p29, %rd79, 0;
	@%p29 bra 	$L__BB7_52;

	cvt.s64.s32 	%rd185, %r322;
	mul.lo.s64 	%rd186, %rd185, %rd36;
	add.s64 	%rd184, %rd79, %rd186;
	// begin inline asm
	{ atom.add.f64 %fd195,[%rd184],%fd73; }

	// end inline asm

$L__BB7_52:
	setp.gt.s32 	%p36, %r13, 0;
	cvt.u32.u64 	%r316, %rd202;
	selp.b32 	%r315, %r316, 0, %p36;
	setp.ge.s32 	%p35, %r315, %r3;
	@%p35 bra 	$L__BB7_57;

	setp.eq.s64 	%p31, %rd96, 0;
	@%p31 bra 	$L__BB7_55;

	cvt.s64.s32 	%rd188, %rd206;
	mul.lo.s64 	%rd189, %rd188, %rd50;
	add.s64 	%rd187, %rd96, %rd189;
	// begin inline asm
	{ atom.add.f64 %fd197,[%rd187],%fd73; }

	// end inline asm
	bra.uni 	$L__BB7_57;

$L__BB7_55:
	setp.eq.s64 	%p32, %rd79, 0;
	@%p32 bra 	$L__BB7_57;

	cvt.s64.s32 	%rd191, %rd206;
	mul.lo.s64 	%rd192, %rd191, %rd36;
	add.s64 	%rd190, %rd79, %rd192;
	// begin inline asm
	{ atom.add.f64 %fd199,[%rd190],%fd73; }

	// end inline asm

$L__BB7_57:
	ld.param.u64 	%rd195, [cg_one_iter_cuda_kernel_backward_param_0+24];
	add.s64 	%rd200, %rd200, %rd34;
	setp.lt.u64 	%p33, %rd200, %rd195;
	@%p33 bra 	$L__BB7_2;

$L__BB7_58:
	ret;

}

 