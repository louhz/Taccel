//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32345990
// Cuda compilation tools, release 12.1, V12.1.55
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_75
.address_size 64

	// .globl	compute_affine_kinematic_hess_cuda_kernel_forward
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.const .align 4 .b8 pnanovdb_grid_type_value_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 1, 0, 0, 0, 32, 0, 0, 0, 4, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_table_strides_bits[108] = {64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_minmax_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_minmax_aligns_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_stat_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_leaf_type[108] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_constants[3024] = {28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 4, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 40, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 68, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 44, 32, 0, 0, 56, 32, 0, 0, 60, 32, 0, 0, 64, 32, 0, 0, 64, 32, 8, 0, 32, 4, 0, 0, 44, 4, 0, 0, 56, 4, 0, 0, 60, 4, 0, 0, 64, 4, 0, 0, 64, 4, 1, 0, 80, 0, 0, 0, 92, 0, 0, 0, 104, 0, 0, 0, 108, 0, 0, 0, 128, 0, 0, 0, 128, 24, 0, 0, 32, 0, 0, 0, 56, 0, 0, 0, 80, 0, 0, 0, 104, 0, 0, 0, 112, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 24, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 56, 32, 0, 0, 80, 32, 0, 0, 88, 32, 0, 0, 96, 32, 0, 0, 96, 32, 12, 0, 32, 4, 0, 0, 56, 4, 0, 0, 80, 4, 0, 0, 88, 4, 0, 0, 96, 4, 0, 0, 96, 132, 1, 0, 80, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 160, 48, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 1, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 160, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 1, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 2, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 44, 0, 0, 0, 60, 0, 0, 0, 76, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 68, 32, 0, 0, 96, 32, 0, 0, 96, 32, 8, 0, 32, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 68, 4, 0, 0, 96, 4, 0, 0, 96, 4, 1, 0, 80, 0, 0, 0, 96, 0, 0, 0, 112, 0, 0, 0, 116, 0, 0, 0, 128, 0, 0, 0, 128, 32, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 0, 1, 0, 0, 32, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 64, 32, 0, 0, 96, 32, 0, 0, 104, 32, 0, 0, 128, 32, 0, 0, 128, 32, 16, 0, 32, 4, 0, 0, 64, 4, 0, 0, 96, 4, 0, 0, 104, 4, 0, 0, 128, 4, 0, 0, 128, 4, 2, 0, 80, 0, 0, 0, 112, 0, 0, 0, 144, 0, 0, 0, 152, 0, 0, 0, 160, 0, 0, 0, 160, 64, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 31, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 35, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 35, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 83, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 6, 0, 0, 28, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 38, 32, 0, 0, 44, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 38, 4, 0, 0, 44, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 86, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 128, 12, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 81, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 2, 0, 0};
.const .align 4 .b8 pnanovdb_dither_lut[2048] = {70, 182, 19, 62, 172, 173, 36, 63, 175, 149, 84, 63, 42, 171, 169, 62, 33, 148, 215, 61, 175, 178, 26, 63, 21, 170, 43, 62, 176, 170, 42, 63, 193, 141, 100, 63, 44, 155, 201, 62, 36, 172, 167, 61, 170, 181, 20, 63, 180, 146, 90, 63, 51, 165, 181, 62, 181, 138, 106, 63, 54, 149, 213, 62, 171, 177, 28, 63, 0, 140, 231, 61, 175, 153, 76, 63, 41, 179, 153, 62, 157, 190, 2, 63, 41, 160, 63, 60, 182, 134, 114, 63, 55, 141, 229, 62, 43, 163, 185, 62, 176, 145, 92, 63, 62, 152, 79, 61, 170, 185, 12, 63, 48, 189, 133, 62, 178, 158, 66, 63, 23, 154, 75, 62, 177, 166, 50, 63, 44, 184, 15, 62, 37, 174, 35, 63, 36, 182, 19, 63, 39, 176, 159, 61, 31, 189, 5, 63, 41, 160, 191, 60, 59, 138, 235, 62, 56, 133, 117, 63, 59, 142, 99, 63, 65, 156, 199, 62, 29, 172, 167, 62, 58, 150, 83, 63, 19, 186, 139, 62, 52, 157, 69, 63, 51, 165, 53, 63, 33, 148, 87, 62, 69, 132, 247, 62, 61, 130, 123, 63, 28, 180, 151, 62, 57, 154, 75, 63, 51, 136, 239, 61, 33, 177, 29, 63, 57, 144, 95, 61, 31, 185, 13, 63, 39, 162, 59, 63, 51, 136, 111, 62, 35, 186, 11, 63, 41, 160, 63, 61, 54, 145, 93, 63, 56, 162, 187, 62, 53, 153, 77, 63, 53, 178, 155, 62, 169, 189, 4, 63, 52, 176, 159, 60, 47, 139, 233, 62, 194, 133, 116, 63, 177, 162, 58, 63, 26, 138, 107, 62, 157, 186, 10, 63, 47, 168, 47, 61, 40, 187, 137, 62, 174, 157, 68, 63, 173, 165, 52, 63, 74, 150, 83, 62, 56, 133, 245, 62, 199, 130, 122, 63, 49, 181, 149, 62, 179, 154, 74, 63, 77, 134, 115, 62, 173, 161, 60, 63, 45, 147, 217, 62, 194, 137, 108, 63, 19, 186, 11, 62, 176, 174, 34, 63, 50, 173, 165, 62, 179, 150, 82, 63, 195, 129, 124, 63, 48, 131, 249, 62, 172, 169, 44, 63, 72, 166, 51, 62, 180, 142, 98, 63, 52, 157, 197, 62, 158, 182, 18, 63, 41, 180, 151, 61, 35, 190, 3, 63, 19, 128, 127, 60, 49, 152, 79, 62, 38, 166, 51, 63, 28, 180, 23, 62, 50, 173, 37, 63, 54, 149, 85, 63, 55, 170, 171, 62, 27, 188, 135, 62, 56, 158, 67, 63, 60, 134, 115, 63, 67, 140, 231, 62, 55, 141, 101, 63, 57, 154, 203, 62, 47, 168, 175, 61, 32, 181, 21, 63, 58, 146, 91, 63, 30, 164, 183, 62, 59, 138, 107, 63, 66, 148, 215, 62, 52, 161, 61, 63, 35, 132, 119, 62, 51, 169, 45, 63, 30, 164, 55, 62, 84, 144, 223, 61, 37, 178, 27, 63, 47, 168, 47, 62, 38, 170, 43, 63, 61, 130, 251, 62, 56, 129, 125, 63, 58, 146, 219, 62, 55, 137, 109, 63, 34, 188, 135, 61, 162, 183, 16, 63, 163, 175, 32, 63, 35, 190, 3, 62, 56, 162, 59, 62, 168, 168, 46, 63, 169, 160, 62, 63, 61, 130, 123, 62, 183, 151, 80, 63, 25, 175, 161, 62, 60, 159, 193, 62, 184, 143, 96, 63, 190, 136, 110, 63, 71, 145, 221, 62, 73, 129, 253, 62, 191, 128, 126, 63, 31, 184, 15, 61, 161, 187, 8, 63, 186, 131, 120, 63, 64, 135, 241, 62, 58, 146, 91, 62, 169, 164, 54, 63, 165, 188, 6, 63, 30, 144, 223, 60, 183, 155, 72, 63, 23, 183, 145, 62, 42, 142, 99, 62, 181, 163, 56, 63, 190, 132, 118, 63, 72, 137, 237, 62, 31, 185, 141, 62, 170, 156, 70, 63, 69, 132, 119, 63, 51, 136, 239, 62, 49, 152, 207, 62, 51, 140, 103, 63, 31, 184, 143, 61, 40, 183, 17, 63, 63, 143, 97, 63, 40, 158, 195, 62, 84, 144, 95, 62, 47, 164, 55, 63, 46, 172, 39, 63, 12, 176, 31, 62, 45, 151, 81, 63, 37, 174, 163, 62, 60, 188, 7, 62, 41, 175, 33, 63, 73, 128, 127, 61, 44, 184, 15, 63, 48, 160, 63, 63, 86, 128, 127, 62, 63, 139, 105, 63, 41, 150, 211, 62, 65, 131, 121, 63, 77, 134, 243, 62, 49, 152, 79, 63, 45, 176, 159, 62, 52, 128, 255, 62, 69, 128, 127, 63, 63, 172, 39, 62, 42, 171, 41, 63, 67, 140, 103, 62, 43, 163, 57, 63, 164, 167, 48, 63, 40, 158, 67, 62, 83, 128, 127, 59, 161, 191, 0, 63, 166, 184, 14, 63, 51, 136, 111, 61, 102, 132, 247, 61, 167, 176, 30, 63, 63, 143, 225, 62, 186, 135, 112, 63, 182, 159, 64, 63, 22, 191, 129, 62, 33, 177, 157, 62, 187, 152, 78, 63, 188, 144, 94, 63, 35, 161, 189, 62, 164, 171, 40, 63, 37, 174, 35, 62, 59, 167, 177, 62, 184, 147, 88, 63, 166, 180, 22, 63, 44, 164, 183, 61, 53, 178, 27, 62, 168, 172, 38, 63, 62, 151, 209, 62, 185, 139, 104, 63, 162, 179, 24, 63, 52, 156, 199, 61, 34, 169, 173, 62, 188, 148, 86, 63, 189, 140, 102, 63, 36, 153, 205, 62, 47, 168, 175, 62, 49, 148, 87, 63, 48, 156, 71, 63, 44, 184, 143, 62, 42, 167, 49, 63, 65, 156, 71, 62, 35, 190, 131, 62, 44, 159, 65, 63, 45, 180, 23, 63, 54, 160, 191, 61, 73, 128, 255, 60, 27, 188, 7, 63, 42, 142, 227, 62, 64, 135, 113, 63, 39, 191, 1, 63, 62, 128, 255, 59, 47, 168, 47, 63, 81, 160, 63, 62, 19, 128, 255, 61, 45, 176, 31, 63, 36, 182, 147, 62, 44, 155, 73, 63, 38, 166, 179, 62, 62, 147, 89, 63, 50, 144, 223, 62, 68, 136, 111, 63, 50, 144, 95, 63, 48, 160, 191, 62, 40, 187, 9, 63, 52, 176, 31, 61, 41, 179, 25, 63, 116, 152, 207, 61, 227, 54, 18, 63, 43, 182, 147, 61, 247, 30, 66, 63, 152, 189, 132, 62, 234, 35, 56, 63, 63, 143, 97, 62, 230, 59, 8, 63, 34, 188, 7, 61, 155, 173, 164, 62, 248, 22, 82, 63, 41, 176, 31, 60, 226, 62, 2, 63, 202, 135, 240, 62, 255, 3, 120, 63, 162, 183, 144, 62, 235, 27, 72, 63, 226, 58, 10, 63, 49, 172, 39, 61, 247, 34, 58, 63, 47, 139, 105, 62, 230, 63, 0, 63, 83, 128, 255, 58, 231, 55, 16, 63, 35, 190, 131, 61, 154, 181, 148, 62, 248, 26, 74, 63, 194, 133, 244, 62, 251, 2, 122, 63, 161, 191, 128, 62, 235, 31, 64, 63, 163, 175, 160, 62, 236, 23, 80, 63, 116, 7, 113, 63, 180, 142, 226, 62, 115, 15, 97, 63, 178, 158, 194, 62, 186, 160, 190, 62, 119, 16, 95, 63, 184, 176, 158, 62, 118, 24, 79, 63, 19, 157, 69, 62, 112, 39, 49, 63, 14, 189, 5, 62, 111, 47, 33, 63, 98, 48, 31, 63, 61, 130, 251, 61, 97, 56, 15, 63, 75, 132, 119, 61, 111, 43, 41, 63, 16, 173, 37, 62, 112, 35, 57, 63, 88, 141, 101, 62, 187, 152, 206, 62, 120, 12, 103, 63, 119, 20, 87, 63, 185, 168, 174, 62, 179, 150, 210, 62, 116, 11, 105, 63, 182, 134, 242, 62, 134, 3, 121, 63, 98, 44, 39, 63, 33, 177, 29, 62, 42, 162, 187, 61, 97, 52, 23, 63, 44, 155, 73, 62, 229, 38, 50, 63, 191, 157, 196, 62, 250, 14, 98, 63, 53, 158, 195, 61, 232, 51, 24, 63, 58, 175, 33, 62, 233, 43, 40, 63, 251, 6, 114, 63, 193, 141, 228, 62, 228, 46, 34, 63, 40, 187, 9, 62, 253, 19, 88, 63, 164, 167, 176, 62, 254, 11, 104, 63, 166, 151, 208, 62, 42, 171, 41, 62, 229, 42, 42, 63, 74, 150, 211, 61, 228, 50, 26, 63, 56, 191, 1, 62, 232, 47, 32, 63, 60, 159, 65, 62, 233, 39, 48, 63, 250, 10, 106, 63, 192, 149, 212, 62, 249, 18, 90, 63, 190, 165, 180, 62, 254, 15, 96, 63, 165, 159, 192, 62, 255, 7, 112, 63, 168, 143, 224, 62, 176, 174, 162, 62, 114, 23, 81, 63, 173, 190, 130, 62, 113, 31, 65, 63, 122, 0, 127, 63, 191, 128, 254, 62, 120, 8, 111, 63, 188, 144, 222, 62, 93, 55, 17, 63, 32, 186, 139, 61, 91, 63, 1, 63, 41, 160, 191, 59, 40, 129, 125, 62, 117, 32, 63, 63, 35, 161, 61, 62, 116, 40, 47, 63, 28, 180, 23, 61, 92, 59, 9, 63, 50, 154, 203, 61, 110, 51, 25, 63, 118, 28, 71, 63, 149, 184, 142, 62, 190, 136, 238, 62, 121, 4, 119, 63, 113, 27, 73, 63, 174, 182, 146, 62, 115, 19, 89, 63, 177, 166, 178, 62, 78, 136, 239, 60, 96, 60, 7, 63, 116, 36, 55, 63, 37, 145, 93, 62, 175, 153, 204, 62, 2, 13, 102, 63, 3, 5, 118, 63, 177, 137, 236, 62, 38, 156, 71, 61, 222, 57, 12, 63, 42, 142, 227, 61, 223, 49, 28, 63, 237, 44, 38, 63, 7, 179, 25, 62, 79, 147, 89, 62, 238, 36, 54, 63, 244, 25, 76, 63, 179, 179, 152, 62, 245, 17, 92, 63, 181, 163, 184, 62, 10, 134, 243, 61, 236, 48, 30, 63, 54, 140, 103, 61, 235, 56, 14, 63, 180, 171, 168, 62, 244, 21, 84, 63, 222, 61, 4, 63, 58, 184, 143, 60, 241, 16, 94, 63, 173, 161, 188, 62, 240, 24, 78, 63, 171, 177, 156, 62, 223, 53, 20, 63, 37, 174, 163, 61, 178, 187, 136, 62, 243, 29, 68, 63, 157, 186, 138, 62, 105, 29, 69, 63, 104, 37, 53, 63, 54, 149, 85, 62, 107, 42, 43, 63, 67, 169, 45, 62, 106, 50, 27, 63, 247, 145, 219, 61, 100, 61, 5, 63, 47, 168, 175, 60, 198, 138, 234, 62, 125, 5, 117, 63, 171, 148, 214, 62, 129, 10, 107, 63, 169, 164, 182, 62, 127, 18, 91, 63, 126, 1, 125, 63, 199, 130, 250, 62, 197, 146, 218, 62, 125, 9, 109, 63, 172, 140, 230, 62, 129, 6, 115, 63, 104, 62, 3, 63, 30, 144, 95, 60, 56, 133, 117, 62, 104, 33, 61, 63, 103, 41, 45, 63, 51, 165, 53, 62, 108, 38, 51, 63, 70, 153, 77, 62, 165, 188, 134, 62, 109, 30, 67, 63, 239, 28, 70, 63, 170, 185, 140, 62, 172, 169, 172, 62, 240, 20, 86, 63, 241, 41, 44, 63, 26, 167, 49, 62, 243, 33, 60, 63, 30, 135, 113, 62, 35, 152, 207, 60, 218, 60, 6, 63, 236, 52, 22, 63, 45, 166, 179, 61, 184, 147, 216, 62, 246, 9, 108, 63, 186, 131, 248, 62, 247, 1, 124, 63, 239, 32, 62, 63, 81, 131, 121, 62, 237, 40, 46, 63, 77, 163, 57, 62, 247, 5, 116, 63, 185, 139, 232, 62, 23, 183, 17, 62, 241, 45, 36, 63, 178, 129, 252, 62, 4, 1, 126, 63, 176, 145, 220, 62, 3, 9, 110, 63, 28, 151, 81, 62, 242, 37, 52, 63, 246, 13, 100, 63, 183, 155, 200, 62, 124, 13, 101, 63, 195, 154, 202, 62, 48, 170, 171, 61, 101, 53, 21, 63, 44, 164, 55, 61, 105, 58, 11, 63, 72, 137, 109, 62, 108, 34, 59, 63, 49, 181, 21, 62, 102, 45, 37, 63, 123, 21, 85, 63, 159, 170, 170, 62, 109, 26, 75, 63, 166, 180, 150, 62, 130, 2, 123, 63, 173, 132, 246, 62, 161, 162, 186, 62, 123, 17, 93, 63, 122, 25, 77, 63, 158, 178, 154, 62, 110, 22, 83, 63, 168, 172, 166, 62, 65, 185, 13, 62, 106, 46, 35, 63, 102, 49, 29, 63, 93, 138, 235, 61, 60, 148, 87, 61, 101, 57, 13, 63, 40, 178, 155, 61, 105, 54, 19, 63, 128, 14, 99, 63, 170, 156, 198, 62};
.global .align 8 .f64 _ZN2wp11_svd_configIdE17QR_GIVENS_EPSILONE = 0d3D719799812DEA11;
.global .align 4 .u32 _ZN2wp11_svd_configIdE17JACOBI_ITERATIONSE = 8;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIiE9GRID_TYPEE = 4;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIxE9GRID_TYPEE = 5;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIjE9GRID_TYPEE = 10;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIfE9GRID_TYPEE = 1;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIdE9GRID_TYPEE = 2;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EfEEE9GRID_TYPEE = 6;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EdEEE9GRID_TYPEE = 7;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EfEEE9GRID_TYPEE = 17;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EdEEE9GRID_TYPEE = 18;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9all_hostsE = 1;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_35_bitE = 2;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_37_bitE = 4;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_50_bitE = 8;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_52_bitE = 16;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_53_bitE = 32;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_60_bitE = 64;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_61_bitE = 128;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_62_bitE = 256;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_70_bitE = 512;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_72_bitE = 1024;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_75_bitE = 2048;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_80_bitE = 4096;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_86_bitE = 8192;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_87_bitE = 16384;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_89_bitE = 32768;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail9sm_90_bitE = 65536;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target6detail11all_devicesE = 131070;
.global .align 8 .b8 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target7is_hostE[8] = {1, 0, 0, 0, 0, 0, 0, 0};
.global .align 8 .b8 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target9is_deviceE[8] = {254, 255, 1, 0, 0, 0, 0, 0};
.global .align 8 .b8 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target10any_targetE[8] = {255, 255, 1, 0, 0, 0, 0, 0};
.global .align 8 .b8 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target9no_targetE[8];
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_35E = 35;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_37E = 37;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_50E = 50;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_52E = 52;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_53E = 53;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_60E = 60;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_61E = 61;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_62E = 62;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_70E = 70;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_72E = 72;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_75E = 75;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_80E = 80;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_86E = 86;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_87E = 87;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_89E = 89;
.global .align 8 .u64 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2nv6target5sm_90E = 90;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp6volume7CLOSESTE;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp6volume6LINEARE = 1;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp15LAUNCH_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp14ARRAY_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp18ARRAY_TYPE_REGULARE;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp18ARRAY_TYPE_INDEXEDE = 1;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp17ARRAY_TYPE_FABRICE = 2;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp25ARRAY_TYPE_FABRIC_INDEXEDE = 3;
.global .align 1 .b8 $str[54] = {91, 67, 79, 79, 77, 97, 116, 114, 105, 120, 32, 79, 70, 66, 32, 69, 114, 114, 111, 114, 93, 9, 98, 108, 111, 99, 107, 95, 105, 110, 100, 101, 120, 58, 32, 37, 100, 44, 32, 115, 105, 122, 101, 58, 32, 37, 100, 33, 33, 33, 33, 33, 10, 0};

.visible .entry compute_affine_kinematic_hess_cuda_kernel_forward(
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_forward_param_1[56],
	.param .f64 compute_affine_kinematic_hess_cuda_kernel_forward_param_2,
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_forward_param_3[184],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_forward_param_6[56]
)
{
	.local .align 8 .b8 	__local_depot0[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<47>;
	.reg .b32 	%r<120>;
	.reg .f64 	%fd<76>;
	.reg .b64 	%rd<121>;


	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r51, %r52}, [compute_affine_kinematic_hess_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r53, %r54}, [compute_affine_kinematic_hess_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r59, %r60}, [compute_affine_kinematic_hess_cuda_kernel_forward_param_1+32];
	ld.param.f64 	%fd2, [compute_affine_kinematic_hess_cuda_kernel_forward_param_2];
	mov.b64 	%rd35, compute_affine_kinematic_hess_cuda_kernel_forward_param_3;
	ld.param.v2.u32 	{%r67, %r68}, [compute_affine_kinematic_hess_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r75, %r76}, [compute_affine_kinematic_hess_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r83, %r84}, [compute_affine_kinematic_hess_cuda_kernel_forward_param_6+32];
	ld.param.u64 	%rd40, [compute_affine_kinematic_hess_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd38, [compute_affine_kinematic_hess_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd36, [compute_affine_kinematic_hess_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd33, [compute_affine_kinematic_hess_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd32, [compute_affine_kinematic_hess_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r14, [compute_affine_kinematic_hess_cuda_kernel_forward_param_0+16];
	mov.u32 	%r87, %ntid.x;
	cvt.u64.u32 	%rd1, %r87;
	mov.u32 	%r88, %ctaid.x;
	mul.wide.u32 	%rd42, %r87, %r88;
	mov.u32 	%r89, %tid.x;
	cvt.u64.u32 	%rd43, %r89;
	add.s64 	%rd117, %rd42, %rd43;
	setp.ge.u64 	%p1, %rd117, %rd32;
	@%p1 bra 	$L__BB0_30;

	add.u64 	%rd44, %SP, 0;
	add.u64 	%rd4, %SPL, 0;
	cvta.to.global.u64 	%rd5, %rd40;
	cvta.to.global.u64 	%rd6, %rd38;
	cvta.to.global.u64 	%rd7, %rd36;
	cvt.s64.s32 	%rd8, %r54;
	cvt.s64.s32 	%rd9, %r53;
	cvt.s64.s32 	%rd10, %r52;
	cvt.s64.s32 	%rd11, %r75;
	cvt.s64.s32 	%rd12, %r83;
	mov.u32 	%r90, %nctaid.x;
	cvt.u64.u32 	%rd45, %r90;
	mul.lo.s64 	%rd13, %rd1, %rd45;
	cvt.s64.s32 	%rd14, %r67;
	cvt.s64.s32 	%rd15, %r59;
	cvta.to.global.u64 	%rd16, %rd33;
	mov.u64 	%rd17, %rd35;

$L__BB0_2:
	setp.lt.s32 	%p2, %r14, 4;
	mov.u64 	%rd118, %rd117;
	@%p2 bra 	$L__BB0_6;

	or.b64  	%rd46, %rd117, %rd8;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p3, %rd47, 0;
	@%p3 bra 	$L__BB0_5;

	div.u64 	%rd118, %rd117, %rd8;
	bra.uni 	$L__BB0_6;

$L__BB0_5:
	cvt.u32.u64 	%r91, %rd8;
	cvt.u32.u64 	%r92, %rd117;
	div.u32 	%r93, %r92, %r91;
	cvt.u64.u32 	%rd118, %r93;

$L__BB0_6:
	setp.lt.s32 	%p4, %r14, 3;
	@%p4 bra 	$L__BB0_10;

	or.b64  	%rd48, %rd118, %rd9;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p5, %rd49, 0;
	@%p5 bra 	$L__BB0_9;

	div.u64 	%rd118, %rd118, %rd9;
	bra.uni 	$L__BB0_10;

$L__BB0_9:
	cvt.u32.u64 	%r94, %rd9;
	cvt.u32.u64 	%r95, %rd118;
	div.u32 	%r96, %r95, %r94;
	cvt.u64.u32 	%rd118, %r96;

$L__BB0_10:
	setp.lt.s32 	%p6, %r14, 2;
	@%p6 bra 	$L__BB0_14;

	or.b64  	%rd50, %rd118, %rd10;
	and.b64  	%rd51, %rd50, -4294967296;
	setp.eq.s64 	%p7, %rd51, 0;
	@%p7 bra 	$L__BB0_13;

	div.u64 	%rd118, %rd118, %rd10;
	bra.uni 	$L__BB0_14;

$L__BB0_13:
	cvt.u32.u64 	%r97, %rd10;
	cvt.u32.u64 	%r98, %rd118;
	div.u32 	%r99, %r98, %r97;
	cvt.u64.u32 	%rd118, %r99;

$L__BB0_14:
	cvt.u32.u64 	%r100, %rd118;
	setp.gt.s32 	%p8, %r14, 0;
	selp.b32 	%r101, %r100, 0, %p8;
	cvt.s64.s32 	%rd28, %r101;
	mul.lo.s64 	%rd52, %rd28, %rd11;
	add.s64 	%rd53, %rd6, %rd52;
	ld.global.s32 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd12;
	add.s64 	%rd56, %rd5, %rd55;
	ld.global.u32 	%r102, [%rd56];
	add.s32 	%r103, %r102, -1;
	setp.lt.u32 	%p9, %r103, 2;
	@%p9 bra 	$L__BB0_29;

	mul.lo.s64 	%rd57, %rd28, %rd14;
	add.s64 	%rd58, %rd7, %rd57;
	mul.lo.s64 	%rd59, %rd28, %rd15;
	add.s64 	%rd60, %rd16, %rd59;
	ld.global.f64 	%fd3, [%rd58];
	mul.f64 	%fd1, %fd3, %fd2;
	ld.global.u8 	%rs33, [%rd60];
	setp.eq.s16 	%p10, %rs33, 0;
	@%p10 bra 	$L__BB0_29;

	cvt.u32.u64 	%r104, %rd28;
	shl.b32 	%r2, %r104, 4;
	ld.param.u32 	%r3, [%rd17+172];
	setp.le.s32 	%p11, %r3, %r2;
	selp.u16 	%rs34, 1, 0, %p11;
	shr.u32 	%r105, %r104, 27;
	cvt.u16.u32 	%rs35, %r105;
	and.b16  	%rs36, %rs35, 1;
	or.b16  	%rs37, %rs36, %rs34;
	setp.eq.s16 	%p12, %rs37, 0;
	@%p12 bra 	$L__BB0_18;

	st.local.v2.u32 	[%rd4], {%r2, %r3};
	mov.u64 	%rd61, $str;
	cvta.global.u64 	%rd62, %rd61;
	{ // callseq 79, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd62;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd44;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r106, [retval0+0];
	} // callseq 79
	bra.uni 	$L__BB0_19;

$L__BB0_18:
	ld.param.u32 	%r107, [%rd17+144];
	mul.wide.s32 	%rd73, %r107, %r2;
	ld.param.u64 	%rd74, [%rd17+112];
	add.s64 	%rd64, %rd74, %rd73;
	// begin inline asm
	{ atom.add.f64 %fd4,[%rd64],%fd1; }

	// end inline asm
	add.s64 	%rd65, %rd64, 8;
	mov.f64 	%fd19, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd6,[%rd65],%fd19; }

	// end inline asm
	add.s64 	%rd66, %rd64, 16;
	// begin inline asm
	{ atom.add.f64 %fd8,[%rd66],%fd19; }

	// end inline asm
	add.s64 	%rd67, %rd64, 24;
	// begin inline asm
	{ atom.add.f64 %fd10,[%rd67],%fd19; }

	// end inline asm
	add.s64 	%rd68, %rd64, 32;
	// begin inline asm
	{ atom.add.f64 %fd12,[%rd68],%fd1; }

	// end inline asm
	add.s64 	%rd69, %rd64, 40;
	// begin inline asm
	{ atom.add.f64 %fd14,[%rd69],%fd19; }

	// end inline asm
	add.s64 	%rd70, %rd64, 48;
	// begin inline asm
	{ atom.add.f64 %fd16,[%rd70],%fd19; }

	// end inline asm
	add.s64 	%rd71, %rd64, 56;
	// begin inline asm
	{ atom.add.f64 %fd18,[%rd71],%fd19; }

	// end inline asm
	add.s64 	%rd72, %rd64, 64;
	// begin inline asm
	{ atom.add.f64 %fd20,[%rd72],%fd1; }

	// end inline asm

$L__BB0_19:
	ld.param.u32 	%r4, [%rd17+172];
	add.s32 	%r5, %r2, 5;
	setp.le.s32 	%p13, %r4, %r5;
	selp.u16 	%rs38, 1, 0, %p13;
	shr.u32 	%r108, %r5, 31;
	cvt.u16.u32 	%rs39, %r108;
	or.b16  	%rs40, %rs38, %rs39;
	setp.eq.s16 	%p14, %rs40, 0;
	@%p14 bra 	$L__BB0_21;

	add.s32 	%r117, %r2, 5;
	st.local.v2.u32 	[%rd4], {%r117, %r4};
	mov.u64 	%rd75, $str;
	cvta.global.u64 	%rd76, %rd75;
	{ // callseq 80, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd76;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd44;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r109, [retval0+0];
	} // callseq 80
	bra.uni 	$L__BB0_22;

$L__BB0_21:
	ld.param.u32 	%r110, [%rd17+144];
	mul.wide.s32 	%rd87, %r110, %r5;
	ld.param.u64 	%rd88, [%rd17+112];
	add.s64 	%rd78, %rd88, %rd87;
	// begin inline asm
	{ atom.add.f64 %fd22,[%rd78],%fd1; }

	// end inline asm
	add.s64 	%rd79, %rd78, 8;
	mov.f64 	%fd37, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd24,[%rd79],%fd37; }

	// end inline asm
	add.s64 	%rd80, %rd78, 16;
	// begin inline asm
	{ atom.add.f64 %fd26,[%rd80],%fd37; }

	// end inline asm
	add.s64 	%rd81, %rd78, 24;
	// begin inline asm
	{ atom.add.f64 %fd28,[%rd81],%fd37; }

	// end inline asm
	add.s64 	%rd82, %rd78, 32;
	// begin inline asm
	{ atom.add.f64 %fd30,[%rd82],%fd1; }

	// end inline asm
	add.s64 	%rd83, %rd78, 40;
	// begin inline asm
	{ atom.add.f64 %fd32,[%rd83],%fd37; }

	// end inline asm
	add.s64 	%rd84, %rd78, 48;
	// begin inline asm
	{ atom.add.f64 %fd34,[%rd84],%fd37; }

	// end inline asm
	add.s64 	%rd85, %rd78, 56;
	// begin inline asm
	{ atom.add.f64 %fd36,[%rd85],%fd37; }

	// end inline asm
	add.s64 	%rd86, %rd78, 64;
	// begin inline asm
	{ atom.add.f64 %fd38,[%rd86],%fd1; }

	// end inline asm

$L__BB0_22:
	ld.param.u32 	%r6, [%rd17+172];
	add.s32 	%r7, %r2, 10;
	setp.le.s32 	%p15, %r6, %r7;
	selp.u16 	%rs41, 1, 0, %p15;
	shr.u32 	%r111, %r7, 31;
	cvt.u16.u32 	%rs42, %r111;
	or.b16  	%rs43, %rs41, %rs42;
	setp.eq.s16 	%p16, %rs43, 0;
	@%p16 bra 	$L__BB0_24;

	add.s32 	%r118, %r2, 10;
	st.local.v2.u32 	[%rd4], {%r118, %r6};
	mov.u64 	%rd89, $str;
	cvta.global.u64 	%rd90, %rd89;
	{ // callseq 81, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd90;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd44;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r112, [retval0+0];
	} // callseq 81
	bra.uni 	$L__BB0_25;

$L__BB0_24:
	ld.param.u32 	%r113, [%rd17+144];
	mul.wide.s32 	%rd101, %r113, %r7;
	ld.param.u64 	%rd102, [%rd17+112];
	add.s64 	%rd92, %rd102, %rd101;
	// begin inline asm
	{ atom.add.f64 %fd40,[%rd92],%fd1; }

	// end inline asm
	add.s64 	%rd93, %rd92, 8;
	mov.f64 	%fd55, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd42,[%rd93],%fd55; }

	// end inline asm
	add.s64 	%rd94, %rd92, 16;
	// begin inline asm
	{ atom.add.f64 %fd44,[%rd94],%fd55; }

	// end inline asm
	add.s64 	%rd95, %rd92, 24;
	// begin inline asm
	{ atom.add.f64 %fd46,[%rd95],%fd55; }

	// end inline asm
	add.s64 	%rd96, %rd92, 32;
	// begin inline asm
	{ atom.add.f64 %fd48,[%rd96],%fd1; }

	// end inline asm
	add.s64 	%rd97, %rd92, 40;
	// begin inline asm
	{ atom.add.f64 %fd50,[%rd97],%fd55; }

	// end inline asm
	add.s64 	%rd98, %rd92, 48;
	// begin inline asm
	{ atom.add.f64 %fd52,[%rd98],%fd55; }

	// end inline asm
	add.s64 	%rd99, %rd92, 56;
	// begin inline asm
	{ atom.add.f64 %fd54,[%rd99],%fd55; }

	// end inline asm
	add.s64 	%rd100, %rd92, 64;
	// begin inline asm
	{ atom.add.f64 %fd56,[%rd100],%fd1; }

	// end inline asm

$L__BB0_25:
	ld.param.u32 	%r8, [%rd17+172];
	add.s32 	%r9, %r2, 15;
	setp.le.s32 	%p17, %r8, %r9;
	selp.u16 	%rs44, 1, 0, %p17;
	shr.u32 	%r114, %r9, 31;
	cvt.u16.u32 	%rs45, %r114;
	or.b16  	%rs46, %rs44, %rs45;
	setp.eq.s16 	%p18, %rs46, 0;
	@%p18 bra 	$L__BB0_27;

	add.s32 	%r119, %r2, 15;
	st.local.v2.u32 	[%rd4], {%r119, %r8};
	mov.u64 	%rd103, $str;
	cvta.global.u64 	%rd104, %rd103;
	{ // callseq 82, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd104;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd44;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r115, [retval0+0];
	} // callseq 82
	bra.uni 	$L__BB0_29;

$L__BB0_27:
	ld.param.u32 	%r116, [%rd17+144];
	mul.wide.s32 	%rd115, %r116, %r9;
	ld.param.u64 	%rd116, [%rd17+112];
	add.s64 	%rd106, %rd116, %rd115;
	// begin inline asm
	{ atom.add.f64 %fd58,[%rd106],%fd1; }

	// end inline asm
	add.s64 	%rd107, %rd106, 8;
	mov.f64 	%fd73, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd60,[%rd107],%fd73; }

	// end inline asm
	add.s64 	%rd108, %rd106, 16;
	// begin inline asm
	{ atom.add.f64 %fd62,[%rd108],%fd73; }

	// end inline asm
	add.s64 	%rd109, %rd106, 24;
	// begin inline asm
	{ atom.add.f64 %fd64,[%rd109],%fd73; }

	// end inline asm
	add.s64 	%rd110, %rd106, 32;
	// begin inline asm
	{ atom.add.f64 %fd66,[%rd110],%fd1; }

	// end inline asm
	add.s64 	%rd111, %rd106, 40;
	// begin inline asm
	{ atom.add.f64 %fd68,[%rd111],%fd73; }

	// end inline asm
	add.s64 	%rd112, %rd106, 48;
	// begin inline asm
	{ atom.add.f64 %fd70,[%rd112],%fd73; }

	// end inline asm
	add.s64 	%rd113, %rd106, 56;
	// begin inline asm
	{ atom.add.f64 %fd72,[%rd113],%fd73; }

	// end inline asm
	add.s64 	%rd114, %rd106, 64;
	// begin inline asm
	{ atom.add.f64 %fd74,[%rd114],%fd1; }

	// end inline asm

$L__BB0_29:
	add.s64 	%rd117, %rd117, %rd13;
	setp.lt.u64 	%p19, %rd117, %rd32;
	@%p19 bra 	$L__BB0_2;

$L__BB0_30:
	ret;

}
	// .globl	compute_affine_kinematic_hess_cuda_kernel_backward
.visible .entry compute_affine_kinematic_hess_cuda_kernel_backward(
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_1[56],
	.param .f64 compute_affine_kinematic_hess_cuda_kernel_backward_param_2,
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_3[184],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_7[56],
	.param .f64 compute_affine_kinematic_hess_cuda_kernel_backward_param_8,
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_9[184],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_12[56]
)
{
	.local .align 8 .b8 	__local_depot1[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<35>;
	.reg .b16 	%rs<67>;
	.reg .b32 	%r<178>;
	.reg .f64 	%fd<150>;
	.reg .b64 	%rd<158>;


	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r76, %r77}, [compute_affine_kinematic_hess_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r78, %r79}, [compute_affine_kinematic_hess_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r84, %r85}, [compute_affine_kinematic_hess_cuda_kernel_backward_param_1+32];
	ld.param.f64 	%fd30, [compute_affine_kinematic_hess_cuda_kernel_backward_param_2];
	mov.b64 	%rd43, compute_affine_kinematic_hess_cuda_kernel_backward_param_3;
	ld.param.v2.u32 	{%r92, %r93}, [compute_affine_kinematic_hess_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r100, %r101}, [compute_affine_kinematic_hess_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r108, %r109}, [compute_affine_kinematic_hess_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r116, %r117}, [compute_affine_kinematic_hess_cuda_kernel_backward_param_10+32];
	ld.param.u64 	%rd50, [compute_affine_kinematic_hess_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd48, [compute_affine_kinematic_hess_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd46, [compute_affine_kinematic_hess_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd45, [compute_affine_kinematic_hess_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd44, [compute_affine_kinematic_hess_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd41, [compute_affine_kinematic_hess_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd40, [compute_affine_kinematic_hess_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r30, [compute_affine_kinematic_hess_cuda_kernel_backward_param_0+16];
	mov.u32 	%r120, %ntid.x;
	cvt.u64.u32 	%rd1, %r120;
	mov.u32 	%r121, %ctaid.x;
	mul.wide.u32 	%rd52, %r120, %r121;
	mov.u32 	%r122, %tid.x;
	cvt.u64.u32 	%rd53, %r122;
	add.s64 	%rd154, %rd52, %rd53;
	setp.ge.u64 	%p1, %rd154, %rd40;
	@%p1 bra 	$L__BB1_52;

	add.u64 	%rd54, %SP, 0;
	add.u64 	%rd6, %SPL, 0;
	cvta.to.global.u64 	%rd7, %rd48;
	cvta.to.global.u64 	%rd8, %rd46;
	cvta.to.global.u64 	%rd9, %rd44;
	cvt.s64.s32 	%rd10, %r79;
	cvt.s64.s32 	%rd11, %r78;
	cvt.s64.s32 	%rd12, %r77;
	cvt.s64.s32 	%rd13, %r100;
	cvt.s64.s32 	%rd14, %r108;
	mov.u32 	%r124, %nctaid.x;
	cvt.u64.u32 	%rd55, %r124;
	mul.lo.s64 	%rd15, %rd1, %rd55;
	cvt.s64.s32 	%rd16, %r92;
	cvt.s64.s32 	%rd17, %r84;
	cvt.s64.s32 	%rd18, %r116;
	cvta.to.global.u64 	%rd19, %rd41;
	mov.u64 	%rd20, %rd43;

$L__BB1_2:
	setp.lt.s32 	%p2, %r30, 4;
	mov.u64 	%rd155, %rd154;
	@%p2 bra 	$L__BB1_6;

	or.b64  	%rd56, %rd154, %rd10;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p3, %rd57, 0;
	@%p3 bra 	$L__BB1_5;

	div.u64 	%rd155, %rd154, %rd10;
	bra.uni 	$L__BB1_6;

$L__BB1_5:
	cvt.u32.u64 	%r125, %rd10;
	cvt.u32.u64 	%r126, %rd154;
	div.u32 	%r127, %r126, %r125;
	cvt.u64.u32 	%rd155, %r127;

$L__BB1_6:
	setp.lt.s32 	%p4, %r30, 3;
	@%p4 bra 	$L__BB1_10;

	or.b64  	%rd58, %rd155, %rd11;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p5, %rd59, 0;
	@%p5 bra 	$L__BB1_9;

	div.u64 	%rd155, %rd155, %rd11;
	bra.uni 	$L__BB1_10;

$L__BB1_9:
	cvt.u32.u64 	%r128, %rd11;
	cvt.u32.u64 	%r129, %rd155;
	div.u32 	%r130, %r129, %r128;
	cvt.u64.u32 	%rd155, %r130;

$L__BB1_10:
	setp.lt.s32 	%p6, %r30, 2;
	@%p6 bra 	$L__BB1_14;

	or.b64  	%rd60, %rd155, %rd12;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.eq.s64 	%p7, %rd61, 0;
	@%p7 bra 	$L__BB1_13;

	div.u64 	%rd155, %rd155, %rd12;
	bra.uni 	$L__BB1_14;

$L__BB1_13:
	cvt.u32.u64 	%r131, %rd12;
	cvt.u32.u64 	%r132, %rd155;
	div.u32 	%r133, %r132, %r131;
	cvt.u64.u32 	%rd155, %r133;

$L__BB1_14:
	cvt.u32.u64 	%r134, %rd155;
	setp.gt.s32 	%p8, %r30, 0;
	selp.b32 	%r135, %r134, 0, %p8;
	cvt.s64.s32 	%rd31, %r135;
	mul.lo.s64 	%rd62, %rd31, %rd13;
	add.s64 	%rd63, %rd8, %rd62;
	ld.global.s32 	%rd64, [%rd63];
	mul.lo.s64 	%rd65, %rd64, %rd14;
	add.s64 	%rd66, %rd7, %rd65;
	ld.global.u32 	%r136, [%rd66];
	add.s32 	%r137, %r136, -1;
	setp.lt.u32 	%p9, %r137, 2;
	@%p9 bra 	$L__BB1_51;

	mul.lo.s64 	%rd32, %rd31, %rd16;
	add.s64 	%rd67, %rd9, %rd32;
	mul.lo.s64 	%rd68, %rd31, %rd17;
	add.s64 	%rd69, %rd19, %rd68;
	ld.global.f64 	%fd31, [%rd67];
	mul.f64 	%fd1, %fd31, %fd30;
	ld.global.u8 	%rs1, [%rd69];
	setp.eq.s16 	%p10, %rs1, 0;
	@%p10 bra 	$L__BB1_29;

	cvt.u32.u64 	%r138, %rd31;
	shl.b32 	%r174, %r138, 4;
	ld.param.u32 	%r7, [%rd20+172];
	setp.le.s32 	%p11, %r7, %r174;
	selp.u16 	%rs42, 1, 0, %p11;
	shr.u32 	%r139, %r138, 27;
	cvt.u16.u32 	%rs43, %r139;
	and.b16  	%rs44, %rs43, 1;
	or.b16  	%rs45, %rs44, %rs42;
	setp.eq.s16 	%p12, %rs45, 0;
	@%p12 bra 	$L__BB1_18;

	st.local.v2.u32 	[%rd6], {%r174, %r7};
	mov.u64 	%rd70, $str;
	cvta.global.u64 	%rd71, %rd70;
	{ // callseq 83, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd71;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r140, [retval0+0];
	} // callseq 83
	bra.uni 	$L__BB1_19;

$L__BB1_18:
	ld.param.u32 	%r141, [%rd20+144];
	mul.wide.s32 	%rd82, %r141, %r174;
	ld.param.u64 	%rd83, [%rd20+112];
	add.s64 	%rd73, %rd83, %rd82;
	// begin inline asm
	{ atom.add.f64 %fd32,[%rd73],%fd1; }

	// end inline asm
	add.s64 	%rd74, %rd73, 8;
	mov.f64 	%fd47, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd34,[%rd74],%fd47; }

	// end inline asm
	add.s64 	%rd75, %rd73, 16;
	// begin inline asm
	{ atom.add.f64 %fd36,[%rd75],%fd47; }

	// end inline asm
	add.s64 	%rd76, %rd73, 24;
	// begin inline asm
	{ atom.add.f64 %fd38,[%rd76],%fd47; }

	// end inline asm
	add.s64 	%rd77, %rd73, 32;
	// begin inline asm
	{ atom.add.f64 %fd40,[%rd77],%fd1; }

	// end inline asm
	add.s64 	%rd78, %rd73, 40;
	// begin inline asm
	{ atom.add.f64 %fd42,[%rd78],%fd47; }

	// end inline asm
	add.s64 	%rd79, %rd73, 48;
	// begin inline asm
	{ atom.add.f64 %fd44,[%rd79],%fd47; }

	// end inline asm
	add.s64 	%rd80, %rd73, 56;
	// begin inline asm
	{ atom.add.f64 %fd46,[%rd80],%fd47; }

	// end inline asm
	add.s64 	%rd81, %rd73, 64;
	// begin inline asm
	{ atom.add.f64 %fd48,[%rd81],%fd1; }

	// end inline asm

$L__BB1_19:
	ld.param.u32 	%r8, [%rd20+172];
	add.s32 	%r175, %r174, 5;
	setp.le.s32 	%p13, %r8, %r175;
	selp.u16 	%rs46, 1, 0, %p13;
	shr.u32 	%r142, %r175, 31;
	cvt.u16.u32 	%rs47, %r142;
	or.b16  	%rs48, %rs46, %rs47;
	setp.eq.s16 	%p14, %rs48, 0;
	@%p14 bra 	$L__BB1_21;

	add.s32 	%r163, %r174, 5;
	st.local.v2.u32 	[%rd6], {%r163, %r8};
	mov.u64 	%rd84, $str;
	cvta.global.u64 	%rd85, %rd84;
	{ // callseq 84, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd85;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r143, [retval0+0];
	} // callseq 84
	bra.uni 	$L__BB1_22;

$L__BB1_21:
	ld.param.u32 	%r144, [%rd20+144];
	mul.wide.s32 	%rd96, %r144, %r175;
	ld.param.u64 	%rd97, [%rd20+112];
	add.s64 	%rd87, %rd97, %rd96;
	// begin inline asm
	{ atom.add.f64 %fd50,[%rd87],%fd1; }

	// end inline asm
	add.s64 	%rd88, %rd87, 8;
	mov.f64 	%fd65, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd52,[%rd88],%fd65; }

	// end inline asm
	add.s64 	%rd89, %rd87, 16;
	// begin inline asm
	{ atom.add.f64 %fd54,[%rd89],%fd65; }

	// end inline asm
	add.s64 	%rd90, %rd87, 24;
	// begin inline asm
	{ atom.add.f64 %fd56,[%rd90],%fd65; }

	// end inline asm
	add.s64 	%rd91, %rd87, 32;
	// begin inline asm
	{ atom.add.f64 %fd58,[%rd91],%fd1; }

	// end inline asm
	add.s64 	%rd92, %rd87, 40;
	// begin inline asm
	{ atom.add.f64 %fd60,[%rd92],%fd65; }

	// end inline asm
	add.s64 	%rd93, %rd87, 48;
	// begin inline asm
	{ atom.add.f64 %fd62,[%rd93],%fd65; }

	// end inline asm
	add.s64 	%rd94, %rd87, 56;
	// begin inline asm
	{ atom.add.f64 %fd64,[%rd94],%fd65; }

	// end inline asm
	add.s64 	%rd95, %rd87, 64;
	// begin inline asm
	{ atom.add.f64 %fd66,[%rd95],%fd1; }

	// end inline asm

$L__BB1_22:
	ld.param.u32 	%r10, [%rd20+172];
	add.s32 	%r176, %r174, 10;
	setp.le.s32 	%p15, %r10, %r176;
	selp.u16 	%rs49, 1, 0, %p15;
	shr.u32 	%r145, %r176, 31;
	cvt.u16.u32 	%rs50, %r145;
	or.b16  	%rs51, %rs49, %rs50;
	setp.eq.s16 	%p16, %rs51, 0;
	@%p16 bra 	$L__BB1_24;

	add.s32 	%r164, %r174, 10;
	st.local.v2.u32 	[%rd6], {%r164, %r10};
	mov.u64 	%rd98, $str;
	cvta.global.u64 	%rd99, %rd98;
	{ // callseq 85, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd99;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r146, [retval0+0];
	} // callseq 85
	bra.uni 	$L__BB1_25;

$L__BB1_24:
	ld.param.u32 	%r147, [%rd20+144];
	mul.wide.s32 	%rd110, %r147, %r176;
	ld.param.u64 	%rd111, [%rd20+112];
	add.s64 	%rd101, %rd111, %rd110;
	// begin inline asm
	{ atom.add.f64 %fd68,[%rd101],%fd1; }

	// end inline asm
	add.s64 	%rd102, %rd101, 8;
	mov.f64 	%fd83, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd70,[%rd102],%fd83; }

	// end inline asm
	add.s64 	%rd103, %rd101, 16;
	// begin inline asm
	{ atom.add.f64 %fd72,[%rd103],%fd83; }

	// end inline asm
	add.s64 	%rd104, %rd101, 24;
	// begin inline asm
	{ atom.add.f64 %fd74,[%rd104],%fd83; }

	// end inline asm
	add.s64 	%rd105, %rd101, 32;
	// begin inline asm
	{ atom.add.f64 %fd76,[%rd105],%fd1; }

	// end inline asm
	add.s64 	%rd106, %rd101, 40;
	// begin inline asm
	{ atom.add.f64 %fd78,[%rd106],%fd83; }

	// end inline asm
	add.s64 	%rd107, %rd101, 48;
	// begin inline asm
	{ atom.add.f64 %fd80,[%rd107],%fd83; }

	// end inline asm
	add.s64 	%rd108, %rd101, 56;
	// begin inline asm
	{ atom.add.f64 %fd82,[%rd108],%fd83; }

	// end inline asm
	add.s64 	%rd109, %rd101, 64;
	// begin inline asm
	{ atom.add.f64 %fd84,[%rd109],%fd1; }

	// end inline asm

$L__BB1_25:
	ld.param.u32 	%r12, [%rd20+172];
	add.s32 	%r177, %r174, 15;
	setp.le.s32 	%p17, %r12, %r177;
	selp.u16 	%rs52, 1, 0, %p17;
	shr.u32 	%r148, %r177, 31;
	cvt.u16.u32 	%rs53, %r148;
	or.b16  	%rs54, %rs52, %rs53;
	setp.eq.s16 	%p18, %rs54, 0;
	@%p18 bra 	$L__BB1_27;

	add.s32 	%r165, %r174, 15;
	st.local.v2.u32 	[%rd6], {%r165, %r12};
	mov.u64 	%rd112, $str;
	cvta.global.u64 	%rd113, %rd112;
	{ // callseq 86, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd113;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r149, [retval0+0];
	} // callseq 86
	bra.uni 	$L__BB1_29;

$L__BB1_27:
	ld.param.u32 	%r150, [%rd20+144];
	mul.wide.s32 	%rd124, %r150, %r177;
	ld.param.u64 	%rd125, [%rd20+112];
	add.s64 	%rd115, %rd125, %rd124;
	// begin inline asm
	{ atom.add.f64 %fd86,[%rd115],%fd1; }

	// end inline asm
	add.s64 	%rd116, %rd115, 8;
	mov.f64 	%fd101, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd88,[%rd116],%fd101; }

	// end inline asm
	add.s64 	%rd117, %rd115, 16;
	// begin inline asm
	{ atom.add.f64 %fd90,[%rd117],%fd101; }

	// end inline asm
	add.s64 	%rd118, %rd115, 24;
	// begin inline asm
	{ atom.add.f64 %fd92,[%rd118],%fd101; }

	// end inline asm
	add.s64 	%rd119, %rd115, 32;
	// begin inline asm
	{ atom.add.f64 %fd94,[%rd119],%fd1; }

	// end inline asm
	add.s64 	%rd120, %rd115, 40;
	// begin inline asm
	{ atom.add.f64 %fd96,[%rd120],%fd101; }

	// end inline asm
	add.s64 	%rd121, %rd115, 48;
	// begin inline asm
	{ atom.add.f64 %fd98,[%rd121],%fd101; }

	// end inline asm
	add.s64 	%rd122, %rd115, 56;
	// begin inline asm
	{ atom.add.f64 %fd100,[%rd122],%fd101; }

	// end inline asm
	add.s64 	%rd123, %rd115, 64;
	// begin inline asm
	{ atom.add.f64 %fd102,[%rd123],%fd1; }

	// end inline asm

$L__BB1_29:
	mov.f64 	%fd138, 0d0000000000000000;
	mov.f64 	%fd139, %fd138;
	mov.f64 	%fd140, %fd138;
	@%p10 bra 	$L__BB1_47;

	ld.param.u32 	%r18, [%rd20+172];
	setp.le.s32 	%p20, %r18, %r177;
	selp.u16 	%rs55, 1, 0, %p20;
	shr.u32 	%r151, %r177, 31;
	cvt.u16.u32 	%rs56, %r151;
	or.b16  	%rs57, %rs55, %rs56;
	setp.eq.s16 	%p21, %rs57, 0;
	mov.f64 	%fd138, 0d0000000000000000;
	mov.f64 	%fd139, 0d0000000000000000;
	mov.f64 	%fd140, 0d0000000000000000;
	@%p21 bra 	$L__BB1_32;

	st.local.v2.u32 	[%rd6], {%r177, %r18};
	mov.u64 	%rd126, $str;
	cvta.global.u64 	%rd127, %rd126;
	{ // callseq 87, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd127;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r152, [retval0+0];
	} // callseq 87
	bra.uni 	$L__BB1_34;

$L__BB1_32:
	ld.param.u64 	%rd35, [%rd20+120];
	setp.eq.s64 	%p22, %rd35, 0;
	@%p22 bra 	$L__BB1_34;

	cvta.to.global.u64 	%rd129, %rd35;
	ld.param.u32 	%r153, [%rd20+144];
	mul.wide.s32 	%rd130, %r153, %r177;
	add.s64 	%rd131, %rd129, %rd130;
	ld.global.f64 	%fd113, [%rd131];
	add.f64 	%fd140, %fd113, 0d0000000000000000;
	ld.global.f64 	%fd114, [%rd131+32];
	add.f64 	%fd139, %fd114, 0d0000000000000000;
	ld.global.f64 	%fd115, [%rd131+64];
	add.f64 	%fd138, %fd115, 0d0000000000000000;

$L__BB1_34:
	ld.param.u32 	%r19, [%rd20+172];
	setp.le.s32 	%p23, %r19, %r176;
	selp.u16 	%rs58, 1, 0, %p23;
	shr.u32 	%r154, %r176, 31;
	cvt.u16.u32 	%rs59, %r154;
	or.b16  	%rs60, %rs58, %rs59;
	setp.eq.s16 	%p24, %rs60, 0;
	@%p24 bra 	$L__BB1_36;

	st.local.v2.u32 	[%rd6], {%r176, %r19};
	mov.u64 	%rd132, $str;
	cvta.global.u64 	%rd133, %rd132;
	{ // callseq 88, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd133;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r155, [retval0+0];
	} // callseq 88
	bra.uni 	$L__BB1_38;

$L__BB1_36:
	ld.param.u64 	%rd36, [%rd20+120];
	setp.eq.s64 	%p25, %rd36, 0;
	@%p25 bra 	$L__BB1_38;

	cvta.to.global.u64 	%rd135, %rd36;
	ld.param.u32 	%r156, [%rd20+144];
	mul.wide.s32 	%rd136, %r156, %r176;
	add.s64 	%rd137, %rd135, %rd136;
	ld.global.f64 	%fd116, [%rd137];
	add.f64 	%fd140, %fd140, %fd116;
	ld.global.f64 	%fd117, [%rd137+32];
	add.f64 	%fd139, %fd139, %fd117;
	ld.global.f64 	%fd118, [%rd137+64];
	add.f64 	%fd138, %fd138, %fd118;

$L__BB1_38:
	ld.param.u32 	%r20, [%rd20+172];
	setp.le.s32 	%p26, %r20, %r175;
	selp.u16 	%rs61, 1, 0, %p26;
	shr.u32 	%r157, %r175, 31;
	cvt.u16.u32 	%rs62, %r157;
	or.b16  	%rs63, %rs61, %rs62;
	setp.eq.s16 	%p27, %rs63, 0;
	@%p27 bra 	$L__BB1_40;

	st.local.v2.u32 	[%rd6], {%r175, %r20};
	mov.u64 	%rd138, $str;
	cvta.global.u64 	%rd139, %rd138;
	{ // callseq 89, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd139;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r158, [retval0+0];
	} // callseq 89
	bra.uni 	$L__BB1_42;

$L__BB1_40:
	ld.param.u64 	%rd37, [%rd20+120];
	setp.eq.s64 	%p28, %rd37, 0;
	@%p28 bra 	$L__BB1_42;

	cvta.to.global.u64 	%rd141, %rd37;
	ld.param.u32 	%r159, [%rd20+144];
	mul.wide.s32 	%rd142, %r159, %r175;
	add.s64 	%rd143, %rd141, %rd142;
	ld.global.f64 	%fd119, [%rd143];
	add.f64 	%fd140, %fd140, %fd119;
	ld.global.f64 	%fd120, [%rd143+32];
	add.f64 	%fd139, %fd139, %fd120;
	ld.global.f64 	%fd121, [%rd143+64];
	add.f64 	%fd138, %fd138, %fd121;

$L__BB1_42:
	ld.param.u32 	%r21, [%rd20+172];
	setp.le.s32 	%p29, %r21, %r174;
	selp.u16 	%rs64, 1, 0, %p29;
	shr.u32 	%r160, %r174, 31;
	cvt.u16.u32 	%rs65, %r160;
	or.b16  	%rs66, %rs64, %rs65;
	setp.eq.s16 	%p30, %rs66, 0;
	@%p30 bra 	$L__BB1_44;

	st.local.v2.u32 	[%rd6], {%r174, %r21};
	mov.u64 	%rd144, $str;
	cvta.global.u64 	%rd145, %rd144;
	{ // callseq 90, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r161, [retval0+0];
	} // callseq 90
	bra.uni 	$L__BB1_47;

$L__BB1_44:
	ld.param.u64 	%rd38, [%rd20+120];
	setp.eq.s64 	%p31, %rd38, 0;
	@%p31 bra 	$L__BB1_47;

	cvta.to.global.u64 	%rd147, %rd38;
	ld.param.u32 	%r162, [%rd20+144];
	mul.wide.s32 	%rd148, %r162, %r174;
	add.s64 	%rd149, %rd147, %rd148;
	ld.global.f64 	%fd122, [%rd149];
	add.f64 	%fd140, %fd140, %fd122;
	ld.global.f64 	%fd123, [%rd149+32];
	add.f64 	%fd139, %fd139, %fd123;
	ld.global.f64 	%fd124, [%rd149+64];
	add.f64 	%fd138, %fd138, %fd124;

$L__BB1_47:
	add.f64 	%fd125, %fd140, 0d0000000000000000;
	add.f64 	%fd126, %fd125, %fd139;
	add.f64 	%fd127, %fd126, %fd138;
	fma.rn.f64 	%fd29, %fd127, %fd30, 0d0000000000000000;
	setp.eq.s64 	%p32, %rd50, 0;
	@%p32 bra 	$L__BB1_49;

	mul.lo.s64 	%rd151, %rd31, %rd18;
	add.s64 	%rd150, %rd50, %rd151;
	// begin inline asm
	{ atom.add.f64 %fd128,[%rd150],%fd29; }

	// end inline asm
	bra.uni 	$L__BB1_51;

$L__BB1_49:
	setp.eq.s64 	%p33, %rd45, 0;
	@%p33 bra 	$L__BB1_51;

	add.s64 	%rd152, %rd45, %rd32;
	// begin inline asm
	{ atom.add.f64 %fd130,[%rd152],%fd29; }

	// end inline asm

$L__BB1_51:
	ld.param.u64 	%rd153, [compute_affine_kinematic_hess_cuda_kernel_backward_param_0+24];
	add.s64 	%rd154, %rd154, %rd15;
	setp.lt.u64 	%p34, %rd154, %rd153;
	@%p34 bra 	$L__BB1_2;

$L__BB1_52:
	ret;

}
	// .globl	compute_soft_kinematic_grad_cuda_kernel_forward
.visible .entry compute_soft_kinematic_grad_cuda_kernel_forward(
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_3[56],
	.param .f64 compute_soft_kinematic_grad_cuda_kernel_forward_param_4,
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_5[56],
	.param .u32 compute_soft_kinematic_grad_cuda_kernel_forward_param_6,
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_8[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_9[56]
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<58>;
	.reg .b32 	%r<149>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<80>;


	ld.param.v2.u32 	{%r72, %r73}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r74, %r75}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r80, %r81}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r88, %r89}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r96, %r97}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_3+32];
	ld.param.f64 	%fd1, [compute_soft_kinematic_grad_cuda_kernel_forward_param_4];
	ld.param.v2.u32 	{%r104, %r105}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_5+32];
	ld.param.u32 	%r44, [compute_soft_kinematic_grad_cuda_kernel_forward_param_6];
	ld.param.v2.u32 	{%r112, %r113}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r120, %r121}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_8+32];
	ld.param.v2.u32 	{%r128, %r129}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_9+32];
	ld.param.u64 	%rd48, [compute_soft_kinematic_grad_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd46, [compute_soft_kinematic_grad_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd44, [compute_soft_kinematic_grad_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd42, [compute_soft_kinematic_grad_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd40, [compute_soft_kinematic_grad_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd38, [compute_soft_kinematic_grad_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd36, [compute_soft_kinematic_grad_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd35, [compute_soft_kinematic_grad_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r7, [compute_soft_kinematic_grad_cuda_kernel_forward_param_0+16];
	mov.u32 	%r132, %ntid.x;
	cvt.u64.u32 	%rd1, %r132;
	mov.u32 	%r133, %ctaid.x;
	mul.wide.u32 	%rd50, %r132, %r133;
	mov.u32 	%r134, %tid.x;
	cvt.u64.u32 	%rd51, %r134;
	add.s64 	%rd76, %rd50, %rd51;
	setp.ge.u64 	%p1, %rd76, %rd35;
	@%p1 bra 	$L__BB2_18;

	cvta.to.global.u64 	%rd4, %rd48;
	cvta.to.global.u64 	%rd5, %rd46;
	cvta.to.global.u64 	%rd6, %rd44;
	cvta.to.global.u64 	%rd8, %rd40;
	cvta.to.global.u64 	%rd9, %rd36;
	cvt.s64.s32 	%rd10, %r75;
	cvt.s64.s32 	%rd11, %r74;
	cvt.s64.s32 	%rd12, %r73;
	cvt.s64.s32 	%rd13, %r120;
	cvt.s64.s32 	%rd14, %r128;
	mov.u32 	%r135, %nctaid.x;
	cvt.u64.u32 	%rd52, %r135;
	mul.lo.s64 	%rd15, %rd1, %rd52;
	cvt.s64.s32 	%rd16, %r112;
	cvt.s64.s32 	%rd17, %r88;
	cvt.s64.s32 	%rd18, %r80;
	cvt.s64.s32 	%rd19, %r96;
	cvt.s64.s32 	%rd20, %r104;
	cvta.to.global.u64 	%rd21, %rd38;

$L__BB2_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd77, %rd76;
	@%p2 bra 	$L__BB2_6;

	or.b64  	%rd53, %rd76, %rd10;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p3, %rd54, 0;
	@%p3 bra 	$L__BB2_5;

	div.u64 	%rd77, %rd76, %rd10;
	bra.uni 	$L__BB2_6;

$L__BB2_5:
	cvt.u32.u64 	%r136, %rd10;
	cvt.u32.u64 	%r137, %rd76;
	div.u32 	%r138, %r137, %r136;
	cvt.u64.u32 	%rd77, %r138;

$L__BB2_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB2_10;

	or.b64  	%rd55, %rd77, %rd11;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p5, %rd56, 0;
	@%p5 bra 	$L__BB2_9;

	div.u64 	%rd77, %rd77, %rd11;
	bra.uni 	$L__BB2_10;

$L__BB2_9:
	cvt.u32.u64 	%r139, %rd11;
	cvt.u32.u64 	%r140, %rd77;
	div.u32 	%r141, %r140, %r139;
	cvt.u64.u32 	%rd77, %r141;

$L__BB2_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB2_14;

	or.b64  	%rd57, %rd77, %rd12;
	and.b64  	%rd58, %rd57, -4294967296;
	setp.eq.s64 	%p7, %rd58, 0;
	@%p7 bra 	$L__BB2_13;

	div.u64 	%rd77, %rd77, %rd12;
	bra.uni 	$L__BB2_14;

$L__BB2_13:
	cvt.u32.u64 	%r142, %rd12;
	cvt.u32.u64 	%r143, %rd77;
	div.u32 	%r144, %r143, %r142;
	cvt.u64.u32 	%rd77, %r144;

$L__BB2_14:
	cvt.u32.u64 	%r145, %rd77;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r145, 0, %p8;
	add.s32 	%r146, %r2, %r44;
	cvt.s64.s32 	%rd32, %r146;
	mul.lo.s64 	%rd59, %rd32, %rd13;
	add.s64 	%rd60, %rd5, %rd59;
	ld.global.s32 	%rd61, [%rd60];
	mul.lo.s64 	%rd62, %rd61, %rd14;
	add.s64 	%rd63, %rd4, %rd62;
	ld.global.u32 	%r147, [%rd63];
	add.s32 	%r148, %r147, -1;
	setp.lt.u32 	%p9, %r148, 2;
	@%p9 bra 	$L__BB2_17;

	cvt.s64.s32 	%rd33, %r2;
	mul.lo.s64 	%rd64, %rd33, %rd17;
	add.s64 	%rd65, %rd21, %rd64;
	ld.global.u8 	%rs57, [%rd65];
	setp.eq.s16 	%p10, %rs57, 0;
	@%p10 bra 	$L__BB2_17;

	mul.lo.s64 	%rd69, %rd33, %rd16;
	add.s64 	%rd70, %rd6, %rd69;
	ld.global.f64 	%fd8, [%rd70];
	mul.f64 	%fd9, %fd8, %fd1;
	mul.lo.s64 	%rd71, %rd32, %rd18;
	add.s64 	%rd72, %rd9, %rd71;
	mul.lo.s64 	%rd73, %rd33, %rd19;
	add.s64 	%rd74, %rd8, %rd73;
	ld.global.f64 	%fd10, [%rd74];
	ld.global.f64 	%fd11, [%rd72];
	sub.f64 	%fd12, %fd11, %fd10;
	ld.global.f64 	%fd13, [%rd74+8];
	ld.global.f64 	%fd14, [%rd72+8];
	sub.f64 	%fd15, %fd14, %fd13;
	ld.global.f64 	%fd16, [%rd74+16];
	ld.global.f64 	%fd17, [%rd72+16];
	sub.f64 	%fd18, %fd17, %fd16;
	mul.f64 	%fd3, %fd9, %fd12;
	mul.f64 	%fd5, %fd9, %fd15;
	mul.f64 	%fd7, %fd9, %fd18;
	mul.lo.s64 	%rd75, %rd33, %rd20;
	add.s64 	%rd66, %rd42, %rd75;
	// begin inline asm
	{ atom.add.f64 %fd2,[%rd66],%fd3; }

	// end inline asm
	add.s64 	%rd67, %rd66, 8;
	// begin inline asm
	{ atom.add.f64 %fd4,[%rd67],%fd5; }

	// end inline asm
	add.s64 	%rd68, %rd66, 16;
	// begin inline asm
	{ atom.add.f64 %fd6,[%rd68],%fd7; }

	// end inline asm

$L__BB2_17:
	add.s64 	%rd76, %rd76, %rd15;
	setp.lt.u64 	%p11, %rd76, %rd35;
	@%p11 bra 	$L__BB2_2;

$L__BB2_18:
	ret;

}
	// .globl	compute_soft_kinematic_grad_cuda_kernel_backward
.visible .entry compute_soft_kinematic_grad_cuda_kernel_backward(
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_3[56],
	.param .f64 compute_soft_kinematic_grad_cuda_kernel_backward_param_4,
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_5[56],
	.param .u32 compute_soft_kinematic_grad_cuda_kernel_backward_param_6,
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_12[56],
	.param .f64 compute_soft_kinematic_grad_cuda_kernel_backward_param_13,
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_14[56],
	.param .u32 compute_soft_kinematic_grad_cuda_kernel_backward_param_15,
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_16[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_17[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_18[56]
)
{
	.reg .pred 	%p<21>;
	.reg .b16 	%rs<90>;
	.reg .b32 	%r<225>;
	.reg .f64 	%fd<100>;
	.reg .b64 	%rd<125>;


	ld.param.v2.u32 	{%r112, %r113}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r114, %r115}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r120, %r121}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r128, %r129}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r136, %r137}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_3+32];
	ld.param.f64 	%fd30, [compute_soft_kinematic_grad_cuda_kernel_backward_param_4];
	ld.param.v2.u32 	{%r144, %r145}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r152, %r153}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r160, %r161}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r168, %r169}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r176, %r177}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r184, %r185}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_12+32];
	ld.param.v2.u32 	{%r192, %r193}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_14+32];
	ld.param.v2.u32 	{%r200, %r201}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_16+32];
	ld.param.u64 	%rd70, [compute_soft_kinematic_grad_cuda_kernel_backward_param_16];
	ld.param.u64 	%rd68, [compute_soft_kinematic_grad_cuda_kernel_backward_param_14];
	ld.param.u64 	%rd66, [compute_soft_kinematic_grad_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd64, [compute_soft_kinematic_grad_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd62, [compute_soft_kinematic_grad_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd60, [compute_soft_kinematic_grad_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd59, [compute_soft_kinematic_grad_cuda_kernel_backward_param_7+8];
	ld.param.u64 	%rd58, [compute_soft_kinematic_grad_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd57, [compute_soft_kinematic_grad_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd55, [compute_soft_kinematic_grad_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd54, [compute_soft_kinematic_grad_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd52, [compute_soft_kinematic_grad_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd51, [compute_soft_kinematic_grad_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd50, [compute_soft_kinematic_grad_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd49, [compute_soft_kinematic_grad_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r11, [compute_soft_kinematic_grad_cuda_kernel_backward_param_0+16];
	mov.u32 	%r204, %ntid.x;
	cvt.u64.u32 	%rd1, %r204;
	mov.u32 	%r205, %ctaid.x;
	mul.wide.u32 	%rd72, %r204, %r205;
	mov.u32 	%r206, %tid.x;
	cvt.u64.u32 	%rd73, %r206;
	add.s64 	%rd121, %rd72, %rd73;
	setp.ge.u64 	%p1, %rd121, %rd49;
	@%p1 bra 	$L__BB3_36;

	cvta.to.global.u64 	%rd12, %rd68;
	cvta.to.global.u64 	%rd13, %rd62;
	cvta.to.global.u64 	%rd14, %rd60;
	cvta.to.global.u64 	%rd15, %rd58;
	cvta.to.global.u64 	%rd16, %rd57;
	cvta.to.global.u64 	%rd17, %rd54;
	cvta.to.global.u64 	%rd18, %rd50;
	cvt.s64.s32 	%rd19, %r115;
	cvt.s64.s32 	%rd20, %r114;
	cvt.s64.s32 	%rd21, %r113;
	cvt.s64.s32 	%rd22, %r160;
	cvt.s64.s32 	%rd23, %r168;
	mov.u32 	%r208, %nctaid.x;
	cvt.u64.u32 	%rd74, %r208;
	mul.lo.s64 	%rd24, %rd1, %rd74;
	cvt.s64.s32 	%rd25, %r152;
	cvt.s64.s32 	%rd26, %r128;
	cvt.s64.s32 	%rd27, %r120;
	cvt.s64.s32 	%rd28, %r136;
	cvt.s64.s32 	%rd29, %r200;
	cvt.s64.s32 	%rd30, %r192;
	cvt.s64.s32 	%rd31, %r144;
	cvt.s64.s32 	%rd32, %r184;
	cvt.s64.s32 	%rd33, %r176;
	cvta.to.global.u64 	%rd34, %rd52;

$L__BB3_2:
	setp.lt.s32 	%p2, %r11, 4;
	mov.u64 	%rd122, %rd121;
	@%p2 bra 	$L__BB3_6;

	or.b64  	%rd75, %rd121, %rd19;
	and.b64  	%rd76, %rd75, -4294967296;
	setp.eq.s64 	%p3, %rd76, 0;
	@%p3 bra 	$L__BB3_5;

	div.u64 	%rd122, %rd121, %rd19;
	bra.uni 	$L__BB3_6;

$L__BB3_5:
	cvt.u32.u64 	%r209, %rd19;
	cvt.u32.u64 	%r210, %rd121;
	div.u32 	%r211, %r210, %r209;
	cvt.u64.u32 	%rd122, %r211;

$L__BB3_6:
	setp.lt.s32 	%p4, %r11, 3;
	@%p4 bra 	$L__BB3_10;

	or.b64  	%rd77, %rd122, %rd20;
	and.b64  	%rd78, %rd77, -4294967296;
	setp.eq.s64 	%p5, %rd78, 0;
	@%p5 bra 	$L__BB3_9;

	div.u64 	%rd122, %rd122, %rd20;
	bra.uni 	$L__BB3_10;

$L__BB3_9:
	cvt.u32.u64 	%r212, %rd20;
	cvt.u32.u64 	%r213, %rd122;
	div.u32 	%r214, %r213, %r212;
	cvt.u64.u32 	%rd122, %r214;

$L__BB3_10:
	setp.lt.s32 	%p6, %r11, 2;
	@%p6 bra 	$L__BB3_14;

	or.b64  	%rd79, %rd122, %rd21;
	and.b64  	%rd80, %rd79, -4294967296;
	setp.eq.s64 	%p7, %rd80, 0;
	@%p7 bra 	$L__BB3_13;

	div.u64 	%rd122, %rd122, %rd21;
	bra.uni 	$L__BB3_14;

$L__BB3_13:
	cvt.u32.u64 	%r215, %rd21;
	cvt.u32.u64 	%r216, %rd122;
	div.u32 	%r217, %r216, %r215;
	cvt.u64.u32 	%rd122, %r217;

$L__BB3_14:
	ld.param.u32 	%r221, [compute_soft_kinematic_grad_cuda_kernel_backward_param_6];
	cvt.u32.u64 	%r218, %rd122;
	setp.gt.s32 	%p8, %r11, 0;
	selp.b32 	%r3, %r218, 0, %p8;
	add.s32 	%r4, %r3, %r221;
	cvt.s64.s32 	%rd45, %r4;
	mul.lo.s64 	%rd81, %rd45, %rd22;
	add.s64 	%rd82, %rd14, %rd81;
	ld.global.s32 	%rd83, [%rd82];
	mul.lo.s64 	%rd84, %rd83, %rd23;
	add.s64 	%rd85, %rd13, %rd84;
	ld.global.u32 	%r219, [%rd85];
	add.s32 	%r220, %r219, -1;
	setp.lt.u32 	%p9, %r220, 2;
	@%p9 bra 	$L__BB3_35;

	cvt.s64.s32 	%rd46, %r3;
	mul.lo.s64 	%rd47, %rd46, %rd25;
	mul.lo.s64 	%rd86, %rd46, %rd26;
	add.s64 	%rd87, %rd34, %rd86;
	ld.global.u8 	%rs1, [%rd87];
	setp.eq.s16 	%p10, %rs1, 0;
	mov.f64 	%fd98, 0d0000000000000000;
	mov.f64 	%fd91, %fd98;
	mov.f64 	%fd92, %fd98;
	mov.f64 	%fd93, %fd98;
	@%p10 bra 	$L__BB3_17;

	add.s64 	%rd88, %rd15, %rd47;
	ld.global.f64 	%fd35, [%rd88];
	mul.f64 	%fd99, %fd35, %fd30;
	mul.lo.s64 	%rd89, %rd45, %rd27;
	add.s64 	%rd90, %rd18, %rd89;
	mul.lo.s64 	%rd91, %rd46, %rd28;
	add.s64 	%rd92, %rd17, %rd91;
	ld.global.f64 	%fd36, [%rd92];
	ld.global.f64 	%fd37, [%rd90];
	sub.f64 	%fd91, %fd37, %fd36;
	ld.global.f64 	%fd38, [%rd92+8];
	ld.global.f64 	%fd39, [%rd90+8];
	sub.f64 	%fd92, %fd39, %fd38;
	ld.global.f64 	%fd40, [%rd92+16];
	ld.global.f64 	%fd41, [%rd90+16];
	sub.f64 	%fd93, %fd41, %fd40;
	mov.u32 	%r224, %r4;

$L__BB3_17:
	@%p10 bra 	$L__BB3_31;

	ld.param.u64 	%rd119, [compute_soft_kinematic_grad_cuda_kernel_backward_param_14];
	setp.eq.s64 	%p12, %rd119, 0;
	@%p12 bra 	$L__BB3_20;

	mul.lo.s64 	%rd93, %rd46, %rd30;
	add.s64 	%rd94, %rd12, %rd93;
	ld.global.f64 	%fd43, [%rd94];
	add.f64 	%fd97, %fd43, 0d0000000000000000;
	ld.global.f64 	%fd44, [%rd94+8];
	add.f64 	%fd96, %fd44, 0d0000000000000000;
	ld.global.f64 	%fd45, [%rd94+16];
	add.f64 	%fd95, %fd45, 0d0000000000000000;
	bra.uni 	$L__BB3_22;

$L__BB3_20:
	ld.param.u64 	%rd120, [compute_soft_kinematic_grad_cuda_kernel_backward_param_5+8];
	setp.eq.s64 	%p13, %rd120, 0;
	mov.f64 	%fd95, 0d0000000000000000;
	mov.f64 	%fd96, %fd95;
	mov.f64 	%fd97, %fd95;
	@%p13 bra 	$L__BB3_22;

	mul.lo.s64 	%rd95, %rd46, %rd31;
	add.s64 	%rd96, %rd16, %rd95;
	ld.global.f64 	%fd49, [%rd96];
	add.f64 	%fd97, %fd49, 0d0000000000000000;
	ld.global.f64 	%fd50, [%rd96+8];
	add.f64 	%fd96, %fd50, 0d0000000000000000;
	ld.global.f64 	%fd51, [%rd96+16];
	add.f64 	%fd95, %fd51, 0d0000000000000000;

$L__BB3_22:
	fma.rn.f64 	%fd19, %fd99, %fd97, 0d0000000000000000;
	mov.f64 	%fd52, 0d0000000000000000;
	fma.rn.f64 	%fd20, %fd99, %fd96, 0d0000000000000000;
	fma.rn.f64 	%fd21, %fd99, %fd95, 0d0000000000000000;
	mul.f64 	%fd53, %fd92, %fd96;
	fma.rn.f64 	%fd54, %fd91, %fd97, %fd53;
	fma.rn.f64 	%fd22, %fd93, %fd95, %fd54;
	sub.f64 	%fd23, %fd52, %fd19;
	sub.f64 	%fd24, %fd52, %fd20;
	sub.f64 	%fd25, %fd52, %fd21;
	setp.eq.s64 	%p14, %rd66, 0;
	@%p14 bra 	$L__BB3_24;

	mul.lo.s64 	%rd100, %rd46, %rd32;
	add.s64 	%rd97, %rd66, %rd100;
	// begin inline asm
	{ atom.add.f64 %fd55,[%rd97],%fd23; }

	// end inline asm
	add.s64 	%rd98, %rd97, 8;
	// begin inline asm
	{ atom.add.f64 %fd57,[%rd98],%fd24; }

	// end inline asm
	add.s64 	%rd99, %rd97, 16;
	// begin inline asm
	{ atom.add.f64 %fd59,[%rd99],%fd25; }

	// end inline asm
	bra.uni 	$L__BB3_26;

$L__BB3_24:
	setp.eq.s64 	%p15, %rd55, 0;
	@%p15 bra 	$L__BB3_26;

	mul.lo.s64 	%rd104, %rd46, %rd28;
	add.s64 	%rd101, %rd55, %rd104;
	// begin inline asm
	{ atom.add.f64 %fd61,[%rd101],%fd23; }

	// end inline asm
	add.s64 	%rd102, %rd101, 8;
	// begin inline asm
	{ atom.add.f64 %fd63,[%rd102],%fd24; }

	// end inline asm
	add.s64 	%rd103, %rd101, 16;
	// begin inline asm
	{ atom.add.f64 %fd65,[%rd103],%fd25; }

	// end inline asm

$L__BB3_26:
	setp.eq.s64 	%p16, %rd64, 0;
	@%p16 bra 	$L__BB3_28;

	fma.rn.f64 	%fd86, %fd99, %fd95, 0d0000000000000000;
	fma.rn.f64 	%fd85, %fd99, %fd96, 0d0000000000000000;
	fma.rn.f64 	%fd84, %fd99, %fd97, 0d0000000000000000;
	cvt.s64.s32 	%rd108, %r224;
	mul.lo.s64 	%rd109, %rd108, %rd33;
	add.s64 	%rd105, %rd64, %rd109;
	// begin inline asm
	{ atom.add.f64 %fd67,[%rd105],%fd84; }

	// end inline asm
	add.s64 	%rd106, %rd105, 8;
	// begin inline asm
	{ atom.add.f64 %fd69,[%rd106],%fd85; }

	// end inline asm
	add.s64 	%rd107, %rd105, 16;
	// begin inline asm
	{ atom.add.f64 %fd71,[%rd107],%fd86; }

	// end inline asm
	bra.uni 	$L__BB3_30;

$L__BB3_28:
	setp.eq.s64 	%p17, %rd51, 0;
	@%p17 bra 	$L__BB3_30;

	fma.rn.f64 	%fd89, %fd99, %fd95, 0d0000000000000000;
	fma.rn.f64 	%fd88, %fd99, %fd96, 0d0000000000000000;
	fma.rn.f64 	%fd87, %fd99, %fd97, 0d0000000000000000;
	cvt.s64.s32 	%rd113, %r224;
	mul.lo.s64 	%rd114, %rd113, %rd27;
	add.s64 	%rd110, %rd51, %rd114;
	// begin inline asm
	{ atom.add.f64 %fd73,[%rd110],%fd87; }

	// end inline asm
	add.s64 	%rd111, %rd110, 8;
	// begin inline asm
	{ atom.add.f64 %fd75,[%rd111],%fd88; }

	// end inline asm
	add.s64 	%rd112, %rd110, 16;
	// begin inline asm
	{ atom.add.f64 %fd77,[%rd112],%fd89; }

	// end inline asm

$L__BB3_30:
	add.f64 	%fd79, %fd22, 0d0000000000000000;
	fma.rn.f64 	%fd98, %fd79, %fd30, 0d0000000000000000;

$L__BB3_31:
	add.f64 	%fd28, %fd98, 0d0000000000000000;
	setp.eq.s64 	%p18, %rd70, 0;
	@%p18 bra 	$L__BB3_33;

	mul.lo.s64 	%rd116, %rd46, %rd29;
	add.s64 	%rd115, %rd70, %rd116;
	// begin inline asm
	{ atom.add.f64 %fd80,[%rd115],%fd28; }

	// end inline asm
	bra.uni 	$L__BB3_35;

$L__BB3_33:
	setp.eq.s64 	%p19, %rd59, 0;
	@%p19 bra 	$L__BB3_35;

	add.s64 	%rd117, %rd59, %rd47;
	// begin inline asm
	{ atom.add.f64 %fd82,[%rd117],%fd28; }

	// end inline asm

$L__BB3_35:
	ld.param.u64 	%rd118, [compute_soft_kinematic_grad_cuda_kernel_backward_param_0+24];
	add.s64 	%rd121, %rd121, %rd24;
	setp.lt.u64 	%p20, %rd121, %rd118;
	@%p20 bra 	$L__BB3_2;

$L__BB3_36:
	ret;

}
	// .globl	compute_soft_kinematic_hess_cuda_kernel_forward
.visible .entry compute_soft_kinematic_hess_cuda_kernel_forward(
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_forward_param_1[56],
	.param .f64 compute_soft_kinematic_hess_cuda_kernel_forward_param_2,
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_forward_param_3[184],
	.param .u32 compute_soft_kinematic_hess_cuda_kernel_forward_param_4,
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_forward_param_7[56]
)
{
	.local .align 8 .b8 	__local_depot4[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<37>;
	.reg .b32 	%r<101>;
	.reg .f64 	%fd<21>;
	.reg .b64 	%rd<80>;


	mov.u64 	%SPL, __local_depot4;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r46, %r47}, [compute_soft_kinematic_hess_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r48, %r49}, [compute_soft_kinematic_hess_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r54, %r55}, [compute_soft_kinematic_hess_cuda_kernel_forward_param_1+32];
	ld.param.f64 	%fd2, [compute_soft_kinematic_hess_cuda_kernel_forward_param_2];
	mov.b64 	%rd34, compute_soft_kinematic_hess_cuda_kernel_forward_param_3;
	ld.param.u32 	%r18, [compute_soft_kinematic_hess_cuda_kernel_forward_param_4];
	ld.param.v2.u32 	{%r62, %r63}, [compute_soft_kinematic_hess_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r70, %r71}, [compute_soft_kinematic_hess_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r78, %r79}, [compute_soft_kinematic_hess_cuda_kernel_forward_param_7+32];
	ld.param.u64 	%rd39, [compute_soft_kinematic_hess_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd37, [compute_soft_kinematic_hess_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd35, [compute_soft_kinematic_hess_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd32, [compute_soft_kinematic_hess_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd31, [compute_soft_kinematic_hess_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r8, [compute_soft_kinematic_hess_cuda_kernel_forward_param_0+16];
	mov.u32 	%r82, %ntid.x;
	cvt.u64.u32 	%rd1, %r82;
	mov.u32 	%r83, %ctaid.x;
	mul.wide.u32 	%rd41, %r82, %r83;
	mov.u32 	%r84, %tid.x;
	cvt.u64.u32 	%rd42, %r84;
	add.s64 	%rd76, %rd41, %rd42;
	setp.ge.u64 	%p1, %rd76, %rd31;
	@%p1 bra 	$L__BB4_21;

	add.u64 	%rd43, %SP, 0;
	add.u64 	%rd4, %SPL, 0;
	cvta.to.global.u64 	%rd5, %rd39;
	cvta.to.global.u64 	%rd6, %rd37;
	cvta.to.global.u64 	%rd7, %rd35;
	cvt.s64.s32 	%rd8, %r49;
	cvt.s64.s32 	%rd9, %r48;
	cvt.s64.s32 	%rd10, %r47;
	cvt.s64.s32 	%rd11, %r70;
	cvt.s64.s32 	%rd12, %r78;
	mov.u32 	%r85, %nctaid.x;
	cvt.u64.u32 	%rd44, %r85;
	mul.lo.s64 	%rd13, %rd1, %rd44;
	cvt.s64.s32 	%rd14, %r62;
	cvt.s64.s32 	%rd15, %r54;
	cvta.to.global.u64 	%rd16, %rd32;
	mov.u64 	%rd17, %rd34;

$L__BB4_2:
	setp.lt.s32 	%p2, %r8, 4;
	mov.u64 	%rd77, %rd76;
	@%p2 bra 	$L__BB4_6;

	or.b64  	%rd45, %rd76, %rd8;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p3, %rd46, 0;
	@%p3 bra 	$L__BB4_5;

	div.u64 	%rd77, %rd76, %rd8;
	bra.uni 	$L__BB4_6;

$L__BB4_5:
	cvt.u32.u64 	%r86, %rd8;
	cvt.u32.u64 	%r87, %rd76;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd77, %r88;

$L__BB4_6:
	setp.lt.s32 	%p4, %r8, 3;
	@%p4 bra 	$L__BB4_10;

	or.b64  	%rd47, %rd77, %rd9;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p5, %rd48, 0;
	@%p5 bra 	$L__BB4_9;

	div.u64 	%rd77, %rd77, %rd9;
	bra.uni 	$L__BB4_10;

$L__BB4_9:
	cvt.u32.u64 	%r89, %rd9;
	cvt.u32.u64 	%r90, %rd77;
	div.u32 	%r91, %r90, %r89;
	cvt.u64.u32 	%rd77, %r91;

$L__BB4_10:
	setp.lt.s32 	%p6, %r8, 2;
	@%p6 bra 	$L__BB4_14;

	or.b64  	%rd49, %rd77, %rd10;
	and.b64  	%rd50, %rd49, -4294967296;
	setp.eq.s64 	%p7, %rd50, 0;
	@%p7 bra 	$L__BB4_13;

	div.u64 	%rd77, %rd77, %rd10;
	bra.uni 	$L__BB4_14;

$L__BB4_13:
	cvt.u32.u64 	%r92, %rd10;
	cvt.u32.u64 	%r93, %rd77;
	div.u32 	%r94, %r93, %r92;
	cvt.u64.u32 	%rd77, %r94;

$L__BB4_14:
	cvt.u32.u64 	%r95, %rd77;
	setp.gt.s32 	%p8, %r8, 0;
	selp.b32 	%r2, %r95, 0, %p8;
	add.s32 	%r96, %r2, %r18;
	cvt.s64.s32 	%rd51, %r96;
	mul.lo.s64 	%rd52, %rd51, %rd11;
	add.s64 	%rd53, %rd6, %rd52;
	ld.global.s32 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd12;
	add.s64 	%rd56, %rd5, %rd55;
	ld.global.u32 	%r97, [%rd56];
	add.s32 	%r98, %r97, -1;
	setp.lt.u32 	%p9, %r98, 2;
	@%p9 bra 	$L__BB4_20;

	cvt.s64.s32 	%rd28, %r2;
	mul.lo.s64 	%rd57, %rd28, %rd15;
	add.s64 	%rd58, %rd16, %rd57;
	ld.global.u8 	%rs33, [%rd58];
	setp.eq.s16 	%p10, %rs33, 0;
	@%p10 bra 	$L__BB4_20;

	mul.lo.s64 	%rd59, %rd28, %rd14;
	add.s64 	%rd60, %rd7, %rd59;
	ld.global.f64 	%fd1, [%rd60];
	ld.param.u32 	%r3, [%rd17+172];
	setp.le.s32 	%p11, %r3, %r2;
	selp.u16 	%rs34, 1, 0, %p11;
	shr.u32 	%r99, %r2, 31;
	cvt.u16.u32 	%rs35, %r99;
	or.b16  	%rs36, %rs34, %rs35;
	setp.eq.s16 	%p12, %rs36, 0;
	@%p12 bra 	$L__BB4_18;

	st.local.v2.u32 	[%rd4], {%r2, %r3};
	mov.u64 	%rd61, $str;
	cvta.global.u64 	%rd62, %rd61;
	{ // callseq 91, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd62;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd43;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r100, [retval0+0];
	} // callseq 91
	bra.uni 	$L__BB4_20;

$L__BB4_18:
	mul.f64 	%fd20, %fd1, %fd2;
	ld.param.s32 	%rd73, [%rd17+144];
	mul.lo.s64 	%rd74, %rd73, %rd28;
	ld.param.u64 	%rd75, [%rd17+112];
	add.s64 	%rd64, %rd75, %rd74;
	// begin inline asm
	{ atom.add.f64 %fd3,[%rd64],%fd20; }

	// end inline asm
	add.s64 	%rd65, %rd64, 8;
	mov.f64 	%fd18, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd5,[%rd65],%fd18; }

	// end inline asm
	add.s64 	%rd66, %rd64, 16;
	// begin inline asm
	{ atom.add.f64 %fd7,[%rd66],%fd18; }

	// end inline asm
	add.s64 	%rd67, %rd64, 24;
	// begin inline asm
	{ atom.add.f64 %fd9,[%rd67],%fd18; }

	// end inline asm
	add.s64 	%rd68, %rd64, 32;
	// begin inline asm
	{ atom.add.f64 %fd11,[%rd68],%fd20; }

	// end inline asm
	add.s64 	%rd69, %rd64, 40;
	// begin inline asm
	{ atom.add.f64 %fd13,[%rd69],%fd18; }

	// end inline asm
	add.s64 	%rd70, %rd64, 48;
	// begin inline asm
	{ atom.add.f64 %fd15,[%rd70],%fd18; }

	// end inline asm
	add.s64 	%rd71, %rd64, 56;
	// begin inline asm
	{ atom.add.f64 %fd17,[%rd71],%fd18; }

	// end inline asm
	add.s64 	%rd72, %rd64, 64;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd72],%fd20; }

	// end inline asm

$L__BB4_20:
	add.s64 	%rd76, %rd76, %rd13;
	setp.lt.u64 	%p13, %rd76, %rd31;
	@%p13 bra 	$L__BB4_2;

$L__BB4_21:
	ret;

}
	// .globl	compute_soft_kinematic_hess_cuda_kernel_backward
.visible .entry compute_soft_kinematic_hess_cuda_kernel_backward(
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_1[56],
	.param .f64 compute_soft_kinematic_hess_cuda_kernel_backward_param_2,
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_3[184],
	.param .u32 compute_soft_kinematic_hess_cuda_kernel_backward_param_4,
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_8[56],
	.param .f64 compute_soft_kinematic_hess_cuda_kernel_backward_param_9,
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_10[184],
	.param .u32 compute_soft_kinematic_hess_cuda_kernel_backward_param_11,
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_12[56],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_14[56]
)
{
	.local .align 8 .b8 	__local_depot5[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<47>;
	.reg .b32 	%r<121>;
	.reg .f64 	%fd<59>;
	.reg .b64 	%rd<98>;


	mov.u64 	%SPL, __local_depot5;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r56, %r57}, [compute_soft_kinematic_hess_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r58, %r59}, [compute_soft_kinematic_hess_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r64, %r65}, [compute_soft_kinematic_hess_cuda_kernel_backward_param_1+32];
	ld.param.f64 	%fd12, [compute_soft_kinematic_hess_cuda_kernel_backward_param_2];
	mov.b64 	%rd40, compute_soft_kinematic_hess_cuda_kernel_backward_param_3;
	ld.param.v2.u32 	{%r72, %r73}, [compute_soft_kinematic_hess_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r80, %r81}, [compute_soft_kinematic_hess_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r88, %r89}, [compute_soft_kinematic_hess_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r96, %r97}, [compute_soft_kinematic_hess_cuda_kernel_backward_param_12+32];
	ld.param.u64 	%rd47, [compute_soft_kinematic_hess_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd45, [compute_soft_kinematic_hess_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd43, [compute_soft_kinematic_hess_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd42, [compute_soft_kinematic_hess_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd41, [compute_soft_kinematic_hess_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd38, [compute_soft_kinematic_hess_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd37, [compute_soft_kinematic_hess_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r9, [compute_soft_kinematic_hess_cuda_kernel_backward_param_0+16];
	mov.u32 	%r100, %ntid.x;
	cvt.u64.u32 	%rd1, %r100;
	mov.u32 	%r101, %ctaid.x;
	mul.wide.u32 	%rd49, %r100, %r101;
	mov.u32 	%r102, %tid.x;
	cvt.u64.u32 	%rd50, %r102;
	add.s64 	%rd94, %rd49, %rd50;
	setp.ge.u64 	%p1, %rd94, %rd37;
	@%p1 bra 	$L__BB5_29;

	add.u64 	%rd51, %SP, 0;
	add.u64 	%rd6, %SPL, 0;
	cvta.to.global.u64 	%rd7, %rd45;
	cvta.to.global.u64 	%rd8, %rd43;
	cvta.to.global.u64 	%rd9, %rd41;
	cvt.s64.s32 	%rd10, %r59;
	cvt.s64.s32 	%rd11, %r58;
	cvt.s64.s32 	%rd12, %r57;
	cvt.s64.s32 	%rd13, %r80;
	cvt.s64.s32 	%rd14, %r88;
	mov.u32 	%r103, %nctaid.x;
	cvt.u64.u32 	%rd52, %r103;
	mul.lo.s64 	%rd15, %rd1, %rd52;
	cvt.s64.s32 	%rd16, %r72;
	cvt.s64.s32 	%rd17, %r64;
	cvt.s64.s32 	%rd18, %r96;
	cvta.to.global.u64 	%rd19, %rd38;
	mov.u64 	%rd20, %rd40;

$L__BB5_2:
	setp.lt.s32 	%p2, %r9, 4;
	mov.u64 	%rd95, %rd94;
	@%p2 bra 	$L__BB5_6;

	or.b64  	%rd53, %rd94, %rd10;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p3, %rd54, 0;
	@%p3 bra 	$L__BB5_5;

	div.u64 	%rd95, %rd94, %rd10;
	bra.uni 	$L__BB5_6;

$L__BB5_5:
	cvt.u32.u64 	%r104, %rd10;
	cvt.u32.u64 	%r105, %rd94;
	div.u32 	%r106, %r105, %r104;
	cvt.u64.u32 	%rd95, %r106;

$L__BB5_6:
	setp.lt.s32 	%p4, %r9, 3;
	@%p4 bra 	$L__BB5_10;

	or.b64  	%rd55, %rd95, %rd11;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p5, %rd56, 0;
	@%p5 bra 	$L__BB5_9;

	div.u64 	%rd95, %rd95, %rd11;
	bra.uni 	$L__BB5_10;

$L__BB5_9:
	cvt.u32.u64 	%r107, %rd11;
	cvt.u32.u64 	%r108, %rd95;
	div.u32 	%r109, %r108, %r107;
	cvt.u64.u32 	%rd95, %r109;

$L__BB5_10:
	setp.lt.s32 	%p6, %r9, 2;
	@%p6 bra 	$L__BB5_14;

	or.b64  	%rd57, %rd95, %rd12;
	and.b64  	%rd58, %rd57, -4294967296;
	setp.eq.s64 	%p7, %rd58, 0;
	@%p7 bra 	$L__BB5_13;

	div.u64 	%rd95, %rd95, %rd12;
	bra.uni 	$L__BB5_14;

$L__BB5_13:
	cvt.u32.u64 	%r110, %rd12;
	cvt.u32.u64 	%r111, %rd95;
	div.u32 	%r112, %r111, %r110;
	cvt.u64.u32 	%rd95, %r112;

$L__BB5_14:
	ld.param.u32 	%r120, [compute_soft_kinematic_hess_cuda_kernel_backward_param_4];
	cvt.u32.u64 	%r113, %rd95;
	setp.gt.s32 	%p8, %r9, 0;
	selp.b32 	%r2, %r113, 0, %p8;
	add.s32 	%r114, %r2, %r120;
	cvt.s64.s32 	%rd59, %r114;
	mul.lo.s64 	%rd60, %rd59, %rd13;
	add.s64 	%rd61, %rd8, %rd60;
	ld.global.s32 	%rd62, [%rd61];
	mul.lo.s64 	%rd63, %rd62, %rd14;
	add.s64 	%rd64, %rd7, %rd63;
	ld.global.u32 	%r115, [%rd64];
	add.s32 	%r116, %r115, -1;
	setp.lt.u32 	%p9, %r116, 2;
	@%p9 bra 	$L__BB5_28;

	cvt.s64.s32 	%rd31, %r2;
	mul.lo.s64 	%rd32, %rd31, %rd16;
	mul.lo.s64 	%rd65, %rd31, %rd17;
	add.s64 	%rd66, %rd19, %rd65;
	ld.global.u8 	%rs42, [%rd66];
	setp.eq.s16 	%p10, %rs42, 0;
	mov.f64 	%fd53, 0d0000000000000000;
	mov.f64 	%fd54, %fd53;
	mov.f64 	%fd55, %fd53;
	@%p10 bra 	$L__BB5_24;

	add.s64 	%rd67, %rd9, %rd32;
	ld.global.f64 	%fd1, [%rd67];
	ld.param.u32 	%r3, [%rd20+172];
	setp.le.s32 	%p11, %r3, %r2;
	selp.u16 	%rs43, 1, 0, %p11;
	shr.u32 	%r117, %r2, 31;
	cvt.u16.u32 	%rs1, %r117;
	or.b16  	%rs44, %rs43, %rs1;
	setp.eq.s16 	%p12, %rs44, 0;
	@%p12 bra 	$L__BB5_18;

	st.local.v2.u32 	[%rd6], {%r2, %r3};
	mov.u64 	%rd68, $str;
	cvta.global.u64 	%rd69, %rd68;
	{ // callseq 92, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd69;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd51;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r118, [retval0+0];
	} // callseq 92
	bra.uni 	$L__BB5_19;

$L__BB5_18:
	mul.f64 	%fd33, %fd1, %fd12;
	ld.param.s32 	%rd80, [%rd20+144];
	mul.lo.s64 	%rd81, %rd80, %rd31;
	ld.param.u64 	%rd82, [%rd20+112];
	add.s64 	%rd71, %rd82, %rd81;
	// begin inline asm
	{ atom.add.f64 %fd16,[%rd71],%fd33; }

	// end inline asm
	add.s64 	%rd72, %rd71, 8;
	mov.f64 	%fd31, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd18,[%rd72],%fd31; }

	// end inline asm
	add.s64 	%rd73, %rd71, 16;
	// begin inline asm
	{ atom.add.f64 %fd20,[%rd73],%fd31; }

	// end inline asm
	add.s64 	%rd74, %rd71, 24;
	// begin inline asm
	{ atom.add.f64 %fd22,[%rd74],%fd31; }

	// end inline asm
	add.s64 	%rd75, %rd71, 32;
	// begin inline asm
	{ atom.add.f64 %fd24,[%rd75],%fd33; }

	// end inline asm
	add.s64 	%rd76, %rd71, 40;
	// begin inline asm
	{ atom.add.f64 %fd26,[%rd76],%fd31; }

	// end inline asm
	add.s64 	%rd77, %rd71, 48;
	// begin inline asm
	{ atom.add.f64 %fd28,[%rd77],%fd31; }

	// end inline asm
	add.s64 	%rd78, %rd71, 56;
	// begin inline asm
	{ atom.add.f64 %fd30,[%rd78],%fd31; }

	// end inline asm
	add.s64 	%rd79, %rd71, 64;
	// begin inline asm
	{ atom.add.f64 %fd32,[%rd79],%fd33; }

	// end inline asm

$L__BB5_19:
	ld.param.u32 	%r4, [%rd20+172];
	setp.le.s32 	%p13, %r4, %r2;
	selp.u16 	%rs45, 1, 0, %p13;
	or.b16  	%rs46, %rs45, %rs1;
	setp.eq.s16 	%p14, %rs46, 0;
	mov.f64 	%fd53, 0d0000000000000000;
	mov.f64 	%fd54, 0d0000000000000000;
	mov.f64 	%fd55, 0d0000000000000000;
	@%p14 bra 	$L__BB5_21;

	st.local.v2.u32 	[%rd6], {%r2, %r4};
	mov.u64 	%rd83, $str;
	cvta.global.u64 	%rd84, %rd83;
	{ // callseq 93, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd84;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd51;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r119, [retval0+0];
	} // callseq 93
	bra.uni 	$L__BB5_24;

$L__BB5_21:
	ld.param.u64 	%rd35, [%rd20+120];
	setp.eq.s64 	%p15, %rd35, 0;
	@%p15 bra 	$L__BB5_24;

	cvta.to.global.u64 	%rd86, %rd35;
	ld.param.s32 	%rd87, [%rd20+144];
	mul.lo.s64 	%rd88, %rd87, %rd31;
	add.s64 	%rd89, %rd86, %rd88;
	ld.global.f64 	%fd40, [%rd89];
	add.f64 	%fd55, %fd40, 0d0000000000000000;
	ld.global.f64 	%fd41, [%rd89+32];
	add.f64 	%fd54, %fd41, 0d0000000000000000;
	ld.global.f64 	%fd42, [%rd89+64];
	add.f64 	%fd53, %fd42, 0d0000000000000000;

$L__BB5_24:
	add.f64 	%fd43, %fd55, 0d0000000000000000;
	add.f64 	%fd44, %fd43, %fd54;
	add.f64 	%fd45, %fd44, %fd53;
	fma.rn.f64 	%fd11, %fd45, %fd12, 0d0000000000000000;
	setp.eq.s64 	%p16, %rd47, 0;
	@%p16 bra 	$L__BB5_26;

	mul.lo.s64 	%rd91, %rd31, %rd18;
	add.s64 	%rd90, %rd47, %rd91;
	// begin inline asm
	{ atom.add.f64 %fd46,[%rd90],%fd11; }

	// end inline asm
	bra.uni 	$L__BB5_28;

$L__BB5_26:
	setp.eq.s64 	%p17, %rd42, 0;
	@%p17 bra 	$L__BB5_28;

	add.s64 	%rd92, %rd42, %rd32;
	// begin inline asm
	{ atom.add.f64 %fd48,[%rd92],%fd11; }

	// end inline asm

$L__BB5_28:
	ld.param.u64 	%rd93, [compute_soft_kinematic_hess_cuda_kernel_backward_param_0+24];
	add.s64 	%rd94, %rd94, %rd15;
	setp.lt.u64 	%p18, %rd94, %rd93;
	@%p18 bra 	$L__BB5_2;

$L__BB5_29:
	ret;

}
	// .globl	init_affine_kinematic_target_kernel_cuda_kernel_forward
.visible .entry init_affine_kinematic_target_kernel_cuda_kernel_forward(
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_forward_param_5[56]
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<42>;
	.reg .b32 	%r<109>;
	.reg .f64 	%fd<91>;
	.reg .b64 	%rd<65>;


	ld.param.v2.u32 	{%r52, %r53}, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r54, %r55}, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r60, %r61}, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r68, %r69}, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r76, %r77}, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r84, %r85}, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r92, %r93}, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_5+32];
	ld.param.u64 	%rd39, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd37, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd35, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd33, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd31, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd30, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_0+16];
	mov.u32 	%r96, %ntid.x;
	cvt.u64.u32 	%rd1, %r96;
	mov.u32 	%r97, %ctaid.x;
	mul.wide.u32 	%rd41, %r96, %r97;
	mov.u32 	%r98, %tid.x;
	cvt.u64.u32 	%rd42, %r98;
	add.s64 	%rd61, %rd41, %rd42;
	setp.ge.u64 	%p1, %rd61, %rd30;
	@%p1 bra 	$L__BB6_17;

	cvta.to.global.u64 	%rd4, %rd39;
	cvta.to.global.u64 	%rd5, %rd37;
	cvta.to.global.u64 	%rd6, %rd35;
	cvta.to.global.u64 	%rd7, %rd33;
	cvt.s64.s32 	%rd8, %r55;
	cvt.s64.s32 	%rd9, %r54;
	cvt.s64.s32 	%rd10, %r53;
	cvt.s64.s32 	%rd11, %r60;
	mov.u32 	%r99, %nctaid.x;
	cvt.u64.u32 	%rd43, %r99;
	mul.lo.s64 	%rd12, %rd1, %rd43;
	cvt.s64.s32 	%rd13, %r68;
	cvt.s64.s32 	%rd14, %r84;
	cvt.s64.s32 	%rd15, %r92;
	cvt.s64.s32 	%rd16, %r76;
	cvta.to.global.u64 	%rd17, %rd31;

$L__BB6_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd62, %rd61;
	@%p2 bra 	$L__BB6_6;

	or.b64  	%rd44, %rd61, %rd8;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p3, %rd45, 0;
	@%p3 bra 	$L__BB6_5;

	div.u64 	%rd62, %rd61, %rd8;
	bra.uni 	$L__BB6_6;

$L__BB6_5:
	cvt.u32.u64 	%r100, %rd8;
	cvt.u32.u64 	%r101, %rd61;
	div.u32 	%r102, %r101, %r100;
	cvt.u64.u32 	%rd62, %r102;

$L__BB6_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB6_10;

	or.b64  	%rd46, %rd62, %rd9;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p5, %rd47, 0;
	@%p5 bra 	$L__BB6_9;

	div.u64 	%rd62, %rd62, %rd9;
	bra.uni 	$L__BB6_10;

$L__BB6_9:
	cvt.u32.u64 	%r103, %rd9;
	cvt.u32.u64 	%r104, %rd62;
	div.u32 	%r105, %r104, %r103;
	cvt.u64.u32 	%rd62, %r105;

$L__BB6_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB6_14;

	or.b64  	%rd48, %rd62, %rd10;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p7, %rd49, 0;
	@%p7 bra 	$L__BB6_13;

	div.u64 	%rd62, %rd62, %rd10;
	bra.uni 	$L__BB6_14;

$L__BB6_13:
	cvt.u32.u64 	%r106, %rd10;
	cvt.u32.u64 	%r107, %rd62;
	div.u32 	%r108, %r107, %r106;
	cvt.u64.u32 	%rd62, %r108;

$L__BB6_14:
	cvt.s64.s32 	%rd50, %rd62;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd28, %rd50, 0, %p8;
	mul.lo.s64 	%rd51, %rd28, %rd11;
	add.s64 	%rd52, %rd17, %rd51;
	ld.global.u8 	%rs41, [%rd52];
	setp.eq.s16 	%p9, %rs41, 0;
	@%p9 bra 	$L__BB6_16;

	mul.lo.s64 	%rd53, %rd28, %rd13;
	add.s64 	%rd54, %rd7, %rd53;
	mul.lo.s64 	%rd55, %rd28, %rd14;
	add.s64 	%rd56, %rd5, %rd55;
	mul.lo.s64 	%rd57, %rd28, %rd15;
	add.s64 	%rd58, %rd4, %rd57;
	ld.global.f64 	%fd1, [%rd58];
	add.f64 	%fd2, %fd1, 0d3FF0000000000000;
	ld.global.f64 	%fd3, [%rd58+16];
	add.f64 	%fd4, %fd3, 0d0000000000000000;
	ld.global.f64 	%fd5, [%rd58+8];
	add.f64 	%fd6, %fd5, 0d3FF0000000000000;
	ld.global.f64 	%fd7, [%rd56];
	sub.f64 	%fd8, %fd1, %fd7;
	ld.global.f64 	%fd9, [%rd56+8];
	sub.f64 	%fd10, %fd5, %fd9;
	ld.global.f64 	%fd11, [%rd56+16];
	sub.f64 	%fd12, %fd3, %fd11;
	add.f64 	%fd13, %fd5, 0d0000000000000000;
	add.f64 	%fd14, %fd1, 0d0000000000000000;
	add.f64 	%fd15, %fd3, 0d3FF0000000000000;
	ld.global.f64 	%fd16, [%rd54];
	ld.global.f64 	%fd17, [%rd54+24];
	ld.global.f64 	%fd18, [%rd54+48];
	ld.global.f64 	%fd19, [%rd54+8];
	mul.f64 	%fd20, %fd19, %fd10;
	ld.global.f64 	%fd21, [%rd54+32];
	mul.f64 	%fd22, %fd21, %fd10;
	ld.global.f64 	%fd23, [%rd54+56];
	mul.f64 	%fd24, %fd23, %fd10;
	fma.rn.f64 	%fd25, %fd16, %fd8, %fd20;
	fma.rn.f64 	%fd26, %fd17, %fd8, %fd22;
	fma.rn.f64 	%fd27, %fd18, %fd8, %fd24;
	ld.global.f64 	%fd28, [%rd54+16];
	ld.global.f64 	%fd29, [%rd54+40];
	ld.global.f64 	%fd30, [%rd54+64];
	fma.rn.f64 	%fd31, %fd28, %fd12, %fd25;
	fma.rn.f64 	%fd32, %fd29, %fd12, %fd26;
	fma.rn.f64 	%fd33, %fd30, %fd12, %fd27;
	add.f64 	%fd34, %fd7, %fd31;
	add.f64 	%fd35, %fd9, %fd32;
	add.f64 	%fd36, %fd11, %fd33;
	sub.f64 	%fd37, %fd2, %fd7;
	sub.f64 	%fd38, %fd13, %fd9;
	sub.f64 	%fd39, %fd4, %fd11;
	ld.global.f64 	%fd40, [%rd54+72];
	add.f64 	%fd41, %fd40, %fd34;
	ld.global.f64 	%fd42, [%rd54+80];
	add.f64 	%fd43, %fd42, %fd35;
	ld.global.f64 	%fd44, [%rd54+88];
	add.f64 	%fd45, %fd44, %fd36;
	mul.f64 	%fd46, %fd19, %fd38;
	mul.f64 	%fd47, %fd21, %fd38;
	mul.f64 	%fd48, %fd23, %fd38;
	fma.rn.f64 	%fd49, %fd16, %fd37, %fd46;
	fma.rn.f64 	%fd50, %fd17, %fd37, %fd47;
	fma.rn.f64 	%fd51, %fd18, %fd37, %fd48;
	fma.rn.f64 	%fd52, %fd28, %fd39, %fd49;
	fma.rn.f64 	%fd53, %fd29, %fd39, %fd50;
	fma.rn.f64 	%fd54, %fd30, %fd39, %fd51;
	add.f64 	%fd55, %fd7, %fd52;
	add.f64 	%fd56, %fd9, %fd53;
	add.f64 	%fd57, %fd11, %fd54;
	sub.f64 	%fd58, %fd14, %fd7;
	sub.f64 	%fd59, %fd6, %fd9;
	add.f64 	%fd60, %fd40, %fd55;
	add.f64 	%fd61, %fd42, %fd56;
	add.f64 	%fd62, %fd44, %fd57;
	mul.f64 	%fd63, %fd19, %fd59;
	mul.f64 	%fd64, %fd21, %fd59;
	mul.f64 	%fd65, %fd23, %fd59;
	fma.rn.f64 	%fd66, %fd16, %fd58, %fd63;
	fma.rn.f64 	%fd67, %fd17, %fd58, %fd64;
	fma.rn.f64 	%fd68, %fd18, %fd58, %fd65;
	fma.rn.f64 	%fd69, %fd28, %fd39, %fd66;
	fma.rn.f64 	%fd70, %fd29, %fd39, %fd67;
	fma.rn.f64 	%fd71, %fd30, %fd39, %fd68;
	add.f64 	%fd72, %fd7, %fd69;
	add.f64 	%fd73, %fd9, %fd70;
	add.f64 	%fd74, %fd11, %fd71;
	sub.f64 	%fd75, %fd15, %fd11;
	add.f64 	%fd76, %fd40, %fd72;
	add.f64 	%fd77, %fd42, %fd73;
	add.f64 	%fd78, %fd44, %fd74;
	fma.rn.f64 	%fd79, %fd16, %fd58, %fd46;
	fma.rn.f64 	%fd80, %fd17, %fd58, %fd47;
	fma.rn.f64 	%fd81, %fd18, %fd58, %fd48;
	fma.rn.f64 	%fd82, %fd28, %fd75, %fd79;
	fma.rn.f64 	%fd83, %fd29, %fd75, %fd80;
	fma.rn.f64 	%fd84, %fd30, %fd75, %fd81;
	add.f64 	%fd85, %fd7, %fd82;
	add.f64 	%fd86, %fd9, %fd83;
	add.f64 	%fd87, %fd11, %fd84;
	add.f64 	%fd88, %fd40, %fd85;
	add.f64 	%fd89, %fd42, %fd86;
	add.f64 	%fd90, %fd44, %fd87;
	mul.lo.s64 	%rd59, %rd28, %rd16;
	add.s64 	%rd60, %rd6, %rd59;
	st.global.f64 	[%rd60], %fd41;
	st.global.f64 	[%rd60+8], %fd43;
	st.global.f64 	[%rd60+16], %fd45;
	st.global.f64 	[%rd60+24], %fd60;
	st.global.f64 	[%rd60+32], %fd61;
	st.global.f64 	[%rd60+40], %fd62;
	st.global.f64 	[%rd60+48], %fd76;
	st.global.f64 	[%rd60+56], %fd77;
	st.global.f64 	[%rd60+64], %fd78;
	st.global.f64 	[%rd60+72], %fd88;
	st.global.f64 	[%rd60+80], %fd89;
	st.global.f64 	[%rd60+88], %fd90;

$L__BB6_16:
	add.s64 	%rd61, %rd61, %rd12;
	setp.lt.u64 	%p10, %rd61, %rd30;
	@%p10 bra 	$L__BB6_2;

$L__BB6_17:
	ret;

}
	// .globl	init_affine_kinematic_target_kernel_cuda_kernel_backward
.visible .entry init_affine_kinematic_target_kernel_cuda_kernel_backward(
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_10[56]
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<74>;
	.reg .b32 	%r<177>;
	.reg .f64 	%fd<340>;
	.reg .b64 	%rd<130>;


	ld.param.v2.u32 	{%r88, %r89}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r90, %r91}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r96, %r97}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r104, %r105}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r112, %r113}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r120, %r121}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r128, %r129}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r136, %r137}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r144, %r145}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r152, %r153}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r160, %r161}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_10+32];
	ld.param.u64 	%rd63, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd61, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd59, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd57, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd56, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd55, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd54, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd53, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd52, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd50, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd49, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd47, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd46, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_0+16];
	mov.u32 	%r164, %ntid.x;
	cvt.u64.u32 	%rd1, %r164;
	mov.u32 	%r165, %ctaid.x;
	mul.wide.u32 	%rd65, %r164, %r165;
	mov.u32 	%r166, %tid.x;
	cvt.u64.u32 	%rd66, %r166;
	add.s64 	%rd126, %rd65, %rd66;
	setp.ge.u64 	%p1, %rd126, %rd46;
	@%p1 bra 	$L__BB7_32;

	cvta.to.global.u64 	%rd12, %rd59;
	cvta.to.global.u64 	%rd13, %rd55;
	cvta.to.global.u64 	%rd14, %rd53;
	cvta.to.global.u64 	%rd15, %rd52;
	cvta.to.global.u64 	%rd16, %rd49;
	cvt.s64.s32 	%rd17, %r91;
	cvt.s64.s32 	%rd18, %r90;
	cvt.s64.s32 	%rd19, %r89;
	cvt.s64.s32 	%rd20, %r96;
	mov.u32 	%r167, %nctaid.x;
	cvt.u64.u32 	%rd67, %r167;
	mul.lo.s64 	%rd21, %rd1, %rd67;
	cvt.s64.s32 	%rd22, %r104;
	cvt.s64.s32 	%rd23, %r120;
	cvt.s64.s32 	%rd24, %r128;
	cvt.s64.s32 	%rd25, %r144;
	cvt.s64.s32 	%rd26, %r112;
	cvt.s64.s32 	%rd27, %r160;
	cvt.s64.s32 	%rd28, %r152;
	cvt.s64.s32 	%rd29, %r136;
	cvta.to.global.u64 	%rd30, %rd47;

$L__BB7_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd127, %rd126;
	@%p2 bra 	$L__BB7_6;

	or.b64  	%rd68, %rd126, %rd17;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.eq.s64 	%p3, %rd69, 0;
	@%p3 bra 	$L__BB7_5;

	div.u64 	%rd127, %rd126, %rd17;
	bra.uni 	$L__BB7_6;

$L__BB7_5:
	cvt.u32.u64 	%r168, %rd17;
	cvt.u32.u64 	%r169, %rd126;
	div.u32 	%r170, %r169, %r168;
	cvt.u64.u32 	%rd127, %r170;

$L__BB7_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB7_10;

	or.b64  	%rd70, %rd127, %rd18;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.eq.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB7_9;

	div.u64 	%rd127, %rd127, %rd18;
	bra.uni 	$L__BB7_10;

$L__BB7_9:
	cvt.u32.u64 	%r171, %rd18;
	cvt.u32.u64 	%r172, %rd127;
	div.u32 	%r173, %r172, %r171;
	cvt.u64.u32 	%rd127, %r173;

$L__BB7_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB7_14;

	or.b64  	%rd72, %rd127, %rd19;
	and.b64  	%rd73, %rd72, -4294967296;
	setp.eq.s64 	%p7, %rd73, 0;
	@%p7 bra 	$L__BB7_13;

	div.u64 	%rd127, %rd127, %rd19;
	bra.uni 	$L__BB7_14;

$L__BB7_13:
	cvt.u32.u64 	%r174, %rd19;
	cvt.u32.u64 	%r175, %rd127;
	div.u32 	%r176, %r175, %r174;
	cvt.u64.u32 	%rd127, %r176;

$L__BB7_14:
	cvt.s64.s32 	%rd74, %rd127;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd41, %rd74, 0, %p8;
	mul.lo.s64 	%rd75, %rd41, %rd20;
	add.s64 	%rd76, %rd30, %rd75;
	ld.global.u8 	%rs73, [%rd76];
	setp.eq.s16 	%p9, %rs73, 0;
	@%p9 bra 	$L__BB7_31;

	ld.param.u64 	%rd124, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_8];
	setp.eq.s64 	%p10, %rd124, 0;
	mul.lo.s64 	%rd42, %rd41, %rd22;
	add.s64 	%rd77, %rd16, %rd42;
	ld.global.f64 	%fd1, [%rd77];
	ld.global.f64 	%fd2, [%rd77+8];
	ld.global.f64 	%fd3, [%rd77+16];
	ld.global.f64 	%fd4, [%rd77+24];
	ld.global.f64 	%fd5, [%rd77+32];
	ld.global.f64 	%fd6, [%rd77+40];
	ld.global.f64 	%fd7, [%rd77+48];
	ld.global.f64 	%fd8, [%rd77+56];
	ld.global.f64 	%fd9, [%rd77+64];
	mul.lo.s64 	%rd43, %rd41, %rd23;
	add.s64 	%rd78, %rd14, %rd43;
	mul.lo.s64 	%rd44, %rd41, %rd24;
	add.s64 	%rd79, %rd13, %rd44;
	ld.global.f64 	%fd10, [%rd79];
	ld.global.f64 	%fd11, [%rd79+16];
	ld.global.f64 	%fd12, [%rd79+8];
	ld.global.f64 	%fd13, [%rd78];
	ld.global.f64 	%fd14, [%rd78+8];
	ld.global.f64 	%fd15, [%rd78+16];
	@%p10 bra 	$L__BB7_17;

	mul.lo.s64 	%rd80, %rd41, %rd25;
	add.s64 	%rd81, %rd12, %rd80;
	ld.global.f64 	%fd73, [%rd81];
	add.f64 	%fd339, %fd73, 0d0000000000000000;
	ld.global.f64 	%fd74, [%rd81+8];
	add.f64 	%fd338, %fd74, 0d0000000000000000;
	ld.global.f64 	%fd75, [%rd81+16];
	add.f64 	%fd337, %fd75, 0d0000000000000000;
	ld.global.f64 	%fd76, [%rd81+24];
	add.f64 	%fd336, %fd76, 0d0000000000000000;
	ld.global.f64 	%fd77, [%rd81+32];
	add.f64 	%fd335, %fd77, 0d0000000000000000;
	ld.global.f64 	%fd78, [%rd81+40];
	add.f64 	%fd334, %fd78, 0d0000000000000000;
	ld.global.f64 	%fd79, [%rd81+48];
	add.f64 	%fd333, %fd79, 0d0000000000000000;
	ld.global.f64 	%fd80, [%rd81+56];
	add.f64 	%fd332, %fd80, 0d0000000000000000;
	ld.global.f64 	%fd81, [%rd81+64];
	add.f64 	%fd331, %fd81, 0d0000000000000000;
	ld.global.f64 	%fd82, [%rd81+72];
	add.f64 	%fd330, %fd82, 0d0000000000000000;
	ld.global.f64 	%fd83, [%rd81+80];
	add.f64 	%fd329, %fd83, 0d0000000000000000;
	ld.global.f64 	%fd84, [%rd81+88];
	add.f64 	%fd328, %fd84, 0d0000000000000000;
	bra.uni 	$L__BB7_19;

$L__BB7_17:
	ld.param.u64 	%rd125, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_3+8];
	setp.eq.s64 	%p11, %rd125, 0;
	mov.f64 	%fd328, 0d0000000000000000;
	mov.f64 	%fd329, %fd328;
	mov.f64 	%fd330, %fd328;
	mov.f64 	%fd331, %fd328;
	mov.f64 	%fd332, %fd328;
	mov.f64 	%fd333, %fd328;
	mov.f64 	%fd334, %fd328;
	mov.f64 	%fd335, %fd328;
	mov.f64 	%fd336, %fd328;
	mov.f64 	%fd337, %fd328;
	mov.f64 	%fd338, %fd328;
	mov.f64 	%fd339, %fd328;
	@%p11 bra 	$L__BB7_19;

	mul.lo.s64 	%rd82, %rd41, %rd26;
	add.s64 	%rd83, %rd15, %rd82;
	ld.global.f64 	%fd97, [%rd83];
	add.f64 	%fd339, %fd97, 0d0000000000000000;
	ld.global.f64 	%fd98, [%rd83+8];
	add.f64 	%fd338, %fd98, 0d0000000000000000;
	ld.global.f64 	%fd99, [%rd83+16];
	add.f64 	%fd337, %fd99, 0d0000000000000000;
	ld.global.f64 	%fd100, [%rd83+24];
	add.f64 	%fd336, %fd100, 0d0000000000000000;
	ld.global.f64 	%fd101, [%rd83+32];
	add.f64 	%fd335, %fd101, 0d0000000000000000;
	ld.global.f64 	%fd102, [%rd83+40];
	add.f64 	%fd334, %fd102, 0d0000000000000000;
	ld.global.f64 	%fd103, [%rd83+48];
	add.f64 	%fd333, %fd103, 0d0000000000000000;
	ld.global.f64 	%fd104, [%rd83+56];
	add.f64 	%fd332, %fd104, 0d0000000000000000;
	ld.global.f64 	%fd105, [%rd83+64];
	add.f64 	%fd331, %fd105, 0d0000000000000000;
	ld.global.f64 	%fd106, [%rd83+72];
	add.f64 	%fd330, %fd106, 0d0000000000000000;
	ld.global.f64 	%fd107, [%rd83+80];
	add.f64 	%fd329, %fd107, 0d0000000000000000;
	ld.global.f64 	%fd108, [%rd83+88];
	add.f64 	%fd328, %fd108, 0d0000000000000000;

$L__BB7_19:
	sub.f64 	%fd109, %fd10, %fd13;
	sub.f64 	%fd110, %fd12, %fd14;
	sub.f64 	%fd111, %fd11, %fd15;
	add.f64 	%fd112, %fd10, 0d3FF0000000000000;
	sub.f64 	%fd113, %fd112, %fd13;
	add.f64 	%fd114, %fd12, 0d0000000000000000;
	sub.f64 	%fd115, %fd114, %fd14;
	add.f64 	%fd116, %fd11, 0d0000000000000000;
	sub.f64 	%fd117, %fd116, %fd15;
	add.f64 	%fd118, %fd10, 0d0000000000000000;
	sub.f64 	%fd119, %fd118, %fd13;
	add.f64 	%fd120, %fd12, 0d3FF0000000000000;
	sub.f64 	%fd121, %fd120, %fd14;
	add.f64 	%fd122, %fd11, 0d3FF0000000000000;
	sub.f64 	%fd123, %fd122, %fd15;
	add.f64 	%fd124, %fd330, 0d0000000000000000;
	add.f64 	%fd125, %fd329, 0d0000000000000000;
	add.f64 	%fd126, %fd328, 0d0000000000000000;
	fma.rn.f64 	%fd127, %fd124, %fd119, 0d0000000000000000;
	fma.rn.f64 	%fd128, %fd124, %fd115, 0d0000000000000000;
	fma.rn.f64 	%fd129, %fd124, %fd123, 0d0000000000000000;
	fma.rn.f64 	%fd130, %fd125, %fd119, 0d0000000000000000;
	fma.rn.f64 	%fd131, %fd125, %fd115, 0d0000000000000000;
	fma.rn.f64 	%fd132, %fd125, %fd123, 0d0000000000000000;
	fma.rn.f64 	%fd133, %fd126, %fd119, 0d0000000000000000;
	fma.rn.f64 	%fd134, %fd126, %fd115, 0d0000000000000000;
	fma.rn.f64 	%fd135, %fd126, %fd123, 0d0000000000000000;
	mul.f64 	%fd136, %fd4, %fd125;
	mul.f64 	%fd137, %fd5, %fd125;
	mul.f64 	%fd138, %fd6, %fd125;
	fma.rn.f64 	%fd139, %fd1, %fd124, %fd136;
	fma.rn.f64 	%fd140, %fd2, %fd124, %fd137;
	fma.rn.f64 	%fd141, %fd3, %fd124, %fd138;
	fma.rn.f64 	%fd142, %fd7, %fd126, %fd139;
	fma.rn.f64 	%fd143, %fd8, %fd126, %fd140;
	fma.rn.f64 	%fd144, %fd9, %fd126, %fd141;
	add.f64 	%fd145, %fd142, 0d0000000000000000;
	add.f64 	%fd146, %fd143, 0d0000000000000000;
	add.f64 	%fd147, %fd144, 0d0000000000000000;
	sub.f64 	%fd148, %fd124, %fd145;
	sub.f64 	%fd149, %fd125, %fd146;
	sub.f64 	%fd150, %fd126, %fd147;
	add.f64 	%fd151, %fd333, 0d0000000000000000;
	add.f64 	%fd152, %fd332, 0d0000000000000000;
	add.f64 	%fd153, %fd331, 0d0000000000000000;
	fma.rn.f64 	%fd154, %fd151, %fd119, %fd127;
	fma.rn.f64 	%fd155, %fd151, %fd121, %fd128;
	fma.rn.f64 	%fd156, %fd151, %fd117, %fd129;
	fma.rn.f64 	%fd157, %fd152, %fd119, %fd130;
	fma.rn.f64 	%fd158, %fd152, %fd121, %fd131;
	fma.rn.f64 	%fd159, %fd152, %fd117, %fd132;
	fma.rn.f64 	%fd160, %fd153, %fd119, %fd133;
	fma.rn.f64 	%fd161, %fd153, %fd121, %fd134;
	fma.rn.f64 	%fd162, %fd153, %fd117, %fd135;
	add.f64 	%fd163, %fd124, %fd151;
	add.f64 	%fd164, %fd125, %fd152;
	add.f64 	%fd165, %fd126, %fd153;
	add.f64 	%fd166, %fd151, %fd148;
	add.f64 	%fd167, %fd152, %fd149;
	add.f64 	%fd168, %fd153, %fd150;
	mul.f64 	%fd169, %fd4, %fd152;
	mul.f64 	%fd170, %fd5, %fd152;
	mul.f64 	%fd171, %fd6, %fd152;
	fma.rn.f64 	%fd172, %fd1, %fd151, %fd169;
	fma.rn.f64 	%fd173, %fd2, %fd151, %fd170;
	fma.rn.f64 	%fd174, %fd3, %fd151, %fd171;
	fma.rn.f64 	%fd175, %fd7, %fd153, %fd172;
	fma.rn.f64 	%fd176, %fd8, %fd153, %fd173;
	fma.rn.f64 	%fd177, %fd9, %fd153, %fd174;
	add.f64 	%fd178, %fd175, 0d0000000000000000;
	add.f64 	%fd179, %fd176, 0d0000000000000000;
	add.f64 	%fd180, %fd177, 0d0000000000000000;
	sub.f64 	%fd181, %fd166, %fd178;
	sub.f64 	%fd182, %fd167, %fd179;
	sub.f64 	%fd183, %fd168, %fd180;
	add.f64 	%fd184, %fd336, 0d0000000000000000;
	add.f64 	%fd185, %fd335, 0d0000000000000000;
	add.f64 	%fd186, %fd334, 0d0000000000000000;
	fma.rn.f64 	%fd187, %fd184, %fd113, %fd154;
	fma.rn.f64 	%fd188, %fd184, %fd115, %fd155;
	fma.rn.f64 	%fd189, %fd184, %fd117, %fd156;
	fma.rn.f64 	%fd190, %fd185, %fd113, %fd157;
	fma.rn.f64 	%fd191, %fd185, %fd115, %fd158;
	fma.rn.f64 	%fd192, %fd185, %fd117, %fd159;
	fma.rn.f64 	%fd193, %fd186, %fd113, %fd160;
	fma.rn.f64 	%fd194, %fd186, %fd115, %fd161;
	fma.rn.f64 	%fd195, %fd186, %fd117, %fd162;
	add.f64 	%fd196, %fd163, %fd184;
	add.f64 	%fd197, %fd164, %fd185;
	add.f64 	%fd198, %fd165, %fd186;
	add.f64 	%fd199, %fd184, %fd181;
	add.f64 	%fd200, %fd185, %fd182;
	add.f64 	%fd201, %fd186, %fd183;
	mul.f64 	%fd202, %fd4, %fd185;
	mul.f64 	%fd203, %fd5, %fd185;
	mul.f64 	%fd204, %fd6, %fd185;
	fma.rn.f64 	%fd205, %fd1, %fd184, %fd202;
	fma.rn.f64 	%fd206, %fd2, %fd184, %fd203;
	fma.rn.f64 	%fd207, %fd3, %fd184, %fd204;
	fma.rn.f64 	%fd208, %fd7, %fd186, %fd205;
	fma.rn.f64 	%fd209, %fd8, %fd186, %fd206;
	fma.rn.f64 	%fd210, %fd9, %fd186, %fd207;
	add.f64 	%fd211, %fd208, 0d0000000000000000;
	add.f64 	%fd212, %fd209, 0d0000000000000000;
	add.f64 	%fd213, %fd210, 0d0000000000000000;
	sub.f64 	%fd214, %fd199, %fd211;
	sub.f64 	%fd215, %fd200, %fd212;
	sub.f64 	%fd216, %fd201, %fd213;
	add.f64 	%fd217, %fd339, 0d0000000000000000;
	add.f64 	%fd218, %fd338, 0d0000000000000000;
	add.f64 	%fd219, %fd337, 0d0000000000000000;
	fma.rn.f64 	%fd220, %fd217, %fd109, %fd187;
	fma.rn.f64 	%fd221, %fd217, %fd110, %fd188;
	fma.rn.f64 	%fd222, %fd217, %fd111, %fd189;
	fma.rn.f64 	%fd223, %fd218, %fd109, %fd190;
	fma.rn.f64 	%fd224, %fd218, %fd110, %fd191;
	fma.rn.f64 	%fd225, %fd218, %fd111, %fd192;
	fma.rn.f64 	%fd226, %fd219, %fd109, %fd193;
	fma.rn.f64 	%fd227, %fd219, %fd110, %fd194;
	fma.rn.f64 	%fd228, %fd219, %fd111, %fd195;
	add.f64 	%fd229, %fd196, %fd217;
	add.f64 	%fd230, %fd197, %fd218;
	add.f64 	%fd231, %fd198, %fd219;
	add.f64 	%fd232, %fd217, %fd214;
	add.f64 	%fd233, %fd218, %fd215;
	add.f64 	%fd234, %fd219, %fd216;
	mul.f64 	%fd235, %fd4, %fd218;
	mul.f64 	%fd236, %fd5, %fd218;
	mul.f64 	%fd237, %fd6, %fd218;
	fma.rn.f64 	%fd238, %fd1, %fd217, %fd235;
	fma.rn.f64 	%fd239, %fd2, %fd217, %fd236;
	fma.rn.f64 	%fd240, %fd3, %fd217, %fd237;
	fma.rn.f64 	%fd241, %fd7, %fd219, %fd238;
	fma.rn.f64 	%fd242, %fd8, %fd219, %fd239;
	fma.rn.f64 	%fd243, %fd9, %fd219, %fd240;
	add.f64 	%fd244, %fd241, 0d0000000000000000;
	add.f64 	%fd245, %fd242, 0d0000000000000000;
	add.f64 	%fd246, %fd243, 0d0000000000000000;
	sub.f64 	%fd52, %fd232, %fd244;
	sub.f64 	%fd53, %fd233, %fd245;
	sub.f64 	%fd54, %fd234, %fd246;
	add.f64 	%fd55, %fd229, 0d0000000000000000;
	add.f64 	%fd56, %fd230, 0d0000000000000000;
	add.f64 	%fd57, %fd231, 0d0000000000000000;
	add.f64 	%fd58, %fd220, 0d0000000000000000;
	add.f64 	%fd59, %fd221, 0d0000000000000000;
	add.f64 	%fd60, %fd222, 0d0000000000000000;
	add.f64 	%fd61, %fd223, 0d0000000000000000;
	add.f64 	%fd62, %fd224, 0d0000000000000000;
	add.f64 	%fd63, %fd225, 0d0000000000000000;
	add.f64 	%fd64, %fd226, 0d0000000000000000;
	add.f64 	%fd65, %fd227, 0d0000000000000000;
	add.f64 	%fd66, %fd228, 0d0000000000000000;
	add.f64 	%fd247, %fd145, %fd244;
	add.f64 	%fd248, %fd146, %fd245;
	add.f64 	%fd249, %fd147, %fd246;
	add.f64 	%fd250, %fd178, %fd247;
	add.f64 	%fd251, %fd179, %fd248;
	add.f64 	%fd252, %fd180, %fd249;
	add.f64 	%fd253, %fd211, %fd250;
	add.f64 	%fd254, %fd212, %fd251;
	add.f64 	%fd255, %fd213, %fd252;
	add.f64 	%fd67, %fd253, 0d0000000000000000;
	add.f64 	%fd68, %fd254, 0d0000000000000000;
	add.f64 	%fd69, %fd255, 0d0000000000000000;
	setp.eq.s64 	%p12, %rd63, 0;
	@%p12 bra 	$L__BB7_21;

	mul.lo.s64 	%rd87, %rd41, %rd27;
	add.s64 	%rd84, %rd63, %rd87;
	// begin inline asm
	{ atom.add.f64 %fd256,[%rd84],%fd67; }

	// end inline asm
	add.s64 	%rd85, %rd84, 8;
	// begin inline asm
	{ atom.add.f64 %fd258,[%rd85],%fd68; }

	// end inline asm
	add.s64 	%rd86, %rd84, 16;
	// begin inline asm
	{ atom.add.f64 %fd260,[%rd86],%fd69; }

	// end inline asm
	bra.uni 	$L__BB7_23;

$L__BB7_21:
	setp.eq.s64 	%p13, %rd56, 0;
	@%p13 bra 	$L__BB7_23;

	add.s64 	%rd88, %rd56, %rd44;
	// begin inline asm
	{ atom.add.f64 %fd262,[%rd88],%fd67; }

	// end inline asm
	add.s64 	%rd89, %rd88, 8;
	// begin inline asm
	{ atom.add.f64 %fd264,[%rd89],%fd68; }

	// end inline asm
	add.s64 	%rd90, %rd88, 16;
	// begin inline asm
	{ atom.add.f64 %fd266,[%rd90],%fd69; }

	// end inline asm

$L__BB7_23:
	setp.eq.s64 	%p14, %rd61, 0;
	add.f64 	%fd70, %fd52, 0d0000000000000000;
	add.f64 	%fd71, %fd53, 0d0000000000000000;
	add.f64 	%fd72, %fd54, 0d0000000000000000;
	@%p14 bra 	$L__BB7_25;

	mul.lo.s64 	%rd94, %rd41, %rd28;
	add.s64 	%rd91, %rd61, %rd94;
	// begin inline asm
	{ atom.add.f64 %fd268,[%rd91],%fd70; }

	// end inline asm
	add.s64 	%rd92, %rd91, 8;
	// begin inline asm
	{ atom.add.f64 %fd270,[%rd92],%fd71; }

	// end inline asm
	add.s64 	%rd93, %rd91, 16;
	// begin inline asm
	{ atom.add.f64 %fd272,[%rd93],%fd72; }

	// end inline asm
	bra.uni 	$L__BB7_27;

$L__BB7_25:
	setp.eq.s64 	%p15, %rd54, 0;
	@%p15 bra 	$L__BB7_27;

	add.s64 	%rd95, %rd54, %rd43;
	// begin inline asm
	{ atom.add.f64 %fd274,[%rd95],%fd70; }

	// end inline asm
	add.s64 	%rd96, %rd95, 8;
	// begin inline asm
	{ atom.add.f64 %fd276,[%rd96],%fd71; }

	// end inline asm
	add.s64 	%rd97, %rd95, 16;
	// begin inline asm
	{ atom.add.f64 %fd278,[%rd97],%fd72; }

	// end inline asm

$L__BB7_27:
	setp.eq.s64 	%p16, %rd57, 0;
	@%p16 bra 	$L__BB7_29;

	mul.lo.s64 	%rd110, %rd41, %rd29;
	add.s64 	%rd98, %rd57, %rd110;
	// begin inline asm
	{ atom.add.f64 %fd280,[%rd98],%fd58; }

	// end inline asm
	add.s64 	%rd99, %rd98, 8;
	// begin inline asm
	{ atom.add.f64 %fd282,[%rd99],%fd59; }

	// end inline asm
	add.s64 	%rd100, %rd98, 16;
	// begin inline asm
	{ atom.add.f64 %fd284,[%rd100],%fd60; }

	// end inline asm
	add.s64 	%rd101, %rd98, 24;
	// begin inline asm
	{ atom.add.f64 %fd286,[%rd101],%fd61; }

	// end inline asm
	add.s64 	%rd102, %rd98, 32;
	// begin inline asm
	{ atom.add.f64 %fd288,[%rd102],%fd62; }

	// end inline asm
	add.s64 	%rd103, %rd98, 40;
	// begin inline asm
	{ atom.add.f64 %fd290,[%rd103],%fd63; }

	// end inline asm
	add.s64 	%rd104, %rd98, 48;
	// begin inline asm
	{ atom.add.f64 %fd292,[%rd104],%fd64; }

	// end inline asm
	add.s64 	%rd105, %rd98, 56;
	// begin inline asm
	{ atom.add.f64 %fd294,[%rd105],%fd65; }

	// end inline asm
	add.s64 	%rd106, %rd98, 64;
	// begin inline asm
	{ atom.add.f64 %fd296,[%rd106],%fd66; }

	// end inline asm
	add.s64 	%rd107, %rd98, 72;
	// begin inline asm
	{ atom.add.f64 %fd298,[%rd107],%fd55; }

	// end inline asm
	add.s64 	%rd108, %rd98, 80;
	// begin inline asm
	{ atom.add.f64 %fd300,[%rd108],%fd56; }

	// end inline asm
	add.s64 	%rd109, %rd98, 88;
	// begin inline asm
	{ atom.add.f64 %fd302,[%rd109],%fd57; }

	// end inline asm
	bra.uni 	$L__BB7_31;

$L__BB7_29:
	setp.eq.s64 	%p17, %rd50, 0;
	@%p17 bra 	$L__BB7_31;

	add.s64 	%rd111, %rd50, %rd42;
	// begin inline asm
	{ atom.add.f64 %fd304,[%rd111],%fd58; }

	// end inline asm
	add.s64 	%rd112, %rd111, 8;
	// begin inline asm
	{ atom.add.f64 %fd306,[%rd112],%fd59; }

	// end inline asm
	add.s64 	%rd113, %rd111, 16;
	// begin inline asm
	{ atom.add.f64 %fd308,[%rd113],%fd60; }

	// end inline asm
	add.s64 	%rd114, %rd111, 24;
	// begin inline asm
	{ atom.add.f64 %fd310,[%rd114],%fd61; }

	// end inline asm
	add.s64 	%rd115, %rd111, 32;
	// begin inline asm
	{ atom.add.f64 %fd312,[%rd115],%fd62; }

	// end inline asm
	add.s64 	%rd116, %rd111, 40;
	// begin inline asm
	{ atom.add.f64 %fd314,[%rd116],%fd63; }

	// end inline asm
	add.s64 	%rd117, %rd111, 48;
	// begin inline asm
	{ atom.add.f64 %fd316,[%rd117],%fd64; }

	// end inline asm
	add.s64 	%rd118, %rd111, 56;
	// begin inline asm
	{ atom.add.f64 %fd318,[%rd118],%fd65; }

	// end inline asm
	add.s64 	%rd119, %rd111, 64;
	// begin inline asm
	{ atom.add.f64 %fd320,[%rd119],%fd66; }

	// end inline asm
	add.s64 	%rd120, %rd111, 72;
	// begin inline asm
	{ atom.add.f64 %fd322,[%rd120],%fd55; }

	// end inline asm
	add.s64 	%rd121, %rd111, 80;
	// begin inline asm
	{ atom.add.f64 %fd324,[%rd121],%fd56; }

	// end inline asm
	add.s64 	%rd122, %rd111, 88;
	// begin inline asm
	{ atom.add.f64 %fd326,[%rd122],%fd57; }

	// end inline asm

$L__BB7_31:
	ld.param.u64 	%rd123, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_0+24];
	add.s64 	%rd126, %rd126, %rd21;
	setp.lt.u64 	%p18, %rd126, %rd123;
	@%p18 bra 	$L__BB7_2;

$L__BB7_32:
	ret;

}
	// .globl	compute_soft_kinematic_energy_cuda_kernel_forward
.visible .entry compute_soft_kinematic_energy_cuda_kernel_forward(
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_forward_param_3[56],
	.param .f64 compute_soft_kinematic_energy_cuda_kernel_forward_param_4,
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_forward_param_5[56],
	.param .u32 compute_soft_kinematic_energy_cuda_kernel_forward_param_6,
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_forward_param_7[56]
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<42>;
	.reg .b32 	%r<114>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<65>;


	ld.param.v2.u32 	{%r53, %r54}, [compute_soft_kinematic_energy_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r55, %r56}, [compute_soft_kinematic_energy_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r61, %r62}, [compute_soft_kinematic_energy_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r69, %r70}, [compute_soft_kinematic_energy_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r77, %r78}, [compute_soft_kinematic_energy_cuda_kernel_forward_param_3+32];
	ld.param.f64 	%fd2, [compute_soft_kinematic_energy_cuda_kernel_forward_param_4];
	ld.param.v2.u32 	{%r85, %r86}, [compute_soft_kinematic_energy_cuda_kernel_forward_param_5+32];
	ld.param.u32 	%r43, [compute_soft_kinematic_energy_cuda_kernel_forward_param_6];
	ld.param.v2.u32 	{%r93, %r94}, [compute_soft_kinematic_energy_cuda_kernel_forward_param_7+32];
	ld.param.u64 	%rd39, [compute_soft_kinematic_energy_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd37, [compute_soft_kinematic_energy_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd35, [compute_soft_kinematic_energy_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd33, [compute_soft_kinematic_energy_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd31, [compute_soft_kinematic_energy_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd30, [compute_soft_kinematic_energy_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [compute_soft_kinematic_energy_cuda_kernel_forward_param_0+16];
	mov.u32 	%r97, %ntid.x;
	cvt.u64.u32 	%rd1, %r97;
	mov.u32 	%r98, %ctaid.x;
	mul.wide.u32 	%rd41, %r97, %r98;
	mov.u32 	%r99, %tid.x;
	cvt.u64.u32 	%rd42, %r99;
	add.s64 	%rd61, %rd41, %rd42;
	setp.ge.u64 	%p1, %rd61, %rd30;
	@%p1 bra 	$L__BB8_17;

	cvta.to.global.u64 	%rd4, %rd39;
	cvta.to.global.u64 	%rd6, %rd35;
	cvta.to.global.u64 	%rd7, %rd31;
	cvt.s64.s32 	%rd8, %r56;
	cvt.s64.s32 	%rd9, %r55;
	cvt.s64.s32 	%rd10, %r54;
	cvt.s64.s32 	%rd11, %r69;
	cvt.s64.s32 	%rd12, %r93;
	mov.u32 	%r100, %nctaid.x;
	cvt.u64.u32 	%rd43, %r100;
	mul.lo.s64 	%rd13, %rd1, %rd43;
	mul.f64 	%fd1, %fd2, 0d3FE0000000000000;
	cvt.s64.s32 	%rd14, %r61;
	cvt.s64.s32 	%rd15, %r77;
	cvt.s64.s32 	%rd16, %r85;
	cvta.to.global.u64 	%rd17, %rd33;

$L__BB8_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd62, %rd61;
	@%p2 bra 	$L__BB8_6;

	or.b64  	%rd44, %rd61, %rd8;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p3, %rd45, 0;
	@%p3 bra 	$L__BB8_5;

	div.u64 	%rd62, %rd61, %rd8;
	bra.uni 	$L__BB8_6;

$L__BB8_5:
	cvt.u32.u64 	%r101, %rd8;
	cvt.u32.u64 	%r102, %rd61;
	div.u32 	%r103, %r102, %r101;
	cvt.u64.u32 	%rd62, %r103;

$L__BB8_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB8_10;

	or.b64  	%rd46, %rd62, %rd9;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p5, %rd47, 0;
	@%p5 bra 	$L__BB8_9;

	div.u64 	%rd62, %rd62, %rd9;
	bra.uni 	$L__BB8_10;

$L__BB8_9:
	cvt.u32.u64 	%r104, %rd9;
	cvt.u32.u64 	%r105, %rd62;
	div.u32 	%r106, %r105, %r104;
	cvt.u64.u32 	%rd62, %r106;

$L__BB8_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB8_14;

	or.b64  	%rd48, %rd62, %rd10;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p7, %rd49, 0;
	@%p7 bra 	$L__BB8_13;

	div.u64 	%rd62, %rd62, %rd10;
	bra.uni 	$L__BB8_14;

$L__BB8_13:
	cvt.u32.u64 	%r107, %rd10;
	cvt.u32.u64 	%r108, %rd62;
	div.u32 	%r109, %r108, %r107;
	cvt.u64.u32 	%rd62, %r109;

$L__BB8_14:
	cvt.u32.u64 	%r110, %rd62;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b32 	%r111, %r110, 0, %p8;
	cvt.s64.s32 	%rd28, %r111;
	mul.lo.s64 	%rd50, %rd28, %rd11;
	add.s64 	%rd51, %rd17, %rd50;
	ld.global.u8 	%rs41, [%rd51];
	setp.eq.s16 	%p9, %rs41, 0;
	@%p9 bra 	$L__BB8_16;

	cvt.u32.u64 	%r112, %rd28;
	mul.lo.s64 	%rd53, %rd28, %rd12;
	add.s64 	%rd54, %rd4, %rd53;
	ld.global.f64 	%fd5, [%rd54];
	mul.f64 	%fd6, %fd1, %fd5;
	add.s32 	%r113, %r112, %r43;
	cvt.s64.s32 	%rd55, %r113;
	mul.lo.s64 	%rd56, %rd55, %rd14;
	add.s64 	%rd57, %rd7, %rd56;
	mul.lo.s64 	%rd58, %rd28, %rd15;
	add.s64 	%rd59, %rd6, %rd58;
	ld.global.f64 	%fd7, [%rd59];
	ld.global.f64 	%fd8, [%rd57];
	sub.f64 	%fd9, %fd8, %fd7;
	ld.global.f64 	%fd10, [%rd59+8];
	ld.global.f64 	%fd11, [%rd57+8];
	sub.f64 	%fd12, %fd11, %fd10;
	ld.global.f64 	%fd13, [%rd59+16];
	ld.global.f64 	%fd14, [%rd57+16];
	sub.f64 	%fd15, %fd14, %fd13;
	mul.f64 	%fd16, %fd12, %fd12;
	fma.rn.f64 	%fd17, %fd9, %fd9, %fd16;
	fma.rn.f64 	%fd18, %fd15, %fd15, %fd17;
	mul.f64 	%fd4, %fd6, %fd18;
	mul.lo.s64 	%rd60, %rd28, %rd16;
	add.s64 	%rd52, %rd37, %rd60;
	// begin inline asm
	{ atom.add.f64 %fd3,[%rd52],%fd4; }

	// end inline asm

$L__BB8_16:
	add.s64 	%rd61, %rd61, %rd13;
	setp.lt.u64 	%p10, %rd61, %rd30;
	@%p10 bra 	$L__BB8_2;

$L__BB8_17:
	ret;

}
	// .globl	compute_soft_kinematic_energy_cuda_kernel_backward
.visible .entry compute_soft_kinematic_energy_cuda_kernel_backward(
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_3[56],
	.param .f64 compute_soft_kinematic_energy_cuda_kernel_backward_param_4,
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_5[56],
	.param .u32 compute_soft_kinematic_energy_cuda_kernel_backward_param_6,
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_10[56],
	.param .f64 compute_soft_kinematic_energy_cuda_kernel_backward_param_11,
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_12[56],
	.param .u32 compute_soft_kinematic_energy_cuda_kernel_backward_param_13,
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_14[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<74>;
	.reg .b32 	%r<188>;
	.reg .f64 	%fd<91>;
	.reg .b64 	%rd<112>;


	ld.param.v2.u32 	{%r92, %r93}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r94, %r95}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r100, %r101}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r108, %r109}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r116, %r117}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_3+32];
	ld.param.f64 	%fd29, [compute_soft_kinematic_energy_cuda_kernel_backward_param_4];
	ld.param.v2.u32 	{%r124, %r125}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r132, %r133}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r140, %r141}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r148, %r149}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r156, %r157}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_12+32];
	ld.param.v2.u32 	{%r164, %r165}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_14+32];
	ld.param.u64 	%rd61, [compute_soft_kinematic_energy_cuda_kernel_backward_param_14];
	ld.param.u64 	%rd59, [compute_soft_kinematic_energy_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd57, [compute_soft_kinematic_energy_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd55, [compute_soft_kinematic_energy_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd54, [compute_soft_kinematic_energy_cuda_kernel_backward_param_7+8];
	ld.param.u64 	%rd53, [compute_soft_kinematic_energy_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd52, [compute_soft_kinematic_energy_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd50, [compute_soft_kinematic_energy_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd49, [compute_soft_kinematic_energy_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd47, [compute_soft_kinematic_energy_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd46, [compute_soft_kinematic_energy_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd45, [compute_soft_kinematic_energy_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd44, [compute_soft_kinematic_energy_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r9, [compute_soft_kinematic_energy_cuda_kernel_backward_param_0+16];
	mov.u32 	%r168, %ntid.x;
	cvt.u64.u32 	%rd1, %r168;
	mov.u32 	%r169, %ctaid.x;
	mul.wide.u32 	%rd63, %r168, %r169;
	mov.u32 	%r170, %tid.x;
	cvt.u64.u32 	%rd64, %r170;
	add.s64 	%rd108, %rd63, %rd64;
	setp.ge.u64 	%p1, %rd108, %rd44;
	@%p1 bra 	$L__BB9_35;

	cvta.to.global.u64 	%rd12, %rd59;
	cvta.to.global.u64 	%rd13, %rd53;
	cvta.to.global.u64 	%rd14, %rd52;
	cvta.to.global.u64 	%rd15, %rd49;
	cvta.to.global.u64 	%rd16, %rd45;
	cvt.s64.s32 	%rd17, %r95;
	cvt.s64.s32 	%rd18, %r94;
	cvt.s64.s32 	%rd19, %r93;
	cvt.s64.s32 	%rd20, %r108;
	cvt.s64.s32 	%rd21, %r132;
	mul.f64 	%fd86, %fd29, 0d3FE0000000000000;
	cvt.s64.s32 	%rd22, %r100;
	cvt.s64.s32 	%rd23, %r116;
	mov.u32 	%r172, %nctaid.x;
	cvt.u64.u32 	%rd65, %r172;
	mul.lo.s64 	%rd24, %rd1, %rd65;
	cvt.s64.s32 	%rd25, %r164;
	cvt.s64.s32 	%rd26, %r156;
	cvt.s64.s32 	%rd27, %r124;
	cvt.s64.s32 	%rd28, %r148;
	cvt.s64.s32 	%rd29, %r140;
	cvta.to.global.u64 	%rd30, %rd47;

$L__BB9_2:
	setp.lt.s32 	%p2, %r9, 4;
	mov.u64 	%rd109, %rd108;
	@%p2 bra 	$L__BB9_6;

	or.b64  	%rd66, %rd108, %rd17;
	and.b64  	%rd67, %rd66, -4294967296;
	setp.eq.s64 	%p3, %rd67, 0;
	@%p3 bra 	$L__BB9_5;

	div.u64 	%rd109, %rd108, %rd17;
	bra.uni 	$L__BB9_6;

$L__BB9_5:
	cvt.u32.u64 	%r173, %rd17;
	cvt.u32.u64 	%r174, %rd108;
	div.u32 	%r175, %r174, %r173;
	cvt.u64.u32 	%rd109, %r175;

$L__BB9_6:
	setp.lt.s32 	%p4, %r9, 3;
	@%p4 bra 	$L__BB9_10;

	or.b64  	%rd68, %rd109, %rd18;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.eq.s64 	%p5, %rd69, 0;
	@%p5 bra 	$L__BB9_9;

	div.u64 	%rd109, %rd109, %rd18;
	bra.uni 	$L__BB9_10;

$L__BB9_9:
	cvt.u32.u64 	%r176, %rd18;
	cvt.u32.u64 	%r177, %rd109;
	div.u32 	%r178, %r177, %r176;
	cvt.u64.u32 	%rd109, %r178;

$L__BB9_10:
	setp.lt.s32 	%p6, %r9, 2;
	@%p6 bra 	$L__BB9_14;

	or.b64  	%rd70, %rd109, %rd19;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.eq.s64 	%p7, %rd71, 0;
	@%p7 bra 	$L__BB9_13;

	div.u64 	%rd109, %rd109, %rd19;
	bra.uni 	$L__BB9_14;

$L__BB9_13:
	cvt.u32.u64 	%r179, %rd19;
	cvt.u32.u64 	%r180, %rd109;
	div.u32 	%r181, %r180, %r179;
	cvt.u64.u32 	%rd109, %r181;

$L__BB9_14:
	cvt.u32.u64 	%r182, %rd109;
	setp.gt.s32 	%p8, %r9, 0;
	selp.b32 	%r183, %r182, 0, %p8;
	cvt.s64.s32 	%rd41, %r183;
	mul.lo.s64 	%rd72, %rd41, %rd20;
	add.s64 	%rd73, %rd30, %rd72;
	mul.lo.s64 	%rd42, %rd41, %rd21;
	ld.global.u8 	%rs1, [%rd73];
	setp.eq.s16 	%p9, %rs1, 0;
	mov.f64 	%fd90, 0d0000000000000000;
	mov.f64 	%fd83, %fd90;
	mov.f64 	%fd84, %fd90;
	mov.f64 	%fd85, %fd90;
	@%p9 bra 	$L__BB9_16;

	ld.param.u32 	%r185, [compute_soft_kinematic_energy_cuda_kernel_backward_param_6];
	cvt.u32.u64 	%r184, %rd41;
	add.s64 	%rd74, %rd13, %rd42;
	ld.global.f64 	%fd34, [%rd74];
	mul.f64 	%fd87, %fd86, %fd34;
	add.s32 	%r187, %r184, %r185;
	cvt.s64.s32 	%rd75, %r187;
	mul.lo.s64 	%rd76, %rd75, %rd22;
	add.s64 	%rd77, %rd16, %rd76;
	mul.lo.s64 	%rd78, %rd41, %rd23;
	add.s64 	%rd79, %rd15, %rd78;
	ld.global.f64 	%fd35, [%rd79];
	ld.global.f64 	%fd36, [%rd77];
	sub.f64 	%fd83, %fd36, %fd35;
	ld.global.f64 	%fd37, [%rd79+8];
	ld.global.f64 	%fd38, [%rd77+8];
	sub.f64 	%fd84, %fd38, %fd37;
	ld.global.f64 	%fd39, [%rd79+16];
	ld.global.f64 	%fd40, [%rd77+16];
	sub.f64 	%fd85, %fd40, %fd39;
	mul.f64 	%fd41, %fd84, %fd84;
	fma.rn.f64 	%fd42, %fd83, %fd83, %fd41;
	fma.rn.f64 	%fd88, %fd85, %fd85, %fd42;

$L__BB9_16:
	@%p9 bra 	$L__BB9_30;

	ld.param.u64 	%rd106, [compute_soft_kinematic_energy_cuda_kernel_backward_param_12];
	setp.eq.s64 	%p11, %rd106, 0;
	@%p11 bra 	$L__BB9_19;

	mul.lo.s64 	%rd80, %rd41, %rd26;
	add.s64 	%rd81, %rd12, %rd80;
	ld.global.f64 	%fd44, [%rd81];
	add.f64 	%fd89, %fd44, 0d0000000000000000;
	bra.uni 	$L__BB9_21;

$L__BB9_19:
	ld.param.u64 	%rd107, [compute_soft_kinematic_energy_cuda_kernel_backward_param_5+8];
	setp.eq.s64 	%p12, %rd107, 0;
	mov.f64 	%fd89, 0d0000000000000000;
	@%p12 bra 	$L__BB9_21;

	mul.lo.s64 	%rd82, %rd41, %rd27;
	add.s64 	%rd83, %rd14, %rd82;
	ld.global.f64 	%fd46, [%rd83];
	add.f64 	%fd89, %fd46, 0d0000000000000000;

$L__BB9_21:
	fma.rn.f64 	%fd19, %fd88, %fd89, 0d0000000000000000;
	mov.f64 	%fd47, 0d0000000000000000;
	fma.rn.f64 	%fd48, %fd87, %fd89, 0d0000000000000000;
	add.f64 	%fd49, %fd83, %fd83;
	add.f64 	%fd50, %fd84, %fd84;
	add.f64 	%fd51, %fd85, %fd85;
	fma.rn.f64 	%fd20, %fd49, %fd48, 0d0000000000000000;
	fma.rn.f64 	%fd21, %fd50, %fd48, 0d0000000000000000;
	fma.rn.f64 	%fd22, %fd51, %fd48, 0d0000000000000000;
	sub.f64 	%fd23, %fd47, %fd20;
	sub.f64 	%fd24, %fd47, %fd21;
	sub.f64 	%fd25, %fd47, %fd22;
	setp.eq.s64 	%p13, %rd57, 0;
	@%p13 bra 	$L__BB9_23;

	mul.lo.s64 	%rd87, %rd41, %rd28;
	add.s64 	%rd84, %rd57, %rd87;
	// begin inline asm
	{ atom.add.f64 %fd52,[%rd84],%fd23; }

	// end inline asm
	add.s64 	%rd85, %rd84, 8;
	// begin inline asm
	{ atom.add.f64 %fd54,[%rd85],%fd24; }

	// end inline asm
	add.s64 	%rd86, %rd84, 16;
	// begin inline asm
	{ atom.add.f64 %fd56,[%rd86],%fd25; }

	// end inline asm
	bra.uni 	$L__BB9_25;

$L__BB9_23:
	setp.eq.s64 	%p14, %rd50, 0;
	@%p14 bra 	$L__BB9_25;

	mul.lo.s64 	%rd91, %rd41, %rd23;
	add.s64 	%rd88, %rd50, %rd91;
	// begin inline asm
	{ atom.add.f64 %fd58,[%rd88],%fd23; }

	// end inline asm
	add.s64 	%rd89, %rd88, 8;
	// begin inline asm
	{ atom.add.f64 %fd60,[%rd89],%fd24; }

	// end inline asm
	add.s64 	%rd90, %rd88, 16;
	// begin inline asm
	{ atom.add.f64 %fd62,[%rd90],%fd25; }

	// end inline asm

$L__BB9_25:
	setp.eq.s64 	%p15, %rd55, 0;
	@%p15 bra 	$L__BB9_27;

	cvt.s64.s32 	%rd95, %r187;
	mul.lo.s64 	%rd96, %rd95, %rd29;
	add.s64 	%rd92, %rd55, %rd96;
	// begin inline asm
	{ atom.add.f64 %fd64,[%rd92],%fd20; }

	// end inline asm
	add.s64 	%rd93, %rd92, 8;
	// begin inline asm
	{ atom.add.f64 %fd66,[%rd93],%fd21; }

	// end inline asm
	add.s64 	%rd94, %rd92, 16;
	// begin inline asm
	{ atom.add.f64 %fd68,[%rd94],%fd22; }

	// end inline asm
	bra.uni 	$L__BB9_29;

$L__BB9_27:
	setp.eq.s64 	%p16, %rd46, 0;
	@%p16 bra 	$L__BB9_29;

	cvt.s64.s32 	%rd100, %r187;
	mul.lo.s64 	%rd101, %rd100, %rd22;
	add.s64 	%rd97, %rd46, %rd101;
	// begin inline asm
	{ atom.add.f64 %fd70,[%rd97],%fd20; }

	// end inline asm
	add.s64 	%rd98, %rd97, 8;
	// begin inline asm
	{ atom.add.f64 %fd72,[%rd98],%fd21; }

	// end inline asm
	add.s64 	%rd99, %rd97, 16;
	// begin inline asm
	{ atom.add.f64 %fd74,[%rd99],%fd22; }

	// end inline asm

$L__BB9_29:
	fma.rn.f64 	%fd90, %fd86, %fd19, 0d0000000000000000;

$L__BB9_30:
	add.f64 	%fd28, %fd90, 0d0000000000000000;
	setp.eq.s64 	%p17, %rd61, 0;
	@%p17 bra 	$L__BB9_32;

	mul.lo.s64 	%rd103, %rd41, %rd25;
	add.s64 	%rd102, %rd61, %rd103;
	// begin inline asm
	{ atom.add.f64 %fd76,[%rd102],%fd28; }

	// end inline asm
	bra.uni 	$L__BB9_34;

$L__BB9_32:
	setp.eq.s64 	%p18, %rd54, 0;
	@%p18 bra 	$L__BB9_34;

	add.s64 	%rd104, %rd54, %rd42;
	// begin inline asm
	{ atom.add.f64 %fd78,[%rd104],%fd28; }

	// end inline asm

$L__BB9_34:
	ld.param.u64 	%rd105, [compute_soft_kinematic_energy_cuda_kernel_backward_param_0+24];
	add.s64 	%rd108, %rd108, %rd24;
	setp.lt.u64 	%p19, %rd108, %rd105;
	@%p19 bra 	$L__BB9_2;

$L__BB9_35:
	ret;

}
	// .globl	compute_affine_kinematic_grad_cuda_kernel_forward
.visible .entry compute_affine_kinematic_grad_cuda_kernel_forward(
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_3[56],
	.param .f64 compute_affine_kinematic_grad_cuda_kernel_forward_param_4,
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_8[56]
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<58>;
	.reg .b32 	%r<145>;
	.reg .f64 	%fd<64>;
	.reg .b64 	%rd<89>;


	ld.param.v2.u32 	{%r70, %r71}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r72, %r73}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r78, %r79}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r86, %r87}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r94, %r95}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_3+32];
	ld.param.f64 	%fd1, [compute_affine_kinematic_grad_cuda_kernel_forward_param_4];
	ld.param.v2.u32 	{%r102, %r103}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r110, %r111}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r118, %r119}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r126, %r127}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_8+32];
	ld.param.u64 	%rd47, [compute_affine_kinematic_grad_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd45, [compute_affine_kinematic_grad_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd43, [compute_affine_kinematic_grad_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd41, [compute_affine_kinematic_grad_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd39, [compute_affine_kinematic_grad_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd37, [compute_affine_kinematic_grad_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd35, [compute_affine_kinematic_grad_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd34, [compute_affine_kinematic_grad_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [compute_affine_kinematic_grad_cuda_kernel_forward_param_0+16];
	mov.u32 	%r130, %ntid.x;
	cvt.u64.u32 	%rd1, %r130;
	mov.u32 	%r131, %ctaid.x;
	mul.wide.u32 	%rd49, %r130, %r131;
	mov.u32 	%r132, %tid.x;
	cvt.u64.u32 	%rd50, %r132;
	add.s64 	%rd85, %rd49, %rd50;
	setp.ge.u64 	%p1, %rd85, %rd34;
	@%p1 bra 	$L__BB10_18;

	cvta.to.global.u64 	%rd4, %rd47;
	cvta.to.global.u64 	%rd5, %rd45;
	cvta.to.global.u64 	%rd6, %rd43;
	cvta.to.global.u64 	%rd8, %rd39;
	cvta.to.global.u64 	%rd9, %rd35;
	cvt.s64.s32 	%rd10, %r73;
	cvt.s64.s32 	%rd11, %r72;
	cvt.s64.s32 	%rd12, %r71;
	cvt.s64.s32 	%rd13, %r118;
	cvt.s64.s32 	%rd14, %r126;
	mov.u32 	%r133, %nctaid.x;
	cvt.u64.u32 	%rd51, %r133;
	mul.lo.s64 	%rd15, %rd1, %rd51;
	cvt.s64.s32 	%rd16, %r110;
	cvt.s64.s32 	%rd17, %r86;
	cvt.s64.s32 	%rd18, %r78;
	cvt.s64.s32 	%rd19, %r94;
	cvt.s64.s32 	%rd20, %r102;
	cvta.to.global.u64 	%rd21, %rd37;

$L__BB10_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd86, %rd85;
	@%p2 bra 	$L__BB10_6;

	or.b64  	%rd52, %rd85, %rd10;
	and.b64  	%rd53, %rd52, -4294967296;
	setp.eq.s64 	%p3, %rd53, 0;
	@%p3 bra 	$L__BB10_5;

	div.u64 	%rd86, %rd85, %rd10;
	bra.uni 	$L__BB10_6;

$L__BB10_5:
	cvt.u32.u64 	%r134, %rd10;
	cvt.u32.u64 	%r135, %rd85;
	div.u32 	%r136, %r135, %r134;
	cvt.u64.u32 	%rd86, %r136;

$L__BB10_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB10_10;

	or.b64  	%rd54, %rd86, %rd11;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p5, %rd55, 0;
	@%p5 bra 	$L__BB10_9;

	div.u64 	%rd86, %rd86, %rd11;
	bra.uni 	$L__BB10_10;

$L__BB10_9:
	cvt.u32.u64 	%r137, %rd11;
	cvt.u32.u64 	%r138, %rd86;
	div.u32 	%r139, %r138, %r137;
	cvt.u64.u32 	%rd86, %r139;

$L__BB10_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB10_14;

	or.b64  	%rd56, %rd86, %rd12;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p7, %rd57, 0;
	@%p7 bra 	$L__BB10_13;

	div.u64 	%rd86, %rd86, %rd12;
	bra.uni 	$L__BB10_14;

$L__BB10_13:
	cvt.u32.u64 	%r140, %rd12;
	cvt.u32.u64 	%r141, %rd86;
	div.u32 	%r142, %r141, %r140;
	cvt.u64.u32 	%rd86, %r142;

$L__BB10_14:
	cvt.s64.s32 	%rd58, %rd86;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd32, %rd58, 0, %p8;
	mul.lo.s64 	%rd59, %rd32, %rd13;
	add.s64 	%rd60, %rd5, %rd59;
	ld.global.s32 	%rd61, [%rd60];
	mul.lo.s64 	%rd62, %rd61, %rd14;
	add.s64 	%rd63, %rd4, %rd62;
	ld.global.u32 	%r143, [%rd63];
	add.s32 	%r144, %r143, -1;
	setp.lt.u32 	%p9, %r144, 2;
	@%p9 bra 	$L__BB10_17;

	mul.lo.s64 	%rd64, %rd32, %rd17;
	add.s64 	%rd65, %rd21, %rd64;
	ld.global.u8 	%rs57, [%rd65];
	setp.eq.s16 	%p10, %rs57, 0;
	@%p10 bra 	$L__BB10_17;

	mul.lo.s64 	%rd78, %rd32, %rd18;
	add.s64 	%rd79, %rd9, %rd78;
	mul.lo.s64 	%rd80, %rd32, %rd19;
	add.s64 	%rd81, %rd8, %rd80;
	ld.global.f64 	%fd26, [%rd81];
	ld.global.f64 	%fd27, [%rd79];
	sub.f64 	%fd28, %fd27, %fd26;
	ld.global.f64 	%fd29, [%rd81+8];
	ld.global.f64 	%fd30, [%rd79+8];
	sub.f64 	%fd31, %fd30, %fd29;
	ld.global.f64 	%fd32, [%rd81+16];
	ld.global.f64 	%fd33, [%rd79+16];
	sub.f64 	%fd34, %fd33, %fd32;
	ld.global.f64 	%fd35, [%rd81+24];
	ld.global.f64 	%fd36, [%rd79+24];
	sub.f64 	%fd37, %fd36, %fd35;
	ld.global.f64 	%fd38, [%rd81+32];
	ld.global.f64 	%fd39, [%rd79+32];
	sub.f64 	%fd40, %fd39, %fd38;
	ld.global.f64 	%fd41, [%rd81+40];
	ld.global.f64 	%fd42, [%rd79+40];
	sub.f64 	%fd43, %fd42, %fd41;
	ld.global.f64 	%fd44, [%rd81+48];
	ld.global.f64 	%fd45, [%rd79+48];
	sub.f64 	%fd46, %fd45, %fd44;
	ld.global.f64 	%fd47, [%rd81+56];
	ld.global.f64 	%fd48, [%rd79+56];
	sub.f64 	%fd49, %fd48, %fd47;
	ld.global.f64 	%fd50, [%rd81+64];
	ld.global.f64 	%fd51, [%rd79+64];
	sub.f64 	%fd52, %fd51, %fd50;
	ld.global.f64 	%fd53, [%rd81+72];
	ld.global.f64 	%fd54, [%rd79+72];
	sub.f64 	%fd55, %fd54, %fd53;
	ld.global.f64 	%fd56, [%rd81+80];
	ld.global.f64 	%fd57, [%rd79+80];
	sub.f64 	%fd58, %fd57, %fd56;
	ld.global.f64 	%fd59, [%rd81+88];
	ld.global.f64 	%fd60, [%rd79+88];
	sub.f64 	%fd61, %fd60, %fd59;
	mul.lo.s64 	%rd82, %rd32, %rd16;
	add.s64 	%rd83, %rd6, %rd82;
	ld.global.f64 	%fd62, [%rd83];
	mul.f64 	%fd63, %fd62, %fd1;
	mul.f64 	%fd3, %fd63, %fd28;
	mul.f64 	%fd5, %fd63, %fd31;
	mul.f64 	%fd7, %fd63, %fd34;
	mul.f64 	%fd9, %fd63, %fd37;
	mul.f64 	%fd11, %fd63, %fd40;
	mul.f64 	%fd13, %fd63, %fd43;
	mul.f64 	%fd15, %fd63, %fd46;
	mul.f64 	%fd17, %fd63, %fd49;
	mul.f64 	%fd19, %fd63, %fd52;
	mul.f64 	%fd21, %fd63, %fd55;
	mul.f64 	%fd23, %fd63, %fd58;
	mul.f64 	%fd25, %fd63, %fd61;
	mul.lo.s64 	%rd84, %rd32, %rd20;
	add.s64 	%rd66, %rd41, %rd84;
	// begin inline asm
	{ atom.add.f64 %fd2,[%rd66],%fd3; }

	// end inline asm
	add.s64 	%rd67, %rd66, 8;
	// begin inline asm
	{ atom.add.f64 %fd4,[%rd67],%fd5; }

	// end inline asm
	add.s64 	%rd68, %rd66, 16;
	// begin inline asm
	{ atom.add.f64 %fd6,[%rd68],%fd7; }

	// end inline asm
	add.s64 	%rd69, %rd66, 24;
	// begin inline asm
	{ atom.add.f64 %fd8,[%rd69],%fd9; }

	// end inline asm
	add.s64 	%rd70, %rd66, 32;
	// begin inline asm
	{ atom.add.f64 %fd10,[%rd70],%fd11; }

	// end inline asm
	add.s64 	%rd71, %rd66, 40;
	// begin inline asm
	{ atom.add.f64 %fd12,[%rd71],%fd13; }

	// end inline asm
	add.s64 	%rd72, %rd66, 48;
	// begin inline asm
	{ atom.add.f64 %fd14,[%rd72],%fd15; }

	// end inline asm
	add.s64 	%rd73, %rd66, 56;
	// begin inline asm
	{ atom.add.f64 %fd16,[%rd73],%fd17; }

	// end inline asm
	add.s64 	%rd74, %rd66, 64;
	// begin inline asm
	{ atom.add.f64 %fd18,[%rd74],%fd19; }

	// end inline asm
	add.s64 	%rd75, %rd66, 72;
	// begin inline asm
	{ atom.add.f64 %fd20,[%rd75],%fd21; }

	// end inline asm
	add.s64 	%rd76, %rd66, 80;
	// begin inline asm
	{ atom.add.f64 %fd22,[%rd76],%fd23; }

	// end inline asm
	add.s64 	%rd77, %rd66, 88;
	// begin inline asm
	{ atom.add.f64 %fd24,[%rd77],%fd25; }

	// end inline asm

$L__BB10_17:
	add.s64 	%rd85, %rd85, %rd15;
	setp.lt.u64 	%p11, %rd85, %rd34;
	@%p11 bra 	$L__BB10_2;

$L__BB10_18:
	ret;

}
	// .globl	compute_affine_kinematic_grad_cuda_kernel_backward
.visible .entry compute_affine_kinematic_grad_cuda_kernel_backward(
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_3[56],
	.param .f64 compute_affine_kinematic_grad_cuda_kernel_backward_param_4,
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_11[56],
	.param .f64 compute_affine_kinematic_grad_cuda_kernel_backward_param_12,
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_14[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_15[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_16[56]
)
{
	.reg .pred 	%p<21>;
	.reg .b16 	%rs<90>;
	.reg .b32 	%r<213>;
	.reg .f64 	%fd<334>;
	.reg .b64 	%rd<159>;


	ld.param.v2.u32 	{%r106, %r107}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r108, %r109}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r114, %r115}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r122, %r123}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r130, %r131}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_3+32];
	ld.param.f64 	%fd93, [compute_affine_kinematic_grad_cuda_kernel_backward_param_4];
	ld.param.v2.u32 	{%r138, %r139}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r146, %r147}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r154, %r155}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r162, %r163}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r170, %r171}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r178, %r179}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r186, %r187}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_13+32];
	ld.param.v2.u32 	{%r194, %r195}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_14+32];
	ld.param.u64 	%rd69, [compute_affine_kinematic_grad_cuda_kernel_backward_param_14];
	ld.param.u64 	%rd67, [compute_affine_kinematic_grad_cuda_kernel_backward_param_13];
	ld.param.u64 	%rd65, [compute_affine_kinematic_grad_cuda_kernel_backward_param_11];
	ld.param.u64 	%rd63, [compute_affine_kinematic_grad_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd61, [compute_affine_kinematic_grad_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd59, [compute_affine_kinematic_grad_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd58, [compute_affine_kinematic_grad_cuda_kernel_backward_param_6+8];
	ld.param.u64 	%rd57, [compute_affine_kinematic_grad_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd56, [compute_affine_kinematic_grad_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd54, [compute_affine_kinematic_grad_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd53, [compute_affine_kinematic_grad_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd51, [compute_affine_kinematic_grad_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd50, [compute_affine_kinematic_grad_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd49, [compute_affine_kinematic_grad_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd48, [compute_affine_kinematic_grad_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [compute_affine_kinematic_grad_cuda_kernel_backward_param_0+16];
	mov.u32 	%r198, %ntid.x;
	cvt.u64.u32 	%rd1, %r198;
	mov.u32 	%r199, %ctaid.x;
	mul.wide.u32 	%rd71, %r198, %r199;
	mov.u32 	%r200, %tid.x;
	cvt.u64.u32 	%rd72, %r200;
	add.s64 	%rd155, %rd71, %rd72;
	setp.ge.u64 	%p1, %rd155, %rd48;
	@%p1 bra 	$L__BB11_36;

	cvta.to.global.u64 	%rd12, %rd67;
	cvta.to.global.u64 	%rd13, %rd61;
	cvta.to.global.u64 	%rd14, %rd59;
	cvta.to.global.u64 	%rd15, %rd57;
	cvta.to.global.u64 	%rd16, %rd56;
	cvta.to.global.u64 	%rd17, %rd53;
	cvta.to.global.u64 	%rd18, %rd49;
	cvt.s64.s32 	%rd19, %r109;
	cvt.s64.s32 	%rd20, %r108;
	cvt.s64.s32 	%rd21, %r107;
	cvt.s64.s32 	%rd22, %r154;
	cvt.s64.s32 	%rd23, %r162;
	mov.u32 	%r201, %nctaid.x;
	cvt.u64.u32 	%rd73, %r201;
	mul.lo.s64 	%rd24, %rd1, %rd73;
	cvt.s64.s32 	%rd25, %r146;
	cvt.s64.s32 	%rd26, %r122;
	cvt.s64.s32 	%rd27, %r114;
	cvt.s64.s32 	%rd28, %r130;
	cvt.s64.s32 	%rd29, %r194;
	cvt.s64.s32 	%rd30, %r186;
	cvt.s64.s32 	%rd31, %r138;
	cvt.s64.s32 	%rd32, %r178;
	cvt.s64.s32 	%rd33, %r170;
	cvta.to.global.u64 	%rd34, %rd51;

$L__BB11_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd156, %rd155;
	@%p2 bra 	$L__BB11_6;

	or.b64  	%rd74, %rd155, %rd19;
	and.b64  	%rd75, %rd74, -4294967296;
	setp.eq.s64 	%p3, %rd75, 0;
	@%p3 bra 	$L__BB11_5;

	div.u64 	%rd156, %rd155, %rd19;
	bra.uni 	$L__BB11_6;

$L__BB11_5:
	cvt.u32.u64 	%r202, %rd19;
	cvt.u32.u64 	%r203, %rd155;
	div.u32 	%r204, %r203, %r202;
	cvt.u64.u32 	%rd156, %r204;

$L__BB11_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB11_10;

	or.b64  	%rd76, %rd156, %rd20;
	and.b64  	%rd77, %rd76, -4294967296;
	setp.eq.s64 	%p5, %rd77, 0;
	@%p5 bra 	$L__BB11_9;

	div.u64 	%rd156, %rd156, %rd20;
	bra.uni 	$L__BB11_10;

$L__BB11_9:
	cvt.u32.u64 	%r205, %rd20;
	cvt.u32.u64 	%r206, %rd156;
	div.u32 	%r207, %r206, %r205;
	cvt.u64.u32 	%rd156, %r207;

$L__BB11_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB11_14;

	or.b64  	%rd78, %rd156, %rd21;
	and.b64  	%rd79, %rd78, -4294967296;
	setp.eq.s64 	%p7, %rd79, 0;
	@%p7 bra 	$L__BB11_13;

	div.u64 	%rd156, %rd156, %rd21;
	bra.uni 	$L__BB11_14;

$L__BB11_13:
	cvt.u32.u64 	%r208, %rd21;
	cvt.u32.u64 	%r209, %rd156;
	div.u32 	%r210, %r209, %r208;
	cvt.u64.u32 	%rd156, %r210;

$L__BB11_14:
	cvt.s64.s32 	%rd80, %rd156;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd45, %rd80, 0, %p8;
	mul.lo.s64 	%rd81, %rd45, %rd22;
	add.s64 	%rd82, %rd14, %rd81;
	ld.global.s32 	%rd83, [%rd82];
	mul.lo.s64 	%rd84, %rd83, %rd23;
	add.s64 	%rd85, %rd13, %rd84;
	ld.global.u32 	%r211, [%rd85];
	add.s32 	%r212, %r211, -1;
	setp.lt.u32 	%p9, %r212, 2;
	@%p9 bra 	$L__BB11_35;

	mul.lo.s64 	%rd46, %rd45, %rd25;
	mul.lo.s64 	%rd86, %rd45, %rd26;
	add.s64 	%rd87, %rd34, %rd86;
	ld.global.u8 	%rs1, [%rd87];
	setp.eq.s16 	%p10, %rs1, 0;
	mov.f64 	%fd332, 0d0000000000000000;
	mov.f64 	%fd307, %fd332;
	mov.f64 	%fd308, %fd332;
	mov.f64 	%fd309, %fd332;
	mov.f64 	%fd310, %fd332;
	mov.f64 	%fd311, %fd332;
	mov.f64 	%fd312, %fd332;
	mov.f64 	%fd313, %fd332;
	mov.f64 	%fd314, %fd332;
	mov.f64 	%fd315, %fd332;
	mov.f64 	%fd316, %fd332;
	mov.f64 	%fd317, %fd332;
	mov.f64 	%fd318, %fd332;
	@%p10 bra 	$L__BB11_17;

	mul.lo.s64 	%rd88, %rd45, %rd27;
	add.s64 	%rd89, %rd18, %rd88;
	mul.lo.s64 	%rd90, %rd45, %rd28;
	add.s64 	%rd91, %rd17, %rd90;
	ld.global.f64 	%fd107, [%rd91];
	ld.global.f64 	%fd108, [%rd89];
	sub.f64 	%fd307, %fd108, %fd107;
	ld.global.f64 	%fd109, [%rd91+8];
	ld.global.f64 	%fd110, [%rd89+8];
	sub.f64 	%fd308, %fd110, %fd109;
	ld.global.f64 	%fd111, [%rd91+16];
	ld.global.f64 	%fd112, [%rd89+16];
	sub.f64 	%fd309, %fd112, %fd111;
	ld.global.f64 	%fd113, [%rd91+24];
	ld.global.f64 	%fd114, [%rd89+24];
	sub.f64 	%fd310, %fd114, %fd113;
	ld.global.f64 	%fd115, [%rd91+32];
	ld.global.f64 	%fd116, [%rd89+32];
	sub.f64 	%fd311, %fd116, %fd115;
	ld.global.f64 	%fd117, [%rd91+40];
	ld.global.f64 	%fd118, [%rd89+40];
	sub.f64 	%fd312, %fd118, %fd117;
	ld.global.f64 	%fd119, [%rd91+48];
	ld.global.f64 	%fd120, [%rd89+48];
	sub.f64 	%fd313, %fd120, %fd119;
	ld.global.f64 	%fd121, [%rd91+56];
	ld.global.f64 	%fd122, [%rd89+56];
	sub.f64 	%fd314, %fd122, %fd121;
	ld.global.f64 	%fd123, [%rd91+64];
	ld.global.f64 	%fd124, [%rd89+64];
	sub.f64 	%fd315, %fd124, %fd123;
	ld.global.f64 	%fd125, [%rd91+72];
	ld.global.f64 	%fd126, [%rd89+72];
	sub.f64 	%fd316, %fd126, %fd125;
	ld.global.f64 	%fd127, [%rd91+80];
	ld.global.f64 	%fd128, [%rd89+80];
	sub.f64 	%fd317, %fd128, %fd127;
	ld.global.f64 	%fd129, [%rd91+88];
	ld.global.f64 	%fd130, [%rd89+88];
	sub.f64 	%fd318, %fd130, %fd129;
	add.s64 	%rd92, %rd15, %rd46;
	ld.global.f64 	%fd131, [%rd92];
	mul.f64 	%fd333, %fd131, %fd93;

$L__BB11_17:
	@%p10 bra 	$L__BB11_31;

	ld.param.u64 	%rd153, [compute_affine_kinematic_grad_cuda_kernel_backward_param_13];
	setp.eq.s64 	%p12, %rd153, 0;
	@%p12 bra 	$L__BB11_20;

	mul.lo.s64 	%rd93, %rd45, %rd30;
	add.s64 	%rd94, %rd12, %rd93;
	ld.global.f64 	%fd133, [%rd94];
	add.f64 	%fd331, %fd133, 0d0000000000000000;
	ld.global.f64 	%fd134, [%rd94+8];
	add.f64 	%fd330, %fd134, 0d0000000000000000;
	ld.global.f64 	%fd135, [%rd94+16];
	add.f64 	%fd329, %fd135, 0d0000000000000000;
	ld.global.f64 	%fd136, [%rd94+24];
	add.f64 	%fd328, %fd136, 0d0000000000000000;
	ld.global.f64 	%fd137, [%rd94+32];
	add.f64 	%fd327, %fd137, 0d0000000000000000;
	ld.global.f64 	%fd138, [%rd94+40];
	add.f64 	%fd326, %fd138, 0d0000000000000000;
	ld.global.f64 	%fd139, [%rd94+48];
	add.f64 	%fd325, %fd139, 0d0000000000000000;
	ld.global.f64 	%fd140, [%rd94+56];
	add.f64 	%fd324, %fd140, 0d0000000000000000;
	ld.global.f64 	%fd141, [%rd94+64];
	add.f64 	%fd323, %fd141, 0d0000000000000000;
	ld.global.f64 	%fd142, [%rd94+72];
	add.f64 	%fd322, %fd142, 0d0000000000000000;
	ld.global.f64 	%fd143, [%rd94+80];
	add.f64 	%fd321, %fd143, 0d0000000000000000;
	ld.global.f64 	%fd144, [%rd94+88];
	add.f64 	%fd320, %fd144, 0d0000000000000000;
	bra.uni 	$L__BB11_22;

$L__BB11_20:
	ld.param.u64 	%rd154, [compute_affine_kinematic_grad_cuda_kernel_backward_param_5+8];
	setp.eq.s64 	%p13, %rd154, 0;
	mov.f64 	%fd320, 0d0000000000000000;
	mov.f64 	%fd321, %fd320;
	mov.f64 	%fd322, %fd320;
	mov.f64 	%fd323, %fd320;
	mov.f64 	%fd324, %fd320;
	mov.f64 	%fd325, %fd320;
	mov.f64 	%fd326, %fd320;
	mov.f64 	%fd327, %fd320;
	mov.f64 	%fd328, %fd320;
	mov.f64 	%fd329, %fd320;
	mov.f64 	%fd330, %fd320;
	mov.f64 	%fd331, %fd320;
	@%p13 bra 	$L__BB11_22;

	mul.lo.s64 	%rd95, %rd45, %rd31;
	add.s64 	%rd96, %rd16, %rd95;
	ld.global.f64 	%fd157, [%rd96];
	add.f64 	%fd331, %fd157, 0d0000000000000000;
	ld.global.f64 	%fd158, [%rd96+8];
	add.f64 	%fd330, %fd158, 0d0000000000000000;
	ld.global.f64 	%fd159, [%rd96+16];
	add.f64 	%fd329, %fd159, 0d0000000000000000;
	ld.global.f64 	%fd160, [%rd96+24];
	add.f64 	%fd328, %fd160, 0d0000000000000000;
	ld.global.f64 	%fd161, [%rd96+32];
	add.f64 	%fd327, %fd161, 0d0000000000000000;
	ld.global.f64 	%fd162, [%rd96+40];
	add.f64 	%fd326, %fd162, 0d0000000000000000;
	ld.global.f64 	%fd163, [%rd96+48];
	add.f64 	%fd325, %fd163, 0d0000000000000000;
	ld.global.f64 	%fd164, [%rd96+56];
	add.f64 	%fd324, %fd164, 0d0000000000000000;
	ld.global.f64 	%fd165, [%rd96+64];
	add.f64 	%fd323, %fd165, 0d0000000000000000;
	ld.global.f64 	%fd166, [%rd96+72];
	add.f64 	%fd322, %fd166, 0d0000000000000000;
	ld.global.f64 	%fd167, [%rd96+80];
	add.f64 	%fd321, %fd167, 0d0000000000000000;
	ld.global.f64 	%fd168, [%rd96+88];
	add.f64 	%fd320, %fd168, 0d0000000000000000;

$L__BB11_22:
	fma.rn.f64 	%fd64, %fd333, %fd331, 0d0000000000000000;
	mov.f64 	%fd169, 0d0000000000000000;
	fma.rn.f64 	%fd65, %fd333, %fd330, 0d0000000000000000;
	fma.rn.f64 	%fd66, %fd333, %fd329, 0d0000000000000000;
	fma.rn.f64 	%fd67, %fd333, %fd328, 0d0000000000000000;
	fma.rn.f64 	%fd68, %fd333, %fd327, 0d0000000000000000;
	fma.rn.f64 	%fd69, %fd333, %fd326, 0d0000000000000000;
	fma.rn.f64 	%fd70, %fd333, %fd325, 0d0000000000000000;
	fma.rn.f64 	%fd71, %fd333, %fd324, 0d0000000000000000;
	fma.rn.f64 	%fd72, %fd333, %fd323, 0d0000000000000000;
	fma.rn.f64 	%fd73, %fd333, %fd322, 0d0000000000000000;
	fma.rn.f64 	%fd74, %fd333, %fd321, 0d0000000000000000;
	fma.rn.f64 	%fd75, %fd333, %fd320, 0d0000000000000000;
	fma.rn.f64 	%fd170, %fd307, %fd331, 0d0000000000000000;
	fma.rn.f64 	%fd171, %fd308, %fd330, %fd170;
	fma.rn.f64 	%fd172, %fd309, %fd329, %fd171;
	fma.rn.f64 	%fd173, %fd310, %fd328, %fd172;
	fma.rn.f64 	%fd174, %fd311, %fd327, %fd173;
	fma.rn.f64 	%fd175, %fd312, %fd326, %fd174;
	fma.rn.f64 	%fd176, %fd313, %fd325, %fd175;
	fma.rn.f64 	%fd177, %fd314, %fd324, %fd176;
	fma.rn.f64 	%fd178, %fd315, %fd323, %fd177;
	fma.rn.f64 	%fd179, %fd316, %fd322, %fd178;
	fma.rn.f64 	%fd180, %fd317, %fd321, %fd179;
	fma.rn.f64 	%fd76, %fd318, %fd320, %fd180;
	sub.f64 	%fd77, %fd169, %fd64;
	sub.f64 	%fd78, %fd169, %fd65;
	sub.f64 	%fd79, %fd169, %fd66;
	sub.f64 	%fd80, %fd169, %fd67;
	sub.f64 	%fd81, %fd169, %fd68;
	sub.f64 	%fd82, %fd169, %fd69;
	sub.f64 	%fd83, %fd169, %fd70;
	sub.f64 	%fd84, %fd169, %fd71;
	sub.f64 	%fd85, %fd169, %fd72;
	sub.f64 	%fd86, %fd169, %fd73;
	sub.f64 	%fd87, %fd169, %fd74;
	sub.f64 	%fd88, %fd169, %fd75;
	setp.eq.s64 	%p14, %rd65, 0;
	@%p14 bra 	$L__BB11_24;

	mul.lo.s64 	%rd109, %rd45, %rd32;
	add.s64 	%rd97, %rd65, %rd109;
	// begin inline asm
	{ atom.add.f64 %fd181,[%rd97],%fd77; }

	// end inline asm
	add.s64 	%rd98, %rd97, 8;
	// begin inline asm
	{ atom.add.f64 %fd183,[%rd98],%fd78; }

	// end inline asm
	add.s64 	%rd99, %rd97, 16;
	// begin inline asm
	{ atom.add.f64 %fd185,[%rd99],%fd79; }

	// end inline asm
	add.s64 	%rd100, %rd97, 24;
	// begin inline asm
	{ atom.add.f64 %fd187,[%rd100],%fd80; }

	// end inline asm
	add.s64 	%rd101, %rd97, 32;
	// begin inline asm
	{ atom.add.f64 %fd189,[%rd101],%fd81; }

	// end inline asm
	add.s64 	%rd102, %rd97, 40;
	// begin inline asm
	{ atom.add.f64 %fd191,[%rd102],%fd82; }

	// end inline asm
	add.s64 	%rd103, %rd97, 48;
	// begin inline asm
	{ atom.add.f64 %fd193,[%rd103],%fd83; }

	// end inline asm
	add.s64 	%rd104, %rd97, 56;
	// begin inline asm
	{ atom.add.f64 %fd195,[%rd104],%fd84; }

	// end inline asm
	add.s64 	%rd105, %rd97, 64;
	// begin inline asm
	{ atom.add.f64 %fd197,[%rd105],%fd85; }

	// end inline asm
	add.s64 	%rd106, %rd97, 72;
	// begin inline asm
	{ atom.add.f64 %fd199,[%rd106],%fd86; }

	// end inline asm
	add.s64 	%rd107, %rd97, 80;
	// begin inline asm
	{ atom.add.f64 %fd201,[%rd107],%fd87; }

	// end inline asm
	add.s64 	%rd108, %rd97, 88;
	// begin inline asm
	{ atom.add.f64 %fd203,[%rd108],%fd88; }

	// end inline asm
	bra.uni 	$L__BB11_26;

$L__BB11_24:
	setp.eq.s64 	%p15, %rd54, 0;
	@%p15 bra 	$L__BB11_26;

	mul.lo.s64 	%rd122, %rd45, %rd28;
	add.s64 	%rd110, %rd54, %rd122;
	// begin inline asm
	{ atom.add.f64 %fd205,[%rd110],%fd77; }

	// end inline asm
	add.s64 	%rd111, %rd110, 8;
	// begin inline asm
	{ atom.add.f64 %fd207,[%rd111],%fd78; }

	// end inline asm
	add.s64 	%rd112, %rd110, 16;
	// begin inline asm
	{ atom.add.f64 %fd209,[%rd112],%fd79; }

	// end inline asm
	add.s64 	%rd113, %rd110, 24;
	// begin inline asm
	{ atom.add.f64 %fd211,[%rd113],%fd80; }

	// end inline asm
	add.s64 	%rd114, %rd110, 32;
	// begin inline asm
	{ atom.add.f64 %fd213,[%rd114],%fd81; }

	// end inline asm
	add.s64 	%rd115, %rd110, 40;
	// begin inline asm
	{ atom.add.f64 %fd215,[%rd115],%fd82; }

	// end inline asm
	add.s64 	%rd116, %rd110, 48;
	// begin inline asm
	{ atom.add.f64 %fd217,[%rd116],%fd83; }

	// end inline asm
	add.s64 	%rd117, %rd110, 56;
	// begin inline asm
	{ atom.add.f64 %fd219,[%rd117],%fd84; }

	// end inline asm
	add.s64 	%rd118, %rd110, 64;
	// begin inline asm
	{ atom.add.f64 %fd221,[%rd118],%fd85; }

	// end inline asm
	add.s64 	%rd119, %rd110, 72;
	// begin inline asm
	{ atom.add.f64 %fd223,[%rd119],%fd86; }

	// end inline asm
	add.s64 	%rd120, %rd110, 80;
	// begin inline asm
	{ atom.add.f64 %fd225,[%rd120],%fd87; }

	// end inline asm
	add.s64 	%rd121, %rd110, 88;
	// begin inline asm
	{ atom.add.f64 %fd227,[%rd121],%fd88; }

	// end inline asm

$L__BB11_26:
	setp.eq.s64 	%p16, %rd63, 0;
	@%p16 bra 	$L__BB11_28;

	fma.rn.f64 	%fd293, %fd333, %fd320, 0d0000000000000000;
	fma.rn.f64 	%fd292, %fd333, %fd321, 0d0000000000000000;
	fma.rn.f64 	%fd291, %fd333, %fd322, 0d0000000000000000;
	fma.rn.f64 	%fd290, %fd333, %fd323, 0d0000000000000000;
	fma.rn.f64 	%fd289, %fd333, %fd324, 0d0000000000000000;
	fma.rn.f64 	%fd288, %fd333, %fd325, 0d0000000000000000;
	fma.rn.f64 	%fd287, %fd333, %fd326, 0d0000000000000000;
	fma.rn.f64 	%fd286, %fd333, %fd327, 0d0000000000000000;
	fma.rn.f64 	%fd285, %fd333, %fd328, 0d0000000000000000;
	fma.rn.f64 	%fd284, %fd333, %fd329, 0d0000000000000000;
	fma.rn.f64 	%fd283, %fd333, %fd330, 0d0000000000000000;
	fma.rn.f64 	%fd282, %fd333, %fd331, 0d0000000000000000;
	mul.lo.s64 	%rd135, %rd45, %rd33;
	add.s64 	%rd123, %rd63, %rd135;
	// begin inline asm
	{ atom.add.f64 %fd229,[%rd123],%fd282; }

	// end inline asm
	add.s64 	%rd124, %rd123, 8;
	// begin inline asm
	{ atom.add.f64 %fd231,[%rd124],%fd283; }

	// end inline asm
	add.s64 	%rd125, %rd123, 16;
	// begin inline asm
	{ atom.add.f64 %fd233,[%rd125],%fd284; }

	// end inline asm
	add.s64 	%rd126, %rd123, 24;
	// begin inline asm
	{ atom.add.f64 %fd235,[%rd126],%fd285; }

	// end inline asm
	add.s64 	%rd127, %rd123, 32;
	// begin inline asm
	{ atom.add.f64 %fd237,[%rd127],%fd286; }

	// end inline asm
	add.s64 	%rd128, %rd123, 40;
	// begin inline asm
	{ atom.add.f64 %fd239,[%rd128],%fd287; }

	// end inline asm
	add.s64 	%rd129, %rd123, 48;
	// begin inline asm
	{ atom.add.f64 %fd241,[%rd129],%fd288; }

	// end inline asm
	add.s64 	%rd130, %rd123, 56;
	// begin inline asm
	{ atom.add.f64 %fd243,[%rd130],%fd289; }

	// end inline asm
	add.s64 	%rd131, %rd123, 64;
	// begin inline asm
	{ atom.add.f64 %fd245,[%rd131],%fd290; }

	// end inline asm
	add.s64 	%rd132, %rd123, 72;
	// begin inline asm
	{ atom.add.f64 %fd247,[%rd132],%fd291; }

	// end inline asm
	add.s64 	%rd133, %rd123, 80;
	// begin inline asm
	{ atom.add.f64 %fd249,[%rd133],%fd292; }

	// end inline asm
	add.s64 	%rd134, %rd123, 88;
	// begin inline asm
	{ atom.add.f64 %fd251,[%rd134],%fd293; }

	// end inline asm
	bra.uni 	$L__BB11_30;

$L__BB11_28:
	setp.eq.s64 	%p17, %rd50, 0;
	@%p17 bra 	$L__BB11_30;

	fma.rn.f64 	%fd305, %fd333, %fd320, 0d0000000000000000;
	fma.rn.f64 	%fd304, %fd333, %fd321, 0d0000000000000000;
	fma.rn.f64 	%fd303, %fd333, %fd322, 0d0000000000000000;
	fma.rn.f64 	%fd302, %fd333, %fd323, 0d0000000000000000;
	fma.rn.f64 	%fd301, %fd333, %fd324, 0d0000000000000000;
	fma.rn.f64 	%fd300, %fd333, %fd325, 0d0000000000000000;
	fma.rn.f64 	%fd299, %fd333, %fd326, 0d0000000000000000;
	fma.rn.f64 	%fd298, %fd333, %fd327, 0d0000000000000000;
	fma.rn.f64 	%fd297, %fd333, %fd328, 0d0000000000000000;
	fma.rn.f64 	%fd296, %fd333, %fd329, 0d0000000000000000;
	fma.rn.f64 	%fd295, %fd333, %fd330, 0d0000000000000000;
	fma.rn.f64 	%fd294, %fd333, %fd331, 0d0000000000000000;
	mul.lo.s64 	%rd148, %rd45, %rd27;
	add.s64 	%rd136, %rd50, %rd148;
	// begin inline asm
	{ atom.add.f64 %fd253,[%rd136],%fd294; }

	// end inline asm
	add.s64 	%rd137, %rd136, 8;
	// begin inline asm
	{ atom.add.f64 %fd255,[%rd137],%fd295; }

	// end inline asm
	add.s64 	%rd138, %rd136, 16;
	// begin inline asm
	{ atom.add.f64 %fd257,[%rd138],%fd296; }

	// end inline asm
	add.s64 	%rd139, %rd136, 24;
	// begin inline asm
	{ atom.add.f64 %fd259,[%rd139],%fd297; }

	// end inline asm
	add.s64 	%rd140, %rd136, 32;
	// begin inline asm
	{ atom.add.f64 %fd261,[%rd140],%fd298; }

	// end inline asm
	add.s64 	%rd141, %rd136, 40;
	// begin inline asm
	{ atom.add.f64 %fd263,[%rd141],%fd299; }

	// end inline asm
	add.s64 	%rd142, %rd136, 48;
	// begin inline asm
	{ atom.add.f64 %fd265,[%rd142],%fd300; }

	// end inline asm
	add.s64 	%rd143, %rd136, 56;
	// begin inline asm
	{ atom.add.f64 %fd267,[%rd143],%fd301; }

	// end inline asm
	add.s64 	%rd144, %rd136, 64;
	// begin inline asm
	{ atom.add.f64 %fd269,[%rd144],%fd302; }

	// end inline asm
	add.s64 	%rd145, %rd136, 72;
	// begin inline asm
	{ atom.add.f64 %fd271,[%rd145],%fd303; }

	// end inline asm
	add.s64 	%rd146, %rd136, 80;
	// begin inline asm
	{ atom.add.f64 %fd273,[%rd146],%fd304; }

	// end inline asm
	add.s64 	%rd147, %rd136, 88;
	// begin inline asm
	{ atom.add.f64 %fd275,[%rd147],%fd305; }

	// end inline asm

$L__BB11_30:
	add.f64 	%fd277, %fd76, 0d0000000000000000;
	fma.rn.f64 	%fd332, %fd277, %fd93, 0d0000000000000000;

$L__BB11_31:
	add.f64 	%fd91, %fd332, 0d0000000000000000;
	setp.eq.s64 	%p18, %rd69, 0;
	@%p18 bra 	$L__BB11_33;

	mul.lo.s64 	%rd150, %rd45, %rd29;
	add.s64 	%rd149, %rd69, %rd150;
	// begin inline asm
	{ atom.add.f64 %fd278,[%rd149],%fd91; }

	// end inline asm
	bra.uni 	$L__BB11_35;

$L__BB11_33:
	setp.eq.s64 	%p19, %rd58, 0;
	@%p19 bra 	$L__BB11_35;

	add.s64 	%rd151, %rd58, %rd46;
	// begin inline asm
	{ atom.add.f64 %fd280,[%rd151],%fd91; }

	// end inline asm

$L__BB11_35:
	ld.param.u64 	%rd152, [compute_affine_kinematic_grad_cuda_kernel_backward_param_0+24];
	add.s64 	%rd155, %rd155, %rd24;
	setp.lt.u64 	%p20, %rd155, %rd152;
	@%p20 bra 	$L__BB11_2;

$L__BB11_36:
	ret;

}
	// .globl	compute_affine_kinematic_energy_cuda_kernel_forward
.visible .entry compute_affine_kinematic_energy_cuda_kernel_forward(
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_forward_param_3[56],
	.param .f64 compute_affine_kinematic_energy_cuda_kernel_forward_param_4,
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_forward_param_6[56]
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<42>;
	.reg .b32 	%r<109>;
	.reg .f64 	%fd<77>;
	.reg .b64 	%rd<76>;


	ld.param.v2.u32 	{%r52, %r53}, [compute_affine_kinematic_energy_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r54, %r55}, [compute_affine_kinematic_energy_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r60, %r61}, [compute_affine_kinematic_energy_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r68, %r69}, [compute_affine_kinematic_energy_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r76, %r77}, [compute_affine_kinematic_energy_cuda_kernel_forward_param_3+32];
	ld.param.f64 	%fd2, [compute_affine_kinematic_energy_cuda_kernel_forward_param_4];
	ld.param.v2.u32 	{%r84, %r85}, [compute_affine_kinematic_energy_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r92, %r93}, [compute_affine_kinematic_energy_cuda_kernel_forward_param_6+32];
	ld.param.u64 	%rd39, [compute_affine_kinematic_energy_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd37, [compute_affine_kinematic_energy_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd35, [compute_affine_kinematic_energy_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd33, [compute_affine_kinematic_energy_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd31, [compute_affine_kinematic_energy_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd30, [compute_affine_kinematic_energy_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [compute_affine_kinematic_energy_cuda_kernel_forward_param_0+16];
	mov.u32 	%r96, %ntid.x;
	cvt.u64.u32 	%rd1, %r96;
	mov.u32 	%r97, %ctaid.x;
	mul.wide.u32 	%rd41, %r96, %r97;
	mov.u32 	%r98, %tid.x;
	cvt.u64.u32 	%rd42, %r98;
	add.s64 	%rd72, %rd41, %rd42;
	setp.ge.u64 	%p1, %rd72, %rd30;
	@%p1 bra 	$L__BB12_17;

	cvta.to.global.u64 	%rd4, %rd39;
	cvta.to.global.u64 	%rd6, %rd35;
	cvta.to.global.u64 	%rd7, %rd31;
	cvt.s64.s32 	%rd8, %r55;
	cvt.s64.s32 	%rd9, %r54;
	cvt.s64.s32 	%rd10, %r53;
	cvt.s64.s32 	%rd11, %r92;
	cvt.s64.s32 	%rd12, %r68;
	mov.u32 	%r99, %nctaid.x;
	cvt.u64.u32 	%rd43, %r99;
	mul.lo.s64 	%rd13, %rd1, %rd43;
	mul.f64 	%fd1, %fd2, 0d3FE0000000000000;
	cvt.s64.s32 	%rd14, %r60;
	cvt.s64.s32 	%rd15, %r76;
	cvt.s64.s32 	%rd16, %r84;
	cvta.to.global.u64 	%rd17, %rd33;

$L__BB12_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd73, %rd72;
	@%p2 bra 	$L__BB12_6;

	or.b64  	%rd44, %rd72, %rd8;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p3, %rd45, 0;
	@%p3 bra 	$L__BB12_5;

	div.u64 	%rd73, %rd72, %rd8;
	bra.uni 	$L__BB12_6;

$L__BB12_5:
	cvt.u32.u64 	%r100, %rd8;
	cvt.u32.u64 	%r101, %rd72;
	div.u32 	%r102, %r101, %r100;
	cvt.u64.u32 	%rd73, %r102;

$L__BB12_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB12_10;

	or.b64  	%rd46, %rd73, %rd9;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p5, %rd47, 0;
	@%p5 bra 	$L__BB12_9;

	div.u64 	%rd73, %rd73, %rd9;
	bra.uni 	$L__BB12_10;

$L__BB12_9:
	cvt.u32.u64 	%r103, %rd9;
	cvt.u32.u64 	%r104, %rd73;
	div.u32 	%r105, %r104, %r103;
	cvt.u64.u32 	%rd73, %r105;

$L__BB12_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB12_14;

	or.b64  	%rd48, %rd73, %rd10;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p7, %rd49, 0;
	@%p7 bra 	$L__BB12_13;

	div.u64 	%rd73, %rd73, %rd10;
	bra.uni 	$L__BB12_14;

$L__BB12_13:
	cvt.u32.u64 	%r106, %rd10;
	cvt.u32.u64 	%r107, %rd73;
	div.u32 	%r108, %r107, %r106;
	cvt.u64.u32 	%rd73, %r108;

$L__BB12_14:
	cvt.s64.s32 	%rd50, %rd73;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd28, %rd50, 0, %p8;
	mul.lo.s64 	%rd51, %rd28, %rd12;
	add.s64 	%rd52, %rd17, %rd51;
	ld.global.u8 	%rs41, [%rd52];
	setp.eq.s16 	%p9, %rs41, 0;
	@%p9 bra 	$L__BB12_16;

	mul.lo.s64 	%rd65, %rd28, %rd11;
	add.s64 	%rd66, %rd4, %rd65;
	ld.global.f64 	%fd27, [%rd66];
	mul.f64 	%fd28, %fd1, %fd27;
	mul.lo.s64 	%rd67, %rd28, %rd14;
	add.s64 	%rd68, %rd7, %rd67;
	mul.lo.s64 	%rd69, %rd28, %rd15;
	add.s64 	%rd70, %rd6, %rd69;
	ld.global.f64 	%fd29, [%rd70];
	ld.global.f64 	%fd30, [%rd68];
	sub.f64 	%fd31, %fd30, %fd29;
	mul.f64 	%fd32, %fd31, %fd31;
	mul.f64 	%fd4, %fd28, %fd32;
	mul.lo.s64 	%rd71, %rd28, %rd16;
	add.s64 	%rd64, %rd37, %rd71;
	// begin inline asm
	{ atom.add.f64 %fd3,[%rd64],%fd4; }

	// end inline asm
	ld.global.f64 	%fd33, [%rd70+8];
	ld.global.f64 	%fd34, [%rd68+8];
	sub.f64 	%fd35, %fd34, %fd33;
	mul.f64 	%fd36, %fd35, %fd35;
	mul.f64 	%fd6, %fd28, %fd36;
	// begin inline asm
	{ atom.add.f64 %fd5,[%rd64],%fd6; }

	// end inline asm
	ld.global.f64 	%fd37, [%rd70+16];
	ld.global.f64 	%fd38, [%rd68+16];
	sub.f64 	%fd39, %fd38, %fd37;
	mul.f64 	%fd40, %fd39, %fd39;
	mul.f64 	%fd8, %fd28, %fd40;
	// begin inline asm
	{ atom.add.f64 %fd7,[%rd64],%fd8; }

	// end inline asm
	ld.global.f64 	%fd41, [%rd70+24];
	ld.global.f64 	%fd42, [%rd68+24];
	sub.f64 	%fd43, %fd42, %fd41;
	mul.f64 	%fd44, %fd43, %fd43;
	mul.f64 	%fd10, %fd28, %fd44;
	// begin inline asm
	{ atom.add.f64 %fd9,[%rd64],%fd10; }

	// end inline asm
	ld.global.f64 	%fd45, [%rd70+32];
	ld.global.f64 	%fd46, [%rd68+32];
	sub.f64 	%fd47, %fd46, %fd45;
	mul.f64 	%fd48, %fd47, %fd47;
	mul.f64 	%fd12, %fd28, %fd48;
	// begin inline asm
	{ atom.add.f64 %fd11,[%rd64],%fd12; }

	// end inline asm
	ld.global.f64 	%fd49, [%rd70+40];
	ld.global.f64 	%fd50, [%rd68+40];
	sub.f64 	%fd51, %fd50, %fd49;
	mul.f64 	%fd52, %fd51, %fd51;
	mul.f64 	%fd14, %fd28, %fd52;
	// begin inline asm
	{ atom.add.f64 %fd13,[%rd64],%fd14; }

	// end inline asm
	ld.global.f64 	%fd53, [%rd70+48];
	ld.global.f64 	%fd54, [%rd68+48];
	sub.f64 	%fd55, %fd54, %fd53;
	mul.f64 	%fd56, %fd55, %fd55;
	mul.f64 	%fd16, %fd28, %fd56;
	// begin inline asm
	{ atom.add.f64 %fd15,[%rd64],%fd16; }

	// end inline asm
	ld.global.f64 	%fd57, [%rd70+56];
	ld.global.f64 	%fd58, [%rd68+56];
	sub.f64 	%fd59, %fd58, %fd57;
	mul.f64 	%fd60, %fd59, %fd59;
	mul.f64 	%fd18, %fd28, %fd60;
	// begin inline asm
	{ atom.add.f64 %fd17,[%rd64],%fd18; }

	// end inline asm
	ld.global.f64 	%fd61, [%rd70+64];
	ld.global.f64 	%fd62, [%rd68+64];
	sub.f64 	%fd63, %fd62, %fd61;
	mul.f64 	%fd64, %fd63, %fd63;
	mul.f64 	%fd20, %fd28, %fd64;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd64],%fd20; }

	// end inline asm
	ld.global.f64 	%fd65, [%rd70+72];
	ld.global.f64 	%fd66, [%rd68+72];
	sub.f64 	%fd67, %fd66, %fd65;
	mul.f64 	%fd68, %fd67, %fd67;
	mul.f64 	%fd22, %fd28, %fd68;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd64],%fd22; }

	// end inline asm
	ld.global.f64 	%fd69, [%rd70+80];
	ld.global.f64 	%fd70, [%rd68+80];
	sub.f64 	%fd71, %fd70, %fd69;
	mul.f64 	%fd72, %fd71, %fd71;
	mul.f64 	%fd24, %fd28, %fd72;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd64],%fd24; }

	// end inline asm
	ld.global.f64 	%fd73, [%rd70+88];
	ld.global.f64 	%fd74, [%rd68+88];
	sub.f64 	%fd75, %fd74, %fd73;
	mul.f64 	%fd76, %fd75, %fd75;
	mul.f64 	%fd26, %fd28, %fd76;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd64],%fd26; }

	// end inline asm

$L__BB12_16:
	add.s64 	%rd72, %rd72, %rd13;
	setp.lt.u64 	%p10, %rd72, %rd30;
	@%p10 bra 	$L__BB12_2;

$L__BB12_17:
	ret;

}
	// .globl	compute_affine_kinematic_energy_cuda_kernel_backward
.visible .entry compute_affine_kinematic_energy_cuda_kernel_backward(
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_3[56],
	.param .f64 compute_affine_kinematic_energy_cuda_kernel_backward_param_4,
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_9[56],
	.param .f64 compute_affine_kinematic_energy_cuda_kernel_backward_param_10,
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_12[56]
)
{
	.reg .pred 	%p<86>;
	.reg .b16 	%rs<74>;
	.reg .b32 	%r<177>;
	.reg .f64 	%fd<1610>;
	.reg .b64 	%rd<718>;


	ld.param.v2.u32 	{%r88, %r89}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r90, %r91}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r96, %r97}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r104, %r105}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r112, %r113}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_3+32];
	ld.param.f64 	%fd209, [compute_affine_kinematic_energy_cuda_kernel_backward_param_4];
	ld.param.v2.u32 	{%r120, %r121}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r128, %r129}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r136, %r137}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r144, %r145}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r152, %r153}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r160, %r161}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_12+32];
	ld.param.u64 	%rd63, [compute_affine_kinematic_energy_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd61, [compute_affine_kinematic_energy_cuda_kernel_backward_param_11];
	ld.param.u64 	%rd59, [compute_affine_kinematic_energy_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd57, [compute_affine_kinematic_energy_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd56, [compute_affine_kinematic_energy_cuda_kernel_backward_param_6+8];
	ld.param.u64 	%rd55, [compute_affine_kinematic_energy_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd54, [compute_affine_kinematic_energy_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd52, [compute_affine_kinematic_energy_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd51, [compute_affine_kinematic_energy_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd49, [compute_affine_kinematic_energy_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd48, [compute_affine_kinematic_energy_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd47, [compute_affine_kinematic_energy_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd46, [compute_affine_kinematic_energy_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [compute_affine_kinematic_energy_cuda_kernel_backward_param_0+16];
	mov.u32 	%r164, %ntid.x;
	cvt.u64.u32 	%rd1, %r164;
	mov.u32 	%r165, %ctaid.x;
	mul.wide.u32 	%rd65, %r164, %r165;
	mov.u32 	%r166, %tid.x;
	cvt.u64.u32 	%rd66, %r166;
	add.s64 	%rd714, %rd65, %rd66;
	setp.ge.u64 	%p1, %rd714, %rd46;
	@%p1 bra 	$L__BB13_167;

	cvta.to.global.u64 	%rd12, %rd61;
	cvta.to.global.u64 	%rd14, %rd55;
	cvta.to.global.u64 	%rd15, %rd51;
	cvta.to.global.u64 	%rd16, %rd47;
	cvt.s64.s32 	%rd17, %r91;
	cvt.s64.s32 	%rd18, %r90;
	cvt.s64.s32 	%rd19, %r89;
	cvt.s64.s32 	%rd20, %r128;
	cvt.s64.s32 	%rd21, %r104;
	mul.f64 	%fd1593, %fd209, 0d3FE0000000000000;
	cvt.s64.s32 	%rd22, %r96;
	cvt.s64.s32 	%rd23, %r112;
	mov.u32 	%r167, %nctaid.x;
	cvt.u64.u32 	%rd67, %r167;
	mul.lo.s64 	%rd24, %rd1, %rd67;
	cvt.s64.s32 	%rd25, %r160;
	cvt.s64.s32 	%rd26, %r152;
	cvt.s64.s32 	%rd27, %r120;
	cvt.s64.s32 	%rd28, %r144;
	cvt.s64.s32 	%rd29, %r136;
	cvta.to.global.u64 	%rd30, %rd49;

$L__BB13_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd715, %rd714;
	@%p2 bra 	$L__BB13_6;

	or.b64  	%rd68, %rd714, %rd17;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.eq.s64 	%p3, %rd69, 0;
	@%p3 bra 	$L__BB13_5;

	div.u64 	%rd715, %rd714, %rd17;
	bra.uni 	$L__BB13_6;

$L__BB13_5:
	cvt.u32.u64 	%r168, %rd17;
	cvt.u32.u64 	%r169, %rd714;
	div.u32 	%r170, %r169, %r168;
	cvt.u64.u32 	%rd715, %r170;

$L__BB13_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB13_10;

	or.b64  	%rd70, %rd715, %rd18;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.eq.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB13_9;

	div.u64 	%rd715, %rd715, %rd18;
	bra.uni 	$L__BB13_10;

$L__BB13_9:
	cvt.u32.u64 	%r171, %rd18;
	cvt.u32.u64 	%r172, %rd715;
	div.u32 	%r173, %r172, %r171;
	cvt.u64.u32 	%rd715, %r173;

$L__BB13_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB13_14;

	or.b64  	%rd72, %rd715, %rd19;
	and.b64  	%rd73, %rd72, -4294967296;
	setp.eq.s64 	%p7, %rd73, 0;
	@%p7 bra 	$L__BB13_13;

	div.u64 	%rd715, %rd715, %rd19;
	bra.uni 	$L__BB13_14;

$L__BB13_13:
	cvt.u32.u64 	%r174, %rd19;
	cvt.u32.u64 	%r175, %rd715;
	div.u32 	%r176, %r175, %r174;
	cvt.u64.u32 	%rd715, %r176;

$L__BB13_14:
	cvt.s64.s32 	%rd74, %rd715;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd41, %rd74, 0, %p8;
	mul.lo.s64 	%rd42, %rd41, %rd20;
	mul.lo.s64 	%rd75, %rd41, %rd21;
	add.s64 	%rd76, %rd30, %rd75;
	ld.global.u8 	%rs1, [%rd76];
	setp.eq.s16 	%p9, %rs1, 0;
	@%p9 bra 	$L__BB13_16;

	add.s64 	%rd77, %rd14, %rd42;
	ld.global.f64 	%fd211, [%rd77];
	mul.f64 	%fd1594, %fd1593, %fd211;
	mul.lo.s64 	%rd78, %rd41, %rd22;
	add.s64 	%rd79, %rd16, %rd78;
	mul.lo.s64 	%rd80, %rd41, %rd23;
	add.s64 	%rd81, %rd15, %rd80;
	ld.global.f64 	%fd212, [%rd81];
	ld.global.f64 	%fd213, [%rd79];
	sub.f64 	%fd1551, %fd213, %fd212;
	mul.f64 	%fd1552, %fd1551, %fd1551;
	ld.global.f64 	%fd214, [%rd81+8];
	ld.global.f64 	%fd215, [%rd79+8];
	sub.f64 	%fd1555, %fd215, %fd214;
	mul.f64 	%fd1556, %fd1555, %fd1555;
	ld.global.f64 	%fd216, [%rd81+16];
	ld.global.f64 	%fd217, [%rd79+16];
	sub.f64 	%fd1559, %fd217, %fd216;
	mul.f64 	%fd1560, %fd1559, %fd1559;
	ld.global.f64 	%fd218, [%rd81+24];
	ld.global.f64 	%fd219, [%rd79+24];
	sub.f64 	%fd1563, %fd219, %fd218;
	mul.f64 	%fd1564, %fd1563, %fd1563;
	ld.global.f64 	%fd220, [%rd81+32];
	ld.global.f64 	%fd221, [%rd79+32];
	sub.f64 	%fd1567, %fd221, %fd220;
	mul.f64 	%fd1568, %fd1567, %fd1567;
	ld.global.f64 	%fd222, [%rd81+40];
	ld.global.f64 	%fd223, [%rd79+40];
	sub.f64 	%fd1571, %fd223, %fd222;
	mul.f64 	%fd1572, %fd1571, %fd1571;
	ld.global.f64 	%fd224, [%rd81+48];
	ld.global.f64 	%fd225, [%rd79+48];
	sub.f64 	%fd1575, %fd225, %fd224;
	mul.f64 	%fd1576, %fd1575, %fd1575;
	ld.global.f64 	%fd226, [%rd81+56];
	ld.global.f64 	%fd227, [%rd79+56];
	sub.f64 	%fd1579, %fd227, %fd226;
	mul.f64 	%fd1580, %fd1579, %fd1579;
	ld.global.f64 	%fd228, [%rd81+64];
	ld.global.f64 	%fd229, [%rd79+64];
	sub.f64 	%fd1583, %fd229, %fd228;
	mul.f64 	%fd1584, %fd1583, %fd1583;
	ld.global.f64 	%fd230, [%rd81+72];
	ld.global.f64 	%fd231, [%rd79+72];
	sub.f64 	%fd1587, %fd231, %fd230;
	mul.f64 	%fd1588, %fd1587, %fd1587;
	ld.global.f64 	%fd232, [%rd81+80];
	ld.global.f64 	%fd233, [%rd79+80];
	sub.f64 	%fd1591, %fd233, %fd232;
	mul.f64 	%fd1592, %fd1591, %fd1591;
	ld.global.f64 	%fd234, [%rd81+88];
	ld.global.f64 	%fd235, [%rd79+88];
	sub.f64 	%fd1595, %fd235, %fd234;
	mul.f64 	%fd1596, %fd1595, %fd1595;

$L__BB13_16:
	mov.f64 	%fd1609, 0d0000000000000000;
	@%p9 bra 	$L__BB13_162;

	cvta.to.global.u64 	%rd713, %rd54;
	ld.param.u64 	%rd712, [compute_affine_kinematic_energy_cuda_kernel_backward_param_11];
	setp.eq.s64 	%p11, %rd712, 0;
	mul.lo.s64 	%rd82, %rd41, %rd26;
	add.s64 	%rd43, %rd12, %rd82;
	mul.lo.s64 	%rd83, %rd41, %rd27;
	add.s64 	%rd44, %rd713, %rd83;
	@%p11 bra 	$L__BB13_19;

	ld.global.f64 	%fd237, [%rd43];
	add.f64 	%fd1597, %fd237, 0d0000000000000000;
	bra.uni 	$L__BB13_21;

$L__BB13_19:
	setp.eq.s64 	%p12, %rd54, 0;
	mov.f64 	%fd1597, 0d0000000000000000;
	@%p12 bra 	$L__BB13_21;

	ld.global.f64 	%fd239, [%rd44];
	add.f64 	%fd1597, %fd239, 0d0000000000000000;

$L__BB13_21:
	mov.f64 	%fd240, 0d0000000000000000;
	fma.rn.f64 	%fd241, %fd1594, %fd1597, 0d0000000000000000;
	fma.rn.f64 	%fd242, %fd1595, %fd241, 0d0000000000000000;
	fma.rn.f64 	%fd243, %fd1595, %fd241, %fd242;
	add.f64 	%fd127, %fd243, 0d0000000000000000;
	sub.f64 	%fd244, %fd240, %fd243;
	add.f64 	%fd128, %fd244, 0d0000000000000000;
	setp.eq.s64 	%p13, %rd59, 0;
	@%p13 bra 	$L__BB13_23;

	mul.lo.s64 	%rd96, %rd41, %rd28;
	add.s64 	%rd84, %rd59, %rd96;
	// begin inline asm
	{ atom.add.f64 %fd245,[%rd84],%fd240; }

	// end inline asm
	add.s64 	%rd85, %rd84, 8;
	// begin inline asm
	{ atom.add.f64 %fd247,[%rd85],%fd240; }

	// end inline asm
	add.s64 	%rd86, %rd84, 16;
	// begin inline asm
	{ atom.add.f64 %fd249,[%rd86],%fd240; }

	// end inline asm
	add.s64 	%rd87, %rd84, 24;
	// begin inline asm
	{ atom.add.f64 %fd251,[%rd87],%fd240; }

	// end inline asm
	add.s64 	%rd88, %rd84, 32;
	// begin inline asm
	{ atom.add.f64 %fd253,[%rd88],%fd240; }

	// end inline asm
	add.s64 	%rd89, %rd84, 40;
	// begin inline asm
	{ atom.add.f64 %fd255,[%rd89],%fd240; }

	// end inline asm
	add.s64 	%rd90, %rd84, 48;
	// begin inline asm
	{ atom.add.f64 %fd257,[%rd90],%fd240; }

	// end inline asm
	add.s64 	%rd91, %rd84, 56;
	// begin inline asm
	{ atom.add.f64 %fd259,[%rd91],%fd240; }

	// end inline asm
	add.s64 	%rd92, %rd84, 64;
	// begin inline asm
	{ atom.add.f64 %fd261,[%rd92],%fd240; }

	// end inline asm
	add.s64 	%rd93, %rd84, 72;
	// begin inline asm
	{ atom.add.f64 %fd263,[%rd93],%fd240; }

	// end inline asm
	add.s64 	%rd94, %rd84, 80;
	// begin inline asm
	{ atom.add.f64 %fd265,[%rd94],%fd240; }

	// end inline asm
	add.s64 	%rd95, %rd84, 88;
	// begin inline asm
	{ atom.add.f64 %fd267,[%rd95],%fd128; }

	// end inline asm
	bra.uni 	$L__BB13_25;

$L__BB13_23:
	setp.eq.s64 	%p14, %rd52, 0;
	@%p14 bra 	$L__BB13_25;

	mul.lo.s64 	%rd109, %rd41, %rd23;
	add.s64 	%rd97, %rd52, %rd109;
	mov.f64 	%fd290, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd269,[%rd97],%fd290; }

	// end inline asm
	add.s64 	%rd98, %rd97, 8;
	// begin inline asm
	{ atom.add.f64 %fd271,[%rd98],%fd290; }

	// end inline asm
	add.s64 	%rd99, %rd97, 16;
	// begin inline asm
	{ atom.add.f64 %fd273,[%rd99],%fd290; }

	// end inline asm
	add.s64 	%rd100, %rd97, 24;
	// begin inline asm
	{ atom.add.f64 %fd275,[%rd100],%fd290; }

	// end inline asm
	add.s64 	%rd101, %rd97, 32;
	// begin inline asm
	{ atom.add.f64 %fd277,[%rd101],%fd290; }

	// end inline asm
	add.s64 	%rd102, %rd97, 40;
	// begin inline asm
	{ atom.add.f64 %fd279,[%rd102],%fd290; }

	// end inline asm
	add.s64 	%rd103, %rd97, 48;
	// begin inline asm
	{ atom.add.f64 %fd281,[%rd103],%fd290; }

	// end inline asm
	add.s64 	%rd104, %rd97, 56;
	// begin inline asm
	{ atom.add.f64 %fd283,[%rd104],%fd290; }

	// end inline asm
	add.s64 	%rd105, %rd97, 64;
	// begin inline asm
	{ atom.add.f64 %fd285,[%rd105],%fd290; }

	// end inline asm
	add.s64 	%rd106, %rd97, 72;
	// begin inline asm
	{ atom.add.f64 %fd287,[%rd106],%fd290; }

	// end inline asm
	add.s64 	%rd107, %rd97, 80;
	// begin inline asm
	{ atom.add.f64 %fd289,[%rd107],%fd290; }

	// end inline asm
	add.s64 	%rd108, %rd97, 88;
	// begin inline asm
	{ atom.add.f64 %fd291,[%rd108],%fd128; }

	// end inline asm

$L__BB13_25:
	setp.eq.s64 	%p15, %rd57, 0;
	@%p15 bra 	$L__BB13_27;

	mul.lo.s64 	%rd122, %rd41, %rd29;
	add.s64 	%rd110, %rd57, %rd122;
	mov.f64 	%fd314, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd293,[%rd110],%fd314; }

	// end inline asm
	add.s64 	%rd111, %rd110, 8;
	// begin inline asm
	{ atom.add.f64 %fd295,[%rd111],%fd314; }

	// end inline asm
	add.s64 	%rd112, %rd110, 16;
	// begin inline asm
	{ atom.add.f64 %fd297,[%rd112],%fd314; }

	// end inline asm
	add.s64 	%rd113, %rd110, 24;
	// begin inline asm
	{ atom.add.f64 %fd299,[%rd113],%fd314; }

	// end inline asm
	add.s64 	%rd114, %rd110, 32;
	// begin inline asm
	{ atom.add.f64 %fd301,[%rd114],%fd314; }

	// end inline asm
	add.s64 	%rd115, %rd110, 40;
	// begin inline asm
	{ atom.add.f64 %fd303,[%rd115],%fd314; }

	// end inline asm
	add.s64 	%rd116, %rd110, 48;
	// begin inline asm
	{ atom.add.f64 %fd305,[%rd116],%fd314; }

	// end inline asm
	add.s64 	%rd117, %rd110, 56;
	// begin inline asm
	{ atom.add.f64 %fd307,[%rd117],%fd314; }

	// end inline asm
	add.s64 	%rd118, %rd110, 64;
	// begin inline asm
	{ atom.add.f64 %fd309,[%rd118],%fd314; }

	// end inline asm
	add.s64 	%rd119, %rd110, 72;
	// begin inline asm
	{ atom.add.f64 %fd311,[%rd119],%fd314; }

	// end inline asm
	add.s64 	%rd120, %rd110, 80;
	// begin inline asm
	{ atom.add.f64 %fd313,[%rd120],%fd314; }

	// end inline asm
	add.s64 	%rd121, %rd110, 88;
	// begin inline asm
	{ atom.add.f64 %fd315,[%rd121],%fd127; }

	// end inline asm
	bra.uni 	$L__BB13_29;

$L__BB13_27:
	setp.eq.s64 	%p16, %rd48, 0;
	@%p16 bra 	$L__BB13_29;

	mul.lo.s64 	%rd135, %rd41, %rd22;
	add.s64 	%rd123, %rd48, %rd135;
	mov.f64 	%fd338, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd317,[%rd123],%fd338; }

	// end inline asm
	add.s64 	%rd124, %rd123, 8;
	// begin inline asm
	{ atom.add.f64 %fd319,[%rd124],%fd338; }

	// end inline asm
	add.s64 	%rd125, %rd123, 16;
	// begin inline asm
	{ atom.add.f64 %fd321,[%rd125],%fd338; }

	// end inline asm
	add.s64 	%rd126, %rd123, 24;
	// begin inline asm
	{ atom.add.f64 %fd323,[%rd126],%fd338; }

	// end inline asm
	add.s64 	%rd127, %rd123, 32;
	// begin inline asm
	{ atom.add.f64 %fd325,[%rd127],%fd338; }

	// end inline asm
	add.s64 	%rd128, %rd123, 40;
	// begin inline asm
	{ atom.add.f64 %fd327,[%rd128],%fd338; }

	// end inline asm
	add.s64 	%rd129, %rd123, 48;
	// begin inline asm
	{ atom.add.f64 %fd329,[%rd129],%fd338; }

	// end inline asm
	add.s64 	%rd130, %rd123, 56;
	// begin inline asm
	{ atom.add.f64 %fd331,[%rd130],%fd338; }

	// end inline asm
	add.s64 	%rd131, %rd123, 64;
	// begin inline asm
	{ atom.add.f64 %fd333,[%rd131],%fd338; }

	// end inline asm
	add.s64 	%rd132, %rd123, 72;
	// begin inline asm
	{ atom.add.f64 %fd335,[%rd132],%fd338; }

	// end inline asm
	add.s64 	%rd133, %rd123, 80;
	// begin inline asm
	{ atom.add.f64 %fd337,[%rd133],%fd338; }

	// end inline asm
	add.s64 	%rd134, %rd123, 88;
	// begin inline asm
	{ atom.add.f64 %fd339,[%rd134],%fd127; }

	// end inline asm

$L__BB13_29:
	fma.rn.f64 	%fd1499, %fd1596, %fd1597, 0d0000000000000000;
	fma.rn.f64 	%fd129, %fd1593, %fd1499, 0d0000000000000000;
	@%p11 bra 	$L__BB13_31;

	ld.global.f64 	%fd341, [%rd43];
	add.f64 	%fd1598, %fd341, 0d0000000000000000;
	bra.uni 	$L__BB13_33;

$L__BB13_31:
	setp.eq.s64 	%p18, %rd54, 0;
	mov.f64 	%fd1598, 0d0000000000000000;
	@%p18 bra 	$L__BB13_33;

	ld.global.f64 	%fd343, [%rd44];
	add.f64 	%fd1598, %fd343, 0d0000000000000000;

$L__BB13_33:
	mov.f64 	%fd344, 0d0000000000000000;
	fma.rn.f64 	%fd345, %fd1594, %fd1598, 0d0000000000000000;
	fma.rn.f64 	%fd346, %fd1591, %fd345, 0d0000000000000000;
	fma.rn.f64 	%fd347, %fd1591, %fd345, %fd346;
	add.f64 	%fd134, %fd347, 0d0000000000000000;
	sub.f64 	%fd348, %fd344, %fd347;
	add.f64 	%fd135, %fd348, 0d0000000000000000;
	@%p13 bra 	$L__BB13_35;

	mul.lo.s64 	%rd148, %rd41, %rd28;
	add.s64 	%rd136, %rd59, %rd148;
	// begin inline asm
	{ atom.add.f64 %fd349,[%rd136],%fd344; }

	// end inline asm
	add.s64 	%rd137, %rd136, 8;
	// begin inline asm
	{ atom.add.f64 %fd351,[%rd137],%fd344; }

	// end inline asm
	add.s64 	%rd138, %rd136, 16;
	// begin inline asm
	{ atom.add.f64 %fd353,[%rd138],%fd344; }

	// end inline asm
	add.s64 	%rd139, %rd136, 24;
	// begin inline asm
	{ atom.add.f64 %fd355,[%rd139],%fd344; }

	// end inline asm
	add.s64 	%rd140, %rd136, 32;
	// begin inline asm
	{ atom.add.f64 %fd357,[%rd140],%fd344; }

	// end inline asm
	add.s64 	%rd141, %rd136, 40;
	// begin inline asm
	{ atom.add.f64 %fd359,[%rd141],%fd344; }

	// end inline asm
	add.s64 	%rd142, %rd136, 48;
	// begin inline asm
	{ atom.add.f64 %fd361,[%rd142],%fd344; }

	// end inline asm
	add.s64 	%rd143, %rd136, 56;
	// begin inline asm
	{ atom.add.f64 %fd363,[%rd143],%fd344; }

	// end inline asm
	add.s64 	%rd144, %rd136, 64;
	// begin inline asm
	{ atom.add.f64 %fd365,[%rd144],%fd344; }

	// end inline asm
	add.s64 	%rd145, %rd136, 72;
	// begin inline asm
	{ atom.add.f64 %fd367,[%rd145],%fd344; }

	// end inline asm
	add.s64 	%rd146, %rd136, 80;
	// begin inline asm
	{ atom.add.f64 %fd369,[%rd146],%fd135; }

	// end inline asm
	add.s64 	%rd147, %rd136, 88;
	// begin inline asm
	{ atom.add.f64 %fd371,[%rd147],%fd344; }

	// end inline asm
	bra.uni 	$L__BB13_37;

$L__BB13_35:
	setp.eq.s64 	%p20, %rd52, 0;
	@%p20 bra 	$L__BB13_37;

	mul.lo.s64 	%rd161, %rd41, %rd23;
	add.s64 	%rd149, %rd52, %rd161;
	mov.f64 	%fd396, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd373,[%rd149],%fd396; }

	// end inline asm
	add.s64 	%rd150, %rd149, 8;
	// begin inline asm
	{ atom.add.f64 %fd375,[%rd150],%fd396; }

	// end inline asm
	add.s64 	%rd151, %rd149, 16;
	// begin inline asm
	{ atom.add.f64 %fd377,[%rd151],%fd396; }

	// end inline asm
	add.s64 	%rd152, %rd149, 24;
	// begin inline asm
	{ atom.add.f64 %fd379,[%rd152],%fd396; }

	// end inline asm
	add.s64 	%rd153, %rd149, 32;
	// begin inline asm
	{ atom.add.f64 %fd381,[%rd153],%fd396; }

	// end inline asm
	add.s64 	%rd154, %rd149, 40;
	// begin inline asm
	{ atom.add.f64 %fd383,[%rd154],%fd396; }

	// end inline asm
	add.s64 	%rd155, %rd149, 48;
	// begin inline asm
	{ atom.add.f64 %fd385,[%rd155],%fd396; }

	// end inline asm
	add.s64 	%rd156, %rd149, 56;
	// begin inline asm
	{ atom.add.f64 %fd387,[%rd156],%fd396; }

	// end inline asm
	add.s64 	%rd157, %rd149, 64;
	// begin inline asm
	{ atom.add.f64 %fd389,[%rd157],%fd396; }

	// end inline asm
	add.s64 	%rd158, %rd149, 72;
	// begin inline asm
	{ atom.add.f64 %fd391,[%rd158],%fd396; }

	// end inline asm
	add.s64 	%rd159, %rd149, 80;
	// begin inline asm
	{ atom.add.f64 %fd393,[%rd159],%fd135; }

	// end inline asm
	add.s64 	%rd160, %rd149, 88;
	// begin inline asm
	{ atom.add.f64 %fd395,[%rd160],%fd396; }

	// end inline asm

$L__BB13_37:
	@%p15 bra 	$L__BB13_39;

	mul.lo.s64 	%rd174, %rd41, %rd29;
	add.s64 	%rd162, %rd57, %rd174;
	mov.f64 	%fd420, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd397,[%rd162],%fd420; }

	// end inline asm
	add.s64 	%rd163, %rd162, 8;
	// begin inline asm
	{ atom.add.f64 %fd399,[%rd163],%fd420; }

	// end inline asm
	add.s64 	%rd164, %rd162, 16;
	// begin inline asm
	{ atom.add.f64 %fd401,[%rd164],%fd420; }

	// end inline asm
	add.s64 	%rd165, %rd162, 24;
	// begin inline asm
	{ atom.add.f64 %fd403,[%rd165],%fd420; }

	// end inline asm
	add.s64 	%rd166, %rd162, 32;
	// begin inline asm
	{ atom.add.f64 %fd405,[%rd166],%fd420; }

	// end inline asm
	add.s64 	%rd167, %rd162, 40;
	// begin inline asm
	{ atom.add.f64 %fd407,[%rd167],%fd420; }

	// end inline asm
	add.s64 	%rd168, %rd162, 48;
	// begin inline asm
	{ atom.add.f64 %fd409,[%rd168],%fd420; }

	// end inline asm
	add.s64 	%rd169, %rd162, 56;
	// begin inline asm
	{ atom.add.f64 %fd411,[%rd169],%fd420; }

	// end inline asm
	add.s64 	%rd170, %rd162, 64;
	// begin inline asm
	{ atom.add.f64 %fd413,[%rd170],%fd420; }

	// end inline asm
	add.s64 	%rd171, %rd162, 72;
	// begin inline asm
	{ atom.add.f64 %fd415,[%rd171],%fd420; }

	// end inline asm
	add.s64 	%rd172, %rd162, 80;
	// begin inline asm
	{ atom.add.f64 %fd417,[%rd172],%fd134; }

	// end inline asm
	add.s64 	%rd173, %rd162, 88;
	// begin inline asm
	{ atom.add.f64 %fd419,[%rd173],%fd420; }

	// end inline asm
	bra.uni 	$L__BB13_41;

$L__BB13_39:
	setp.eq.s64 	%p22, %rd48, 0;
	@%p22 bra 	$L__BB13_41;

	mul.lo.s64 	%rd187, %rd41, %rd22;
	add.s64 	%rd175, %rd48, %rd187;
	mov.f64 	%fd444, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd421,[%rd175],%fd444; }

	// end inline asm
	add.s64 	%rd176, %rd175, 8;
	// begin inline asm
	{ atom.add.f64 %fd423,[%rd176],%fd444; }

	// end inline asm
	add.s64 	%rd177, %rd175, 16;
	// begin inline asm
	{ atom.add.f64 %fd425,[%rd177],%fd444; }

	// end inline asm
	add.s64 	%rd178, %rd175, 24;
	// begin inline asm
	{ atom.add.f64 %fd427,[%rd178],%fd444; }

	// end inline asm
	add.s64 	%rd179, %rd175, 32;
	// begin inline asm
	{ atom.add.f64 %fd429,[%rd179],%fd444; }

	// end inline asm
	add.s64 	%rd180, %rd175, 40;
	// begin inline asm
	{ atom.add.f64 %fd431,[%rd180],%fd444; }

	// end inline asm
	add.s64 	%rd181, %rd175, 48;
	// begin inline asm
	{ atom.add.f64 %fd433,[%rd181],%fd444; }

	// end inline asm
	add.s64 	%rd182, %rd175, 56;
	// begin inline asm
	{ atom.add.f64 %fd435,[%rd182],%fd444; }

	// end inline asm
	add.s64 	%rd183, %rd175, 64;
	// begin inline asm
	{ atom.add.f64 %fd437,[%rd183],%fd444; }

	// end inline asm
	add.s64 	%rd184, %rd175, 72;
	// begin inline asm
	{ atom.add.f64 %fd439,[%rd184],%fd444; }

	// end inline asm
	add.s64 	%rd185, %rd175, 80;
	// begin inline asm
	{ atom.add.f64 %fd441,[%rd185],%fd134; }

	// end inline asm
	add.s64 	%rd186, %rd175, 88;
	// begin inline asm
	{ atom.add.f64 %fd443,[%rd186],%fd444; }

	// end inline asm

$L__BB13_41:
	fma.rn.f64 	%fd1489, %fd1592, %fd1598, 0d0000000000000000;
	fma.rn.f64 	%fd136, %fd1593, %fd1489, %fd129;
	@%p11 bra 	$L__BB13_43;

	ld.global.f64 	%fd445, [%rd43];
	add.f64 	%fd1599, %fd445, 0d0000000000000000;
	bra.uni 	$L__BB13_45;

$L__BB13_43:
	setp.eq.s64 	%p24, %rd54, 0;
	mov.f64 	%fd1599, 0d0000000000000000;
	@%p24 bra 	$L__BB13_45;

	ld.global.f64 	%fd447, [%rd44];
	add.f64 	%fd1599, %fd447, 0d0000000000000000;

$L__BB13_45:
	mov.f64 	%fd448, 0d0000000000000000;
	fma.rn.f64 	%fd449, %fd1594, %fd1599, 0d0000000000000000;
	fma.rn.f64 	%fd450, %fd1587, %fd449, 0d0000000000000000;
	fma.rn.f64 	%fd451, %fd1587, %fd449, %fd450;
	add.f64 	%fd141, %fd451, 0d0000000000000000;
	sub.f64 	%fd452, %fd448, %fd451;
	add.f64 	%fd142, %fd452, 0d0000000000000000;
	@%p13 bra 	$L__BB13_47;

	mul.lo.s64 	%rd200, %rd41, %rd28;
	add.s64 	%rd188, %rd59, %rd200;
	// begin inline asm
	{ atom.add.f64 %fd453,[%rd188],%fd448; }

	// end inline asm
	add.s64 	%rd189, %rd188, 8;
	// begin inline asm
	{ atom.add.f64 %fd455,[%rd189],%fd448; }

	// end inline asm
	add.s64 	%rd190, %rd188, 16;
	// begin inline asm
	{ atom.add.f64 %fd457,[%rd190],%fd448; }

	// end inline asm
	add.s64 	%rd191, %rd188, 24;
	// begin inline asm
	{ atom.add.f64 %fd459,[%rd191],%fd448; }

	// end inline asm
	add.s64 	%rd192, %rd188, 32;
	// begin inline asm
	{ atom.add.f64 %fd461,[%rd192],%fd448; }

	// end inline asm
	add.s64 	%rd193, %rd188, 40;
	// begin inline asm
	{ atom.add.f64 %fd463,[%rd193],%fd448; }

	// end inline asm
	add.s64 	%rd194, %rd188, 48;
	// begin inline asm
	{ atom.add.f64 %fd465,[%rd194],%fd448; }

	// end inline asm
	add.s64 	%rd195, %rd188, 56;
	// begin inline asm
	{ atom.add.f64 %fd467,[%rd195],%fd448; }

	// end inline asm
	add.s64 	%rd196, %rd188, 64;
	// begin inline asm
	{ atom.add.f64 %fd469,[%rd196],%fd448; }

	// end inline asm
	add.s64 	%rd197, %rd188, 72;
	// begin inline asm
	{ atom.add.f64 %fd471,[%rd197],%fd142; }

	// end inline asm
	add.s64 	%rd198, %rd188, 80;
	// begin inline asm
	{ atom.add.f64 %fd473,[%rd198],%fd448; }

	// end inline asm
	add.s64 	%rd199, %rd188, 88;
	// begin inline asm
	{ atom.add.f64 %fd475,[%rd199],%fd448; }

	// end inline asm
	bra.uni 	$L__BB13_49;

$L__BB13_47:
	setp.eq.s64 	%p26, %rd52, 0;
	@%p26 bra 	$L__BB13_49;

	mul.lo.s64 	%rd213, %rd41, %rd23;
	add.s64 	%rd201, %rd52, %rd213;
	mov.f64 	%fd500, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd477,[%rd201],%fd500; }

	// end inline asm
	add.s64 	%rd202, %rd201, 8;
	// begin inline asm
	{ atom.add.f64 %fd479,[%rd202],%fd500; }

	// end inline asm
	add.s64 	%rd203, %rd201, 16;
	// begin inline asm
	{ atom.add.f64 %fd481,[%rd203],%fd500; }

	// end inline asm
	add.s64 	%rd204, %rd201, 24;
	// begin inline asm
	{ atom.add.f64 %fd483,[%rd204],%fd500; }

	// end inline asm
	add.s64 	%rd205, %rd201, 32;
	// begin inline asm
	{ atom.add.f64 %fd485,[%rd205],%fd500; }

	// end inline asm
	add.s64 	%rd206, %rd201, 40;
	// begin inline asm
	{ atom.add.f64 %fd487,[%rd206],%fd500; }

	// end inline asm
	add.s64 	%rd207, %rd201, 48;
	// begin inline asm
	{ atom.add.f64 %fd489,[%rd207],%fd500; }

	// end inline asm
	add.s64 	%rd208, %rd201, 56;
	// begin inline asm
	{ atom.add.f64 %fd491,[%rd208],%fd500; }

	// end inline asm
	add.s64 	%rd209, %rd201, 64;
	// begin inline asm
	{ atom.add.f64 %fd493,[%rd209],%fd500; }

	// end inline asm
	add.s64 	%rd210, %rd201, 72;
	// begin inline asm
	{ atom.add.f64 %fd495,[%rd210],%fd142; }

	// end inline asm
	add.s64 	%rd211, %rd201, 80;
	// begin inline asm
	{ atom.add.f64 %fd497,[%rd211],%fd500; }

	// end inline asm
	add.s64 	%rd212, %rd201, 88;
	// begin inline asm
	{ atom.add.f64 %fd499,[%rd212],%fd500; }

	// end inline asm

$L__BB13_49:
	@%p15 bra 	$L__BB13_51;

	mul.lo.s64 	%rd226, %rd41, %rd29;
	add.s64 	%rd214, %rd57, %rd226;
	mov.f64 	%fd524, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd501,[%rd214],%fd524; }

	// end inline asm
	add.s64 	%rd215, %rd214, 8;
	// begin inline asm
	{ atom.add.f64 %fd503,[%rd215],%fd524; }

	// end inline asm
	add.s64 	%rd216, %rd214, 16;
	// begin inline asm
	{ atom.add.f64 %fd505,[%rd216],%fd524; }

	// end inline asm
	add.s64 	%rd217, %rd214, 24;
	// begin inline asm
	{ atom.add.f64 %fd507,[%rd217],%fd524; }

	// end inline asm
	add.s64 	%rd218, %rd214, 32;
	// begin inline asm
	{ atom.add.f64 %fd509,[%rd218],%fd524; }

	// end inline asm
	add.s64 	%rd219, %rd214, 40;
	// begin inline asm
	{ atom.add.f64 %fd511,[%rd219],%fd524; }

	// end inline asm
	add.s64 	%rd220, %rd214, 48;
	// begin inline asm
	{ atom.add.f64 %fd513,[%rd220],%fd524; }

	// end inline asm
	add.s64 	%rd221, %rd214, 56;
	// begin inline asm
	{ atom.add.f64 %fd515,[%rd221],%fd524; }

	// end inline asm
	add.s64 	%rd222, %rd214, 64;
	// begin inline asm
	{ atom.add.f64 %fd517,[%rd222],%fd524; }

	// end inline asm
	add.s64 	%rd223, %rd214, 72;
	// begin inline asm
	{ atom.add.f64 %fd519,[%rd223],%fd141; }

	// end inline asm
	add.s64 	%rd224, %rd214, 80;
	// begin inline asm
	{ atom.add.f64 %fd521,[%rd224],%fd524; }

	// end inline asm
	add.s64 	%rd225, %rd214, 88;
	// begin inline asm
	{ atom.add.f64 %fd523,[%rd225],%fd524; }

	// end inline asm
	bra.uni 	$L__BB13_53;

$L__BB13_51:
	setp.eq.s64 	%p28, %rd48, 0;
	@%p28 bra 	$L__BB13_53;

	mul.lo.s64 	%rd239, %rd41, %rd22;
	add.s64 	%rd227, %rd48, %rd239;
	mov.f64 	%fd548, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd525,[%rd227],%fd548; }

	// end inline asm
	add.s64 	%rd228, %rd227, 8;
	// begin inline asm
	{ atom.add.f64 %fd527,[%rd228],%fd548; }

	// end inline asm
	add.s64 	%rd229, %rd227, 16;
	// begin inline asm
	{ atom.add.f64 %fd529,[%rd229],%fd548; }

	// end inline asm
	add.s64 	%rd230, %rd227, 24;
	// begin inline asm
	{ atom.add.f64 %fd531,[%rd230],%fd548; }

	// end inline asm
	add.s64 	%rd231, %rd227, 32;
	// begin inline asm
	{ atom.add.f64 %fd533,[%rd231],%fd548; }

	// end inline asm
	add.s64 	%rd232, %rd227, 40;
	// begin inline asm
	{ atom.add.f64 %fd535,[%rd232],%fd548; }

	// end inline asm
	add.s64 	%rd233, %rd227, 48;
	// begin inline asm
	{ atom.add.f64 %fd537,[%rd233],%fd548; }

	// end inline asm
	add.s64 	%rd234, %rd227, 56;
	// begin inline asm
	{ atom.add.f64 %fd539,[%rd234],%fd548; }

	// end inline asm
	add.s64 	%rd235, %rd227, 64;
	// begin inline asm
	{ atom.add.f64 %fd541,[%rd235],%fd548; }

	// end inline asm
	add.s64 	%rd236, %rd227, 72;
	// begin inline asm
	{ atom.add.f64 %fd543,[%rd236],%fd141; }

	// end inline asm
	add.s64 	%rd237, %rd227, 80;
	// begin inline asm
	{ atom.add.f64 %fd545,[%rd237],%fd548; }

	// end inline asm
	add.s64 	%rd238, %rd227, 88;
	// begin inline asm
	{ atom.add.f64 %fd547,[%rd238],%fd548; }

	// end inline asm

$L__BB13_53:
	fma.rn.f64 	%fd1490, %fd1588, %fd1599, 0d0000000000000000;
	fma.rn.f64 	%fd143, %fd1593, %fd1490, %fd136;
	@%p11 bra 	$L__BB13_55;

	ld.global.f64 	%fd549, [%rd43];
	add.f64 	%fd1600, %fd549, 0d0000000000000000;
	bra.uni 	$L__BB13_57;

$L__BB13_55:
	setp.eq.s64 	%p30, %rd54, 0;
	mov.f64 	%fd1600, 0d0000000000000000;
	@%p30 bra 	$L__BB13_57;

	ld.global.f64 	%fd551, [%rd44];
	add.f64 	%fd1600, %fd551, 0d0000000000000000;

$L__BB13_57:
	mov.f64 	%fd552, 0d0000000000000000;
	fma.rn.f64 	%fd553, %fd1594, %fd1600, 0d0000000000000000;
	fma.rn.f64 	%fd554, %fd1583, %fd553, 0d0000000000000000;
	fma.rn.f64 	%fd555, %fd1583, %fd553, %fd554;
	add.f64 	%fd148, %fd555, 0d0000000000000000;
	sub.f64 	%fd556, %fd552, %fd555;
	add.f64 	%fd149, %fd556, 0d0000000000000000;
	@%p13 bra 	$L__BB13_59;

	mul.lo.s64 	%rd252, %rd41, %rd28;
	add.s64 	%rd240, %rd59, %rd252;
	// begin inline asm
	{ atom.add.f64 %fd557,[%rd240],%fd552; }

	// end inline asm
	add.s64 	%rd241, %rd240, 8;
	// begin inline asm
	{ atom.add.f64 %fd559,[%rd241],%fd552; }

	// end inline asm
	add.s64 	%rd242, %rd240, 16;
	// begin inline asm
	{ atom.add.f64 %fd561,[%rd242],%fd552; }

	// end inline asm
	add.s64 	%rd243, %rd240, 24;
	// begin inline asm
	{ atom.add.f64 %fd563,[%rd243],%fd552; }

	// end inline asm
	add.s64 	%rd244, %rd240, 32;
	// begin inline asm
	{ atom.add.f64 %fd565,[%rd244],%fd552; }

	// end inline asm
	add.s64 	%rd245, %rd240, 40;
	// begin inline asm
	{ atom.add.f64 %fd567,[%rd245],%fd552; }

	// end inline asm
	add.s64 	%rd246, %rd240, 48;
	// begin inline asm
	{ atom.add.f64 %fd569,[%rd246],%fd552; }

	// end inline asm
	add.s64 	%rd247, %rd240, 56;
	// begin inline asm
	{ atom.add.f64 %fd571,[%rd247],%fd552; }

	// end inline asm
	add.s64 	%rd248, %rd240, 64;
	// begin inline asm
	{ atom.add.f64 %fd573,[%rd248],%fd149; }

	// end inline asm
	add.s64 	%rd249, %rd240, 72;
	// begin inline asm
	{ atom.add.f64 %fd575,[%rd249],%fd552; }

	// end inline asm
	add.s64 	%rd250, %rd240, 80;
	// begin inline asm
	{ atom.add.f64 %fd577,[%rd250],%fd552; }

	// end inline asm
	add.s64 	%rd251, %rd240, 88;
	// begin inline asm
	{ atom.add.f64 %fd579,[%rd251],%fd552; }

	// end inline asm
	bra.uni 	$L__BB13_61;

$L__BB13_59:
	setp.eq.s64 	%p32, %rd52, 0;
	@%p32 bra 	$L__BB13_61;

	mul.lo.s64 	%rd265, %rd41, %rd23;
	add.s64 	%rd253, %rd52, %rd265;
	mov.f64 	%fd604, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd581,[%rd253],%fd604; }

	// end inline asm
	add.s64 	%rd254, %rd253, 8;
	// begin inline asm
	{ atom.add.f64 %fd583,[%rd254],%fd604; }

	// end inline asm
	add.s64 	%rd255, %rd253, 16;
	// begin inline asm
	{ atom.add.f64 %fd585,[%rd255],%fd604; }

	// end inline asm
	add.s64 	%rd256, %rd253, 24;
	// begin inline asm
	{ atom.add.f64 %fd587,[%rd256],%fd604; }

	// end inline asm
	add.s64 	%rd257, %rd253, 32;
	// begin inline asm
	{ atom.add.f64 %fd589,[%rd257],%fd604; }

	// end inline asm
	add.s64 	%rd258, %rd253, 40;
	// begin inline asm
	{ atom.add.f64 %fd591,[%rd258],%fd604; }

	// end inline asm
	add.s64 	%rd259, %rd253, 48;
	// begin inline asm
	{ atom.add.f64 %fd593,[%rd259],%fd604; }

	// end inline asm
	add.s64 	%rd260, %rd253, 56;
	// begin inline asm
	{ atom.add.f64 %fd595,[%rd260],%fd604; }

	// end inline asm
	add.s64 	%rd261, %rd253, 64;
	// begin inline asm
	{ atom.add.f64 %fd597,[%rd261],%fd149; }

	// end inline asm
	add.s64 	%rd262, %rd253, 72;
	// begin inline asm
	{ atom.add.f64 %fd599,[%rd262],%fd604; }

	// end inline asm
	add.s64 	%rd263, %rd253, 80;
	// begin inline asm
	{ atom.add.f64 %fd601,[%rd263],%fd604; }

	// end inline asm
	add.s64 	%rd264, %rd253, 88;
	// begin inline asm
	{ atom.add.f64 %fd603,[%rd264],%fd604; }

	// end inline asm

$L__BB13_61:
	@%p15 bra 	$L__BB13_63;

	mul.lo.s64 	%rd278, %rd41, %rd29;
	add.s64 	%rd266, %rd57, %rd278;
	mov.f64 	%fd628, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd605,[%rd266],%fd628; }

	// end inline asm
	add.s64 	%rd267, %rd266, 8;
	// begin inline asm
	{ atom.add.f64 %fd607,[%rd267],%fd628; }

	// end inline asm
	add.s64 	%rd268, %rd266, 16;
	// begin inline asm
	{ atom.add.f64 %fd609,[%rd268],%fd628; }

	// end inline asm
	add.s64 	%rd269, %rd266, 24;
	// begin inline asm
	{ atom.add.f64 %fd611,[%rd269],%fd628; }

	// end inline asm
	add.s64 	%rd270, %rd266, 32;
	// begin inline asm
	{ atom.add.f64 %fd613,[%rd270],%fd628; }

	// end inline asm
	add.s64 	%rd271, %rd266, 40;
	// begin inline asm
	{ atom.add.f64 %fd615,[%rd271],%fd628; }

	// end inline asm
	add.s64 	%rd272, %rd266, 48;
	// begin inline asm
	{ atom.add.f64 %fd617,[%rd272],%fd628; }

	// end inline asm
	add.s64 	%rd273, %rd266, 56;
	// begin inline asm
	{ atom.add.f64 %fd619,[%rd273],%fd628; }

	// end inline asm
	add.s64 	%rd274, %rd266, 64;
	// begin inline asm
	{ atom.add.f64 %fd621,[%rd274],%fd148; }

	// end inline asm
	add.s64 	%rd275, %rd266, 72;
	// begin inline asm
	{ atom.add.f64 %fd623,[%rd275],%fd628; }

	// end inline asm
	add.s64 	%rd276, %rd266, 80;
	// begin inline asm
	{ atom.add.f64 %fd625,[%rd276],%fd628; }

	// end inline asm
	add.s64 	%rd277, %rd266, 88;
	// begin inline asm
	{ atom.add.f64 %fd627,[%rd277],%fd628; }

	// end inline asm
	bra.uni 	$L__BB13_65;

$L__BB13_63:
	setp.eq.s64 	%p34, %rd48, 0;
	@%p34 bra 	$L__BB13_65;

	mul.lo.s64 	%rd291, %rd41, %rd22;
	add.s64 	%rd279, %rd48, %rd291;
	mov.f64 	%fd652, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd629,[%rd279],%fd652; }

	// end inline asm
	add.s64 	%rd280, %rd279, 8;
	// begin inline asm
	{ atom.add.f64 %fd631,[%rd280],%fd652; }

	// end inline asm
	add.s64 	%rd281, %rd279, 16;
	// begin inline asm
	{ atom.add.f64 %fd633,[%rd281],%fd652; }

	// end inline asm
	add.s64 	%rd282, %rd279, 24;
	// begin inline asm
	{ atom.add.f64 %fd635,[%rd282],%fd652; }

	// end inline asm
	add.s64 	%rd283, %rd279, 32;
	// begin inline asm
	{ atom.add.f64 %fd637,[%rd283],%fd652; }

	// end inline asm
	add.s64 	%rd284, %rd279, 40;
	// begin inline asm
	{ atom.add.f64 %fd639,[%rd284],%fd652; }

	// end inline asm
	add.s64 	%rd285, %rd279, 48;
	// begin inline asm
	{ atom.add.f64 %fd641,[%rd285],%fd652; }

	// end inline asm
	add.s64 	%rd286, %rd279, 56;
	// begin inline asm
	{ atom.add.f64 %fd643,[%rd286],%fd652; }

	// end inline asm
	add.s64 	%rd287, %rd279, 64;
	// begin inline asm
	{ atom.add.f64 %fd645,[%rd287],%fd148; }

	// end inline asm
	add.s64 	%rd288, %rd279, 72;
	// begin inline asm
	{ atom.add.f64 %fd647,[%rd288],%fd652; }

	// end inline asm
	add.s64 	%rd289, %rd279, 80;
	// begin inline asm
	{ atom.add.f64 %fd649,[%rd289],%fd652; }

	// end inline asm
	add.s64 	%rd290, %rd279, 88;
	// begin inline asm
	{ atom.add.f64 %fd651,[%rd290],%fd652; }

	// end inline asm

$L__BB13_65:
	fma.rn.f64 	%fd1491, %fd1584, %fd1600, 0d0000000000000000;
	fma.rn.f64 	%fd150, %fd1593, %fd1491, %fd143;
	@%p11 bra 	$L__BB13_67;

	ld.global.f64 	%fd653, [%rd43];
	add.f64 	%fd1601, %fd653, 0d0000000000000000;
	bra.uni 	$L__BB13_69;

$L__BB13_67:
	setp.eq.s64 	%p36, %rd54, 0;
	mov.f64 	%fd1601, 0d0000000000000000;
	@%p36 bra 	$L__BB13_69;

	ld.global.f64 	%fd655, [%rd44];
	add.f64 	%fd1601, %fd655, 0d0000000000000000;

$L__BB13_69:
	mov.f64 	%fd656, 0d0000000000000000;
	fma.rn.f64 	%fd657, %fd1594, %fd1601, 0d0000000000000000;
	fma.rn.f64 	%fd658, %fd1579, %fd657, 0d0000000000000000;
	fma.rn.f64 	%fd659, %fd1579, %fd657, %fd658;
	add.f64 	%fd155, %fd659, 0d0000000000000000;
	sub.f64 	%fd660, %fd656, %fd659;
	add.f64 	%fd156, %fd660, 0d0000000000000000;
	@%p13 bra 	$L__BB13_71;

	mul.lo.s64 	%rd304, %rd41, %rd28;
	add.s64 	%rd292, %rd59, %rd304;
	// begin inline asm
	{ atom.add.f64 %fd661,[%rd292],%fd656; }

	// end inline asm
	add.s64 	%rd293, %rd292, 8;
	// begin inline asm
	{ atom.add.f64 %fd663,[%rd293],%fd656; }

	// end inline asm
	add.s64 	%rd294, %rd292, 16;
	// begin inline asm
	{ atom.add.f64 %fd665,[%rd294],%fd656; }

	// end inline asm
	add.s64 	%rd295, %rd292, 24;
	// begin inline asm
	{ atom.add.f64 %fd667,[%rd295],%fd656; }

	// end inline asm
	add.s64 	%rd296, %rd292, 32;
	// begin inline asm
	{ atom.add.f64 %fd669,[%rd296],%fd656; }

	// end inline asm
	add.s64 	%rd297, %rd292, 40;
	// begin inline asm
	{ atom.add.f64 %fd671,[%rd297],%fd656; }

	// end inline asm
	add.s64 	%rd298, %rd292, 48;
	// begin inline asm
	{ atom.add.f64 %fd673,[%rd298],%fd656; }

	// end inline asm
	add.s64 	%rd299, %rd292, 56;
	// begin inline asm
	{ atom.add.f64 %fd675,[%rd299],%fd156; }

	// end inline asm
	add.s64 	%rd300, %rd292, 64;
	// begin inline asm
	{ atom.add.f64 %fd677,[%rd300],%fd656; }

	// end inline asm
	add.s64 	%rd301, %rd292, 72;
	// begin inline asm
	{ atom.add.f64 %fd679,[%rd301],%fd656; }

	// end inline asm
	add.s64 	%rd302, %rd292, 80;
	// begin inline asm
	{ atom.add.f64 %fd681,[%rd302],%fd656; }

	// end inline asm
	add.s64 	%rd303, %rd292, 88;
	// begin inline asm
	{ atom.add.f64 %fd683,[%rd303],%fd656; }

	// end inline asm
	bra.uni 	$L__BB13_73;

$L__BB13_71:
	setp.eq.s64 	%p38, %rd52, 0;
	@%p38 bra 	$L__BB13_73;

	mul.lo.s64 	%rd317, %rd41, %rd23;
	add.s64 	%rd305, %rd52, %rd317;
	mov.f64 	%fd708, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd685,[%rd305],%fd708; }

	// end inline asm
	add.s64 	%rd306, %rd305, 8;
	// begin inline asm
	{ atom.add.f64 %fd687,[%rd306],%fd708; }

	// end inline asm
	add.s64 	%rd307, %rd305, 16;
	// begin inline asm
	{ atom.add.f64 %fd689,[%rd307],%fd708; }

	// end inline asm
	add.s64 	%rd308, %rd305, 24;
	// begin inline asm
	{ atom.add.f64 %fd691,[%rd308],%fd708; }

	// end inline asm
	add.s64 	%rd309, %rd305, 32;
	// begin inline asm
	{ atom.add.f64 %fd693,[%rd309],%fd708; }

	// end inline asm
	add.s64 	%rd310, %rd305, 40;
	// begin inline asm
	{ atom.add.f64 %fd695,[%rd310],%fd708; }

	// end inline asm
	add.s64 	%rd311, %rd305, 48;
	// begin inline asm
	{ atom.add.f64 %fd697,[%rd311],%fd708; }

	// end inline asm
	add.s64 	%rd312, %rd305, 56;
	// begin inline asm
	{ atom.add.f64 %fd699,[%rd312],%fd156; }

	// end inline asm
	add.s64 	%rd313, %rd305, 64;
	// begin inline asm
	{ atom.add.f64 %fd701,[%rd313],%fd708; }

	// end inline asm
	add.s64 	%rd314, %rd305, 72;
	// begin inline asm
	{ atom.add.f64 %fd703,[%rd314],%fd708; }

	// end inline asm
	add.s64 	%rd315, %rd305, 80;
	// begin inline asm
	{ atom.add.f64 %fd705,[%rd315],%fd708; }

	// end inline asm
	add.s64 	%rd316, %rd305, 88;
	// begin inline asm
	{ atom.add.f64 %fd707,[%rd316],%fd708; }

	// end inline asm

$L__BB13_73:
	@%p15 bra 	$L__BB13_75;

	mul.lo.s64 	%rd330, %rd41, %rd29;
	add.s64 	%rd318, %rd57, %rd330;
	mov.f64 	%fd732, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd709,[%rd318],%fd732; }

	// end inline asm
	add.s64 	%rd319, %rd318, 8;
	// begin inline asm
	{ atom.add.f64 %fd711,[%rd319],%fd732; }

	// end inline asm
	add.s64 	%rd320, %rd318, 16;
	// begin inline asm
	{ atom.add.f64 %fd713,[%rd320],%fd732; }

	// end inline asm
	add.s64 	%rd321, %rd318, 24;
	// begin inline asm
	{ atom.add.f64 %fd715,[%rd321],%fd732; }

	// end inline asm
	add.s64 	%rd322, %rd318, 32;
	// begin inline asm
	{ atom.add.f64 %fd717,[%rd322],%fd732; }

	// end inline asm
	add.s64 	%rd323, %rd318, 40;
	// begin inline asm
	{ atom.add.f64 %fd719,[%rd323],%fd732; }

	// end inline asm
	add.s64 	%rd324, %rd318, 48;
	// begin inline asm
	{ atom.add.f64 %fd721,[%rd324],%fd732; }

	// end inline asm
	add.s64 	%rd325, %rd318, 56;
	// begin inline asm
	{ atom.add.f64 %fd723,[%rd325],%fd155; }

	// end inline asm
	add.s64 	%rd326, %rd318, 64;
	// begin inline asm
	{ atom.add.f64 %fd725,[%rd326],%fd732; }

	// end inline asm
	add.s64 	%rd327, %rd318, 72;
	// begin inline asm
	{ atom.add.f64 %fd727,[%rd327],%fd732; }

	// end inline asm
	add.s64 	%rd328, %rd318, 80;
	// begin inline asm
	{ atom.add.f64 %fd729,[%rd328],%fd732; }

	// end inline asm
	add.s64 	%rd329, %rd318, 88;
	// begin inline asm
	{ atom.add.f64 %fd731,[%rd329],%fd732; }

	// end inline asm
	bra.uni 	$L__BB13_77;

$L__BB13_75:
	setp.eq.s64 	%p40, %rd48, 0;
	@%p40 bra 	$L__BB13_77;

	mul.lo.s64 	%rd343, %rd41, %rd22;
	add.s64 	%rd331, %rd48, %rd343;
	mov.f64 	%fd756, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd733,[%rd331],%fd756; }

	// end inline asm
	add.s64 	%rd332, %rd331, 8;
	// begin inline asm
	{ atom.add.f64 %fd735,[%rd332],%fd756; }

	// end inline asm
	add.s64 	%rd333, %rd331, 16;
	// begin inline asm
	{ atom.add.f64 %fd737,[%rd333],%fd756; }

	// end inline asm
	add.s64 	%rd334, %rd331, 24;
	// begin inline asm
	{ atom.add.f64 %fd739,[%rd334],%fd756; }

	// end inline asm
	add.s64 	%rd335, %rd331, 32;
	// begin inline asm
	{ atom.add.f64 %fd741,[%rd335],%fd756; }

	// end inline asm
	add.s64 	%rd336, %rd331, 40;
	// begin inline asm
	{ atom.add.f64 %fd743,[%rd336],%fd756; }

	// end inline asm
	add.s64 	%rd337, %rd331, 48;
	// begin inline asm
	{ atom.add.f64 %fd745,[%rd337],%fd756; }

	// end inline asm
	add.s64 	%rd338, %rd331, 56;
	// begin inline asm
	{ atom.add.f64 %fd747,[%rd338],%fd155; }

	// end inline asm
	add.s64 	%rd339, %rd331, 64;
	// begin inline asm
	{ atom.add.f64 %fd749,[%rd339],%fd756; }

	// end inline asm
	add.s64 	%rd340, %rd331, 72;
	// begin inline asm
	{ atom.add.f64 %fd751,[%rd340],%fd756; }

	// end inline asm
	add.s64 	%rd341, %rd331, 80;
	// begin inline asm
	{ atom.add.f64 %fd753,[%rd341],%fd756; }

	// end inline asm
	add.s64 	%rd342, %rd331, 88;
	// begin inline asm
	{ atom.add.f64 %fd755,[%rd342],%fd756; }

	// end inline asm

$L__BB13_77:
	fma.rn.f64 	%fd1492, %fd1580, %fd1601, 0d0000000000000000;
	fma.rn.f64 	%fd157, %fd1593, %fd1492, %fd150;
	@%p11 bra 	$L__BB13_79;

	ld.global.f64 	%fd757, [%rd43];
	add.f64 	%fd1602, %fd757, 0d0000000000000000;
	bra.uni 	$L__BB13_81;

$L__BB13_79:
	setp.eq.s64 	%p42, %rd54, 0;
	mov.f64 	%fd1602, 0d0000000000000000;
	@%p42 bra 	$L__BB13_81;

	ld.global.f64 	%fd759, [%rd44];
	add.f64 	%fd1602, %fd759, 0d0000000000000000;

$L__BB13_81:
	mov.f64 	%fd760, 0d0000000000000000;
	fma.rn.f64 	%fd761, %fd1594, %fd1602, 0d0000000000000000;
	fma.rn.f64 	%fd762, %fd1575, %fd761, 0d0000000000000000;
	fma.rn.f64 	%fd763, %fd1575, %fd761, %fd762;
	add.f64 	%fd162, %fd763, 0d0000000000000000;
	sub.f64 	%fd764, %fd760, %fd763;
	add.f64 	%fd163, %fd764, 0d0000000000000000;
	@%p13 bra 	$L__BB13_83;

	mul.lo.s64 	%rd356, %rd41, %rd28;
	add.s64 	%rd344, %rd59, %rd356;
	// begin inline asm
	{ atom.add.f64 %fd765,[%rd344],%fd760; }

	// end inline asm
	add.s64 	%rd345, %rd344, 8;
	// begin inline asm
	{ atom.add.f64 %fd767,[%rd345],%fd760; }

	// end inline asm
	add.s64 	%rd346, %rd344, 16;
	// begin inline asm
	{ atom.add.f64 %fd769,[%rd346],%fd760; }

	// end inline asm
	add.s64 	%rd347, %rd344, 24;
	// begin inline asm
	{ atom.add.f64 %fd771,[%rd347],%fd760; }

	// end inline asm
	add.s64 	%rd348, %rd344, 32;
	// begin inline asm
	{ atom.add.f64 %fd773,[%rd348],%fd760; }

	// end inline asm
	add.s64 	%rd349, %rd344, 40;
	// begin inline asm
	{ atom.add.f64 %fd775,[%rd349],%fd760; }

	// end inline asm
	add.s64 	%rd350, %rd344, 48;
	// begin inline asm
	{ atom.add.f64 %fd777,[%rd350],%fd163; }

	// end inline asm
	add.s64 	%rd351, %rd344, 56;
	// begin inline asm
	{ atom.add.f64 %fd779,[%rd351],%fd760; }

	// end inline asm
	add.s64 	%rd352, %rd344, 64;
	// begin inline asm
	{ atom.add.f64 %fd781,[%rd352],%fd760; }

	// end inline asm
	add.s64 	%rd353, %rd344, 72;
	// begin inline asm
	{ atom.add.f64 %fd783,[%rd353],%fd760; }

	// end inline asm
	add.s64 	%rd354, %rd344, 80;
	// begin inline asm
	{ atom.add.f64 %fd785,[%rd354],%fd760; }

	// end inline asm
	add.s64 	%rd355, %rd344, 88;
	// begin inline asm
	{ atom.add.f64 %fd787,[%rd355],%fd760; }

	// end inline asm
	bra.uni 	$L__BB13_85;

$L__BB13_83:
	setp.eq.s64 	%p44, %rd52, 0;
	@%p44 bra 	$L__BB13_85;

	mul.lo.s64 	%rd369, %rd41, %rd23;
	add.s64 	%rd357, %rd52, %rd369;
	mov.f64 	%fd812, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd789,[%rd357],%fd812; }

	// end inline asm
	add.s64 	%rd358, %rd357, 8;
	// begin inline asm
	{ atom.add.f64 %fd791,[%rd358],%fd812; }

	// end inline asm
	add.s64 	%rd359, %rd357, 16;
	// begin inline asm
	{ atom.add.f64 %fd793,[%rd359],%fd812; }

	// end inline asm
	add.s64 	%rd360, %rd357, 24;
	// begin inline asm
	{ atom.add.f64 %fd795,[%rd360],%fd812; }

	// end inline asm
	add.s64 	%rd361, %rd357, 32;
	// begin inline asm
	{ atom.add.f64 %fd797,[%rd361],%fd812; }

	// end inline asm
	add.s64 	%rd362, %rd357, 40;
	// begin inline asm
	{ atom.add.f64 %fd799,[%rd362],%fd812; }

	// end inline asm
	add.s64 	%rd363, %rd357, 48;
	// begin inline asm
	{ atom.add.f64 %fd801,[%rd363],%fd163; }

	// end inline asm
	add.s64 	%rd364, %rd357, 56;
	// begin inline asm
	{ atom.add.f64 %fd803,[%rd364],%fd812; }

	// end inline asm
	add.s64 	%rd365, %rd357, 64;
	// begin inline asm
	{ atom.add.f64 %fd805,[%rd365],%fd812; }

	// end inline asm
	add.s64 	%rd366, %rd357, 72;
	// begin inline asm
	{ atom.add.f64 %fd807,[%rd366],%fd812; }

	// end inline asm
	add.s64 	%rd367, %rd357, 80;
	// begin inline asm
	{ atom.add.f64 %fd809,[%rd367],%fd812; }

	// end inline asm
	add.s64 	%rd368, %rd357, 88;
	// begin inline asm
	{ atom.add.f64 %fd811,[%rd368],%fd812; }

	// end inline asm

$L__BB13_85:
	@%p15 bra 	$L__BB13_87;

	mul.lo.s64 	%rd382, %rd41, %rd29;
	add.s64 	%rd370, %rd57, %rd382;
	mov.f64 	%fd836, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd813,[%rd370],%fd836; }

	// end inline asm
	add.s64 	%rd371, %rd370, 8;
	// begin inline asm
	{ atom.add.f64 %fd815,[%rd371],%fd836; }

	// end inline asm
	add.s64 	%rd372, %rd370, 16;
	// begin inline asm
	{ atom.add.f64 %fd817,[%rd372],%fd836; }

	// end inline asm
	add.s64 	%rd373, %rd370, 24;
	// begin inline asm
	{ atom.add.f64 %fd819,[%rd373],%fd836; }

	// end inline asm
	add.s64 	%rd374, %rd370, 32;
	// begin inline asm
	{ atom.add.f64 %fd821,[%rd374],%fd836; }

	// end inline asm
	add.s64 	%rd375, %rd370, 40;
	// begin inline asm
	{ atom.add.f64 %fd823,[%rd375],%fd836; }

	// end inline asm
	add.s64 	%rd376, %rd370, 48;
	// begin inline asm
	{ atom.add.f64 %fd825,[%rd376],%fd162; }

	// end inline asm
	add.s64 	%rd377, %rd370, 56;
	// begin inline asm
	{ atom.add.f64 %fd827,[%rd377],%fd836; }

	// end inline asm
	add.s64 	%rd378, %rd370, 64;
	// begin inline asm
	{ atom.add.f64 %fd829,[%rd378],%fd836; }

	// end inline asm
	add.s64 	%rd379, %rd370, 72;
	// begin inline asm
	{ atom.add.f64 %fd831,[%rd379],%fd836; }

	// end inline asm
	add.s64 	%rd380, %rd370, 80;
	// begin inline asm
	{ atom.add.f64 %fd833,[%rd380],%fd836; }

	// end inline asm
	add.s64 	%rd381, %rd370, 88;
	// begin inline asm
	{ atom.add.f64 %fd835,[%rd381],%fd836; }

	// end inline asm
	bra.uni 	$L__BB13_89;

$L__BB13_87:
	setp.eq.s64 	%p46, %rd48, 0;
	@%p46 bra 	$L__BB13_89;

	mul.lo.s64 	%rd395, %rd41, %rd22;
	add.s64 	%rd383, %rd48, %rd395;
	mov.f64 	%fd860, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd837,[%rd383],%fd860; }

	// end inline asm
	add.s64 	%rd384, %rd383, 8;
	// begin inline asm
	{ atom.add.f64 %fd839,[%rd384],%fd860; }

	// end inline asm
	add.s64 	%rd385, %rd383, 16;
	// begin inline asm
	{ atom.add.f64 %fd841,[%rd385],%fd860; }

	// end inline asm
	add.s64 	%rd386, %rd383, 24;
	// begin inline asm
	{ atom.add.f64 %fd843,[%rd386],%fd860; }

	// end inline asm
	add.s64 	%rd387, %rd383, 32;
	// begin inline asm
	{ atom.add.f64 %fd845,[%rd387],%fd860; }

	// end inline asm
	add.s64 	%rd388, %rd383, 40;
	// begin inline asm
	{ atom.add.f64 %fd847,[%rd388],%fd860; }

	// end inline asm
	add.s64 	%rd389, %rd383, 48;
	// begin inline asm
	{ atom.add.f64 %fd849,[%rd389],%fd162; }

	// end inline asm
	add.s64 	%rd390, %rd383, 56;
	// begin inline asm
	{ atom.add.f64 %fd851,[%rd390],%fd860; }

	// end inline asm
	add.s64 	%rd391, %rd383, 64;
	// begin inline asm
	{ atom.add.f64 %fd853,[%rd391],%fd860; }

	// end inline asm
	add.s64 	%rd392, %rd383, 72;
	// begin inline asm
	{ atom.add.f64 %fd855,[%rd392],%fd860; }

	// end inline asm
	add.s64 	%rd393, %rd383, 80;
	// begin inline asm
	{ atom.add.f64 %fd857,[%rd393],%fd860; }

	// end inline asm
	add.s64 	%rd394, %rd383, 88;
	// begin inline asm
	{ atom.add.f64 %fd859,[%rd394],%fd860; }

	// end inline asm

$L__BB13_89:
	fma.rn.f64 	%fd1493, %fd1576, %fd1602, 0d0000000000000000;
	fma.rn.f64 	%fd164, %fd1593, %fd1493, %fd157;
	@%p11 bra 	$L__BB13_91;

	ld.global.f64 	%fd861, [%rd43];
	add.f64 	%fd1603, %fd861, 0d0000000000000000;
	bra.uni 	$L__BB13_93;

$L__BB13_91:
	setp.eq.s64 	%p48, %rd54, 0;
	mov.f64 	%fd1603, 0d0000000000000000;
	@%p48 bra 	$L__BB13_93;

	ld.global.f64 	%fd863, [%rd44];
	add.f64 	%fd1603, %fd863, 0d0000000000000000;

$L__BB13_93:
	mov.f64 	%fd864, 0d0000000000000000;
	fma.rn.f64 	%fd865, %fd1594, %fd1603, 0d0000000000000000;
	fma.rn.f64 	%fd866, %fd1571, %fd865, 0d0000000000000000;
	fma.rn.f64 	%fd867, %fd1571, %fd865, %fd866;
	add.f64 	%fd169, %fd867, 0d0000000000000000;
	sub.f64 	%fd868, %fd864, %fd867;
	add.f64 	%fd170, %fd868, 0d0000000000000000;
	@%p13 bra 	$L__BB13_95;

	mul.lo.s64 	%rd408, %rd41, %rd28;
	add.s64 	%rd396, %rd59, %rd408;
	// begin inline asm
	{ atom.add.f64 %fd869,[%rd396],%fd864; }

	// end inline asm
	add.s64 	%rd397, %rd396, 8;
	// begin inline asm
	{ atom.add.f64 %fd871,[%rd397],%fd864; }

	// end inline asm
	add.s64 	%rd398, %rd396, 16;
	// begin inline asm
	{ atom.add.f64 %fd873,[%rd398],%fd864; }

	// end inline asm
	add.s64 	%rd399, %rd396, 24;
	// begin inline asm
	{ atom.add.f64 %fd875,[%rd399],%fd864; }

	// end inline asm
	add.s64 	%rd400, %rd396, 32;
	// begin inline asm
	{ atom.add.f64 %fd877,[%rd400],%fd864; }

	// end inline asm
	add.s64 	%rd401, %rd396, 40;
	// begin inline asm
	{ atom.add.f64 %fd879,[%rd401],%fd170; }

	// end inline asm
	add.s64 	%rd402, %rd396, 48;
	// begin inline asm
	{ atom.add.f64 %fd881,[%rd402],%fd864; }

	// end inline asm
	add.s64 	%rd403, %rd396, 56;
	// begin inline asm
	{ atom.add.f64 %fd883,[%rd403],%fd864; }

	// end inline asm
	add.s64 	%rd404, %rd396, 64;
	// begin inline asm
	{ atom.add.f64 %fd885,[%rd404],%fd864; }

	// end inline asm
	add.s64 	%rd405, %rd396, 72;
	// begin inline asm
	{ atom.add.f64 %fd887,[%rd405],%fd864; }

	// end inline asm
	add.s64 	%rd406, %rd396, 80;
	// begin inline asm
	{ atom.add.f64 %fd889,[%rd406],%fd864; }

	// end inline asm
	add.s64 	%rd407, %rd396, 88;
	// begin inline asm
	{ atom.add.f64 %fd891,[%rd407],%fd864; }

	// end inline asm
	bra.uni 	$L__BB13_97;

$L__BB13_95:
	setp.eq.s64 	%p50, %rd52, 0;
	@%p50 bra 	$L__BB13_97;

	mul.lo.s64 	%rd421, %rd41, %rd23;
	add.s64 	%rd409, %rd52, %rd421;
	mov.f64 	%fd916, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd893,[%rd409],%fd916; }

	// end inline asm
	add.s64 	%rd410, %rd409, 8;
	// begin inline asm
	{ atom.add.f64 %fd895,[%rd410],%fd916; }

	// end inline asm
	add.s64 	%rd411, %rd409, 16;
	// begin inline asm
	{ atom.add.f64 %fd897,[%rd411],%fd916; }

	// end inline asm
	add.s64 	%rd412, %rd409, 24;
	// begin inline asm
	{ atom.add.f64 %fd899,[%rd412],%fd916; }

	// end inline asm
	add.s64 	%rd413, %rd409, 32;
	// begin inline asm
	{ atom.add.f64 %fd901,[%rd413],%fd916; }

	// end inline asm
	add.s64 	%rd414, %rd409, 40;
	// begin inline asm
	{ atom.add.f64 %fd903,[%rd414],%fd170; }

	// end inline asm
	add.s64 	%rd415, %rd409, 48;
	// begin inline asm
	{ atom.add.f64 %fd905,[%rd415],%fd916; }

	// end inline asm
	add.s64 	%rd416, %rd409, 56;
	// begin inline asm
	{ atom.add.f64 %fd907,[%rd416],%fd916; }

	// end inline asm
	add.s64 	%rd417, %rd409, 64;
	// begin inline asm
	{ atom.add.f64 %fd909,[%rd417],%fd916; }

	// end inline asm
	add.s64 	%rd418, %rd409, 72;
	// begin inline asm
	{ atom.add.f64 %fd911,[%rd418],%fd916; }

	// end inline asm
	add.s64 	%rd419, %rd409, 80;
	// begin inline asm
	{ atom.add.f64 %fd913,[%rd419],%fd916; }

	// end inline asm
	add.s64 	%rd420, %rd409, 88;
	// begin inline asm
	{ atom.add.f64 %fd915,[%rd420],%fd916; }

	// end inline asm

$L__BB13_97:
	@%p15 bra 	$L__BB13_99;

	mul.lo.s64 	%rd434, %rd41, %rd29;
	add.s64 	%rd422, %rd57, %rd434;
	mov.f64 	%fd940, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd917,[%rd422],%fd940; }

	// end inline asm
	add.s64 	%rd423, %rd422, 8;
	// begin inline asm
	{ atom.add.f64 %fd919,[%rd423],%fd940; }

	// end inline asm
	add.s64 	%rd424, %rd422, 16;
	// begin inline asm
	{ atom.add.f64 %fd921,[%rd424],%fd940; }

	// end inline asm
	add.s64 	%rd425, %rd422, 24;
	// begin inline asm
	{ atom.add.f64 %fd923,[%rd425],%fd940; }

	// end inline asm
	add.s64 	%rd426, %rd422, 32;
	// begin inline asm
	{ atom.add.f64 %fd925,[%rd426],%fd940; }

	// end inline asm
	add.s64 	%rd427, %rd422, 40;
	// begin inline asm
	{ atom.add.f64 %fd927,[%rd427],%fd169; }

	// end inline asm
	add.s64 	%rd428, %rd422, 48;
	// begin inline asm
	{ atom.add.f64 %fd929,[%rd428],%fd940; }

	// end inline asm
	add.s64 	%rd429, %rd422, 56;
	// begin inline asm
	{ atom.add.f64 %fd931,[%rd429],%fd940; }

	// end inline asm
	add.s64 	%rd430, %rd422, 64;
	// begin inline asm
	{ atom.add.f64 %fd933,[%rd430],%fd940; }

	// end inline asm
	add.s64 	%rd431, %rd422, 72;
	// begin inline asm
	{ atom.add.f64 %fd935,[%rd431],%fd940; }

	// end inline asm
	add.s64 	%rd432, %rd422, 80;
	// begin inline asm
	{ atom.add.f64 %fd937,[%rd432],%fd940; }

	// end inline asm
	add.s64 	%rd433, %rd422, 88;
	// begin inline asm
	{ atom.add.f64 %fd939,[%rd433],%fd940; }

	// end inline asm
	bra.uni 	$L__BB13_101;

$L__BB13_99:
	setp.eq.s64 	%p52, %rd48, 0;
	@%p52 bra 	$L__BB13_101;

	mul.lo.s64 	%rd447, %rd41, %rd22;
	add.s64 	%rd435, %rd48, %rd447;
	mov.f64 	%fd964, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd941,[%rd435],%fd964; }

	// end inline asm
	add.s64 	%rd436, %rd435, 8;
	// begin inline asm
	{ atom.add.f64 %fd943,[%rd436],%fd964; }

	// end inline asm
	add.s64 	%rd437, %rd435, 16;
	// begin inline asm
	{ atom.add.f64 %fd945,[%rd437],%fd964; }

	// end inline asm
	add.s64 	%rd438, %rd435, 24;
	// begin inline asm
	{ atom.add.f64 %fd947,[%rd438],%fd964; }

	// end inline asm
	add.s64 	%rd439, %rd435, 32;
	// begin inline asm
	{ atom.add.f64 %fd949,[%rd439],%fd964; }

	// end inline asm
	add.s64 	%rd440, %rd435, 40;
	// begin inline asm
	{ atom.add.f64 %fd951,[%rd440],%fd169; }

	// end inline asm
	add.s64 	%rd441, %rd435, 48;
	// begin inline asm
	{ atom.add.f64 %fd953,[%rd441],%fd964; }

	// end inline asm
	add.s64 	%rd442, %rd435, 56;
	// begin inline asm
	{ atom.add.f64 %fd955,[%rd442],%fd964; }

	// end inline asm
	add.s64 	%rd443, %rd435, 64;
	// begin inline asm
	{ atom.add.f64 %fd957,[%rd443],%fd964; }

	// end inline asm
	add.s64 	%rd444, %rd435, 72;
	// begin inline asm
	{ atom.add.f64 %fd959,[%rd444],%fd964; }

	// end inline asm
	add.s64 	%rd445, %rd435, 80;
	// begin inline asm
	{ atom.add.f64 %fd961,[%rd445],%fd964; }

	// end inline asm
	add.s64 	%rd446, %rd435, 88;
	// begin inline asm
	{ atom.add.f64 %fd963,[%rd446],%fd964; }

	// end inline asm

$L__BB13_101:
	fma.rn.f64 	%fd1494, %fd1572, %fd1603, 0d0000000000000000;
	fma.rn.f64 	%fd171, %fd1593, %fd1494, %fd164;
	@%p11 bra 	$L__BB13_103;

	ld.global.f64 	%fd965, [%rd43];
	add.f64 	%fd1604, %fd965, 0d0000000000000000;
	bra.uni 	$L__BB13_105;

$L__BB13_103:
	setp.eq.s64 	%p54, %rd54, 0;
	mov.f64 	%fd1604, 0d0000000000000000;
	@%p54 bra 	$L__BB13_105;

	ld.global.f64 	%fd967, [%rd44];
	add.f64 	%fd1604, %fd967, 0d0000000000000000;

$L__BB13_105:
	mov.f64 	%fd968, 0d0000000000000000;
	fma.rn.f64 	%fd969, %fd1594, %fd1604, 0d0000000000000000;
	fma.rn.f64 	%fd970, %fd1567, %fd969, 0d0000000000000000;
	fma.rn.f64 	%fd971, %fd1567, %fd969, %fd970;
	add.f64 	%fd176, %fd971, 0d0000000000000000;
	sub.f64 	%fd972, %fd968, %fd971;
	add.f64 	%fd177, %fd972, 0d0000000000000000;
	@%p13 bra 	$L__BB13_107;

	mul.lo.s64 	%rd460, %rd41, %rd28;
	add.s64 	%rd448, %rd59, %rd460;
	// begin inline asm
	{ atom.add.f64 %fd973,[%rd448],%fd968; }

	// end inline asm
	add.s64 	%rd449, %rd448, 8;
	// begin inline asm
	{ atom.add.f64 %fd975,[%rd449],%fd968; }

	// end inline asm
	add.s64 	%rd450, %rd448, 16;
	// begin inline asm
	{ atom.add.f64 %fd977,[%rd450],%fd968; }

	// end inline asm
	add.s64 	%rd451, %rd448, 24;
	// begin inline asm
	{ atom.add.f64 %fd979,[%rd451],%fd968; }

	// end inline asm
	add.s64 	%rd452, %rd448, 32;
	// begin inline asm
	{ atom.add.f64 %fd981,[%rd452],%fd177; }

	// end inline asm
	add.s64 	%rd453, %rd448, 40;
	// begin inline asm
	{ atom.add.f64 %fd983,[%rd453],%fd968; }

	// end inline asm
	add.s64 	%rd454, %rd448, 48;
	// begin inline asm
	{ atom.add.f64 %fd985,[%rd454],%fd968; }

	// end inline asm
	add.s64 	%rd455, %rd448, 56;
	// begin inline asm
	{ atom.add.f64 %fd987,[%rd455],%fd968; }

	// end inline asm
	add.s64 	%rd456, %rd448, 64;
	// begin inline asm
	{ atom.add.f64 %fd989,[%rd456],%fd968; }

	// end inline asm
	add.s64 	%rd457, %rd448, 72;
	// begin inline asm
	{ atom.add.f64 %fd991,[%rd457],%fd968; }

	// end inline asm
	add.s64 	%rd458, %rd448, 80;
	// begin inline asm
	{ atom.add.f64 %fd993,[%rd458],%fd968; }

	// end inline asm
	add.s64 	%rd459, %rd448, 88;
	// begin inline asm
	{ atom.add.f64 %fd995,[%rd459],%fd968; }

	// end inline asm
	bra.uni 	$L__BB13_109;

$L__BB13_107:
	setp.eq.s64 	%p56, %rd52, 0;
	@%p56 bra 	$L__BB13_109;

	mul.lo.s64 	%rd473, %rd41, %rd23;
	add.s64 	%rd461, %rd52, %rd473;
	mov.f64 	%fd1020, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd997,[%rd461],%fd1020; }

	// end inline asm
	add.s64 	%rd462, %rd461, 8;
	// begin inline asm
	{ atom.add.f64 %fd999,[%rd462],%fd1020; }

	// end inline asm
	add.s64 	%rd463, %rd461, 16;
	// begin inline asm
	{ atom.add.f64 %fd1001,[%rd463],%fd1020; }

	// end inline asm
	add.s64 	%rd464, %rd461, 24;
	// begin inline asm
	{ atom.add.f64 %fd1003,[%rd464],%fd1020; }

	// end inline asm
	add.s64 	%rd465, %rd461, 32;
	// begin inline asm
	{ atom.add.f64 %fd1005,[%rd465],%fd177; }

	// end inline asm
	add.s64 	%rd466, %rd461, 40;
	// begin inline asm
	{ atom.add.f64 %fd1007,[%rd466],%fd1020; }

	// end inline asm
	add.s64 	%rd467, %rd461, 48;
	// begin inline asm
	{ atom.add.f64 %fd1009,[%rd467],%fd1020; }

	// end inline asm
	add.s64 	%rd468, %rd461, 56;
	// begin inline asm
	{ atom.add.f64 %fd1011,[%rd468],%fd1020; }

	// end inline asm
	add.s64 	%rd469, %rd461, 64;
	// begin inline asm
	{ atom.add.f64 %fd1013,[%rd469],%fd1020; }

	// end inline asm
	add.s64 	%rd470, %rd461, 72;
	// begin inline asm
	{ atom.add.f64 %fd1015,[%rd470],%fd1020; }

	// end inline asm
	add.s64 	%rd471, %rd461, 80;
	// begin inline asm
	{ atom.add.f64 %fd1017,[%rd471],%fd1020; }

	// end inline asm
	add.s64 	%rd472, %rd461, 88;
	// begin inline asm
	{ atom.add.f64 %fd1019,[%rd472],%fd1020; }

	// end inline asm

$L__BB13_109:
	@%p15 bra 	$L__BB13_111;

	mul.lo.s64 	%rd486, %rd41, %rd29;
	add.s64 	%rd474, %rd57, %rd486;
	mov.f64 	%fd1044, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1021,[%rd474],%fd1044; }

	// end inline asm
	add.s64 	%rd475, %rd474, 8;
	// begin inline asm
	{ atom.add.f64 %fd1023,[%rd475],%fd1044; }

	// end inline asm
	add.s64 	%rd476, %rd474, 16;
	// begin inline asm
	{ atom.add.f64 %fd1025,[%rd476],%fd1044; }

	// end inline asm
	add.s64 	%rd477, %rd474, 24;
	// begin inline asm
	{ atom.add.f64 %fd1027,[%rd477],%fd1044; }

	// end inline asm
	add.s64 	%rd478, %rd474, 32;
	// begin inline asm
	{ atom.add.f64 %fd1029,[%rd478],%fd176; }

	// end inline asm
	add.s64 	%rd479, %rd474, 40;
	// begin inline asm
	{ atom.add.f64 %fd1031,[%rd479],%fd1044; }

	// end inline asm
	add.s64 	%rd480, %rd474, 48;
	// begin inline asm
	{ atom.add.f64 %fd1033,[%rd480],%fd1044; }

	// end inline asm
	add.s64 	%rd481, %rd474, 56;
	// begin inline asm
	{ atom.add.f64 %fd1035,[%rd481],%fd1044; }

	// end inline asm
	add.s64 	%rd482, %rd474, 64;
	// begin inline asm
	{ atom.add.f64 %fd1037,[%rd482],%fd1044; }

	// end inline asm
	add.s64 	%rd483, %rd474, 72;
	// begin inline asm
	{ atom.add.f64 %fd1039,[%rd483],%fd1044; }

	// end inline asm
	add.s64 	%rd484, %rd474, 80;
	// begin inline asm
	{ atom.add.f64 %fd1041,[%rd484],%fd1044; }

	// end inline asm
	add.s64 	%rd485, %rd474, 88;
	// begin inline asm
	{ atom.add.f64 %fd1043,[%rd485],%fd1044; }

	// end inline asm
	bra.uni 	$L__BB13_113;

$L__BB13_111:
	setp.eq.s64 	%p58, %rd48, 0;
	@%p58 bra 	$L__BB13_113;

	mul.lo.s64 	%rd499, %rd41, %rd22;
	add.s64 	%rd487, %rd48, %rd499;
	mov.f64 	%fd1068, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1045,[%rd487],%fd1068; }

	// end inline asm
	add.s64 	%rd488, %rd487, 8;
	// begin inline asm
	{ atom.add.f64 %fd1047,[%rd488],%fd1068; }

	// end inline asm
	add.s64 	%rd489, %rd487, 16;
	// begin inline asm
	{ atom.add.f64 %fd1049,[%rd489],%fd1068; }

	// end inline asm
	add.s64 	%rd490, %rd487, 24;
	// begin inline asm
	{ atom.add.f64 %fd1051,[%rd490],%fd1068; }

	// end inline asm
	add.s64 	%rd491, %rd487, 32;
	// begin inline asm
	{ atom.add.f64 %fd1053,[%rd491],%fd176; }

	// end inline asm
	add.s64 	%rd492, %rd487, 40;
	// begin inline asm
	{ atom.add.f64 %fd1055,[%rd492],%fd1068; }

	// end inline asm
	add.s64 	%rd493, %rd487, 48;
	// begin inline asm
	{ atom.add.f64 %fd1057,[%rd493],%fd1068; }

	// end inline asm
	add.s64 	%rd494, %rd487, 56;
	// begin inline asm
	{ atom.add.f64 %fd1059,[%rd494],%fd1068; }

	// end inline asm
	add.s64 	%rd495, %rd487, 64;
	// begin inline asm
	{ atom.add.f64 %fd1061,[%rd495],%fd1068; }

	// end inline asm
	add.s64 	%rd496, %rd487, 72;
	// begin inline asm
	{ atom.add.f64 %fd1063,[%rd496],%fd1068; }

	// end inline asm
	add.s64 	%rd497, %rd487, 80;
	// begin inline asm
	{ atom.add.f64 %fd1065,[%rd497],%fd1068; }

	// end inline asm
	add.s64 	%rd498, %rd487, 88;
	// begin inline asm
	{ atom.add.f64 %fd1067,[%rd498],%fd1068; }

	// end inline asm

$L__BB13_113:
	fma.rn.f64 	%fd1495, %fd1568, %fd1604, 0d0000000000000000;
	fma.rn.f64 	%fd178, %fd1593, %fd1495, %fd171;
	@%p11 bra 	$L__BB13_115;

	ld.global.f64 	%fd1069, [%rd43];
	add.f64 	%fd1605, %fd1069, 0d0000000000000000;
	bra.uni 	$L__BB13_117;

$L__BB13_115:
	setp.eq.s64 	%p60, %rd54, 0;
	mov.f64 	%fd1605, 0d0000000000000000;
	@%p60 bra 	$L__BB13_117;

	ld.global.f64 	%fd1071, [%rd44];
	add.f64 	%fd1605, %fd1071, 0d0000000000000000;

$L__BB13_117:
	mov.f64 	%fd1072, 0d0000000000000000;
	fma.rn.f64 	%fd1073, %fd1594, %fd1605, 0d0000000000000000;
	fma.rn.f64 	%fd1074, %fd1563, %fd1073, 0d0000000000000000;
	fma.rn.f64 	%fd1075, %fd1563, %fd1073, %fd1074;
	add.f64 	%fd183, %fd1075, 0d0000000000000000;
	sub.f64 	%fd1076, %fd1072, %fd1075;
	add.f64 	%fd184, %fd1076, 0d0000000000000000;
	@%p13 bra 	$L__BB13_119;

	mul.lo.s64 	%rd512, %rd41, %rd28;
	add.s64 	%rd500, %rd59, %rd512;
	// begin inline asm
	{ atom.add.f64 %fd1077,[%rd500],%fd1072; }

	// end inline asm
	add.s64 	%rd501, %rd500, 8;
	// begin inline asm
	{ atom.add.f64 %fd1079,[%rd501],%fd1072; }

	// end inline asm
	add.s64 	%rd502, %rd500, 16;
	// begin inline asm
	{ atom.add.f64 %fd1081,[%rd502],%fd1072; }

	// end inline asm
	add.s64 	%rd503, %rd500, 24;
	// begin inline asm
	{ atom.add.f64 %fd1083,[%rd503],%fd184; }

	// end inline asm
	add.s64 	%rd504, %rd500, 32;
	// begin inline asm
	{ atom.add.f64 %fd1085,[%rd504],%fd1072; }

	// end inline asm
	add.s64 	%rd505, %rd500, 40;
	// begin inline asm
	{ atom.add.f64 %fd1087,[%rd505],%fd1072; }

	// end inline asm
	add.s64 	%rd506, %rd500, 48;
	// begin inline asm
	{ atom.add.f64 %fd1089,[%rd506],%fd1072; }

	// end inline asm
	add.s64 	%rd507, %rd500, 56;
	// begin inline asm
	{ atom.add.f64 %fd1091,[%rd507],%fd1072; }

	// end inline asm
	add.s64 	%rd508, %rd500, 64;
	// begin inline asm
	{ atom.add.f64 %fd1093,[%rd508],%fd1072; }

	// end inline asm
	add.s64 	%rd509, %rd500, 72;
	// begin inline asm
	{ atom.add.f64 %fd1095,[%rd509],%fd1072; }

	// end inline asm
	add.s64 	%rd510, %rd500, 80;
	// begin inline asm
	{ atom.add.f64 %fd1097,[%rd510],%fd1072; }

	// end inline asm
	add.s64 	%rd511, %rd500, 88;
	// begin inline asm
	{ atom.add.f64 %fd1099,[%rd511],%fd1072; }

	// end inline asm
	bra.uni 	$L__BB13_121;

$L__BB13_119:
	setp.eq.s64 	%p62, %rd52, 0;
	@%p62 bra 	$L__BB13_121;

	mul.lo.s64 	%rd525, %rd41, %rd23;
	add.s64 	%rd513, %rd52, %rd525;
	mov.f64 	%fd1124, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1101,[%rd513],%fd1124; }

	// end inline asm
	add.s64 	%rd514, %rd513, 8;
	// begin inline asm
	{ atom.add.f64 %fd1103,[%rd514],%fd1124; }

	// end inline asm
	add.s64 	%rd515, %rd513, 16;
	// begin inline asm
	{ atom.add.f64 %fd1105,[%rd515],%fd1124; }

	// end inline asm
	add.s64 	%rd516, %rd513, 24;
	// begin inline asm
	{ atom.add.f64 %fd1107,[%rd516],%fd184; }

	// end inline asm
	add.s64 	%rd517, %rd513, 32;
	// begin inline asm
	{ atom.add.f64 %fd1109,[%rd517],%fd1124; }

	// end inline asm
	add.s64 	%rd518, %rd513, 40;
	// begin inline asm
	{ atom.add.f64 %fd1111,[%rd518],%fd1124; }

	// end inline asm
	add.s64 	%rd519, %rd513, 48;
	// begin inline asm
	{ atom.add.f64 %fd1113,[%rd519],%fd1124; }

	// end inline asm
	add.s64 	%rd520, %rd513, 56;
	// begin inline asm
	{ atom.add.f64 %fd1115,[%rd520],%fd1124; }

	// end inline asm
	add.s64 	%rd521, %rd513, 64;
	// begin inline asm
	{ atom.add.f64 %fd1117,[%rd521],%fd1124; }

	// end inline asm
	add.s64 	%rd522, %rd513, 72;
	// begin inline asm
	{ atom.add.f64 %fd1119,[%rd522],%fd1124; }

	// end inline asm
	add.s64 	%rd523, %rd513, 80;
	// begin inline asm
	{ atom.add.f64 %fd1121,[%rd523],%fd1124; }

	// end inline asm
	add.s64 	%rd524, %rd513, 88;
	// begin inline asm
	{ atom.add.f64 %fd1123,[%rd524],%fd1124; }

	// end inline asm

$L__BB13_121:
	@%p15 bra 	$L__BB13_123;

	mul.lo.s64 	%rd538, %rd41, %rd29;
	add.s64 	%rd526, %rd57, %rd538;
	mov.f64 	%fd1148, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1125,[%rd526],%fd1148; }

	// end inline asm
	add.s64 	%rd527, %rd526, 8;
	// begin inline asm
	{ atom.add.f64 %fd1127,[%rd527],%fd1148; }

	// end inline asm
	add.s64 	%rd528, %rd526, 16;
	// begin inline asm
	{ atom.add.f64 %fd1129,[%rd528],%fd1148; }

	// end inline asm
	add.s64 	%rd529, %rd526, 24;
	// begin inline asm
	{ atom.add.f64 %fd1131,[%rd529],%fd183; }

	// end inline asm
	add.s64 	%rd530, %rd526, 32;
	// begin inline asm
	{ atom.add.f64 %fd1133,[%rd530],%fd1148; }

	// end inline asm
	add.s64 	%rd531, %rd526, 40;
	// begin inline asm
	{ atom.add.f64 %fd1135,[%rd531],%fd1148; }

	// end inline asm
	add.s64 	%rd532, %rd526, 48;
	// begin inline asm
	{ atom.add.f64 %fd1137,[%rd532],%fd1148; }

	// end inline asm
	add.s64 	%rd533, %rd526, 56;
	// begin inline asm
	{ atom.add.f64 %fd1139,[%rd533],%fd1148; }

	// end inline asm
	add.s64 	%rd534, %rd526, 64;
	// begin inline asm
	{ atom.add.f64 %fd1141,[%rd534],%fd1148; }

	// end inline asm
	add.s64 	%rd535, %rd526, 72;
	// begin inline asm
	{ atom.add.f64 %fd1143,[%rd535],%fd1148; }

	// end inline asm
	add.s64 	%rd536, %rd526, 80;
	// begin inline asm
	{ atom.add.f64 %fd1145,[%rd536],%fd1148; }

	// end inline asm
	add.s64 	%rd537, %rd526, 88;
	// begin inline asm
	{ atom.add.f64 %fd1147,[%rd537],%fd1148; }

	// end inline asm
	bra.uni 	$L__BB13_125;

$L__BB13_123:
	setp.eq.s64 	%p64, %rd48, 0;
	@%p64 bra 	$L__BB13_125;

	mul.lo.s64 	%rd551, %rd41, %rd22;
	add.s64 	%rd539, %rd48, %rd551;
	mov.f64 	%fd1172, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1149,[%rd539],%fd1172; }

	// end inline asm
	add.s64 	%rd540, %rd539, 8;
	// begin inline asm
	{ atom.add.f64 %fd1151,[%rd540],%fd1172; }

	// end inline asm
	add.s64 	%rd541, %rd539, 16;
	// begin inline asm
	{ atom.add.f64 %fd1153,[%rd541],%fd1172; }

	// end inline asm
	add.s64 	%rd542, %rd539, 24;
	// begin inline asm
	{ atom.add.f64 %fd1155,[%rd542],%fd183; }

	// end inline asm
	add.s64 	%rd543, %rd539, 32;
	// begin inline asm
	{ atom.add.f64 %fd1157,[%rd543],%fd1172; }

	// end inline asm
	add.s64 	%rd544, %rd539, 40;
	// begin inline asm
	{ atom.add.f64 %fd1159,[%rd544],%fd1172; }

	// end inline asm
	add.s64 	%rd545, %rd539, 48;
	// begin inline asm
	{ atom.add.f64 %fd1161,[%rd545],%fd1172; }

	// end inline asm
	add.s64 	%rd546, %rd539, 56;
	// begin inline asm
	{ atom.add.f64 %fd1163,[%rd546],%fd1172; }

	// end inline asm
	add.s64 	%rd547, %rd539, 64;
	// begin inline asm
	{ atom.add.f64 %fd1165,[%rd547],%fd1172; }

	// end inline asm
	add.s64 	%rd548, %rd539, 72;
	// begin inline asm
	{ atom.add.f64 %fd1167,[%rd548],%fd1172; }

	// end inline asm
	add.s64 	%rd549, %rd539, 80;
	// begin inline asm
	{ atom.add.f64 %fd1169,[%rd549],%fd1172; }

	// end inline asm
	add.s64 	%rd550, %rd539, 88;
	// begin inline asm
	{ atom.add.f64 %fd1171,[%rd550],%fd1172; }

	// end inline asm

$L__BB13_125:
	fma.rn.f64 	%fd1496, %fd1564, %fd1605, 0d0000000000000000;
	fma.rn.f64 	%fd185, %fd1593, %fd1496, %fd178;
	@%p11 bra 	$L__BB13_127;

	ld.global.f64 	%fd1173, [%rd43];
	add.f64 	%fd1606, %fd1173, 0d0000000000000000;
	bra.uni 	$L__BB13_129;

$L__BB13_127:
	setp.eq.s64 	%p66, %rd54, 0;
	mov.f64 	%fd1606, 0d0000000000000000;
	@%p66 bra 	$L__BB13_129;

	ld.global.f64 	%fd1175, [%rd44];
	add.f64 	%fd1606, %fd1175, 0d0000000000000000;

$L__BB13_129:
	mov.f64 	%fd1176, 0d0000000000000000;
	fma.rn.f64 	%fd1177, %fd1594, %fd1606, 0d0000000000000000;
	fma.rn.f64 	%fd1178, %fd1559, %fd1177, 0d0000000000000000;
	fma.rn.f64 	%fd1179, %fd1559, %fd1177, %fd1178;
	add.f64 	%fd190, %fd1179, 0d0000000000000000;
	sub.f64 	%fd1180, %fd1176, %fd1179;
	add.f64 	%fd191, %fd1180, 0d0000000000000000;
	@%p13 bra 	$L__BB13_131;

	mul.lo.s64 	%rd564, %rd41, %rd28;
	add.s64 	%rd552, %rd59, %rd564;
	// begin inline asm
	{ atom.add.f64 %fd1181,[%rd552],%fd1176; }

	// end inline asm
	add.s64 	%rd553, %rd552, 8;
	// begin inline asm
	{ atom.add.f64 %fd1183,[%rd553],%fd1176; }

	// end inline asm
	add.s64 	%rd554, %rd552, 16;
	// begin inline asm
	{ atom.add.f64 %fd1185,[%rd554],%fd191; }

	// end inline asm
	add.s64 	%rd555, %rd552, 24;
	// begin inline asm
	{ atom.add.f64 %fd1187,[%rd555],%fd1176; }

	// end inline asm
	add.s64 	%rd556, %rd552, 32;
	// begin inline asm
	{ atom.add.f64 %fd1189,[%rd556],%fd1176; }

	// end inline asm
	add.s64 	%rd557, %rd552, 40;
	// begin inline asm
	{ atom.add.f64 %fd1191,[%rd557],%fd1176; }

	// end inline asm
	add.s64 	%rd558, %rd552, 48;
	// begin inline asm
	{ atom.add.f64 %fd1193,[%rd558],%fd1176; }

	// end inline asm
	add.s64 	%rd559, %rd552, 56;
	// begin inline asm
	{ atom.add.f64 %fd1195,[%rd559],%fd1176; }

	// end inline asm
	add.s64 	%rd560, %rd552, 64;
	// begin inline asm
	{ atom.add.f64 %fd1197,[%rd560],%fd1176; }

	// end inline asm
	add.s64 	%rd561, %rd552, 72;
	// begin inline asm
	{ atom.add.f64 %fd1199,[%rd561],%fd1176; }

	// end inline asm
	add.s64 	%rd562, %rd552, 80;
	// begin inline asm
	{ atom.add.f64 %fd1201,[%rd562],%fd1176; }

	// end inline asm
	add.s64 	%rd563, %rd552, 88;
	// begin inline asm
	{ atom.add.f64 %fd1203,[%rd563],%fd1176; }

	// end inline asm
	bra.uni 	$L__BB13_133;

$L__BB13_131:
	setp.eq.s64 	%p68, %rd52, 0;
	@%p68 bra 	$L__BB13_133;

	mul.lo.s64 	%rd577, %rd41, %rd23;
	add.s64 	%rd565, %rd52, %rd577;
	mov.f64 	%fd1228, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1205,[%rd565],%fd1228; }

	// end inline asm
	add.s64 	%rd566, %rd565, 8;
	// begin inline asm
	{ atom.add.f64 %fd1207,[%rd566],%fd1228; }

	// end inline asm
	add.s64 	%rd567, %rd565, 16;
	// begin inline asm
	{ atom.add.f64 %fd1209,[%rd567],%fd191; }

	// end inline asm
	add.s64 	%rd568, %rd565, 24;
	// begin inline asm
	{ atom.add.f64 %fd1211,[%rd568],%fd1228; }

	// end inline asm
	add.s64 	%rd569, %rd565, 32;
	// begin inline asm
	{ atom.add.f64 %fd1213,[%rd569],%fd1228; }

	// end inline asm
	add.s64 	%rd570, %rd565, 40;
	// begin inline asm
	{ atom.add.f64 %fd1215,[%rd570],%fd1228; }

	// end inline asm
	add.s64 	%rd571, %rd565, 48;
	// begin inline asm
	{ atom.add.f64 %fd1217,[%rd571],%fd1228; }

	// end inline asm
	add.s64 	%rd572, %rd565, 56;
	// begin inline asm
	{ atom.add.f64 %fd1219,[%rd572],%fd1228; }

	// end inline asm
	add.s64 	%rd573, %rd565, 64;
	// begin inline asm
	{ atom.add.f64 %fd1221,[%rd573],%fd1228; }

	// end inline asm
	add.s64 	%rd574, %rd565, 72;
	// begin inline asm
	{ atom.add.f64 %fd1223,[%rd574],%fd1228; }

	// end inline asm
	add.s64 	%rd575, %rd565, 80;
	// begin inline asm
	{ atom.add.f64 %fd1225,[%rd575],%fd1228; }

	// end inline asm
	add.s64 	%rd576, %rd565, 88;
	// begin inline asm
	{ atom.add.f64 %fd1227,[%rd576],%fd1228; }

	// end inline asm

$L__BB13_133:
	@%p15 bra 	$L__BB13_135;

	mul.lo.s64 	%rd590, %rd41, %rd29;
	add.s64 	%rd578, %rd57, %rd590;
	mov.f64 	%fd1252, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1229,[%rd578],%fd1252; }

	// end inline asm
	add.s64 	%rd579, %rd578, 8;
	// begin inline asm
	{ atom.add.f64 %fd1231,[%rd579],%fd1252; }

	// end inline asm
	add.s64 	%rd580, %rd578, 16;
	// begin inline asm
	{ atom.add.f64 %fd1233,[%rd580],%fd190; }

	// end inline asm
	add.s64 	%rd581, %rd578, 24;
	// begin inline asm
	{ atom.add.f64 %fd1235,[%rd581],%fd1252; }

	// end inline asm
	add.s64 	%rd582, %rd578, 32;
	// begin inline asm
	{ atom.add.f64 %fd1237,[%rd582],%fd1252; }

	// end inline asm
	add.s64 	%rd583, %rd578, 40;
	// begin inline asm
	{ atom.add.f64 %fd1239,[%rd583],%fd1252; }

	// end inline asm
	add.s64 	%rd584, %rd578, 48;
	// begin inline asm
	{ atom.add.f64 %fd1241,[%rd584],%fd1252; }

	// end inline asm
	add.s64 	%rd585, %rd578, 56;
	// begin inline asm
	{ atom.add.f64 %fd1243,[%rd585],%fd1252; }

	// end inline asm
	add.s64 	%rd586, %rd578, 64;
	// begin inline asm
	{ atom.add.f64 %fd1245,[%rd586],%fd1252; }

	// end inline asm
	add.s64 	%rd587, %rd578, 72;
	// begin inline asm
	{ atom.add.f64 %fd1247,[%rd587],%fd1252; }

	// end inline asm
	add.s64 	%rd588, %rd578, 80;
	// begin inline asm
	{ atom.add.f64 %fd1249,[%rd588],%fd1252; }

	// end inline asm
	add.s64 	%rd589, %rd578, 88;
	// begin inline asm
	{ atom.add.f64 %fd1251,[%rd589],%fd1252; }

	// end inline asm
	bra.uni 	$L__BB13_137;

$L__BB13_135:
	setp.eq.s64 	%p70, %rd48, 0;
	@%p70 bra 	$L__BB13_137;

	mul.lo.s64 	%rd603, %rd41, %rd22;
	add.s64 	%rd591, %rd48, %rd603;
	mov.f64 	%fd1276, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1253,[%rd591],%fd1276; }

	// end inline asm
	add.s64 	%rd592, %rd591, 8;
	// begin inline asm
	{ atom.add.f64 %fd1255,[%rd592],%fd1276; }

	// end inline asm
	add.s64 	%rd593, %rd591, 16;
	// begin inline asm
	{ atom.add.f64 %fd1257,[%rd593],%fd190; }

	// end inline asm
	add.s64 	%rd594, %rd591, 24;
	// begin inline asm
	{ atom.add.f64 %fd1259,[%rd594],%fd1276; }

	// end inline asm
	add.s64 	%rd595, %rd591, 32;
	// begin inline asm
	{ atom.add.f64 %fd1261,[%rd595],%fd1276; }

	// end inline asm
	add.s64 	%rd596, %rd591, 40;
	// begin inline asm
	{ atom.add.f64 %fd1263,[%rd596],%fd1276; }

	// end inline asm
	add.s64 	%rd597, %rd591, 48;
	// begin inline asm
	{ atom.add.f64 %fd1265,[%rd597],%fd1276; }

	// end inline asm
	add.s64 	%rd598, %rd591, 56;
	// begin inline asm
	{ atom.add.f64 %fd1267,[%rd598],%fd1276; }

	// end inline asm
	add.s64 	%rd599, %rd591, 64;
	// begin inline asm
	{ atom.add.f64 %fd1269,[%rd599],%fd1276; }

	// end inline asm
	add.s64 	%rd600, %rd591, 72;
	// begin inline asm
	{ atom.add.f64 %fd1271,[%rd600],%fd1276; }

	// end inline asm
	add.s64 	%rd601, %rd591, 80;
	// begin inline asm
	{ atom.add.f64 %fd1273,[%rd601],%fd1276; }

	// end inline asm
	add.s64 	%rd602, %rd591, 88;
	// begin inline asm
	{ atom.add.f64 %fd1275,[%rd602],%fd1276; }

	// end inline asm

$L__BB13_137:
	fma.rn.f64 	%fd1497, %fd1560, %fd1606, 0d0000000000000000;
	fma.rn.f64 	%fd192, %fd1593, %fd1497, %fd185;
	@%p11 bra 	$L__BB13_139;

	ld.global.f64 	%fd1277, [%rd43];
	add.f64 	%fd1607, %fd1277, 0d0000000000000000;
	bra.uni 	$L__BB13_141;

$L__BB13_139:
	setp.eq.s64 	%p72, %rd54, 0;
	mov.f64 	%fd1607, 0d0000000000000000;
	@%p72 bra 	$L__BB13_141;

	ld.global.f64 	%fd1279, [%rd44];
	add.f64 	%fd1607, %fd1279, 0d0000000000000000;

$L__BB13_141:
	mov.f64 	%fd1280, 0d0000000000000000;
	fma.rn.f64 	%fd1281, %fd1594, %fd1607, 0d0000000000000000;
	fma.rn.f64 	%fd1282, %fd1555, %fd1281, 0d0000000000000000;
	fma.rn.f64 	%fd1283, %fd1555, %fd1281, %fd1282;
	add.f64 	%fd197, %fd1283, 0d0000000000000000;
	sub.f64 	%fd1284, %fd1280, %fd1283;
	add.f64 	%fd198, %fd1284, 0d0000000000000000;
	@%p13 bra 	$L__BB13_143;

	mul.lo.s64 	%rd616, %rd41, %rd28;
	add.s64 	%rd604, %rd59, %rd616;
	// begin inline asm
	{ atom.add.f64 %fd1285,[%rd604],%fd1280; }

	// end inline asm
	add.s64 	%rd605, %rd604, 8;
	// begin inline asm
	{ atom.add.f64 %fd1287,[%rd605],%fd198; }

	// end inline asm
	add.s64 	%rd606, %rd604, 16;
	// begin inline asm
	{ atom.add.f64 %fd1289,[%rd606],%fd1280; }

	// end inline asm
	add.s64 	%rd607, %rd604, 24;
	// begin inline asm
	{ atom.add.f64 %fd1291,[%rd607],%fd1280; }

	// end inline asm
	add.s64 	%rd608, %rd604, 32;
	// begin inline asm
	{ atom.add.f64 %fd1293,[%rd608],%fd1280; }

	// end inline asm
	add.s64 	%rd609, %rd604, 40;
	// begin inline asm
	{ atom.add.f64 %fd1295,[%rd609],%fd1280; }

	// end inline asm
	add.s64 	%rd610, %rd604, 48;
	// begin inline asm
	{ atom.add.f64 %fd1297,[%rd610],%fd1280; }

	// end inline asm
	add.s64 	%rd611, %rd604, 56;
	// begin inline asm
	{ atom.add.f64 %fd1299,[%rd611],%fd1280; }

	// end inline asm
	add.s64 	%rd612, %rd604, 64;
	// begin inline asm
	{ atom.add.f64 %fd1301,[%rd612],%fd1280; }

	// end inline asm
	add.s64 	%rd613, %rd604, 72;
	// begin inline asm
	{ atom.add.f64 %fd1303,[%rd613],%fd1280; }

	// end inline asm
	add.s64 	%rd614, %rd604, 80;
	// begin inline asm
	{ atom.add.f64 %fd1305,[%rd614],%fd1280; }

	// end inline asm
	add.s64 	%rd615, %rd604, 88;
	// begin inline asm
	{ atom.add.f64 %fd1307,[%rd615],%fd1280; }

	// end inline asm
	bra.uni 	$L__BB13_145;

$L__BB13_143:
	setp.eq.s64 	%p74, %rd52, 0;
	@%p74 bra 	$L__BB13_145;

	mul.lo.s64 	%rd629, %rd41, %rd23;
	add.s64 	%rd617, %rd52, %rd629;
	mov.f64 	%fd1332, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1309,[%rd617],%fd1332; }

	// end inline asm
	add.s64 	%rd618, %rd617, 8;
	// begin inline asm
	{ atom.add.f64 %fd1311,[%rd618],%fd198; }

	// end inline asm
	add.s64 	%rd619, %rd617, 16;
	// begin inline asm
	{ atom.add.f64 %fd1313,[%rd619],%fd1332; }

	// end inline asm
	add.s64 	%rd620, %rd617, 24;
	// begin inline asm
	{ atom.add.f64 %fd1315,[%rd620],%fd1332; }

	// end inline asm
	add.s64 	%rd621, %rd617, 32;
	// begin inline asm
	{ atom.add.f64 %fd1317,[%rd621],%fd1332; }

	// end inline asm
	add.s64 	%rd622, %rd617, 40;
	// begin inline asm
	{ atom.add.f64 %fd1319,[%rd622],%fd1332; }

	// end inline asm
	add.s64 	%rd623, %rd617, 48;
	// begin inline asm
	{ atom.add.f64 %fd1321,[%rd623],%fd1332; }

	// end inline asm
	add.s64 	%rd624, %rd617, 56;
	// begin inline asm
	{ atom.add.f64 %fd1323,[%rd624],%fd1332; }

	// end inline asm
	add.s64 	%rd625, %rd617, 64;
	// begin inline asm
	{ atom.add.f64 %fd1325,[%rd625],%fd1332; }

	// end inline asm
	add.s64 	%rd626, %rd617, 72;
	// begin inline asm
	{ atom.add.f64 %fd1327,[%rd626],%fd1332; }

	// end inline asm
	add.s64 	%rd627, %rd617, 80;
	// begin inline asm
	{ atom.add.f64 %fd1329,[%rd627],%fd1332; }

	// end inline asm
	add.s64 	%rd628, %rd617, 88;
	// begin inline asm
	{ atom.add.f64 %fd1331,[%rd628],%fd1332; }

	// end inline asm

$L__BB13_145:
	@%p15 bra 	$L__BB13_147;

	mul.lo.s64 	%rd642, %rd41, %rd29;
	add.s64 	%rd630, %rd57, %rd642;
	mov.f64 	%fd1356, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1333,[%rd630],%fd1356; }

	// end inline asm
	add.s64 	%rd631, %rd630, 8;
	// begin inline asm
	{ atom.add.f64 %fd1335,[%rd631],%fd197; }

	// end inline asm
	add.s64 	%rd632, %rd630, 16;
	// begin inline asm
	{ atom.add.f64 %fd1337,[%rd632],%fd1356; }

	// end inline asm
	add.s64 	%rd633, %rd630, 24;
	// begin inline asm
	{ atom.add.f64 %fd1339,[%rd633],%fd1356; }

	// end inline asm
	add.s64 	%rd634, %rd630, 32;
	// begin inline asm
	{ atom.add.f64 %fd1341,[%rd634],%fd1356; }

	// end inline asm
	add.s64 	%rd635, %rd630, 40;
	// begin inline asm
	{ atom.add.f64 %fd1343,[%rd635],%fd1356; }

	// end inline asm
	add.s64 	%rd636, %rd630, 48;
	// begin inline asm
	{ atom.add.f64 %fd1345,[%rd636],%fd1356; }

	// end inline asm
	add.s64 	%rd637, %rd630, 56;
	// begin inline asm
	{ atom.add.f64 %fd1347,[%rd637],%fd1356; }

	// end inline asm
	add.s64 	%rd638, %rd630, 64;
	// begin inline asm
	{ atom.add.f64 %fd1349,[%rd638],%fd1356; }

	// end inline asm
	add.s64 	%rd639, %rd630, 72;
	// begin inline asm
	{ atom.add.f64 %fd1351,[%rd639],%fd1356; }

	// end inline asm
	add.s64 	%rd640, %rd630, 80;
	// begin inline asm
	{ atom.add.f64 %fd1353,[%rd640],%fd1356; }

	// end inline asm
	add.s64 	%rd641, %rd630, 88;
	// begin inline asm
	{ atom.add.f64 %fd1355,[%rd641],%fd1356; }

	// end inline asm
	bra.uni 	$L__BB13_149;

$L__BB13_147:
	setp.eq.s64 	%p76, %rd48, 0;
	@%p76 bra 	$L__BB13_149;

	mul.lo.s64 	%rd655, %rd41, %rd22;
	add.s64 	%rd643, %rd48, %rd655;
	mov.f64 	%fd1380, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1357,[%rd643],%fd1380; }

	// end inline asm
	add.s64 	%rd644, %rd643, 8;
	// begin inline asm
	{ atom.add.f64 %fd1359,[%rd644],%fd197; }

	// end inline asm
	add.s64 	%rd645, %rd643, 16;
	// begin inline asm
	{ atom.add.f64 %fd1361,[%rd645],%fd1380; }

	// end inline asm
	add.s64 	%rd646, %rd643, 24;
	// begin inline asm
	{ atom.add.f64 %fd1363,[%rd646],%fd1380; }

	// end inline asm
	add.s64 	%rd647, %rd643, 32;
	// begin inline asm
	{ atom.add.f64 %fd1365,[%rd647],%fd1380; }

	// end inline asm
	add.s64 	%rd648, %rd643, 40;
	// begin inline asm
	{ atom.add.f64 %fd1367,[%rd648],%fd1380; }

	// end inline asm
	add.s64 	%rd649, %rd643, 48;
	// begin inline asm
	{ atom.add.f64 %fd1369,[%rd649],%fd1380; }

	// end inline asm
	add.s64 	%rd650, %rd643, 56;
	// begin inline asm
	{ atom.add.f64 %fd1371,[%rd650],%fd1380; }

	// end inline asm
	add.s64 	%rd651, %rd643, 64;
	// begin inline asm
	{ atom.add.f64 %fd1373,[%rd651],%fd1380; }

	// end inline asm
	add.s64 	%rd652, %rd643, 72;
	// begin inline asm
	{ atom.add.f64 %fd1375,[%rd652],%fd1380; }

	// end inline asm
	add.s64 	%rd653, %rd643, 80;
	// begin inline asm
	{ atom.add.f64 %fd1377,[%rd653],%fd1380; }

	// end inline asm
	add.s64 	%rd654, %rd643, 88;
	// begin inline asm
	{ atom.add.f64 %fd1379,[%rd654],%fd1380; }

	// end inline asm

$L__BB13_149:
	fma.rn.f64 	%fd1498, %fd1556, %fd1607, 0d0000000000000000;
	fma.rn.f64 	%fd199, %fd1593, %fd1498, %fd192;
	@%p11 bra 	$L__BB13_151;

	ld.global.f64 	%fd1381, [%rd43];
	add.f64 	%fd1608, %fd1381, 0d0000000000000000;
	bra.uni 	$L__BB13_153;

$L__BB13_151:
	setp.eq.s64 	%p78, %rd54, 0;
	mov.f64 	%fd1608, 0d0000000000000000;
	@%p78 bra 	$L__BB13_153;

	ld.global.f64 	%fd1383, [%rd44];
	add.f64 	%fd1608, %fd1383, 0d0000000000000000;

$L__BB13_153:
	mov.f64 	%fd1384, 0d0000000000000000;
	fma.rn.f64 	%fd1385, %fd1594, %fd1608, 0d0000000000000000;
	fma.rn.f64 	%fd1386, %fd1551, %fd1385, 0d0000000000000000;
	fma.rn.f64 	%fd1387, %fd1551, %fd1385, %fd1386;
	add.f64 	%fd204, %fd1387, 0d0000000000000000;
	sub.f64 	%fd1388, %fd1384, %fd1387;
	add.f64 	%fd205, %fd1388, 0d0000000000000000;
	@%p13 bra 	$L__BB13_155;

	mul.lo.s64 	%rd668, %rd41, %rd28;
	add.s64 	%rd656, %rd59, %rd668;
	// begin inline asm
	{ atom.add.f64 %fd1389,[%rd656],%fd205; }

	// end inline asm
	add.s64 	%rd657, %rd656, 8;
	// begin inline asm
	{ atom.add.f64 %fd1391,[%rd657],%fd1384; }

	// end inline asm
	add.s64 	%rd658, %rd656, 16;
	// begin inline asm
	{ atom.add.f64 %fd1393,[%rd658],%fd1384; }

	// end inline asm
	add.s64 	%rd659, %rd656, 24;
	// begin inline asm
	{ atom.add.f64 %fd1395,[%rd659],%fd1384; }

	// end inline asm
	add.s64 	%rd660, %rd656, 32;
	// begin inline asm
	{ atom.add.f64 %fd1397,[%rd660],%fd1384; }

	// end inline asm
	add.s64 	%rd661, %rd656, 40;
	// begin inline asm
	{ atom.add.f64 %fd1399,[%rd661],%fd1384; }

	// end inline asm
	add.s64 	%rd662, %rd656, 48;
	// begin inline asm
	{ atom.add.f64 %fd1401,[%rd662],%fd1384; }

	// end inline asm
	add.s64 	%rd663, %rd656, 56;
	// begin inline asm
	{ atom.add.f64 %fd1403,[%rd663],%fd1384; }

	// end inline asm
	add.s64 	%rd664, %rd656, 64;
	// begin inline asm
	{ atom.add.f64 %fd1405,[%rd664],%fd1384; }

	// end inline asm
	add.s64 	%rd665, %rd656, 72;
	// begin inline asm
	{ atom.add.f64 %fd1407,[%rd665],%fd1384; }

	// end inline asm
	add.s64 	%rd666, %rd656, 80;
	// begin inline asm
	{ atom.add.f64 %fd1409,[%rd666],%fd1384; }

	// end inline asm
	add.s64 	%rd667, %rd656, 88;
	// begin inline asm
	{ atom.add.f64 %fd1411,[%rd667],%fd1384; }

	// end inline asm
	bra.uni 	$L__BB13_157;

$L__BB13_155:
	setp.eq.s64 	%p80, %rd52, 0;
	@%p80 bra 	$L__BB13_157;

	mul.lo.s64 	%rd681, %rd41, %rd23;
	add.s64 	%rd669, %rd52, %rd681;
	// begin inline asm
	{ atom.add.f64 %fd1413,[%rd669],%fd205; }

	// end inline asm
	add.s64 	%rd670, %rd669, 8;
	mov.f64 	%fd1436, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1415,[%rd670],%fd1436; }

	// end inline asm
	add.s64 	%rd671, %rd669, 16;
	// begin inline asm
	{ atom.add.f64 %fd1417,[%rd671],%fd1436; }

	// end inline asm
	add.s64 	%rd672, %rd669, 24;
	// begin inline asm
	{ atom.add.f64 %fd1419,[%rd672],%fd1436; }

	// end inline asm
	add.s64 	%rd673, %rd669, 32;
	// begin inline asm
	{ atom.add.f64 %fd1421,[%rd673],%fd1436; }

	// end inline asm
	add.s64 	%rd674, %rd669, 40;
	// begin inline asm
	{ atom.add.f64 %fd1423,[%rd674],%fd1436; }

	// end inline asm
	add.s64 	%rd675, %rd669, 48;
	// begin inline asm
	{ atom.add.f64 %fd1425,[%rd675],%fd1436; }

	// end inline asm
	add.s64 	%rd676, %rd669, 56;
	// begin inline asm
	{ atom.add.f64 %fd1427,[%rd676],%fd1436; }

	// end inline asm
	add.s64 	%rd677, %rd669, 64;
	// begin inline asm
	{ atom.add.f64 %fd1429,[%rd677],%fd1436; }

	// end inline asm
	add.s64 	%rd678, %rd669, 72;
	// begin inline asm
	{ atom.add.f64 %fd1431,[%rd678],%fd1436; }

	// end inline asm
	add.s64 	%rd679, %rd669, 80;
	// begin inline asm
	{ atom.add.f64 %fd1433,[%rd679],%fd1436; }

	// end inline asm
	add.s64 	%rd680, %rd669, 88;
	// begin inline asm
	{ atom.add.f64 %fd1435,[%rd680],%fd1436; }

	// end inline asm

$L__BB13_157:
	@%p15 bra 	$L__BB13_159;

	mul.lo.s64 	%rd694, %rd41, %rd29;
	add.s64 	%rd682, %rd57, %rd694;
	// begin inline asm
	{ atom.add.f64 %fd1437,[%rd682],%fd204; }

	// end inline asm
	add.s64 	%rd683, %rd682, 8;
	mov.f64 	%fd1460, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1439,[%rd683],%fd1460; }

	// end inline asm
	add.s64 	%rd684, %rd682, 16;
	// begin inline asm
	{ atom.add.f64 %fd1441,[%rd684],%fd1460; }

	// end inline asm
	add.s64 	%rd685, %rd682, 24;
	// begin inline asm
	{ atom.add.f64 %fd1443,[%rd685],%fd1460; }

	// end inline asm
	add.s64 	%rd686, %rd682, 32;
	// begin inline asm
	{ atom.add.f64 %fd1445,[%rd686],%fd1460; }

	// end inline asm
	add.s64 	%rd687, %rd682, 40;
	// begin inline asm
	{ atom.add.f64 %fd1447,[%rd687],%fd1460; }

	// end inline asm
	add.s64 	%rd688, %rd682, 48;
	// begin inline asm
	{ atom.add.f64 %fd1449,[%rd688],%fd1460; }

	// end inline asm
	add.s64 	%rd689, %rd682, 56;
	// begin inline asm
	{ atom.add.f64 %fd1451,[%rd689],%fd1460; }

	// end inline asm
	add.s64 	%rd690, %rd682, 64;
	// begin inline asm
	{ atom.add.f64 %fd1453,[%rd690],%fd1460; }

	// end inline asm
	add.s64 	%rd691, %rd682, 72;
	// begin inline asm
	{ atom.add.f64 %fd1455,[%rd691],%fd1460; }

	// end inline asm
	add.s64 	%rd692, %rd682, 80;
	// begin inline asm
	{ atom.add.f64 %fd1457,[%rd692],%fd1460; }

	// end inline asm
	add.s64 	%rd693, %rd682, 88;
	// begin inline asm
	{ atom.add.f64 %fd1459,[%rd693],%fd1460; }

	// end inline asm
	bra.uni 	$L__BB13_161;

$L__BB13_159:
	setp.eq.s64 	%p82, %rd48, 0;
	@%p82 bra 	$L__BB13_161;

	mul.lo.s64 	%rd707, %rd41, %rd22;
	add.s64 	%rd695, %rd48, %rd707;
	// begin inline asm
	{ atom.add.f64 %fd1461,[%rd695],%fd204; }

	// end inline asm
	add.s64 	%rd696, %rd695, 8;
	mov.f64 	%fd1484, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1463,[%rd696],%fd1484; }

	// end inline asm
	add.s64 	%rd697, %rd695, 16;
	// begin inline asm
	{ atom.add.f64 %fd1465,[%rd697],%fd1484; }

	// end inline asm
	add.s64 	%rd698, %rd695, 24;
	// begin inline asm
	{ atom.add.f64 %fd1467,[%rd698],%fd1484; }

	// end inline asm
	add.s64 	%rd699, %rd695, 32;
	// begin inline asm
	{ atom.add.f64 %fd1469,[%rd699],%fd1484; }

	// end inline asm
	add.s64 	%rd700, %rd695, 40;
	// begin inline asm
	{ atom.add.f64 %fd1471,[%rd700],%fd1484; }

	// end inline asm
	add.s64 	%rd701, %rd695, 48;
	// begin inline asm
	{ atom.add.f64 %fd1473,[%rd701],%fd1484; }

	// end inline asm
	add.s64 	%rd702, %rd695, 56;
	// begin inline asm
	{ atom.add.f64 %fd1475,[%rd702],%fd1484; }

	// end inline asm
	add.s64 	%rd703, %rd695, 64;
	// begin inline asm
	{ atom.add.f64 %fd1477,[%rd703],%fd1484; }

	// end inline asm
	add.s64 	%rd704, %rd695, 72;
	// begin inline asm
	{ atom.add.f64 %fd1479,[%rd704],%fd1484; }

	// end inline asm
	add.s64 	%rd705, %rd695, 80;
	// begin inline asm
	{ atom.add.f64 %fd1481,[%rd705],%fd1484; }

	// end inline asm
	add.s64 	%rd706, %rd695, 88;
	// begin inline asm
	{ atom.add.f64 %fd1483,[%rd706],%fd1484; }

	// end inline asm

$L__BB13_161:
	fma.rn.f64 	%fd1500, %fd1552, %fd1608, 0d0000000000000000;
	fma.rn.f64 	%fd1609, %fd1593, %fd1500, %fd199;

$L__BB13_162:
	add.f64 	%fd208, %fd1609, 0d0000000000000000;
	setp.eq.s64 	%p83, %rd63, 0;
	@%p83 bra 	$L__BB13_164;

	mul.lo.s64 	%rd709, %rd41, %rd25;
	add.s64 	%rd708, %rd63, %rd709;
	// begin inline asm
	{ atom.add.f64 %fd1485,[%rd708],%fd208; }

	// end inline asm
	bra.uni 	$L__BB13_166;

$L__BB13_164:
	setp.eq.s64 	%p84, %rd56, 0;
	@%p84 bra 	$L__BB13_166;

	add.s64 	%rd710, %rd56, %rd42;
	// begin inline asm
	{ atom.add.f64 %fd1487,[%rd710],%fd208; }

	// end inline asm

$L__BB13_166:
	ld.param.u64 	%rd711, [compute_affine_kinematic_energy_cuda_kernel_backward_param_0+24];
	add.s64 	%rd714, %rd714, %rd24;
	setp.lt.u64 	%p85, %rd714, %rd711;
	@%p85 bra 	$L__BB13_2;

$L__BB13_167:
	ret;

}

 